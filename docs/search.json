[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Notes",
    "section": "",
    "text": "Series\n\nFastAI Series\nThis series contains notes from the fastai “Practical Deep Learning for Coders” course and related book.\n\n\nGen AI Series\nA series of posts on Generative AI techniques and projects.\n\n\nReact Series\nMaking pretty front-ends.\n\n\n\nAll Posts\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nMathematical Statistics\n\n\n\n\n\n\nMaths\n\n\nStatistics\n\n\n\nFun with Latex ;)\n\n\n\n\n\nAug 5, 2025\n\n\n28 min\n\n\n\n\n\n\n\n\n\n\n\n\nMore Statistics\n\n\n\n\n\n\nMaths\n\n\nStatistics\n\n\n\nSignificantly More\n\n\n\n\n\nAug 2, 2025\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nBasic Statistics\n\n\n\n\n\n\nMaths\n\n\nStatistics\n\n\n\nSignificant.\n\n\n\n\n\nJul 20, 2025\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nAWS Solutions Architect\n\n\n\n\n\n\nDataEngineering\n\n\nEngineering\n\n\nSoftware\n\n\n\nSAA-C03\n\n\n\n\n\nJul 18, 2025\n\n\n98 min\n\n\n\n\n\n\n\n\n\n\n\n\nKubernetes\n\n\n\n\n\n\nSoftware\n\n\nEngineering\n\n\nDataEngineering\n\n\n\nWhat the hell is this? Some kind of kube?\n\n\n\n\n\nJul 10, 2025\n\n\n18 min\n\n\n\n\n\n\n\n\n\n\n\n\nDocker\n\n\n\n\n\n\nSoftware\n\n\nEngineering\n\n\nDataEngineering\n\n\n\nDocker? I Hardly Know ’Er\n\n\n\n\n\nJun 5, 2025\n\n\n35 min\n\n\n\n\n\n\n\n\n\n\n\n\nNeuroscience for Parents\n\n\n\n\n\n\nLearning\n\n\nParenting\n\n\n\nBaby Brain\n\n\n\n\n\nJun 1, 2025\n\n\n30 min\n\n\n\n\n\n\n\n\n\n\n\n\nExecutive Presence\n\n\n\n\n\n\nBusiness\n\n\nLeadership\n\n\n\nMy Presence is a Present\n\n\n\n\n\nMay 22, 2025\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nScience of Leadership\n\n\n\n\n\n\nBusiness\n\n\nLeadership\n\n\n\nNerds in Charge\n\n\n\n\n\nMay 20, 2025\n\n\n20 min\n\n\n\n\n\n\n\n\n\n\n\n\nConflict Resolution\n\n\n\n\n\n\nBusiness\n\n\nLeadership\n\n\n\nCheaper Than Real Therapy\n\n\n\n\n\nApr 28, 2025\n\n\n20 min\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On LLMs: Advanced Text Generation Techniques and Tools\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\nLLM\n\n\n\nPart 7: Chains and Agents\n\n\n\n\n\nMar 5, 2025\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nSnowflake: SnowPro Core\n\n\n\n\n\n\nSoftware\n\n\nDataEngineering\n\n\n\nSnowflake? Snow Problem.\n\n\n\n\n\nMar 4, 2025\n\n\n61 min\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series Analysis\n\n\n\n\n\n\nEngineering\n\n\nTimeseries\n\n\n\nTime well spent\n\n\n\n\n\nFeb 15, 2025\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nSQL\n\n\n\n\n\n\nInterviewPrep\n\n\nSoftware\n\n\nDataEngineering\n\n\n\nPart 1 of 1: No Sequel\n\n\n\n\n\nJan 27, 2025\n\n\n25 min\n\n\n\n\n\n\n\n\n\n\n\n\nDesigning Data Intensive Applications: Part 1\n\n\n\n\n\n\nDataEngineering\n\n\nEngineering\n\n\nSoftware\n\n\n\nData Systems\n\n\n\n\n\nJan 26, 2025\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On LLMs: Text Clustering and Topic Modeling\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\nLLM\n\n\n\nPart 5: Hot Topics\n\n\n\n\n\nJan 16, 2025\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On LLMs: Text Classification\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\nLLM\n\n\n\nPart 4: Stay Classy\n\n\n\n\n\nJan 15, 2025\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On LLMs: Prompt Engineering\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\nLLM\n\n\n\nPart 6: Professional prompts\n\n\n\n\n\nDec 13, 2024\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On LLMs: Tokens and Embeddings\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\nLLM\n\n\n\nPart 2: Tokens and Embeddings\n\n\n\n\n\nNov 28, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On LLMs: Intro\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\nLLM\n\n\n\nPart 1: Introduction to LLMs\n\n\n\n\n\nNov 26, 2024\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nGraph ML: Graph Algorithms\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGraphs\n\n\nGraphML\n\n\nAlgorithms\n\n\n\nPart 0.5: Traditional Graph Algorithms\n\n\n\n\n\nMay 8, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nGraph ML: What’s a Graph?\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGraphs\n\n\nGraphML\n\n\n\nPart 0: Intro to Graphs\n\n\n\n\n\nMay 7, 2024\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nSorting Algorithms\n\n\n\n\n\n\nEngineering\n\n\nComputerScience\n\n\nInterviewPrep\n\n\nAlgorithms\n\n\n\nAlgorithms? Sorted, mate.\n\n\n\n\n\nMay 6, 2024\n\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\n\nData Structures\n\n\n\n\n\n\nEngineering\n\n\nComputerScience\n\n\nInterviewPrep\n\n\n\nAn Array of Possibilities, Trie it Out, You’ll Have Heaps of Fun\n\n\n\n\n\nApr 23, 2024\n\n\n30 min\n\n\n\n\n\n\n\n\n\n\n\n\nAlgorithms\n\n\n\n\n\n\nEngineering\n\n\nComputerScience\n\n\nInterviewPrep\n\n\nAlgorithms\n\n\n\nBig G Explains Big O\n\n\n\n\n\nApr 17, 2024\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI: GANs\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\nGAN\n\n\n\nPart 4: Generative Adversarial Networks\n\n\n\n\n\nApr 10, 2024\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: TypeScript Essentials\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 7: I don’t got no type, bad code is the only thing that I like\n\n\n\n\n\nMar 21, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: Testing\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 6: Testing my patience\n\n\n\n\n\nMar 20, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: Deployment\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 5: Deploying React Apps\n\n\n\n\n\nMar 18, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: Debugging\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 4: A Bug’s Life\n\n\n\n\n\nMar 17, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: Styling\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 3: Styling it Out\n\n\n\n\n\nMar 16, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: JavaScript Essentials\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 2: WTF is JSX\n\n\n\n\n\nMar 14, 2024\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: A Gentle Introduction\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 1: Getting Started with React\n\n\n\n\n\nMar 12, 2024\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\nMeta Learning\n\n\n\n\n\n\nLearning\n\n\nAI\n\n\n\nMeta Meta Learning: What I Learned From Meta Learning by Radek Osmulski\n\n\n\n\n\nMar 10, 2024\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI: VAEs\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\nVAE\n\n\n\nPart 3: Variational Autoencoders\n\n\n\n\n\nMar 6, 2024\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 8: Convolutions\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 8\n\n\n\n\n\nMar 5, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 7: Collaborative Filtering\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 7\n\n\n\n\n\nMar 1, 2024\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI: Deep Learning Foundations\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\n\nPart 2: The Building Blocks for Generative AI\n\n\n\n\n\nFeb 26, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI: Intro\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\n\nPart 1: Introduction to Generative AI\n\n\n\n\n\nFeb 19, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 6.5: Road to the Top\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 6.5\n\n\n\n\n\nFeb 13, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 6: Random Forests\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 6\n\n\n\n\n\nOct 9, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 5: Natural Language Processing\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 5\n\n\n\n\n\nSep 23, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 4: Natural Language Processing\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 4\n\n\n\n\n\nSep 7, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 3: How Does a Neural Network Learn?\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 3\n\n\n\n\n\nSep 1, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 2: Deployment\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 2\n\n\n\n\n\nAug 30, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 1: Image Classification Models\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 1\n\n\n\n\n\nAug 23, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nSystem Design Notes\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\n\nNotes on System Design\n\n\n\n\n\nAug 14, 2023\n\n\n24 min\n\n\n\n\n\n\n\n\n\n\n\n\nPitching Notes\n\n\n\n\n\n\nBusiness\n\n\n\nNotes on Pitching\n\n\n\n\n\nAug 4, 2023\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nKalman Filter Notes\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\n\nNotes on Kalman filters\n\n\n\n\n\nJul 23, 2023\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nPublic Speaking Notes\n\n\n\n\n\n\nBusiness\n\n\n\nNotes on Public Speaking\n\n\n\n\n\nJul 20, 2023\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nMarketing Notes\n\n\n\n\n\n\nBusiness\n\n\n\nNotes on Marketing\n\n\n\n\n\nJul 18, 2023\n\n\n18 min\n\n\n\n\n\n\n\n\n\n\n\n\nSoftware Architecture Notes\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\n\nNotes on Software Architecture\n\n\n\n\n\nJun 23, 2023\n\n\n22 min\n\n\n\n\n\n\n\n\n\n\n\n\nTensorflow Notes\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\n\nNotes on Tensorflow\n\n\n\n\n\nFeb 23, 2023\n\n\n21 min\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n Back to top"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Projects\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBrazilian Jiu Jitsu Taxonomy\n\n\n\nBJJ\n\n\nWebDev\n\n\n\nGrappling with Web Development\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplainable AI in Healthcare\n\n\n\nResearch\n\n\nAI\n\n\nHealthcare\n\n\n\nOpening the black box in medical AI\n\n\n\n\n\n\n\n\n\n\n\n\n\nTradeIntel\n\n\n\nTrading\n\n\nApp\n\n\n\nNext-generation robo advisor\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html",
    "href": "posts/maths/mathematical_statistics/math_stats.html",
    "title": "Mathematical Statistics",
    "section": "",
    "text": "A random variable is a quantity that is random. It has a probability distribution.\nE.g. X = roll of a dice\nThe sample space is the set of all possible outcomes of a random variable. This is also called the support of a function.\n\\[\nS = {1,2,3,4,5,6}\n\\]\nDifferent values occur with different probabilities. We can describe this with a probability distribution.\nP(X=1) = 1/6\nP(X=2) = 1/6\nP(X=3) = 1/6\n…\nA common way to describe this is with a function as above. We give an input (e.g. \\(X=1\\)) and get an output (\\(\\frac{1}{6}\\)). This is a probability mass function. We can represent this graphically.\nIn this case, a dice roll is a discrete random variable.\nWith a continuous random variable, we have a continuous distribution. The probability of any specific value is infinitesimally small, so we consider ranges and call the distribution the probability density function.\nA parameter is a number that controls the properties of a probability distribution."
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#bernoulli-distribution",
    "href": "posts/maths/mathematical_statistics/math_stats.html#bernoulli-distribution",
    "title": "Mathematical Statistics",
    "section": "2.1. Bernoulli Distribution",
    "text": "2.1. Bernoulli Distribution\nA Bernoulli distribution describes an experiment that has two possible outcomes, i.e. success/failure, heads/tails, etc.\n\\[\nX \\sim \\text{Bernoulli}(p)\n\\]\nThe probability of the successful and unsuccessful outcomes are: \\[\nP(X=1) = p\n\\] \\[\nP(X=0) = 1-p\n\\]\nThe sample space is: \\[\nP \\in [0,1]\n\\]\nThe sample space is also known as the support.\nWe can alternatively write this as a probability mass function (PMF):\n\\[\nf(x) =\n\\begin{cases}\np & x = 1 \\\\[6pt]\n1-p & x = 0 \\\\[6pt]\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nA way of rewriting this is: \\[\nf(x) = p^{x} (1-p)^{\\,1-x}\n\\]"
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#uniform-distribution",
    "href": "posts/maths/mathematical_statistics/math_stats.html#uniform-distribution",
    "title": "Mathematical Statistics",
    "section": "2.2. Uniform Distribution",
    "text": "2.2. Uniform Distribution\nThis is a continuous distribution where every value between and min and a max value is equally likely.\n\\[\nX \\sim \\text{Uniform}(a,b)\n\\]\nAs this is a continuous distribution, the mass of any point is infinitesimally small, so we have a probability density function (PDF) rather than a probability mass function (PMF).\n\\[\nf(x) =\n\\begin{cases}\n\\dfrac{1}{b-a} & a \\leq x \\leq b \\\\[6pt]\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nThe sample space is: \\[\nS = (a, b)\n\\]"
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#normal-distribution",
    "href": "posts/maths/mathematical_statistics/math_stats.html#normal-distribution",
    "title": "Mathematical Statistics",
    "section": "2.3. Normal Distribution",
    "text": "2.3. Normal Distribution\nCentred on the mean \\(\\mu\\). The spread is defined the standard deviation \\(\\sigma\\).\n\\[\nX \\sim \\mathcal{N}(\\mu, \\sigma^{2})\n\\]\nIt is defined by these two parameters.\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\!\\left( -\\frac{(x-\\mu)^{2}}{2\\sigma^{2}} \\right),\n\\quad -\\infty &lt; x &lt; \\infty\n\\]"
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#expected-value-of-a-bernoulli-random-variable",
    "href": "posts/maths/mathematical_statistics/math_stats.html#expected-value-of-a-bernoulli-random-variable",
    "title": "Mathematical Statistics",
    "section": "3.1. Expected Value of a Bernoulli Random Variable",
    "text": "3.1. Expected Value of a Bernoulli Random Variable\nRecall the PMF: \\[\nf(x) =\n\\begin{cases}\np & x = 1 \\\\[6pt]\n1-p & x = 0 \\\\[6pt]\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nAnd the definition of expectation: \\[\nE[X] = \\mu = \\sum_{x \\in S} xP(X=x)\n\\]\nSubstituting the first into the second equation, we get: \\[\nE[X] = 0 \\cdot (1-p) + 1 \\cdot p = p\n\\]"
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#expected-value-of-a-uniform-random-variable",
    "href": "posts/maths/mathematical_statistics/math_stats.html#expected-value-of-a-uniform-random-variable",
    "title": "Mathematical Statistics",
    "section": "3.2. Expected Value of a Uniform Random Variable",
    "text": "3.2. Expected Value of a Uniform Random Variable\nThis is continuous, so we integrate the PDF:\nExpected value for a continuous distribution \\[\n\\begin{aligned}\nE[X] &= \\int_{x \\in S} x f(x) \\, dx \\\\[6pt]\n     &= \\int_{0}^{\\theta} x \\cdot \\frac{1}{\\theta} \\, dx \\\\[6pt]\n     &= \\frac{1}{\\theta} \\left[ \\frac{x^{2}}{2} \\right]_{0}^{\\theta} \\\\[6pt]\n     &= \\frac{1}{\\theta} \\cdot \\frac{\\theta^{2}}{2} \\\\[6pt]\n     &= \\frac{\\theta}{2}.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#expected-value-for-a-normal-random-variable",
    "href": "posts/maths/mathematical_statistics/math_stats.html#expected-value-for-a-normal-random-variable",
    "title": "Mathematical Statistics",
    "section": "3.3. Expected Value for a Normal Random Variable",
    "text": "3.3. Expected Value for a Normal Random Variable\nAgain, this is a continuous distribution, so we can integrate the PDF. See here for the integral.\nIntuitively, we know that a Normal distribution is symmetric and defined by \\(\\mu\\) and \\(\\sigma\\). So the expected value is \\(\\mu\\)."
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#method-of-moments-estimator-for-a-bernoulli-distribution",
    "href": "posts/maths/mathematical_statistics/math_stats.html#method-of-moments-estimator-for-a-bernoulli-distribution",
    "title": "Mathematical Statistics",
    "section": "4.1. Method of Moments Estimator for a Bernoulli Distribution",
    "text": "4.1. Method of Moments Estimator for a Bernoulli Distribution\nWe start with our two basic facts:\n\n\\(E[X] \\approx \\bar{X}\\) - our sample mean should be close to our population mean\n\\(E[X] = p\\) - for a Bernoulli distribution, the population mean is \\(p\\)\n\nSubstituting this, we get an estimate of the parameter \\(p\\) using the method of moments: \\[\np ~= \\bar{X}\n\\]\nNote the approximately equal sign in the equation above. We don’t know the true population parameter so it isn’t equal, only an approximation.\nWe can define this estimate as \\(\\hat{p}\\), which is defined as being equal to this estimate.\n\\[\n\\hat{p} = \\bar{X} = \\frac{\\sum x_i}{n}\n\\]"
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#method-of-moments-estimator-for-a-uniform-distribution",
    "href": "posts/maths/mathematical_statistics/math_stats.html#method-of-moments-estimator-for-a-uniform-distribution",
    "title": "Mathematical Statistics",
    "section": "4.2. Method of Moments Estimator for a Uniform Distribution",
    "text": "4.2. Method of Moments Estimator for a Uniform Distribution\nWe start with our two basic facts:\n\n\\(E[X] ~= \\bar{X}\\) - our sample mean should be close to our population mean\n\\(E[X] = \\frac{\\theta}{2}\\) - population mean for a uniform distribution\n\nSo we can substitute this to get our method of moments estimator for \\(\\theta\\): \\[\n\\begin{aligned}\n\\frac{\\theta}{2} &\\approx \\bar{X} \\\\[6pt]\n\\;\\;\\Rightarrow\\;\\; \\theta &\\approx 2\\bar{X} \\\\[6pt]\n\\;\\;\\Rightarrow\\;\\; \\hat{\\theta} &= 2\\bar{X}\n\\end{aligned}\n\\]\n\\[\n\\boxed{\\hat{\\theta} = 2\\bar{X}}\n\\]"
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#method-of-moments-estimator-for-a-normal-distribution",
    "href": "posts/maths/mathematical_statistics/math_stats.html#method-of-moments-estimator-for-a-normal-distribution",
    "title": "Mathematical Statistics",
    "section": "4.3 Method of Moments Estimator for a Normal Distribution",
    "text": "4.3 Method of Moments Estimator for a Normal Distribution\nWe start with our two basic facts:\n\n\\(E[X] ~= \\bar{X}\\) - our sample mean should be close to our population mean\n\\(E[X] = \\mu\\) - population mean for a Normal distribution\n\nTherefore, we substitute to get our method of moments estimator for \\(\\mu\\): \\[\n\\begin{aligned}\n\\mu &\\approx \\bar{X} \\\\[2pt]\n\\hat{\\mu} &= \\bar{X}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#background-on-estimators",
    "href": "posts/maths/mathematical_statistics/math_stats.html#background-on-estimators",
    "title": "Mathematical Statistics",
    "section": "5.0. Background on Estimators",
    "text": "5.0. Background on Estimators\n\n5.0.1. Distribution of Estimators\nOur data is random. It is the input to our estimator, therefore our estimate is also a random variable.\nThe distribution of an estimator is called a sampling distribution.\nWe want the estimator to be centred on the true value. This is an unbiased estimator. In other words, we want the expected value of the estimate to equal its true value, i.e. we want\n\\[\nE[\\hat{\\theta}] = \\theta\n\\]\nWe can define the bias as the difference between these two: \\[\nBias = E[\\hat{\\theta}] - \\theta\n\\]\nIf \\(Bias = 0\\) then we have an unbiased estimator."
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#properties-of-expected-value",
    "href": "posts/maths/mathematical_statistics/math_stats.html#properties-of-expected-value",
    "title": "Mathematical Statistics",
    "section": "5.0.2. Properties of Expected Value",
    "text": "5.0.2. Properties of Expected Value\nThe linearity of expectation results below help us prove properties of the estimators.\nThe law of the unconscious statistician: \\[\nE[cX] = c\\,E[X], \\quad \\text{where $c$ is a constant}\n\\]\nThis follows from the definition of expected value. For example, consider a discrete distribution: \\[\n\\begin{aligned}\nE[X] &= \\sum_{x \\in S} x\\, P(X=x) \\\\[2pt]\n\\Rightarrow E[cX] &= \\sum_{x \\in S} c x \\, P(cX = c x) \\\\[2pt]\n&= \\sum_{x \\in S} c x \\, P(X = x) \\quad \\text{(since } P(cX = c x) = P(X = x)\\text{)} \\\\[2pt]\n&= c \\sum_{x \\in S} x \\, P(X = x) \\\\[2pt]\n&= c E[X]\n\\end{aligned}\n\\]\nThe sum of random variables: \\[\nE[X+Y] = E[X] + E(Y)\n\\]\nOr more generally, the expected value of any sum equals the sum of expected values. This holds regardless of independence.\n\\[\nE\\Bigg[\\sum_{i=1}^{n} X_i\\Bigg] = \\sum_{i=1}^{n} E[X_i]\n\\]"
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#the-bias-of-a-mom-estimator-for-a-bernoulli-distribution",
    "href": "posts/maths/mathematical_statistics/math_stats.html#the-bias-of-a-mom-estimator-for-a-bernoulli-distribution",
    "title": "Mathematical Statistics",
    "section": "5.1. The Bias of a MoM Estimator for a Bernoulli Distribution",
    "text": "5.1. The Bias of a MoM Estimator for a Bernoulli Distribution\nRecall that our method of moments estimator was: \\[\n\\hat{p} = \\bar{X}\n\\]\nWe want to know if this equals \\(E(p)\\). This would make it an unbiased estimator.\n\\[\n\\begin{aligned}\nE[\\hat{p}] &= E\\Bigg[\\frac{1}{n} \\sum_{i=1}^{n} X_i \\Bigg] \\\\[2pt]\n&= \\frac{1}{n} E\\Bigg[\\sum_{i=1}^{n} X_i \\Bigg] \\\\[2pt]\n&= \\frac{1}{n} \\sum_{i=1}^{n} E[X_i] \\quad \\text{(using the linearity of expectation)} \\\\[2pt]\n&= \\frac{1}{n} \\sum_{i=1}^{n} p \\\\[2pt]\n&= p\n\\end{aligned}\n\\]\nThe expected value of our estimator is the population value, therefore it is an unbiased estimator."
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#the-bias-of-a-mom-estimator-for-a-uniform-distribution",
    "href": "posts/maths/mathematical_statistics/math_stats.html#the-bias-of-a-mom-estimator-for-a-uniform-distribution",
    "title": "Mathematical Statistics",
    "section": "5.2. The Bias of a MoM Estimator for a Uniform Distribution",
    "text": "5.2. The Bias of a MoM Estimator for a Uniform Distribution\nRecall that our method of moments estimator is: \\[\n\\hat{\\theta} = 2 \\bar{X}\n\\]\nSo the expected value of the estimate is: \\[\n\\begin{aligned}\nE[\\hat{\\theta}] &= E\\Bigg[ 2 \\bar{X} \\Bigg] \\\\[2pt]\n&= E\\Bigg[ 2 \\cdot \\frac{1}{n} \\sum_{i=1}^{n} X_i \\Bigg] \\\\[2pt]\n&= \\frac{2}{n} E\\Bigg[ \\sum_{i=1}^{n} X_i \\Bigg] \\quad \\text{(taking constant outside)} \\\\[2pt]\n&= \\frac{2}{n} \\sum_{i=1}^{n} E[X_i] \\quad \\text{(linearity of expectation)} \\\\[2pt]\n&= \\frac{2}{n} \\sum_{i=1}^{n} \\frac{\\theta}{2} \\quad \\text{(substitute } E[X_i] = \\theta/2 \\text{)} \\\\[2pt]\n&= \\frac{2}{n} \\cdot n \\cdot \\frac{\\theta}{2} \\\\[2pt]\n&= \\theta\n\\end{aligned}\n\\]\nTherefore, this is an unbiased estimator, because \\(E(\\hat{\\theta}) = \\theta\\)."
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#the-bias-of-a-mom-estimator-for-a-normal-distribution",
    "href": "posts/maths/mathematical_statistics/math_stats.html#the-bias-of-a-mom-estimator-for-a-normal-distribution",
    "title": "Mathematical Statistics",
    "section": "5.3. The Bias of a MoM Estimator for a Normal Distribution",
    "text": "5.3. The Bias of a MoM Estimator for a Normal Distribution\nRecall that: \\[\n\\hat{\\mu} = \\bar{X}\n\\]\nThe expected value of this estimator is: \\[\n\\begin{aligned}\nE[\\hat{\\mu}] &= E[\\bar{X}] \\\\[2pt]\n&= E\\Bigg[ \\frac{1}{n} \\sum_{i=1}^{n} X_i \\Bigg] \\\\[2pt]\n&= \\frac{1}{n} \\sum_{i=1}^{n} E[X_i] \\quad \\text{(Sum of expectations = expectation of sums)} \\\\[2pt]\n&= \\frac{1}{n} \\sum_{i=1}^{n} \\mu \\\\[2pt]\n&= \\frac{1}{n} \\cdot n \\mu \\\\[2pt]\n&= \\mu\n\\end{aligned}\n\\]\nTherefore, this is an unbiased estimator, \\(E(\\hat{\\mu}) = \\mu\\)"
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#variance-of-a-bernoulli-distribution",
    "href": "posts/maths/mathematical_statistics/math_stats.html#variance-of-a-bernoulli-distribution",
    "title": "Mathematical Statistics",
    "section": "6.1. Variance of a Bernoulli Distribution",
    "text": "6.1. Variance of a Bernoulli Distribution\nRemember that in the general case, the variance is defined as:\n\\[\n\\mathrm{Var}(X) = E[X^2] - (E[X])^2\n\\]\nWe already know the mean of a Bernoulli distribution is \\(E[X] = p\\) So we just need to find an expression for \\(E[X^2]\\).\nRecall that for a Bernoulli distribution: \\[\nf(x) =\n\\begin{cases}\np & x = 1 \\\\[6pt]\n1-p & x = 0 \\\\[6pt]\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nSo \\[\n\\begin{aligned}\nE[X^2] &= \\sum_{x} x^2 P(X = x) \\\\[2pt]\n&= 0^2 \\cdot (1-p) + 1^2 \\cdot p \\\\[2pt]\n&= p\n\\end{aligned}\n\\]\nSubstituting into our variance formula: \\[\n\\begin{aligned}\n\\mathrm{Var}(X) &= E[X^2] - (E[X])^2 \\\\[2pt]\n&= p - p^2 \\\\[2pt]\n&= p(1-p)\n\\end{aligned}\n\\]\nSo the variance of a Bernoulli distribution is \\(\\mathrm{Var}(X)  = p(1-p)\\)."
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#variance-of-a-uniform-distribution",
    "href": "posts/maths/mathematical_statistics/math_stats.html#variance-of-a-uniform-distribution",
    "title": "Mathematical Statistics",
    "section": "6.2. Variance of a Uniform Distribution",
    "text": "6.2. Variance of a Uniform Distribution\nAgain we start with that general formula for variance \\[\n\\mathrm{Var}(X) = E[X^2] - (E[X])^2\n\\]\nWe know the the expected value for a uniform distribution is \\(\\frac{\\theta}{2}\\) and the PDF is \\(f(x) = \\frac{1}{\\theta}\\)\nSo \\((E[X])^2 = \\frac{\\theta^2}{4}\\). We just need to calculate \\(E[X^2]\\).\n\\[\n\\begin{aligned}\nE[X^2] &= \\int_{0}^{\\theta} x^2 \\cdot \\frac{1}{\\theta} \\, dx \\\\[2pt]\n&= \\frac{1}{\\theta} \\int_{0}^{\\theta} x^2 \\, dx \\quad \\text{(Pulling constant outside the integral)} \\\\[2pt]\n&= \\frac{1}{\\theta} \\left[ \\frac{x^3}{3} \\right]_{0}^{\\theta} \\\\[2pt]\n&= \\frac{1}{\\theta} \\cdot \\frac{\\theta^3}{3} \\\\[2pt]\n&= \\frac{\\theta^2}{3}\n\\end{aligned}\n\\]\nHence our variance expression is: \\[\n\\begin{aligned}\n\\mathrm{Var}(X) &= E[X^2] - (E[X])^2 \\\\[2pt]\n&= \\frac{\\theta^2}{3} - \\frac{\\theta^2}{4} \\\\[2pt]\n&= \\frac{\\theta^2}{12}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#variance-of-a-normal-distribution",
    "href": "posts/maths/mathematical_statistics/math_stats.html#variance-of-a-normal-distribution",
    "title": "Mathematical Statistics",
    "section": "6.3. Variance of a Normal Distribution",
    "text": "6.3. Variance of a Normal Distribution\nYou can grind through the integrals to get an expression for the variance… we know that it will eventually simplify down to \\(\\mathrm{Var}(X) = \\sigma ^2\\) by the definition of a Normal distribution."
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#background-on-variance",
    "href": "posts/maths/mathematical_statistics/math_stats.html#background-on-variance",
    "title": "Mathematical Statistics",
    "section": "7.0. Background on Variance",
    "text": "7.0. Background on Variance\n\n7.0.1. Why the Variance of an Estimator Matters\nWe want a lower variance for our estimator, i.e. the value we measure is in a tighter range near the true population mean.\nAn estimator with an unbiased mean but massive variance wouldn’t be very useful. On average, the survey results will reflect the true results, but any given survey may be wildly off the true population mean.\n\n\n7.0.2. Properties of Variance\nSimilarly to the linearity properties of expected value, there are analogous results for variance.\nIf we scale our random variable by a constant, the variance squares all distances, so: \\[\n\\mathrm{Var}(cX) = c^2 \\mathrm{Var}(X)\n\\]\nFor i.i.d. random variables: \\[\n\\mathrm{Var}\\Bigg(\\sum_{i=1}^{n} X_i\\Bigg) = \\sum_{i=1}^{n} \\mathrm{Var}(X_i)\n\\]"
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#variance-of-the-mom-estimator-of-a-bernoulli-distribution",
    "href": "posts/maths/mathematical_statistics/math_stats.html#variance-of-the-mom-estimator-of-a-bernoulli-distribution",
    "title": "Mathematical Statistics",
    "section": "7.1. Variance of the MoM Estimator of a Bernoulli Distribution",
    "text": "7.1. Variance of the MoM Estimator of a Bernoulli Distribution\nRecall that the MoM estimate is: \\[\n\\hat{p} = \\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\\]\nWe want to find the variance, so rewrite it in a form where we can apply some of our variance rules: \\[\n\\begin{aligned}\n\\mathrm{Var}(\\hat{p}) &= \\mathrm{Var}\\Bigg( \\frac{1}{n} \\sum_{i=1}^{n} X_i \\Bigg) \\\\[2pt]\n&= \\frac{1}{n^2} \\, \\mathrm{Var}\\Bigg( \\sum_{i=1}^{n} X_i \\Bigg) \\quad \\text{(using } \\mathrm{Var}(cX) = c^2 \\mathrm{Var}(X) \\text{)} \\\\[2pt]\n&= \\frac{1}{n^2} \\sum_{i=1}^{n} \\mathrm{Var}(X_i) \\quad \\text{(variance of sum = sum of variances for i.i.d.)} \\\\[2pt]\n&= \\frac{1}{n^2} \\sum_{i=1}^{n} p(1-p) \\quad \\text{(using variance of a Bernoulli variable)} \\\\[2pt]\n&= \\frac{1}{n^2} \\cdot n \\, p(1-p) \\\\[2pt]\n&= \\frac{p(1-p)}{n}\n\\end{aligned}\n\\]\nSo \\(\\mathrm{Var}(\\hat{p}) = \\frac{p(1-p)}{n}\\)\nThis means the variance always decreases with \\(n\\) more samples. The lowest variance is for \\(p=0.5\\); if positive or negative cases is very likely (e.g. 0.99) then we wouldn’t see much variance in our estimate because it is so certain. In the extreme case of \\(p=0\\) or \\(p=1\\), the variance is 0."
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#variance-of-the-mom-estimator-of-a-uniform-distribution",
    "href": "posts/maths/mathematical_statistics/math_stats.html#variance-of-the-mom-estimator-of-a-uniform-distribution",
    "title": "Mathematical Statistics",
    "section": "7.2. Variance of the MoM Estimator of a Uniform Distribution",
    "text": "7.2. Variance of the MoM Estimator of a Uniform Distribution\nRecall the the MoM Estimator is: \\[\n\\hat{\\theta} = 2 \\bar{X} = \\frac{2}{n} \\sum_{i=1}^{n} x_i\n\\]\nWe want to find an expression for the variance: \\[\n\\begin{aligned}\n\\mathrm{Var}(\\hat{\\theta}) &= \\mathrm{Var}\\Bigg( 2 \\cdot \\frac{1}{n} \\sum_{i=1}^{n} X_i \\Bigg) \\\\[2pt]\n&= \\frac{4}{n^2} \\, \\mathrm{Var}\\Bigg( \\sum_{i=1}^{n} X_i \\Bigg) \\quad \\text{(using } \\mathrm{Var}(cX) = c^2 \\mathrm{Var}(X) \\text{)} \\\\[2pt]\n&= \\frac{4}{n^2} \\sum_{i=1}^{n} \\mathrm{Var}(X_i) \\quad \\text{(variance of sum = sum of variances for i.i.d.)} \\\\[2pt]\n&= \\frac{4}{n^2} \\sum_{i=1}^{n} \\frac{\\theta^2}{12} \\quad \\text{(variance of a uniform distribution)} \\\\[2pt]\n&= \\frac{4}{n^2} \\cdot n \\cdot \\frac{\\theta^2}{12} \\\\[2pt]\n&= \\frac{\\theta^2}{3n}\n\\end{aligned}\n\\]\nThis makes intuitive sense. As the distribution gets wider, the variance gets wider. As the number of samples increases, the variance decreases, i.e. we get a more certain estimate."
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#variance-of-the-mom-estimator-of-a-normal-distribution",
    "href": "posts/maths/mathematical_statistics/math_stats.html#variance-of-the-mom-estimator-of-a-normal-distribution",
    "title": "Mathematical Statistics",
    "section": "7.3. Variance of the MoM Estimator of a Normal Distribution",
    "text": "7.3. Variance of the MoM Estimator of a Normal Distribution\nRecall that \\[\n\\hat{\\mu} = \\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\\]\nSo the variance is: \\[\n\\begin{aligned}\n\\mathrm{Var}(\\hat{\\mu}) &= \\mathrm{Var}\\Bigg( \\frac{1}{n} \\sum_{i=1}^{n} X_i \\Bigg) \\\\[2pt]\n&= \\frac{1}{n^2} \\, \\mathrm{Var}\\Bigg( \\sum_{i=1}^{n} X_i \\Bigg) \\quad \\text{(using } \\mathrm{Var}(cX) = c^2 \\mathrm{Var}(X) \\text{)} \\\\[2pt]\n&= \\frac{1}{n^2} \\sum_{i=1}^{n} \\mathrm{Var}(X_i) \\quad \\text{(variance of sum = sum of variances for i.i.d.)} \\\\[2pt]\n&= \\frac{1}{n^2} \\sum_{i=1}^{n} \\sigma^2 \\quad \\text{(variance of a Normal distribution)} \\\\[2pt]\n&= \\frac{1}{n^2} \\cdot n \\, \\sigma^2 \\\\[2pt]\n&= \\frac{\\sigma^2}{n}\n\\end{aligned}\n\\]\nThis gives the well-known result that for a normally-distributed random variable \\(X_i ~ N(\\mu, \\sigma ^ 2)\\), the mean of N such i.i.d. random variables is: \\[\nX_i \\sim N(\\mu, \\frac{\\sigma^2}{N})\n\\]\nThis comes up a lot in hypothesis testing because of the central limit theorem."
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#background-on-mle",
    "href": "posts/maths/mathematical_statistics/math_stats.html#background-on-mle",
    "title": "Mathematical Statistics",
    "section": "8.0. Background on MLE",
    "text": "8.0. Background on MLE\n\n8.0.1 Intuition\nMaximum Likelihood Estimation (MLE) is an alternative approach to finding estimators using the Method of Moments (MOM)approach.\nThe idea is that we shift our perspective so that rather than thinking of PDFs as a function of the data \\(x\\) where parameters \\(\\mu\\) and \\(\\sigma\\) are unknown, we treat it as a function of the parameter \\(\\mu\\) where our observed data \\(x\\) is known.\nTypically we would think of the PDF for some distribution where \\(\\mu = 2\\) and \\(\\sigma = 3\\) as: \\[\n\\begin{aligned}\nf(x) &= \\frac{1}{\\sigma \\sqrt{2\\pi}}\n       \\exp\\left( -\\frac{1}{2} \\left( \\frac{x-\\mu}{\\sigma} \\right)^2 \\right) \\\\[2pt]\n     &= \\frac{1}{3 \\sqrt{2\\pi}}\n       \\exp\\left( -\\frac{1}{2} \\left( \\frac{x-2}{3} \\right)^2 \\right)\n\\end{aligned}\n\\]\nInstead, say we have observed one data point where \\(x_1 = 4\\). We treat \\(\\mu\\) as the unknown and rewrite this as the likelihood function: \\[\nL(\\mu) = \\frac{1}{\\sigma \\sqrt{2\\pi}}\n         \\exp\\left( -\\frac{1}{2} \\left( \\frac{4-\\mu}{\\sigma} \\right)^2 \\right)\n\\]\nThen to find a good estimate of the parameter, we find the maximum of the likelihood function.\nIn summary:\n\nPDF: Known parameters, unknown data\nLikelihood: Unknown parameters, known data.\n\n\n\n8.0.2 Joint PDF\nThe probability of two things happening in succession is the joint probability.\nFor example, the joint probability of flipping two heads is \\[\nP(H \\cap H) = 0.5 \\cdot 0.5 = 0.25\n\\]\nThis is true of any number of (independent) events. So if we have observed some data \\((x_1, x_2, ... x_n) = X\\) then we can think about the joint probability distribution of all of these events \\(P(X)\\).\n\\[\nf(X) = \\prod_{i=1}^{n} f(x_i)\n\\]\nWe assume all of the data (\\(x_i\\)s) are i.i.d., i.e. the same distribution parameters, for the following discussions.\nFor a Bernoulli distribution, the joint PMF is: \\[\n\\begin{aligned}\nf(X) &= \\prod_{i=1}^{n} f(x_i) \\\\[2pt]\n     &= \\prod_{i=1}^{n} p^{x_i} (1-p)^{1-x_i} \\\\[2pt]\n     &= p^{x_1} (1-p)^{1-x_1} \\cdot p^{x_2} (1-p)^{1-x_2} \\cdots p^{x_n} (1-p)^{1-x_n}\n        \\quad \\text{(Expanding out)} \\\\[2pt]\n     &= p^{\\sum_{i=1}^{n} x_i} (1-p)^{\\sum_{i=1}^{n} (1-x_i)}\n        \\quad \\text{(Collecting terms)} \\\\[2pt]\n     &= p^{\\sum_{i=1}^{n} x_i} (1-p)^{n - \\sum_{i=1}^{n} x_i}\n        \\quad \\text{(Simplifying)}\n\\end{aligned}\n\\]\nFor a Uniform distribution, the joint PDF is: \\[\n\\begin{aligned}\nf(X) &= \\prod_{i=1}^{n} f(x_i) \\\\[2pt]\n     &= \\prod_{i=1}^{n} \\frac{1}{\\theta} \\\\[2pt]\n     &= \\left(\\frac{1}{\\theta}\\right)^{n}\n\\end{aligned}\n\\]\nFor a Normal distribution, the joint PDF is: \\[\n\\begin{aligned}\nf(X) &= \\prod_{i=1}^{n} f(x_i) \\\\[2pt]\n     &= \\prod_{i=1}^{n} (2 \\pi \\sigma^2)^{-1/2} \\,\n        \\exp\\Bigg(-\\frac{(x_i - \\mu)^2}{2 \\sigma^2}\\Bigg) \\\\[2pt]\n     &= (2 \\pi \\sigma^2)^{-n/2} \\,\n        \\exp\\Bigg(-\\frac{\\sum_{i=1}^{n} (x_i - \\mu)^2}{2 \\sigma^2}\\Bigg)\n\\end{aligned}\n\\]\nWe often call the joint PMF/PDF the “joint distribution”.\nRemember that the “likelihood function” is just this same formula, but reframing what we see as the knowns vs unknowns. We treat the parameter as an unknown and the data as a known value.\n\n\n8.0.3. Finding the MLE\nWe differentiate the likelihood function to find the maximum.\nBecause the likelihood function \\(L(\\theta)\\) is a joint distribution, it will often contain lots of products and exponentials. It is therefore often more convenient to work with the log-likelihood function \\(\\ell(\\theta)\\) since logs convert products to sums which are easier to differentiate. We use the natural log to deal with the \\(e\\) values that often crop up. \\[\n\\ell(\\theta) = \\log L(\\theta)\n\\]\nThe log function is monotonic, so the maximum of \\(\\ell(\\theta)\\) occurs for the same value of \\(\\theta\\) as \\(L(\\theta)\\), i.e. we are still maximising our original likelihood function even if we use the log-likelihood in our calculations for tractability.\nTo find the maximum of the log-likelihood, we set its derivative to 0 and solve for \\(\\theta\\): \\[\n\\frac{d \\ell(\\theta)}{d \\theta} = 0\n\\]\nThe derivative function is sometimes called the “score function”."
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#properties-of-logarithms",
    "href": "posts/maths/mathematical_statistics/math_stats.html#properties-of-logarithms",
    "title": "Mathematical Statistics",
    "section": "8.0.4 Properties of Logarithms",
    "text": "8.0.4 Properties of Logarithms\nLog of products = sum of logs \\[\n\\log(xy) = \\log(x) + \\log(y)\n\\]\nLog of exponents \\[\n\\log(a^b) = b \\log(a)\n\\]\nAnd by extension \\[\n\\log(1 /a) = \\log(a^{-1}) =  -\\log(a)\n\\]\n\n8.0.5. Mean Squared Error\nWe used bias and variance to evaluate the goodness of our estimators.\nWe may want to compare a biased estimator vs an unbiased estimator. One method for doing so is to compare the mean squared error (MSE); the average squared distance from the true value.\n\\[\nMSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2]\n\\]\nWe can work through some algebra to get the identity: \\[\n\\mathrm{MSE}(\\hat{\\theta}) = \\mathrm{Bias}(\\hat{\\theta}) ^2 + \\mathrm{Var}(\\theta)\n\\]\nSo for an unbiased estimator, the \\(MSE = Variance\\).\nBiased estimators can still be useful. We want estimators which are close to the mean with small variance. We may have an unbiased estimator which has a large variance. An alternative biased estimator with small bias but much tighter variance could be more useful."
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#mle-estimator-for-a-bernoulli-distribution",
    "href": "posts/maths/mathematical_statistics/math_stats.html#mle-estimator-for-a-bernoulli-distribution",
    "title": "Mathematical Statistics",
    "section": "8.1. MLE Estimator for a Bernoulli Distribution",
    "text": "8.1. MLE Estimator for a Bernoulli Distribution\nThe general process is:\n\n\n\n\n\nflowchart LR\n\n  A(PDF) --&gt; B(Joint PDF) --&gt; C(Likelihood Function) --&gt; D(Log-Likelihood Function) --&gt; E(Maximise)\n\n\n\n\n\n\nRecall that our PDF is: \\[\nf(x_i) = p^{x_i} (1-p)^{1-x_i}\n\\]\nAnd our joint PDF when taking the product of multiple random Bernoulli variables is: \\[\nf(X) = p^{\\sum x_i} (1-p)^{n - \\sum x_i}\n\\]\nOur likelihood function is just rewriting this joint PDF as a function of the parameters. Just the same thing. \\[\nL(p) = p^{\\sum x_i} (1-p)^{n - \\sum x_i}\n\\]\nOur log-likelihood is then: \\[\n\\ell(p) = \\log L(p) = \\sum x_i \\log(p) + (n - \\sum x_i) log(1-p)\n\\]\nNow to find the maximum. \\[\n\\frac{d\\ell}{dp}\n= \\frac{\\sum_{i=1}^{n} x_i}{p} - \\frac{n - \\sum_{i=1}^{n} x_i}{1-p}\n= 0\n\\]\nThe solution to this is \\(\\hat{p}_{MLE}\\).\n$$\n\\[\\begin{aligned}\n&\\text{Multiply both sides by } p(1-p) \\\\[6pt]\n&\\sum_{i=1}^{n} x_i (1-p) - (n - \\sum_{i=1}^{n} x_i)\\,p = 0 \\\\[10pt]\n\n&\\text{Expand sums} \\\\[6pt]\n&\\sum_{i=1}^{n} x_i - p \\sum_{i=1}^{n} x_i - np + p \\sum_{i=1}^{n} x_i = 0 \\\\[10pt]\n\n&\\text{Collect \\(p\\) terms} \\\\[6pt]\n&p \\big(- \\sum_{i=1}^{n} x_i - n + \\sum_{i=1}^{n} x_i \\big) + \\sum_{i=1}^{n} x_i = 0 \\\\[6pt]\n&\\implies \\sum_{i=1}^{n} x_i = np \\\\[6pt]\n&\\implies \\hat{p} = \\frac{\\sum_{i=1}^{n} x_i}{n} = \\bar{X}\n\\end{aligned}\\]\n$$\nSo the MLE estimate of p is the same as the MoM estimate: \\(\\bar{X}\\)."
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#mle-estimator-for-a-uniform-distribution",
    "href": "posts/maths/mathematical_statistics/math_stats.html#mle-estimator-for-a-uniform-distribution",
    "title": "Mathematical Statistics",
    "section": "8.2. MLE Estimator for a Uniform Distribution",
    "text": "8.2. MLE Estimator for a Uniform Distribution\nThe PDF of each observation (random variable) \\(x_i\\) is: \\[\nf(x_i) =\n\\begin{cases}\n\\dfrac{1}{\\theta}, & 0 \\leq x_i \\leq \\theta \\\\[6pt]\n0, & \\text{otherwise}\n\\end{cases}\n\\]\nThe joint PDF is then the product of these: \\[\nf(X) = \\prod_{i=1}^{n} f(x_i) =\n\\begin{cases}\n\\dfrac{1}{\\theta^n}, & 0 \\leq x_i \\leq \\theta \\quad \\text{for all } i \\\\[6pt]\n0, & \\text{otherwise}\n\\end{cases}\n\\]\nThe bound is important as it determines when this function is valid, which we need to consider when maximising the likelihood function. Note that all of the \\(x_i\\) values need to be in the bound, otherwise the product, i.e. the entire joint PDF, is 0.\nThe likelihood function is just reframing the joint PDF as a function of \\(\\theta%.\\)$ L() =\n\\[\\begin{cases}\n\\dfrac{1}{\\theta^n}, & 0 \\leq x_i \\leq \\theta \\quad \\text{for all } i \\\\[6pt]\n0, & \\text{otherwise}\n\\end{cases}\\]\n$$\nWe often take the log at this point to make the differentiation more tractable, but in this case we don’t need to.\nWe can plot \\(L\\), or differentiate it, to realise that the value of \\(\\theta\\) asymptotically approaches infinity as \\(\\theta\\) approaches 0. \\[\n\\frac{dL}{d\\theta} = -n \\theta^{-n-1}\n\\]\n\n\n\nLikelihood function for a uniform distribution where the maximum value observed is 3.5\n\n\nSo is \\(\\theta_{MLE} = 0\\)? Naively we would think so, but remember the bounds of the PDF; \\(\\theta\\) has to be at least as high as the observations we have seen for the PDF to be valid. So the maximum of the observed \\(x_i\\) values is a lower bound. We can write this as \\(x_{(n)}\\), where the brackets indicate this is the nth element in the sorted list. The smallest allowable value of \\(\\theta\\) maximises this function.\nTherefore \\(\\hat{\\theta}_{\\mathrm{MLE}} = \\max_{i} x_i\\).\nNote that this is different to our MoM estimate, which said that \\(\\theta_{MOM} = 2 \\bar{X}\\).\nMLE bases the estimate of the upper bound parameter \\(\\theta\\) on the maximum value we have seen. MoM says to take the average we have observed, that should be somewhere near the middle, so we double it to get an estimate of the upper bound parameter \\(\\theta\\).\nWe know that \\(\\theta_{MOM}\\) was unbiased, so \\(\\theta_{MLE}\\) must be biased."
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#mle-estimator-for-a-normal-distribution",
    "href": "posts/maths/mathematical_statistics/math_stats.html#mle-estimator-for-a-normal-distribution",
    "title": "Mathematical Statistics",
    "section": "8.3. MLE Estimator for a Normal Distribution",
    "text": "8.3. MLE Estimator for a Normal Distribution\nRecall that for a Normal distribution, the joint PDF is: \\[\nf(X) = (2 \\pi \\sigma^2)^{-n/2} e^{-\\sum_{i=1}^{n} \\frac{(x-\\mu)^2}{2 \\sigma^2}}\n\\]\nSo the likelihood is just reframing this as a function of the parameter \\(\\mu\\): \\[\nL(\\mu) = (2 \\pi \\sigma^2)^{-n/2} e^{-\\sum_{i=1}^{n} \\frac{(x-\\mu)^2}{2 \\sigma^2}}\n\\]\nSince we are taking logs the constant term at the front becomes an addition, and then the derivative of a constant is 0. So we can ignore it. The log-likelihood function is then: \\[\n\\ell(\\mu) = \\frac{-1}{2 \\sigma^2} \\sum_{i=1}^{n} (x-\\mu)^2\n\\]\nTo find the maximum, we take the derivative and set it to 0. \\[\n\\begin{aligned}\n\\frac{d l}{d \\mu} &= -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} 2(x_i - \\mu) = 0 \\\\\n\\implies \\sum_{i=1}^{n} (x_i - \\mu) &= 0 \\\\\n\\implies \\sum_{i=1}^{n} x_i - n \\mu &= 0 \\\\\n\\implies \\hat{\\mu} &= \\frac{\\sum_{i=1}^{n} x_i}{n} \\\\\n&= \\boxed{\\bar{X}}\n\\end{aligned}\n\\]\nSo this is the same as the MoM estimator, \\(\\hat{\\mu} = \\bar{X}\\)."
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#crlb-for-a-bernoulli-distribution",
    "href": "posts/maths/mathematical_statistics/math_stats.html#crlb-for-a-bernoulli-distribution",
    "title": "Mathematical Statistics",
    "section": "9.1. CRLB for a Bernoulli Distribution",
    "text": "9.1. CRLB for a Bernoulli Distribution\nRecall that in general for any distribution: \\[\n\\operatorname{Var}(\\hat{\\theta}) \\ge \\frac{1}{\\,n \\, I_1(\\theta)\\,}, \\qquad\nI_1(\\theta) = -\\mathbb{E}[\\ell''(\\theta)]\n\\]\nFor a Bernoulli distribution: \\[\n\\begin{aligned}\nf(x) &= p^x (1-p)^x \\\\\nL(p) &= p^x (1-p)^x \\\\\n\\ell(p) &= x \\log(p) + (1-x) \\log(1-p) \\\\\n\\frac{d\\ell}{dp} &= \\frac{x}{p} - \\frac{1-x}{1-p}\n\\end{aligned}\n\\]\nThe Fisher Information is then found for a single observation by taking the second derivative and finding its expected value. \\[\n\\frac{d^2 \\ell}{dp^2} = \\ell''(p) = -\\frac{x}{p^2} - \\frac{1-x}{(1-p)^2}\n\\]\nThe expectation over x, using \\(\\mathbb{E}[x] = p\\) for a Bernoulli: \\[\n\\begin{aligned}\n\\mathbb{E}[\\ell''(p)] &= \\mathbb{E}\\Bigg[-\\frac{x}{p^2} - \\frac{1-x}{(1-p)^2}\\Bigg] \\\\\n&= -\\frac{p}{p^2} - \\frac{1-p}{(1-p)^2} \\\\\n&= -\\frac{1}{p} - \\frac{1}{1-p} \\\\\n&= -\\frac{1}{\\,p(1-p)\\,}\n\\end{aligned}\n\\]\nThe Fisher information is the negative of this, i.e. \\(I_1(p) = -E[l''(p)] = \\frac{1}{p(1-p)}\\)\nThe CRLB is the reciprocal, i.e. \\(\\text{CRLB} = \\frac{1}{\\,n I_1(p)\\,} = \\frac{p(1-p)}{n}\\)\nThis is the variance we found for both our MoM and MLE estimators, therefore these estimators are efficient."
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#crlb-for-a-uniform-distribution",
    "href": "posts/maths/mathematical_statistics/math_stats.html#crlb-for-a-uniform-distribution",
    "title": "Mathematical Statistics",
    "section": "9.2. CRLB for a Uniform Distribution",
    "text": "9.2. CRLB for a Uniform Distribution\nThe definition of Fisher Information only applies when the support (i.e. the sample space of the distribution) does not depend on the value of \\(\\theta\\). This condition is breached for the uniform distribution.\nThe uniform distribution is not a member of the exponential family."
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#crlb-for-a-normal-distribution",
    "href": "posts/maths/mathematical_statistics/math_stats.html#crlb-for-a-normal-distribution",
    "title": "Mathematical Statistics",
    "section": "9.3. CRLB for a Normal Distribution",
    "text": "9.3. CRLB for a Normal Distribution\nAs in the general case, we need to find the log-likelihood function, then calculate its second derivative.\n\\[\n\\begin{aligned}\n\\ell(\\mu) &= \\log\\left(\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\right) - \\frac{1}{2 \\sigma^2} (x-\\mu)^2 \\\\\n\\frac{d\\ell}{d\\mu} &= \\frac{x-\\mu}{\\sigma^2} \\\\\n\\ell''(\\mu) &= -\\frac{1}{\\sigma^2}\n\\end{aligned}\n\\]\nThe Fisher Information for a single observation is therefore: \\[\n\\begin{aligned}\nI_1(\\mu) &= -\\mathbb{E}\\Big[-\\frac{1}{\\sigma^2}\\Big] \\\\\n         &= \\frac{1}{\\sigma^2} \\quad \\text{Since none of this expression is random}\n\\end{aligned}\n\\]\nThe CRLB is therefore \\[\n\\operatorname{Var}(\\mu) \\ge \\frac{\\sigma^2}{n}\n\\]\nSo our MOM and MLE estimators are both efficient."
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#efficiency",
    "href": "posts/maths/mathematical_statistics/math_stats.html#efficiency",
    "title": "Mathematical Statistics",
    "section": "9.4. Efficiency",
    "text": "9.4. Efficiency\nWe can define the efficiency of an estimator as the ratio of its variance and its optimal variance given by the CRLB: \\[\n\\text{Efficiency}(\\hat{\\theta}) = \\frac{\\text{CRLB}}{\\operatorname{Var}(\\hat{\\theta})}\n\\]\nWe can also compare two different estimators by taking the ratio of their variances. This is the relative efficiency. For example, comparing the MOM and MLE estimators: \\[\n\\text{Relative Efficiency} = \\frac{\\operatorname{Var}(\\hat{\\theta}_{\\text{MOM}})}{\\operatorname{Var}(\\hat{\\theta}_{\\text{MLE}})}\n\\]\nThis is typically a function of \\(n\\). We can analyse the **relative asymptotic efficiency by observing the behaviour as \\(n \\to \\infty\\)."
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#clt-for-a-bernoulli-distribution",
    "href": "posts/maths/mathematical_statistics/math_stats.html#clt-for-a-bernoulli-distribution",
    "title": "Mathematical Statistics",
    "section": "10.1. CLT for a Bernoulli Distribution",
    "text": "10.1. CLT for a Bernoulli Distribution\nRecall that both MOM and MLE estimators for a Bernoulli distribution were \\(\\hat{p} = \\bar{X}\\).\nAs this is an average of data points, CLT applies. A general rule of thumb is that CLT applies when the expected number of successes \\(np \\ge 10\\) and the expected number of failures \\(n(1-p) \\ge 10\\).\nWe’ve already established the mean and variance of the estimator \\(\\hat{p}\\) in earlier sections: \\[\n\\begin{aligned}\n\\mathbb{E}[\\hat{p}] &= p \\\\\n\\operatorname{Var}[\\hat{p}] &= \\frac{p(1-p)}{n}\n\\end{aligned}\n\\]\nSo the value of the estimator is a Normally distributed random variable with those parameters. We now know the entire distribution and can do interesting things with it, like hypothesis testing.\nThe CLT approximation is helpful because the Normal distribution can often be easier to compute than the original distribution, so it reduces the computing power (or historically, the number of calculations required by hand), while still giving a good approximation to the true value if we had used the actual distribution."
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#clt-for-a-uniform-distribution",
    "href": "posts/maths/mathematical_statistics/math_stats.html#clt-for-a-uniform-distribution",
    "title": "Mathematical Statistics",
    "section": "10.2. CLT for a Uniform Distribution",
    "text": "10.2. CLT for a Uniform Distribution\nWe only consider the MOM estimate not the MLE estimate, because the MLE estimate was a maximum of values, meaning the CLT does not apply.\nWe can reason about the asymptotic distribution of the MOM estimate for a uniform distribution if our sample size is large enough \\(n \\ge 30\\).\nWe’ve already established the mean and variance of the estimator \\(\\theta\\) in earlier sections: \\[\n\\begin{aligned}\n\\mathbb{E}[\\hat{\\theta}] &= \\theta \\\\\n\\operatorname{Var}[\\hat{p}] &= \\frac{\\theta^2}{3n}\n\\end{aligned}\n\\]\nSo again, our estimate is a Normally-distributed random variable with these parameters."
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#clt-for-a-normal-distribution",
    "href": "posts/maths/mathematical_statistics/math_stats.html#clt-for-a-normal-distribution",
    "title": "Mathematical Statistics",
    "section": "10.3. CLT for a Normal Distribution",
    "text": "10.3. CLT for a Normal Distribution\nWhen we add Gaussians we get another Gaussian, so the actual (rather than asymptotic) distribution of our estimator for a Normal distribution is exactly Normal.\nWe don’t need any specific sample size for this to be true; it is true for \\(n=1,2,3,...\\), we don’t need our usual rule of \\(n \\ge 30\\).\nWe’ve already established the mean and variance of the estimator \\(\\mu\\) in earlier sections: \\[\n\\begin{aligned}\n\\mathbb{E}[\\hat{\\mu}] &= \\mu \\\\\n\\operatorname{Var}[\\hat{\\mu}] &= \\frac{\\sigma^2}{n}\n\\end{aligned}\n\\]\nSo again, our estimate is a Normally-distributed random variable with these parameters.\nNote that we don’t need a minimum sample size \\(n\\) for the CLT to apply, but a large \\(n\\) reduces the variance of our estimate, thus reducing our uncertainty."
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#consistency",
    "href": "posts/maths/mathematical_statistics/math_stats.html#consistency",
    "title": "Mathematical Statistics",
    "section": "10.4. Consistency",
    "text": "10.4. Consistency\nConsistency is another measure of asymptotic behaviour that says that as the number of samples approaches infinity, the estimator converges to the true value of the parameter.\nMathematically \\[\nP\\big(| \\hat{\\theta} - \\theta | &lt; \\epsilon \\big) \\to 1 \\quad \\text{as } n \\to \\infty,\n\\quad \\text{for } \\epsilon \\text{ arbitrarily small}\n\\]\nUnbiased estimators are consistent if their variance approaches 0 as \\(n\\) approaches infinity."
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#background",
    "href": "posts/maths/mathematical_statistics/math_stats.html#background",
    "title": "Mathematical Statistics",
    "section": "11.0. Background",
    "text": "11.0. Background\nWhen we’ve looked at estimators in previous sections, we’ve focused on point estimates.\nBut we know that these are random variables, so we would prefer to make interval estimates. Then we can make statements like “we are 95% sure that the mean is between 1.5 to 2.5”.\nWe can use the results from previous sections for this. Recall that we were able to reason about the distribution of our estimate with CLT. The estimate \\(\\hat{\\theta}\\) is centred on the true value, and using the \\(2 \\, \\sigma(\\hat{\\theta})\\) value, there is a 95% probability that the estimate lies within 2 (sample) standard deviations of the true mean. Mathematically: \\[\nP\\big(\\theta - 2\\sigma(\\hat{\\theta}) \\le \\hat{\\theta} \\le \\theta + 2\\sigma(\\hat{\\theta})\\big) = 0.95\n\\]\nNote that we have the population parameter value \\(\\theta\\) in the left and right terms and the sample value \\(\\hat{\\theta}\\) in the middle term.\nWe want to shift our perspective when going from probability to statistics. In statistics/the real world, we don’t know the TRUE parameter value, but we can gather data so that our ESTIMATED parameter value is known. So we want to make confidence interval statements about the true value, i.e. we want the population value $$ in the middle of the inequality and estimated values \\(\\hat{\\theta}\\) as the bounds.\nWe can do this with a bit of algebra. Starting with the equation above, subtract \\(\\theta\\) and \\(\\hat{\\theta}\\) from each term inside the probability. \\[\nP\\big(-\\hat{\\theta} - 2\\sigma(\\hat{\\theta}) \\le -\\theta \\le -\\hat{\\theta} + 2\\sigma(\\hat{\\theta})\\big) = 0.95\n\\]\nNow multiply all terms by -1 and note that it flips the sign of the inequality when we multiply by a negative value. \\[\nP\\big(\\hat{\\theta} - 2\\sigma(\\hat{\\theta}) \\le \\theta \\le \\hat{\\theta} + 2\\sigma(\\hat{\\theta})\\big) = 0.95\n\\]\nThis was our goal: we now have bounds for the population value \\(\\theta\\) in terms of the estimated values \\(\\hat{\\theta}\\) which are known values that we can calculate from the data.\nWe have inverted/pivoted the roles of \\(\\theta\\) and \\(\\hat{\\theta}\\).\nWhat does it mean to have a 95% confidence interval? Say we have a process:\n\nCollect data\nCalculate the confidence interval\nTheoretically repeat this process over and over\n\nSo we are doing 20 coin flips. We get a confidence interval \\([0.4, 0.45]\\)\nThen we do 20 more flips and get a confidence interval \\([0.48,0.53]\\)\nThen a third time we do 20 flips and get a confidence interval \\([0.46, 0.55]\\)\nAnd we keep going… We get a list of confidence intervals: \\[\n[0.4, 0.45]\n[0.48,0.53]\n[0.46, 0.55]\n[0.49, 0.56]\n[0.42, 0.50]\n[0.45, 0.55]\n...\n\\]\nOur 95% confidence value means “95% of these hypothetical intervals calculated will contain the true parameter value \\(\\theta\\)”.\n\n11.0.1 The Pivot from Probability to Statistics\nThe pivot from probability to statistics was mentioned in the previous section but is crucial so bears emphasising.\nIn probability, we can make statements like “there is 95% probability that \\(\\hat{\\theta} \\text{ is in } \\theta \\pm 2 \\, \\sigma(\\theta)\\)”.\nIn statistics, thanks to our pivot (algebraic manipulation of the inequality), we make statements like “we are 95% confidence that \\(\\theta\\) is in the interval \\(\\hat{\\theta} \\pm 2 \\times \\text{standard error}(\\hat{\\theta})\\)”.\n\n\n11.0.2. Some Useful Terms\nThe sample standard deviation, e.g. \\(\\frac{\\hat{\\sigma}}{\\sqrt{n}}\\) is called the standard error.\nThe margin of error for a particular confidence interval statement is the standard error multiplied by the number of standard deviations we use. For example, for a 95% confidence interval \\[\n\\text{Margin of error} = 2 \\times \\text{Standard error}\n\\]"
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#bernoulli-confidence-intervals",
    "href": "posts/maths/mathematical_statistics/math_stats.html#bernoulli-confidence-intervals",
    "title": "Mathematical Statistics",
    "section": "11.1 Bernoulli Confidence Intervals",
    "text": "11.1 Bernoulli Confidence Intervals\nWe’ve already established the mean and standard error of our MOM/MLE estimators (they were the same for Bernoulli).\nSo our 95% confidence interval is: \\[\n\\hat{p} \\pm 2 \\, \\sqrt{\\frac{\\hat{p} (1-\\hat{p})}{n}}\n\\]"
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#uniform-distribution-confidence-intervals",
    "href": "posts/maths/mathematical_statistics/math_stats.html#uniform-distribution-confidence-intervals",
    "title": "Mathematical Statistics",
    "section": "11.2. Uniform Distribution Confidence Intervals",
    "text": "11.2. Uniform Distribution Confidence Intervals\nWe skip the MLE uniform estimator since it’s a maximum of values, therefore CLT does not apply and we cannot reason in the same way.\nFor our MOM estimator, the 95% confidence interval is: \\[\n\\hat{\\theta} \\pm \\frac{2 \\, \\theta}{\\sqrt{3n}}\n\\]"
  },
  {
    "objectID": "posts/maths/mathematical_statistics/math_stats.html#normal-distribution-confidence-intervals",
    "href": "posts/maths/mathematical_statistics/math_stats.html#normal-distribution-confidence-intervals",
    "title": "Mathematical Statistics",
    "section": "11.3. Normal Distribution Confidence Intervals",
    "text": "11.3. Normal Distribution Confidence Intervals\nFor our MOM estimator, the 95% confidence interval is: \\[\n\\hat{\\mu} \\pm \\frac{2 \\, \\sigma}{\\sqrt{n}}\n\\]"
  },
  {
    "objectID": "posts/maths/more_statistics/lesson.html",
    "href": "posts/maths/more_statistics/lesson.html",
    "title": "More Statistics",
    "section": "",
    "text": "In practice, the Z-test is quite unrealistic. It assumed the population mean is unknown but the variance is known. We need the mean to calculate the variance, so this generally never happens in practice.\nThe T-test statistic is almost the same as the Z-score. In general, test statistics are of the form:\n\\[\n\\text{test statistic} = \\frac{\\text{observed value} - \\text{expected value}}{\\text{standard error}}\n\\]\nFor the t-test, the standard error divides by \\(n-1\\) rather than \\(n\\) to account for Bessel’s correction. Since we are using the sample mean rather than the population, then data is going to be closer to the sample mean than the population mean; the sample mean is derived from the data itself. So we have one less degree of freedom.\nThe T-test also differs in that it depends on our sample size. The tails are fatter than a normal distribution and tend towards a normal as \\(n\\) grows. \\(DoF=n-1\\)\n\n\n\nIf we are testing whether two populations have different means, we use a two-sample T-test.\n\\[\n\\begin{aligned}\nH_0 &: \\mu_1 = \\mu_2 \\\\\nH_1 &: \\mu_1 \\ne \\mu_2\n\\end{aligned}\n\\]\n\\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n\\]\nThis is the unpooled estimate since we keep the standard deviations of the two populations distinct.\nThe DoF is somewhere between the two. A conservative approach is to use the minimum of n1 and n2.\n\n\n\nIf we have reason to believe the two populations have the same variance, we can pool them together to get the standard error:\n\\[\n\\begin{aligned}\nSE &= \\sqrt{\\sigma^2 \\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)} \\\\\n\\text{DoF} &= n_1 + n_2 - 2\n\\end{aligned}\n\\]\nThis is generally quite a strong assumption so unrealistic to use in practice.\n\n\n\nIf we have paired data, we only really need to know the diff.\nFor example, if we have pairs of brothers and sisters, and want to know if brothers and taller than sisters.\nWe could mistakenly treat this as a 2-sample test. The correct approach is to take the diff for each pair. Think of this like a pre-processing step.\nThen perform a regular one-sample T-test on the diffs."
  },
  {
    "objectID": "posts/maths/more_statistics/lesson.html#one-sample-t-test",
    "href": "posts/maths/more_statistics/lesson.html#one-sample-t-test",
    "title": "More Statistics",
    "section": "",
    "text": "In practice, the Z-test is quite unrealistic. It assumed the population mean is unknown but the variance is known. We need the mean to calculate the variance, so this generally never happens in practice.\nThe T-test statistic is almost the same as the Z-score. In general, test statistics are of the form:\n\\[\n\\text{test statistic} = \\frac{\\text{observed value} - \\text{expected value}}{\\text{standard error}}\n\\]\nFor the t-test, the standard error divides by \\(n-1\\) rather than \\(n\\) to account for Bessel’s correction. Since we are using the sample mean rather than the population, then data is going to be closer to the sample mean than the population mean; the sample mean is derived from the data itself. So we have one less degree of freedom.\nThe T-test also differs in that it depends on our sample size. The tails are fatter than a normal distribution and tend towards a normal as \\(n\\) grows. \\(DoF=n-1\\)"
  },
  {
    "objectID": "posts/maths/more_statistics/lesson.html#two-sample-t-test",
    "href": "posts/maths/more_statistics/lesson.html#two-sample-t-test",
    "title": "More Statistics",
    "section": "",
    "text": "If we are testing whether two populations have different means, we use a two-sample T-test.\n\\[\n\\begin{aligned}\nH_0 &: \\mu_1 = \\mu_2 \\\\\nH_1 &: \\mu_1 \\ne \\mu_2\n\\end{aligned}\n\\]\n\\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n\\]\nThis is the unpooled estimate since we keep the standard deviations of the two populations distinct.\nThe DoF is somewhere between the two. A conservative approach is to use the minimum of n1 and n2."
  },
  {
    "objectID": "posts/maths/more_statistics/lesson.html#pooled-vs-unpooled",
    "href": "posts/maths/more_statistics/lesson.html#pooled-vs-unpooled",
    "title": "More Statistics",
    "section": "",
    "text": "If we have reason to believe the two populations have the same variance, we can pool them together to get the standard error:\n\\[\n\\begin{aligned}\nSE &= \\sqrt{\\sigma^2 \\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)} \\\\\n\\text{DoF} &= n_1 + n_2 - 2\n\\end{aligned}\n\\]\nThis is generally quite a strong assumption so unrealistic to use in practice."
  },
  {
    "objectID": "posts/maths/more_statistics/lesson.html#paired-t-tests",
    "href": "posts/maths/more_statistics/lesson.html#paired-t-tests",
    "title": "More Statistics",
    "section": "",
    "text": "If we have paired data, we only really need to know the diff.\nFor example, if we have pairs of brothers and sisters, and want to know if brothers and taller than sisters.\nWe could mistakenly treat this as a 2-sample test. The correct approach is to take the diff for each pair. Think of this like a pre-processing step.\nThen perform a regular one-sample T-test on the diffs."
  },
  {
    "objectID": "posts/maths/more_statistics/lesson.html#chi-squared-test",
    "href": "posts/maths/more_statistics/lesson.html#chi-squared-test",
    "title": "More Statistics",
    "section": "2.1. Chi-squared Test",
    "text": "2.1. Chi-squared Test\nThis is for comparing two categorical variables where we want to evaluate how “unusual” a result is.\nFor example, we roll a dice and measure the number of 1s, 2s, 3s, 4s, 5s, 6s we get. How do we determine if this is an unusual result?\nRecall the general form of test statistics is typically: \\[\n\\text{test statistic} = \\frac{\\text{observed value} - \\text{expected value}}{\\text{standard error}}\n\\]\nA chi-squared test statistic is: \\[\n\\chi^2 = \\sum_{\\text{categories}} \\frac{(\\text{observed frequency} - \\text{expected frequency})^2}{\\text{expected frequency}}\n\\]\nThe expected frequency acts as the standard error, essentially a scaling term.\nThe expected frequency essentially defines a “null hypothesis distribution” and so the chi-squared test is telling us “does the observed frequency distribution differ in a statistically significant way?”\nThe chi-squared distribution is right-skewed and cannot be negative. We generally only need to do 1-tailed tests.\nThe test statistic depends on the degrees of freedom. For chi squared, \\[\n\\text{DoF} = n_{categories} - 1\n\\]"
  },
  {
    "objectID": "posts/maths/more_statistics/lesson.html#two-way-tables",
    "href": "posts/maths/more_statistics/lesson.html#two-way-tables",
    "title": "More Statistics",
    "section": "2.2. Two-way Tables",
    "text": "2.2. Two-way Tables\nWe looking at the distribution of two variables at the same time, e.g. ice cream sales by flavour and gender of customer.\nThe row and column totals are in the “margins” of the table, so are called the marginal distributions. The values in the table itself are telling us the relationship joining the two variables, so are called the joint distribution.\nWe can answer questions like “are gender and ice cream flavour preference related?”\nWe proceed as before. We have some observed frequencies. We calculate the expected frequency for each cell as:\nExpected count = row total * column total / total total\nThe chi-squared test statistic is calculated in the same way as the one variable case, summing over all categories.\nThe only difference of how we calculate the degrees of freedom: \\[\n\\text{DoF} = (n_{rows} - 1) \\times (n_{cols} - 1)\n\\]\nWe can think of the DoF as “if I know all of the marginal frequencies, how many of the joint values in the middle of the table would I need to know to fully define the table?”\nThink of it like a Sudoku: if you were given 1 joint value and all of the margins, you could fill in all of the other joint values. So DOF=1."
  },
  {
    "objectID": "posts/maths/more_statistics/lesson.html#homogeneity-vs-independence",
    "href": "posts/maths/more_statistics/lesson.html#homogeneity-vs-independence",
    "title": "More Statistics",
    "section": "2.3.Homogeneity vs Independence",
    "text": "2.3.Homogeneity vs Independence\nThere are two different scenarios when we would use the chi-squared test. The calculations are the same in both cases, the differences are in the phrasing of the null hypothesis and how the sample was collected.\n\nHomogeneity\nHypothesising that two categories are the same. E.g. “the ice cream preferences are the same for males and females”. Typically the sample would be collected with a stratified random sample, i.e. we are explicitly asking questions about males vs females, so we design our experiment to sample equal numbers of males and females.\n\n\nIndependence\nHypothesising that two categories are independent of each other. E.g. “gender and ice cream preferences are independent/ unrelated/ uncorrelated”. Typically the sample would be collected with a simple random sample, i.e. we just collect data and then see if there is a dependence between categories.\nThe two types boil down to the same calculations because they are essentially asking the same question of the distribution.\nIndependence is asking, is the following true? \\[\nP(chocolate \\cap male) = P(chocolate)P(male)\n\\]\nHomogeneity is asking, is the following true? \\[\nP(chocolate | male) = P(chocolate | female)\n\\]"
  },
  {
    "objectID": "posts/maths/more_statistics/lesson.html#comparing-quantitative-variables",
    "href": "posts/maths/more_statistics/lesson.html#comparing-quantitative-variables",
    "title": "More Statistics",
    "section": "3.1. Comparing Quantitative Variables",
    "text": "3.1. Comparing Quantitative Variables\nCorrelations measure the relation between two quantitative variables. This is analogous to how the chi-squared test measured the relationship between two categorical variables.\nWe generally want to understand three properties of the relation between the variables:\n\nDirection. Are they positively or negatively correlated?\nForm. Is there a linear relationship between them? Polynomial? Exponential?\nStrength. How closely do the points lie on the fitted line/curve?\n\nA scatter plot is a useful visualisation to get an idea of these three properties."
  },
  {
    "objectID": "posts/maths/more_statistics/lesson.html#pearsons-correlation-coefficient",
    "href": "posts/maths/more_statistics/lesson.html#pearsons-correlation-coefficient",
    "title": "More Statistics",
    "section": "3.2. Pearson’s Correlation Coefficient",
    "text": "3.2. Pearson’s Correlation Coefficient\nThis is denoted with \\(r\\). It can be between -1 and 1.\nIn terms of the three properties of the relationship:\n\nDirection. The sign of \\(r\\) tells us this.\nForm. Correlation coefficient only considers linear forms.\nStrength. The magnitude of \\(r\\) tells us the strength of the relationship.\n\nThe equation for \\(r\\) is:\n\\[\n\\begin{aligned}\nr &= \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}\n         {\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\, \\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}} \\\\\n  &= \\frac{\\operatorname{cov}(X,Y)}{s_x s_y}\n\\end{aligned}\n\\]\nWe can think of this as the same form as our previous test statistic: \\(\\frac{\\text{observed} - \\text{expected}}{\\text{standard error}}\\)\nWe normally think in terms of \\(r\\) rather than covariance because \\(r\\) is normalised. The units of covariance are weird and not easily interpretable.\nThe regression line will always pass through \\((\\bar{x}, \\bar{y})\\). We can split the scatter plot into four quadrants centred on \\((\\bar{x}, \\bar{y})\\).\n\nPoints in the top right and bottom left contribute positive values to the correlation value.\nPoints in the top left and bottom right contribute negative values to the correlation value."
  },
  {
    "objectID": "posts/maths/more_statistics/lesson.html#the-regression-equation",
    "href": "posts/maths/more_statistics/lesson.html#the-regression-equation",
    "title": "More Statistics",
    "section": "3.3. The Regression Equation",
    "text": "3.3. The Regression Equation\nLinear regression problems model the equation between the two variables as:\n\\[\ny = \\beta_0 + \\beta_1 x + \\epsilon,\n\\quad \\text{where } \\epsilon \\text{ is a Gaussian noise term.}\n\\]\nWe are trying to estimate \\(\\beta_0\\) and \\(\\beta_1\\) of the true parameter. We estimate \\(\\hat{y}\\) using \\(b_0\\) and \\(b_1\\).\n\\[\n\\hat{y} = b_0 + b_1 x\n\\] We can perform t-tests on the parameters.\nThe sign of \\(\\beta_1\\) is the same as the sign of \\(r\\); both are telling us the direction of the relationship."
  },
  {
    "objectID": "posts/maths/more_statistics/lesson.html#least-squares",
    "href": "posts/maths/more_statistics/lesson.html#least-squares",
    "title": "More Statistics",
    "section": "3.4. Least Squares",
    "text": "3.4. Least Squares\nLeast squares estimation is how we find the parameters \\(\\beta_0\\) and \\(\\beta_1\\). We want to minimise the sum of squared errors.\nThe error terms, or “residuals”, are: \\[\ne_i = y_i - \\hat{y}_i\n\\]\nWe can think of this as the observed error. It is the vertical distance of each observed point to our fitted line.\nWe could do this by convex optimisation, but least squares does have a closed form solution:\n\\[\n\\begin{aligned}\nb_1 &= r \\frac{s_y}{s_x} \\\\\nb_0 &= \\bar{y} - b_1\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/maths/more_statistics/lesson.html#errors-and-residuals",
    "href": "posts/maths/more_statistics/lesson.html#errors-and-residuals",
    "title": "More Statistics",
    "section": "3.5. Errors and Residuals",
    "text": "3.5. Errors and Residuals\nRecall the error term for our true (but unknowable) distribution \\(y\\) is \\(\\epsilon\\).\nThe error term for our estimated line \\(\\hat{y}\\) is \\(e\\).\nWe assume \\[\n\\epsilon \\sim \\mathcal{N}(0, \\sigma_e^2)\n\\]\nWe use the variance of the observed errors \\(e\\) as an estimate of the variance of \\(\\epsilon\\). The mean must be 0, because if it was anything else, i.e. there was a constant offset of the errors, we could reduce the mean to 0 and therefore achieve a smaller loss by shifting the parameters of our line.\nThe estimated variance is: \\[\ns^2 = \\frac{1}{n-2} \\sum_{i=1}^{n} e_i^2\n\\]\n\\(s\\) is the standard deviation of residuals. The \\(n-2\\) term is the degrees of freedom; we have two parameters in a linear regression hence the \\(n-2\\) term."
  },
  {
    "objectID": "posts/maths/more_statistics/lesson.html#the-coefficient-of-determination",
    "href": "posts/maths/more_statistics/lesson.html#the-coefficient-of-determination",
    "title": "More Statistics",
    "section": "3.6. The Coefficient of Determination",
    "text": "3.6. The Coefficient of Determination\n\\(R^2\\) is simply the \\(r\\) value (correlation coefficient) squared, at least for a simple linear regression with one variable.\nIt measures “the % of variability of Y explained by X”.\nIt does this by measuring the sum of squared residuals for our fitted model and comparing it to the sum of squared residuals if we have just used the mean value of y, \\(\\bar{y}\\) as our estimating function, throwing away any data about \\(X\\).\nIf our model is good, the residuals of the model will be smaller than the mean estimator.\nHence: \\[\nR^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n\\]"
  },
  {
    "objectID": "posts/maths/more_statistics/lesson.html#hypothesis-testing-of-parameters",
    "href": "posts/maths/more_statistics/lesson.html#hypothesis-testing-of-parameters",
    "title": "More Statistics",
    "section": "3.7. Hypothesis Testing of Parameters",
    "text": "3.7. Hypothesis Testing of Parameters\nWe can apply the same concept of hypothesis tests and confidence intervals to our parameters.\nAs an example of checking whether the slope is significant, i.e. is there a relation between X and Y:\n\\[\n\\begin{aligned}\nH_0 &: \\beta_1 = 0 \\\\\nH_1 &: \\beta_1 \\ne 0\n\\end{aligned}\n\\]\nWe use a t-test with \\(n-2\\) degrees of freedom (for the same reason we use \\(n-2\\) in the \\(R^2\\) calculation).\nThe standard error for our estimate \\(b_1\\) is given (not derived in the lecture) as:\n\\[\nSE(b_1) = \\frac{s}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}}\n\\]\nwhere \\(s\\) is the standard deviation of residuals.\nThe t-value is then as usual: \\[\nt = \\frac{(observed - expected)}{SE}\n\\]\nThen we can perform the hypothesis test as usual, using either \\(t_{critical}\\), the p-value or the constructed confidence intervals to determine whether 0 is a possible value of \\(\\beta_1\\) at the given confidence level."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html",
    "href": "posts/ml/meta_learning/meta_learning.html",
    "title": "Meta Learning",
    "section": "",
    "text": "These are notes on the book Meta Learning by Radek Osmulski."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#learning-to-program",
    "href": "posts/ml/meta_learning/meta_learning.html#learning-to-program",
    "title": "Meta Learning",
    "section": "1. Learning to Program",
    "text": "1. Learning to Program\nYou must become a developer before you can be a deep learning expert. Start by tinkering with things you enjoy. Don’t exert yourself too much so that you can stay consistent above all else.\n\n1.1. CS Foundations\nStart with a programming MOOC like Harvard CS50 if coming into this fresh. Don’t get bogged down in tutorial hell, just get familiar enough with high-level concepts to be able to google the rest.\n“It doesn’t make sense to invest all your time into learnng calligraphy if you have nothing to write about”.\n\n\n1.2. IDE\nLearn to use an IDE effectively. If stuck, just start with VSCode.\n\n\n1.3. Version control\nLearn to use git and GitHub.\n\n\n1.4. DevOps\nLearn how to use your computer in the context of writing code: spin up a cloud VM, ssh into it, move data around, etc. A good resource is The Missing Semester.\n\n\n1.5. Start Learning Deep Learning the Right Way\nThe above 4 steps are to get to a stage where you can take the Practical Deep Learning for Coders which is the best starting foundation to get a high-level grounding in ML.\nUse the top-down approach to learning championed by fastai:\n\nWatch a lecture.\nLook through the associated notebook - run the code and understand the inputs and outputs of each cell.\nStart with a new notebook and try to work through the same steps from scratch as an open-book exercise. Also read the documentation as you go along to fill any gaps.\nFind a similar dataset (or make one) and try the same techniques you’ve just learned. Creating a dataset is a great way to think about feature engineering and choices of labels.\n\nLearn the whole game then play out of town."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#theory-vs-practice",
    "href": "posts/ml/meta_learning/meta_learning.html#theory-vs-practice",
    "title": "Meta Learning",
    "section": "2. Theory vs Practice",
    "text": "2. Theory vs Practice\nStarting from an “elements-first” approach can feel like a never ending struggle. You want to learn ML, but then need to study calculus, but then you need to study real analysis before that, but then you need to study set theory before that…\nThe problem with theory: theory follows practice!\nBecome a great practitioner first and the theory will make more sense afterwards, once you have some intuition. Practical problems give you an incredibly useful feedback loop for your learning that you don’t get from following a linear theoretical curriculum.\nFor best results, combine practice and theory, in that order."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#programming-is-about-what-you-have-to-say",
    "href": "posts/ml/meta_learning/meta_learning.html#programming-is-about-what-you-have-to-say",
    "title": "Meta Learning",
    "section": "3. Programming is About What You Have to Say",
    "text": "3. Programming is About What You Have to Say\nYour ability as a developer is measured by the utility of things you can do in your language of choice. What you have to say is the most important thing!\nThe key to getting started is to read and write a lot of code. Start with 100-200 line projects, anything under 1000 lines should be possible to follow. Then graduate to larger problems.\nDomain knowledge comes first. Once you know what you are trying to achieve and broadly how you can achieve it, you can worry about best practices to write clean, maintainable code later.\nThe fastest way to learn to program is to learn to say something useful."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#the-secret-of-good-developers",
    "href": "posts/ml/meta_learning/meta_learning.html#the-secret-of-good-developers",
    "title": "Meta Learning",
    "section": "4. The Secret of Good Developers",
    "text": "4. The Secret of Good Developers\nBecoming familiar with a codebase or problem requires holding multiple things in your head: the layout of the code, how it is tested, how you intend to change it, other places that might be affected by your change, etc. This requires uninterrupted focus. When you are interrupted, you drop those things you were holding in your head, and you might not always pick them up again when you switch back.\nThe price of context switching is extremely high!\nLong, uninterrupted sessions - “deep work” - are the key to progress."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#the-best-way-to-improve-as-a-developer",
    "href": "posts/ml/meta_learning/meta_learning.html#the-best-way-to-improve-as-a-developer",
    "title": "Meta Learning",
    "section": "5. The Best Way to Improve as a Developer",
    "text": "5. The Best Way to Improve as a Developer\nYou sharpen your skills with practice. To get better at a thing, do the thing!\nThe key is to read and write code. Everything else, like MOOCs, books, articles etc are nice as a side dish but they are not the main course."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#achieving-flow",
    "href": "posts/ml/meta_learning/meta_learning.html#achieving-flow",
    "title": "Meta Learning",
    "section": "6. Achieving Flow",
    "text": "6. Achieving Flow\nFlow is difficult to achieve, but we can help ourselves by removing any obvious obstacles.\nLearn your IDE inside out, and know all of the keyboard shortcuts so that you don’t interrupt your flow by switching to your mouse or searching through settings.\nLikewise, address easy things like making sure you’ve charged your laptop, keyboard, mouse etc and you don’t spend time battling connectivity issues or just plugging things in.\nFlow is a spectrum. Don’t think of it as “in flow” or “not in flow”. Rather, “how much are you flowing?”. Take small actions to increase it.\nJust work on what you want to work on. Don’t overthink it and talk yourself out of doing something because you’re not a “real” developer/author/startup founder etc."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#genuine-work-compounds",
    "href": "posts/ml/meta_learning/meta_learning.html#genuine-work-compounds",
    "title": "Meta Learning",
    "section": "7. Genuine Work Compounds",
    "text": "7. Genuine Work Compounds\n\n“Reading a book without taking notes is like discovering a new territory without making a map.”\n\nDoing &gt; thinking\nThinking about something without writing notes or doing more work on it is like running on a treadmill when your goal is to get from A to B. Just do a little bit: write some notes one day, then outline the project then next, then start the first component, etc. Before long, you will have made more progress than you expected.\nThe more “atoms you move” the more feeback you can get, so the more you can reflect and learn."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#how-to-structure-an-ml-project",
    "href": "posts/ml/meta_learning/meta_learning.html#how-to-structure-an-ml-project",
    "title": "Meta Learning",
    "section": "8. How to Structure an ML Project",
    "text": "8. How to Structure an ML Project\n\n“The only way to maintain your sanity in the long run is to be paranoid in the short run.”\n\nThe key is a good train-validation-test set split.\nThen construct a baseline. The smaller and simpler, the better.\nThis helps us know we are moving in the right direction when we iterate. It also gives an idea of what shape our reeal results should be.\nStart broad. Explore different models and architectures first, rather than diving straight in to tuning hyperparameters.\nMake changes incrementally, then run the model and check that your output is the correct shape and not all zeros. You can’t write a complex model all in one sweep! This requires running the entire pipelines regularly. This could be a big time sink to run on the whole dataset, so just train on 5% or less to keep iterations fast.\nTime is a key component of success. You might get a decent solution quite quickly. But going from good to great is a creative endeavour, which requires time to think and mull over in your subconscious."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#how-to-win-at-kaggle",
    "href": "posts/ml/meta_learning/meta_learning.html#how-to-win-at-kaggle",
    "title": "Meta Learning",
    "section": "9. How to Win at Kaggle",
    "text": "9. How to Win at Kaggle\n\nJoin a competition early\n\nDownload the dataset, understand the schema of inputs and outputs\nStart writing code immediately. A good starting point is to just download the data and make a submission (of random numbers or all zeros) to make sure you have all of the mechanics in place before working on your model.\nMore time for more iteration loops\nMore time to mull the problem and think creatively\n\nRead Kaggle forums (daily)\nMake small improvements (daily)\n\nSmall tweaks compound into big results\nInitial exploration should cover as much ground as possible, so try multiple architectures rather than focusing on tuning one specific model in the early stages.\n\nFind an appropriate validation split that mirrors the leaderboard\n\nIs random sampling appropriate or does the split require more thought?\nTrain two models and submit them. The leaderboard results should reflect what you saw locally (i.e. which was the better model). If not, you might have problems with your validation split.\n\nPosts by top Kagglers will take you 80% of the way\nPapers, blogs and creativity will take you the remaining 20%\n\nWhen reading papers, you don’t need to understand every paragraph, and trying to do so would be overly time-consuming and counter-productive. Scan the paper to understand the general idea and whether it is relevant to your problem.\nBlog posts are often more accessible and quicker to implement.\n\nEnsemble your results\n\nCross-validation is a related concept and also essential."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#hardware-for-deep-learning",
    "href": "posts/ml/meta_learning/meta_learning.html#hardware-for-deep-learning",
    "title": "Meta Learning",
    "section": "10. Hardware for Deep Learning",
    "text": "10. Hardware for Deep Learning\nExplore hardware only to the extent that you find it interesting. Otherwise it’s a false economy: you are paying with your time learning about concepts that might save a few $ here and there.\nStart with a cloud VM, colab or Kaggle kernels.\nOnce you know you are serious, a home rig is the most cost effective option. Get a GPU with the most RAM you can afford. This first rig should just get you to a stage where you know what you like to work on and what the bottlenecks worth improving are: more RAM, better CPU, more storage (and how fast does the storage need to be).\nDebugging and profiling your code (use %%timeit in Jupyter) is essential."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#sharing-your-work",
    "href": "posts/ml/meta_learning/meta_learning.html#sharing-your-work",
    "title": "Meta Learning",
    "section": "11. Sharing Your Work",
    "text": "11. Sharing Your Work\nA resume is of limited use nowadays. Meet people who influence hiring decisions where they hang out: conferences, social media, meetups, blogs etc.\nCredibility is key. Helping others and showing your work builds credibility.\nIf you are looking for work, say so! On Twitter, Linkedin etc, and reach out to people you know.\nThe deep learning community is active on Twitter. But keep your time on Twitter limited and focused. Your goal isn’t to become a content creator.\nShare your work. This builds a personal brand. It also gives you a milestone to work towards and defines when it is “done”. The sooner you start sharing your work, the better. There are fewer, if any, of the negative consequences you might anticipate when sharing work online. Failure is just “cheap feedback”; embrace it!\nMost people’s biggest regret when learning ML is not enough time spent doing and too much time spent learning theory.\nThere’s one guaranteed way to fail: stop trying. Learning compounds; you need to give it time before you see exponential results.\nA general technique that works for all aspects of life: oberseve whether you are getting the desired results. If not, change your approach.\nFind a mentor. This might not necessarily be someone you know or interact with directly, or even someone who’s alive. If you follow them (on Twitter) and learn from what they have to say, they are a mentor.\nThe “permissionless apprenticeship” approach to finding a mentor means to give value first before you receive value."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#references",
    "href": "posts/ml/meta_learning/meta_learning.html#references",
    "title": "Meta Learning",
    "section": "References",
    "text": "References\n\nThe Missing Semester\nValidation sets\nPersonal brand"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter4/lesson.html#the-task",
    "href": "posts/ml/hands_on_llm/chapter4/lesson.html#the-task",
    "title": "Hands-On LLMs: Text Classification",
    "section": "1.1. The task",
    "text": "1.1. The task\nThe goal is to assign a label to some input text. This has applications in sentiment analysis, entity recognition and detecting language.\nText classification can be achieve using representation models i.e. encoder-only, or generative models i.e. decoder-only.\nThere are, of course, classical NLP approaches to this that do not rely on LLMs. For example, representing text using TF-IDF and training a logistic classifier on this.\nOne caveat to be aware of when using any pre-trained models is that we often don’t know the training data used; the weights are open-source but the training data isn’t.\nThis makes evaluation trickier, as we can’t be sure if our test data is truly out-of-sample."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter4/lesson.html#using-a-pre-trained-classifier",
    "href": "posts/ml/hands_on_llm/chapter4/lesson.html#using-a-pre-trained-classifier",
    "title": "Hands-On LLMs: Text Classification",
    "section": "2.1. Using a pre-trained classifier",
    "text": "2.1. Using a pre-trained classifier\nWe can load a pre-trained classification model from HuggingFace:\n\nfrom transformers import pipeline \n\n\nmodel_path = \"cardiffnlp/twitter-roberta-base-sentiment-latest\" \npipe = pipeline(model=model_path, tokenizer=model_path, return_all_scores=True, device=device)\n\n\n\n\n\n\n\nSome weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\nThen we use it to infer the sentiment of our test data:\n\nimport numpy as np \nfrom tqdm import tqdm \nfrom transformers.pipelines.pt_utils import KeyDataset \n\n\n# Run inference for each test instance\ny_pred = [] \nfor output in tqdm(pipe(KeyDataset(data[\"test\"], \"text\")), total=len(data[\"test\"])):\n    negative_score = output[0][\"score\"] \n    positive_score = output[2][\"score\"] \n    assignment = np.argmax([negative_score, positive_score])\n    y_pred.append(assignment)\n\n100%|██████████| 1066/1066 [00:33&lt;00:00, 32.03it/s]\n\n\nWe can then evaluate the performance by looking at classification metrics based on the confusion matrix:\n\nfrom sklearn.metrics import classification_report \n\n\nperformance = classification_report(\n    data[\"test\"][\"label\"],\n    y_pred,\n    target_names=[\"Negative Review\", \"Positive Review\"]\n)\nprint(performance)\n\n                 precision    recall  f1-score   support\n\nNegative Review       0.76      0.88      0.81       533\nPositive Review       0.86      0.72      0.78       533\n\n       accuracy                           0.80      1066\n      macro avg       0.81      0.80      0.80      1066\n   weighted avg       0.81      0.80      0.80      1066\n\n\n\nThis is a pretty good result considering we’ve just used a generic off-the-shelf model with no training and not specific to our domain! We get 80% accuracy and an F1 score of 0.81."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter4/lesson.html#using-a-pre-trained-embedding-model",
    "href": "posts/ml/hands_on_llm/chapter4/lesson.html#using-a-pre-trained-embedding-model",
    "title": "Hands-On LLMs: Text Classification",
    "section": "2.2. Using a pre-trained embedding model",
    "text": "2.2. Using a pre-trained embedding model\nThis approach is helpful when we can not find a “similar enough” classification model that has been trained on a similar task.\nWe use an embedding model to generate embedding vectors for our text data. Think of this like the feature engineering step of a classical ML classification task.\nWe can then feed these embeddings / features to train a lightweight classifier. This can be any classifier of choice; there is nothing NLP-specific to the problem at this point.\nWe can load a pre-trained embedding model and use it to generate embeddings for our training and test data. This embedding model is kept frozen.\n\nfrom sentence_transformers import SentenceTransformer \n\n\nmodel = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_embeddings = model.encode(data[\"train\"][\"text\"], show_progress_bar=True) \ntest_embeddings = model.encode(data[\"test\"][\"text\"], show_progress_bar=True)\n\n\n\n\n\n\n\nNext we can use these embeddings to train a classifier. In this case, we just use a simple logistic regression, but this can be any classifier.\n\nfrom sklearn.linear_model import LogisticRegression\n\n\nclf = LogisticRegression(random_state=42)\nclf.fit(train_embeddings, data[\"train\"][\"label\"])\n\nLogisticRegression(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression(random_state=42) \n\n\nNow we can evaluate the model as before:\n\ny_pred = clf.predict(test_embeddings)\nperformance = classification_report(data[\"test\"][\"label\"], y_pred, target_names=[\"Negative Review\", \"Positive Review\"])\n\nprint(performance)\n\n                 precision    recall  f1-score   support\n\nNegative Review       0.85      0.86      0.85       533\nPositive Review       0.86      0.85      0.85       533\n\n       accuracy                           0.85      1066\n      macro avg       0.85      0.85      0.85      1066\n   weighted avg       0.85      0.85      0.85      1066\n\n\n\nEven better! With a pre-trained embedding model and training our own lightweight classifier in a few seconds, we get improved accuracy of 85%."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter4/lesson.html#zero-shot-classification-using-and-embedding-model",
    "href": "posts/ml/hands_on_llm/chapter4/lesson.html#zero-shot-classification-using-and-embedding-model",
    "title": "Hands-On LLMs: Text Classification",
    "section": "2.3. Zero-shot classification using and embedding model",
    "text": "2.3. Zero-shot classification using and embedding model\nGetting labelled data is expensive and time-consuming.\nWe can use zero-shot classification in the absence of labels. This can be useful in cases where we want to assess the feasibility of a task as a first step to determine whether it’s worth the effort of collecting labelled data.\nWe do this by:\n\nDescribe the label, then create the embedding vector of this description. This acts as the baseline vector for the label.\nWe can then compare the embedding vector of any text, using cosine similarity, to get a measure of how well our label matches our text.\n\n\n\n\nimage.png\n\n\nWe create embedding vectors for our labels:\n\nlabel_embeddings = model.encode([\"A negative review\", \"A positive review\"])\n\nThen we can classify our text data by calculating the cosine similarity with each of our labels, and assigning the more similar label.\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\nsim_matrix = cosine_similarity(test_embeddings, label_embeddings)\ny_pred = np.argmax(sim_matrix, axis=1)\n\nAgain, the model evaluation is the same:\n\nperformance = classification_report(data[\"test\"][\"label\"], y_pred, target_names=[\"Negative Review\", \"Positive Review\"])\n\nprint(performance)\n\n                 precision    recall  f1-score   support\n\nNegative Review       0.78      0.77      0.78       533\nPositive Review       0.77      0.79      0.78       533\n\n       accuracy                           0.78      1066\n      macro avg       0.78      0.78      0.78      1066\n   weighted avg       0.78      0.78      0.78      1066\n\n\n\nThe accuracy is 78%. Pretty good considering we have no labelled data and did no training!"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter2/lesson.html#tokenization-in-action",
    "href": "posts/ml/hands_on_llm/chapter2/lesson.html#tokenization-in-action",
    "title": "Hands-On LLMs: Tokens and Embeddings",
    "section": "1.1. Tokenization in Action",
    "text": "1.1. Tokenization in Action\nWe can load a model to see what tokenization looks like practice.\nWe’ll load a smaller open-source model and its corresponding tokenizer.\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nif torch.cuda.is_available():\n    device = \"cuda\"\n    device_map = \"cuda\"\nelif torch.backends.mps.is_available():\n    device = \"mps\"\n    device_map=None\nelse:\n    device = \"cpu\"\n    device_map = \"cpu\"\n\n\n1.1.1. Load the model\nThis can take a few minutes depedning on internet connection.\n\nmodel_name = \"microsoft/Phi-3-mini-4k-instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=device_map,\n    torch_dtype=\"auto\",\n    trust_remote_code=True\n)\nmodel.to(device)\n\n\n\n\n\n\n\nA new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n- configuration_phi3.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n- modeling_phi3.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\nCurrent `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPhi3ForCausalLM(\n  (model): Phi3Model(\n    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n    (embed_dropout): Dropout(p=0.0, inplace=False)\n    (layers): ModuleList(\n      (0-31): 32 x Phi3DecoderLayer(\n        (self_attn): Phi3Attention(\n          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n          (rotary_emb): Phi3RotaryEmbedding()\n        )\n        (mlp): Phi3MLP(\n          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n          (activation_fn): SiLU()\n        )\n        (input_layernorm): Phi3RMSNorm()\n        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n        (post_attention_layernorm): Phi3RMSNorm()\n      )\n    )\n    (norm): Phi3RMSNorm()\n  )\n  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n)\n\n\n\n\n1.1.2. Load the tokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\n\n\n\n1.1.3. Use the model to generate text\nFirst we tokenize the input promt. Then we pass this to the model. We can peek in at each step to see what’s actually being passed around.\nWe’ll start with the following input prompt:\n\ninput_prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.&lt;|assistant|&gt;\"\n\nThe tokenizer converts this text to a list of integers. These are the input IDs that are passed to the model.\n\n# Tokenize the input prompt \ninput_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids.to(device)\n\n\nprint(input_ids)\n\ntensor([[14350,   385,  4876, 27746,  5281,   304, 19235,   363,   278, 25305,\n           293, 16423,   292,   286,   728,   481, 29889, 12027,  7420,   920,\n           372,  9559, 29889, 32001]], device='mps:0')\n\n\nWe can “decode” these input IDs, converting them back to text, to see how the tokenizer splits the text. It uses sub-word tokens, so mishap is split into m, ish, ap. Punctuation is its own token and there is a special token for &lt;|assistant|&gt; Spaces are implicit; parital tokens have a special hidden character preceding them and tokens without that character are assumed to have a space before them.\n\nfor id in input_ids[0]: \n    print(tokenizer.decode(id))\n\nWrite\nan\nemail\napolog\nizing\nto\nSarah\nfor\nthe\ntrag\nic\ngarden\ning\nm\nish\nap\n.\nExp\nlain\nhow\nit\nhappened\n.\n&lt;|assistant|&gt;\n\n\nWe can now pass this tokenized input to the model to generate new tokens.\n\n# Due to a quirk of Macs, we need to explicitly pass it an attention mask as it cannot be inferred\nif device == 'mps':\n    model_kwargs = {'attention_mask': (input_ids != tokenizer.pad_token_id).long()}\nelse:\n    model_kwargs = {}\n\n# Generate the text \ngeneration_output = model.generate(input_ids=input_ids, max_new_tokens=100, **model_kwargs)\n\n\nThe output of the generation appends tokens to the input.\n\ngeneration_output\n\ntensor([[14350,   385,  4876, 27746,  5281,   304, 19235,   363,   278, 25305,\n           293, 16423,   292,   286,   728,   481, 29889, 12027,  7420,   920,\n           372,  9559, 29889, 32001,  3323,   622, 29901, 17778, 29888,  2152,\n          6225, 11763,   363,   278, 19906,   292,   341,   728,   481,    13,\n            13,    13, 29928,   799, 19235, 29892,    13,    13,    13, 29902,\n          4966,   445,  2643, 14061,   366,  1532, 29889,   306,   626,  5007,\n           304,  4653,   590,  6483,   342,  3095, 11763,   363,   278,   443,\n          6477,   403, 15134,   393, 10761,   297,   596, 16423, 22600, 29889,\n            13,    13,    13,  2887,   366,  1073, 29892,   306,   505,  2337,\n          7336,  2859,   278, 15409,   322, 22024,   339,  1793,   310,   596,\n         16423, 29889,   739,   756,  1063,   263,  2752,   310,  8681, 12232,\n           363,   592, 29892,   322,   306,   471,  1468, 24455,   304,   505,\n           278, 15130,   304,  1371]], device='mps:0')\n\n\nAgain, we can decode this to see the output text\n\n# Print the output \nprint(tokenizer.decode(generation_output[0]))\n\nWrite an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.&lt;|assistant|&gt; Subject: Heartfelt Apologies for the Gardening Mishap\n\n\nDear Sarah,\n\n\nI hope this message finds you well. I am writing to express my deepest apologies for the unfortunate incident that occurred in your garden yesterday.\n\n\nAs you know, I have always admired the beauty and tranquility of your garden. It has been a source of inspiration for me, and I was thrilled to have the opportunity to help"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter2/lesson.html#tokenizer-design",
    "href": "posts/ml/hands_on_llm/chapter2/lesson.html#tokenizer-design",
    "title": "Hands-On LLMs: Tokens and Embeddings",
    "section": "1.2. Tokenizer Design",
    "text": "1.2. Tokenizer Design\nThere are three primary decisions that determine how the tokenizer splits a given prompt:\n\nTokenization method: byte pair encoding (BPE), WordPiece\nTokenizer parameters: vocabulary size, choice of special tokens\nTraining data set: a tokenizer trained on English text will give different results to one trained on Punjabi text or Python code, etc.\n\nTokenizers are used on the input (to encode text -&gt; numbers) and on the output (to decode numbers -&gt; text).\n\n1.2.1 Tokenization Methods\nThere are four promininent tokenization schemes:\n\nWord tokens.\n\nPros: Simple to implement and understand; can fit more text in a given context window\nCons: Unable to handle unseen words; vocab has lots of tokens for almost identical words (e.g. write, writing, written, wrote)\n\nSub-word tokens.\n\nPros: Can represent new words by breaking down into other known tokens\nCons: Choice of partial words dictionary requires careful design\n\nCharacter tokens.\n\nPros: Can represent any new word\nCons: Modeling is more difficult; can’t fit as much text in a context window\n\nByte tokens. Breaks tokens down into the individual unicode character bytes. This is also called “tokenization-free representation”.\n\nPros: Can represent text of different alphabets, useful for multilingual models\n\n\nSome tokenizers employ a hybrid approach. For example, GPT-2 uses sub-word tokenization and falls back to byte tokens for other characters.\nParticular cases of interest that distinguish tokenization (and model) performance are the way the tokenizer handles:\n\nCapitalization\nOther languages\nEmojis\nCode - keywords and whitespace. Some models have different tokens for one space, two spaces, three spaces, four spaces etc.\nNumbers and digits - does it encode each digit as a separate number or as a whole? E.g. 420 vs 4,2,0. Separate seems to perform maths better.\nSpecial tokens - beginning of text, end of text, user/system/assistant tags, separator token used to separate two text inputs in similarity models.\n\n\n\n1.2.2. Tokenizer Parameters\nThe LLM designer makes decisions about the paramters of the tokenizer:\n\nVocabulary size: \\(\\approx 50k\\) is typical currently\nSpecial tokens: Particular use cases may warrant special tokens, e.g. coding, research citations, etc\nCapitalisation: Treat upper case and lower case as separate tokens? Or convert all to lower case?\nTraining data domain"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter2/lesson.html#sentencedocument-embeddings",
    "href": "posts/ml/hands_on_llm/chapter2/lesson.html#sentencedocument-embeddings",
    "title": "Hands-On LLMs: Tokens and Embeddings",
    "section": "2.2. Sentence/Document Embeddings",
    "text": "2.2. Sentence/Document Embeddings\nSome models operate on sentences or entire documents.\nA simple approach is to take the word embeddings for each word in the document, then average them. Some LLMs produce “text embeddings” which represent the whole text as an embedding vector directly.\n\nfrom sentence_transformers import SentenceTransformer \n\n# Load model \nmodel = SentenceTransformer(\"sentence-transformers/all-mpnetbase-v2\") \n\n# Convert text to text embeddings \nvector = model.encode(\"Best movie ever!\")"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter2/lesson.html#non-llm-based-embeddings",
    "href": "posts/ml/hands_on_llm/chapter2/lesson.html#non-llm-based-embeddings",
    "title": "Hands-On LLMs: Tokens and Embeddings",
    "section": "2.3. Non-LLM-based Embeddings",
    "text": "2.3. Non-LLM-based Embeddings\nEmbeddings are useful in NLP more generally, and some techniques, such as Word2Vec and GloVe, predate LLMs.\nThese can be useful to apply NLP to non-text applications, such as music recommendations.\nSay we have a data set of songs belonging to playlists. This can help us learn which songs are similar, because similar songs are likely to be neighbouring on playlists, just as similar words are likely to be neighbouring in a sentence.\nSo we can convert each song to an ID, and treat a playlist like a sentence, i.e. it is just a sequence of tokens. Then we can train a Word2Vec model on it to get embedding vectors for each song.\nThen, if we have a song we like, we can look at its embedding vector and find similar songs by finding the songs with the closest embeddings."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter7/lesson.html",
    "href": "posts/ml/hands_on_llm/chapter7/lesson.html",
    "title": "Hands-On LLMs: Advanced Text Generation Techniques and Tools",
    "section": "",
    "text": "Going beyond prompt engineering, there are several areas where we can improve the quality of generated text:\n\nModel I/O\nMemory\nAgents\nChains\n\nLangchain is a framework that provides useful abstractions for these kinds of things and helps connect them together. We will use LangChain here, but alternatives include LlamaIndex, DSPy and Haystack.\n\n\n\nLangChain example"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter7/lesson.html#quantization",
    "href": "posts/ml/hands_on_llm/chapter7/lesson.html#quantization",
    "title": "Hands-On LLMs: Advanced Text Generation Techniques and Tools",
    "section": "1.1. Quantization",
    "text": "1.1. Quantization\nWe can load quantized models using the GGUF file format which is a binary format optimised for fast loading of pytorch models.\nThe benefit of a quantized model is a smaller size in memory while retaining most of the original information. For example, if the model was trained using 32-bit floats for parameters, we can use 16-bit floats instead. This reduces the memory requirement but also reduces the precision. Often this trade-off is worthwhile.\nThis page goes into detail on the mechanics of quantisation.\nThe “best” model is constantly changing, so we can refer to the Open LLM leaderboard.\nWe can download a 16-bit quantized version of the Phi-3 mini model from HuggingFace.\n\n!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instructgguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\n\nThen we can use LangChain to load the GGUF file.\nNote: an alternative is to use the langchain_huggingface library.\n\n# This cell *should* work, but due to some funkiness with incompatible langchain vs llama versions (I think)\n#  it's easier to just create a custom LangChain wrapper in the following cell.\nfrom langchain import LlamaCpp\n\n# Make sure model_path points at the file location of the GGUF file\nMODEL_DIR = \"/Users/gurpreetjohl/workspace/python/ml-practice/ml-practice/models/\"\nMODEL_NAME = \"Phi-3-mini-4k-instruct-fp16.gguf\"\nmodel_path = MODEL_DIR+MODEL_NAME\n\nllm = LlamaCpp(\n    model_path=MODEL_DIR+MODEL_NAME,\n    n_gpu_layers=-1,\n    max_tokens=500,\n    n_ctx=2048,\n    seed=42,\n    verbose=False\n)\n\n/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/langchain_community/llms/__init__.py:312: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n\nFor example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\nwith: `from pydantic import BaseModel`\nor the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet.     from pydantic.v1 import BaseModel\n\n  from langchain_community.llms.llamacpp import LlamaCpp\n\n\nValidationError: 1 validation error for LlamaCpp\nclient\n  Field required [type=missing, input_value={'model_path': '/Users/gu...': 42, 'verbose': False}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\n\n\n\nfrom typing import Any, Dict, List, Optional\nfrom langchain_core.language_models import LLM\nfrom llama_cpp import Llama\n\nclass CustomLlamaLLM(LLM):\n    model_path: str\n    n_gpu_layers: int = -1\n    max_tokens: int = 500\n    n_ctx: int = 2048\n    seed: Optional[int] = 42\n    verbose: bool = False\n    client: Any = None\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.client = Llama(\n            model_path=self.model_path,\n            n_gpu_layers=self.n_gpu_layers,\n            max_tokens=self.max_tokens,\n            n_ctx=self.n_ctx,\n            seed=self.seed,\n            verbose=self.verbose\n        )\n\n    @property\n    def _llm_type(self) -&gt; str:\n        return \"CustomLlama\"\n\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -&gt; str:\n        response = self.client(prompt, stop=stop)\n        return response[\"choices\"][0][\"text\"]\n\n\n# Instantiate the custom LLM class\nllm = CustomLlamaLLM(\n    model_path=model_path,\n    n_gpu_layers=-1,\n    max_tokens=500,\n    n_ctx=2048,\n    seed=42,\n    verbose=True\n)\n\nllama_model_loader: loaded meta data with 23 key-value pairs and 195 tensors from /Users/gurpreetjohl/workspace/python/ml-practice/ml-practice/models/Phi-3-mini-4k-instruct-fp16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = phi3\nllama_model_loader: - kv   1:                               general.name str              = Phi3\nllama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\nllama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\nllama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\nllama_model_loader: - kv   5:                           phi3.block_count u32              = 32\nllama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\nllama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\nllama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\nllama_model_loader: - kv  10:                          general.file_type u32              = 1\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"&lt;unk&gt;\", \"&lt;s&gt;\", \"&lt;/s&gt;\", \"&lt;0x00&gt;\", \"&lt;...\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\nllama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  130 tensors\nllm_load_vocab: special tokens cache size = 323\nllm_load_vocab: token to piece cache size = 0.1687 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = phi3\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32064\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 3072\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_rot            = 96\nllm_load_print_meta: n_embd_head_k    = 96\nllm_load_print_meta: n_embd_head_v    = 96\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 3072\nllm_load_print_meta: n_embd_v_gqa     = 3072\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 8192\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 2\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: model type       = 3B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 3.82 B\nllm_load_print_meta: model size       = 7.12 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Phi3\nllm_load_print_meta: BOS token        = 1 '&lt;s&gt;'\nllm_load_print_meta: EOS token        = 32000 '&lt;|endoftext|&gt;'\nllm_load_print_meta: UNK token        = 0 '&lt;unk&gt;'\nllm_load_print_meta: PAD token        = 32000 '&lt;|endoftext|&gt;'\nllm_load_print_meta: LF token         = 13 '&lt;0x0A&gt;'\nllm_load_print_meta: EOT token        = 32007 '&lt;|end|&gt;'\nllm_load_tensors: ggml ctx size =    0.22 MiB\nggml_backend_metal_log_allocated_size: allocated buffer, size =  7100.64 MiB, (23176.83 / 10922.67)ggml_backend_metal_log_allocated_size: warning: current allocated size is greater than the recommended max working set size\nllm_load_tensors: offloading 32 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloaded 33/33 layers to GPU\nllm_load_tensors:        CPU buffer size =   187.88 MiB\nllm_load_tensors:      Metal buffer size =  7100.64 MiB\n....................................................................................\nllama_new_context_with_model: n_ctx      = 2048\nllama_new_context_with_model: n_batch    = 512\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nggml_metal_init: allocating\nggml_metal_init: found device: Apple M2\nggml_metal_init: picking default device: Apple M2\nggml_metal_init: using embedded metal library\nggml_metal_init: GPU name:   Apple M2\nggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\nggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\nggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\nggml_metal_init: simdgroup reduction support   = true\nggml_metal_init: simdgroup matrix mul. support = true\nggml_metal_init: hasUnifiedMemory              = true\nggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\nllama_kv_cache_init:      Metal KV buffer size =   768.00 MiB\nllama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB\nllama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\nllama_new_context_with_model:      Metal compute buffer size =   168.00 MiB\nllama_new_context_with_model:        CPU compute buffer size =    10.01 MiB\nllama_new_context_with_model: graph nodes  = 1286\nllama_new_context_with_model: graph splits = 2\nAVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \nModel metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'&lt;|user|&gt;' + '\\n' + message['content'] + '&lt;|end|&gt;' + '\\n' + '&lt;|assistant|&gt;' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '&lt;|end|&gt;' + '\\n'}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.pre': 'default', 'general.file_type': '1', 'phi3.rope.dimension_count': '96', 'tokenizer.ggml.bos_token_id': '1', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.attention.head_count_kv': '32', 'phi3.attention.head_count': '32', 'tokenizer.ggml.model': 'llama', 'phi3.block_count': '32', 'general.architecture': 'phi3', 'phi3.feed_forward_length': '8192', 'phi3.embedding_length': '3072', 'general.name': 'Phi3', 'phi3.context_length': '4096'}\nAvailable chat formats from metadata: chat_template.default\nUsing gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'&lt;|user|&gt;' + '\n' + message['content'] + '&lt;|end|&gt;' + '\n' + '&lt;|assistant|&gt;' + '\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '&lt;|end|&gt;' + '\n'}}{% endif %}{% endfor %}\nUsing chat eos_token: &lt;|endoftext|&gt;\nUsing chat bos_token: &lt;s&gt;\nggml_metal_free: deallocating"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter7/lesson.html#basic-prompt-chain",
    "href": "posts/ml/hands_on_llm/chapter7/lesson.html#basic-prompt-chain",
    "title": "Hands-On LLMs: Advanced Text Generation Techniques and Tools",
    "section": "2.1. Basic Prompt Chain",
    "text": "2.1. Basic Prompt Chain\nIn LangChain, we use the invoke function to generate an output.\nHowever, each model requires a specific prompt template. If we blindly called invoke on our model, we get no response:\n\nllm.invoke(\"Hi! My name is Gurp. What is 1 + 1?\")\n\n\nllama_print_timings:        load time =     840.73 ms\nllama_print_timings:      sample time =       0.89 ms /     1 runs   (    0.89 ms per token,  1123.60 tokens per second)\nllama_print_timings: prompt eval time =     840.28 ms /    18 tokens (   46.68 ms per token,    21.42 tokens per second)\nllama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_print_timings:       total time =     841.04 ms /    19 tokens\n\n\n''\n\n\nThis is where the LangChain abstractions come in useful.\nWe will create a simple chain with a single link:\n\n\n\n\n\nflowchart LR\n\n  subgraph PromptChain\n    B(Prompt template) --&gt; C[LLM]\n  end\n\n  A(User prompt) --&gt; B\n  C --&gt; D(Output)\n\n\n\n\n\n\nFor our particular case, Phi-3 prompts require start, end , user and assistant tokens.\n\nfrom langchain import PromptTemplate \n\n# Create a prompt template with the \"input_prompt\" variable \ntemplate = \"\"\"&lt;s&gt;&lt;|user|&gt; {input_prompt}&lt;|end|&gt; &lt;|assistant|&gt;\"\"\" \nprompt = PromptTemplate(template=template, input_variables=[\"input_prompt\"])\n\nWe can then create a chain by chaining the prompt and LLM together. Then we can call invoke and get the intended text generation.\n\nbasic_chain = prompt | llm\nbasic_chain.invoke({\"input_prompt\": \"Hi! My name is Gurp. What is 1 + 1?\"})\n\nllama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading \"&lt;s&gt;\" in prompt, this will likely reduce response quality, consider removing it...\n  inputs = [input]\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     840.73 ms\nllama_print_timings:      sample time =       5.89 ms /    16 runs   (    0.37 ms per token,  2716.93 tokens per second)\nllama_print_timings: prompt eval time =     295.30 ms /    21 tokens (   14.06 ms per token,    71.11 tokens per second)\nllama_print_timings:        eval time =    1349.54 ms /    15 runs   (   89.97 ms per token,    11.11 tokens per second)\nllama_print_timings:       total time =    1668.23 ms /    36 tokens\n\n\n' Hello Gurp! The answer to 1 + 1 is 2'\n\n\nNote that we just passed the entire input_prompt as a variable to the chain, but we can define other variables. For example, if we wanted a more specialised use case where we don’t give te user as much flexibility, we can pre-define some of the prompt.\ntemplate = \"Create a funny name for a business that sells {product}.\" \nname_prompt = PromptTemplate(template=template, input_variables=[\"product\"])"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter7/lesson.html#multiple-prompt-chain",
    "href": "posts/ml/hands_on_llm/chapter7/lesson.html#multiple-prompt-chain",
    "title": "Hands-On LLMs: Advanced Text Generation Techniques and Tools",
    "section": "2.2. Multiple Prompt Chain",
    "text": "2.2. Multiple Prompt Chain\nIf we have more complex prompts or use cases, we can split the task into smaller subtasks that run sequentially. Each link in the chain deals with a specific subtask.\n\n\n\n\n\nflowchart LR\n\n  subgraph MultiPromptChain\n    B1(Prompt 1) --&gt; B2(Prompt 2)\n    B1 &lt;--&gt; C[LLM]\n    B2 &lt;--&gt; C\n  end\n\n  A(User input) --&gt; B1\n\n  B2 --&gt; D(Output)\n\n\n\n\n\n\nAs an example, we can prompt the LLM to create a story. First we ask it for a title based on the user prompt, then characters based on the title, then a story based on the characters and title. The first link is the only one that requires user input.\n\n\n\nStory prompt chain\n\n\n\nfrom langchain import LLMChain\n\n# Create a chain for the title of our story \ntitle_template = \"\"\"&lt;s&gt;&lt;|user|&gt; Create a title for a story about {summary}. Only return the title. &lt;|end|&gt; &lt;|assistant|&gt;\"\"\" \ntitle_prompt = PromptTemplate(template=title_template, input_variables= [\"summary\"]) \ntitle = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")\n\n# Create a chain for the character description using the summary and title \ncharacter_template = \"\"\"&lt;s&gt;&lt;|user|&gt; Describe the main character of a story about {summary} with the title {title}. Use only two sentences.&lt;|end|&gt; &lt;|assistant|&gt;\"\"\" \ncharacter_prompt = PromptTemplate(template=character_template, input_variables=[\"summary\", \"title\"]) \ncharacter = LLMChain(llm=llm, prompt=character_prompt, output_key=\"character\")\n\n# Create a chain for the story using the summary, title, and character description \nstory_template = \"\"\"&lt;s&gt;&lt;|user|&gt; Create a story about {summary} with the title {title}. The main character is: {character}. Only return the story and it cannot be longer than one paragraph. &lt;|end|&gt; &lt;|assistant|&gt;\"\"\" \nstory_prompt = PromptTemplate(template=story_template, input_variables=[\"summary\", \"title\", \"character\"]) \nstory = LLMChain(llm=llm, prompt=story_prompt, output_key=\"story\")\n\n\n# Combine all three components to create the full chain \nllm_chain = title | character | story\n\nggml_metal_free: deallocating\n\n\nNow we can invoke the chain just like before:\n\nllm_chain.invoke({\"summary\": \"a dog that can smell danger\"})\n\nllama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading \"&lt;s&gt;\" in prompt, this will likely reduce response quality, consider removing it...\n  inputs = [input]\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     840.73 ms\nllama_print_timings:      sample time =       1.34 ms /    16 runs   (    0.08 ms per token, 11958.15 tokens per second)\nllama_print_timings: prompt eval time =     295.27 ms /    23 tokens (   12.84 ms per token,    77.89 tokens per second)\nllama_print_timings:        eval time =    1424.98 ms /    15 runs   (   95.00 ms per token,    10.53 tokens per second)\nllama_print_timings:       total time =    1725.28 ms /    38 tokens\nllama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading \"&lt;s&gt;\" in prompt, this will likely reduce response quality, consider removing it...\n  inputs = [input]\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     840.73 ms\nllama_print_timings:      sample time =       1.97 ms /    16 runs   (    0.12 ms per token,  8109.48 tokens per second)\nllama_print_timings: prompt eval time =     190.41 ms /    44 tokens (    4.33 ms per token,   231.08 tokens per second)\nllama_print_timings:        eval time =    1315.12 ms /    15 runs   (   87.67 ms per token,    11.41 tokens per second)\nllama_print_timings:       total time =    1511.43 ms /    59 tokens\nllama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading \"&lt;s&gt;\" in prompt, this will likely reduce response quality, consider removing it...\n  inputs = [input]\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     840.73 ms\nllama_print_timings:      sample time =       1.48 ms /    16 runs   (    0.09 ms per token, 10803.51 tokens per second)\nllama_print_timings: prompt eval time =     290.88 ms /    70 tokens (    4.16 ms per token,   240.65 tokens per second)\nllama_print_timings:        eval time =    1480.88 ms /    15 runs   (   98.73 ms per token,    10.13 tokens per second)\nllama_print_timings:       total time =    1777.98 ms /    85 tokens\n\n\n{'summary': 'a dog that can smell danger',\n 'title': ' \"Scent of Peril: The Canine Detective\\'s N',\n 'character': ' In the heartwarming tale, Scent of Peril follows Max,',\n 'story': ' In a quaint suburban neighborhood, there lived an extraordinary German Shepherd'}"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter7/lesson.html#conversation-buffer",
    "href": "posts/ml/hands_on_llm/chapter7/lesson.html#conversation-buffer",
    "title": "Hands-On LLMs: Advanced Text Generation Techniques and Tools",
    "section": "3.1. Conversation Buffer",
    "text": "3.1. Conversation Buffer\nThe simplest way we can force the LLM to remember previous conversation is by passing the full conversation history in our prompt.\nThis approach is called conversation buffer memory. We update our prompt with the history of the chat.\n\n3.1.1. Simple Conversation Buffer\n\n# Create an updated prompt template to include a chat history \ntemplate = \"\"\"&lt;s&gt;&lt;|user|&gt;Current conversation:{chat_history} {input_prompt}&lt;|end|&gt; &lt;|assistant|&gt;\"\"\" \nprompt = PromptTemplate(template=template, input_variables=[\"input_prompt\", \"chat_history\"])\n\nNext, we create a ConversationBufferMemory link in the chain that will store the conversations we have previously had with the LLM.\n\nfrom langchain.memory import ConversationBufferMemory\n\n# Define the type of memory we will use \nmemory = ConversationBufferMemory(memory_key=\"chat_history\") \n\n# Chain the LLM, prompt, and memory together \nllm_chain = LLMChain(prompt=prompt, llm=llm, memory=memory)\n\nWe can verify if this has worked by seeing if it now remembers our name in later prompts:\n\n# Generate a conversation and ask a basic question \nllm_chain.invoke({\"input_prompt\": \"Hi! My name is Gurp. What is 1 + 1?\"})\n\nllama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading \"&lt;s&gt;\" in prompt, this will likely reduce response quality, consider removing it...\n  inputs = [input]\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     840.73 ms\nllama_print_timings:      sample time =       5.11 ms /    16 runs   (    0.32 ms per token,  3129.89 tokens per second)\nllama_print_timings: prompt eval time =     262.37 ms /    19 tokens (   13.81 ms per token,    72.42 tokens per second)\nllama_print_timings:        eval time =    1334.30 ms /    15 runs   (   88.95 ms per token,    11.24 tokens per second)\nllama_print_timings:       total time =    1608.21 ms /    34 tokens\n\n\n{'input_prompt': 'Hi! My name is Gurp. What is 1 + 1?',\n 'chat_history': '',\n 'text': ' Hello, Gurp! The answer to 1 + 1 is '}\n\n\n\n# Does the LLM remember my name? \nresponse = llm_chain.invoke({\"input_prompt\": \"What is my name?\"})\nprint(response)\n\nllama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading \"&lt;s&gt;\" in prompt, this will likely reduce response quality, consider removing it...\n  inputs = [input]\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     840.73 ms\nllama_print_timings:      sample time =       1.28 ms /    16 runs   (    0.08 ms per token, 12539.18 tokens per second)\nllama_print_timings: prompt eval time =     344.80 ms /    47 tokens (    7.34 ms per token,   136.31 tokens per second)\nllama_print_timings:        eval time =    1425.89 ms /    15 runs   (   95.06 ms per token,    10.52 tokens per second)\nllama_print_timings:       total time =    1775.85 ms /    62 tokens\n\n\n{'input_prompt': 'What is my name?', 'chat_history': 'Human: Hi! My name is Gurp. What is 1 + 1?\\nAI:  Hello, Gurp! The answer to 1 + 1 is ', 'text': ' Hi Gurp! The answer to 1 + 1 is 2'}\n\n\n\nllm_chain.invoke({\"input_prompt\": \"What is my name?\"})\n\nllama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading \"&lt;s&gt;\" in prompt, this will likely reduce response quality, consider removing it...\n  inputs = [input]\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     840.73 ms\nllama_print_timings:      sample time =       6.22 ms /    16 runs   (    0.39 ms per token,  2571.93 tokens per second)\nllama_print_timings: prompt eval time =     346.93 ms /    36 tokens (    9.64 ms per token,   103.77 tokens per second)\nllama_print_timings:        eval time =    1359.76 ms /    15 runs   (   90.65 ms per token,    11.03 tokens per second)\nllama_print_timings:       total time =    1721.41 ms /    51 tokens\n\n\n{'input_prompt': 'What is my name?',\n 'chat_history': 'Human: Hi! My name is Gurp. What is 1 + 1?\\nAI:  Hello, Gurp! The answer to 1 + 1 is \\nHuman: What is my name?\\nAI:  Hi Gurp! The answer to 1 + 1 is 2',\n 'text': \" Hi Gurp! You're asking your own name; I am an\"}\n\n\n\n\n3.1.2 Windowed Conversation Buffer\nAs the conversation goes on, the size of the chat history grows until eventually it may exceed the token limit.\nOne approach to work around this is to only hold the last \\(k\\) conversations in memory rather than the entire history.\n\nfrom langchain.memory import ConversationBufferWindowMemory \n\n# Retain only the last 2 conversations in memory \nmemory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\") \n\n# Chain the LLM, prompt, and memory together \nllm_chain = LLMChain(prompt=prompt, llm=llm, memory=memory)\n\n/var/folders/8k/8jqhnfbd1t99blb07r1hs5440000gn/T/ipykernel_47761/3936398832.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n  memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n\n\nThis approach is not ideal for longer conversations. An alternative is to summarise the chat history to fit in the token limit, rather than truncating it."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter7/lesson.html#conversation-summary",
    "href": "posts/ml/hands_on_llm/chapter7/lesson.html#conversation-summary",
    "title": "Hands-On LLMs: Advanced Text Generation Techniques and Tools",
    "section": "3.2. Conversation Summary",
    "text": "3.2. Conversation Summary\nThis approach uses an LLM to summarise the main points of the history so far to reduce the number of tokens required to pass to the main LLM.\nThe “summary LLM” can be a different model to our “main LLM”. We may want to use a smaller LLM for the “easier” task of summarisation to speed up computation.\nThere will now be two LLM calls per invocation: the user prompt and the summarisation prompt.\n\n\n\nConversation summary diagram\n\n\n\n# Create a summary prompt template \nsummary_prompt_template = \"\"\"\n    &lt;s&gt;&lt;|user|&gt;Summarize the conversations and update with the new lines. \n    Current summary: {summary} new lines of conversation: {new_lines} \n    New summary:&lt;|end|&gt; &lt;|assistant|&gt;\n\"\"\"\nsummary_prompt = PromptTemplate(\n    input_variables=[\"new_lines\", \"summary\"],\n    template=summary_prompt_template\n)\n\nIn this example, we’ll pass both calls to the same LLM, but in general we don’t have to.\n\nfrom langchain.memory import ConversationSummaryMemory \n\n# Define the type of memory we will use \nmemory = ConversationSummaryMemory(\n    llm=llm,\n    memory_key=\"chat_history\",\n    prompt=summary_prompt\n) \n\n# Chain the LLM, prompt, and memory together \nllm_chain = LLMChain(prompt=prompt, llm=llm, memory=memory)\n\n/var/folders/8k/8jqhnfbd1t99blb07r1hs5440000gn/T/ipykernel_47761/178162952.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n  memory = ConversationSummaryMemory(\n\n\nWe can try this out by having a short conversation with the LLM and checking it has retained previous information.\n\n# Generate a conversation and ask for the name \nllm_chain.invoke({\"input_prompt\": \"Hi! My name is Gurp. What is 1 + 1?\"}) \nllm_chain.invoke({\"input_prompt\": \"What is my name?\"})\n\nllama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading \"&lt;s&gt;\" in prompt, this will likely reduce response quality, consider removing it...\n  inputs = [input]\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     840.73 ms\nllama_print_timings:      sample time =       1.33 ms /    16 runs   (    0.08 ms per token, 12066.37 tokens per second)\nllama_print_timings: prompt eval time =     273.75 ms /    19 tokens (   14.41 ms per token,    69.41 tokens per second)\nllama_print_timings:        eval time =    1438.78 ms /    15 runs   (   95.92 ms per token,    10.43 tokens per second)\nllama_print_timings:       total time =    1717.89 ms /    34 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     840.73 ms\nllama_print_timings:      sample time =       5.37 ms /    16 runs   (    0.34 ms per token,  2977.85 tokens per second)\nllama_print_timings: prompt eval time =     291.94 ms /    77 tokens (    3.79 ms per token,   263.75 tokens per second)\nllama_print_timings:        eval time =    1356.44 ms /    15 runs   (   90.43 ms per token,    11.06 tokens per second)\nllama_print_timings:       total time =    1661.81 ms /    92 tokens\nllama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading \"&lt;s&gt;\" in prompt, this will likely reduce response quality, consider removing it...\n  inputs = [input]\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     840.73 ms\nllama_print_timings:      sample time =       1.40 ms /    16 runs   (    0.09 ms per token, 11428.57 tokens per second)\nllama_print_timings: prompt eval time =     133.14 ms /    28 tokens (    4.76 ms per token,   210.30 tokens per second)\nllama_print_timings:        eval time =    1437.07 ms /    15 runs   (   95.80 ms per token,    10.44 tokens per second)\nllama_print_timings:       total time =    1575.77 ms /    43 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     840.73 ms\nllama_print_timings:      sample time =       4.67 ms /    16 runs   (    0.29 ms per token,  3427.59 tokens per second)\nllama_print_timings: prompt eval time =     294.58 ms /    81 tokens (    3.64 ms per token,   274.96 tokens per second)\nllama_print_timings:        eval time =    1346.13 ms /    15 runs   (   89.74 ms per token,    11.14 tokens per second)\nllama_print_timings:       total time =    1654.38 ms /    96 tokens\n\n\n{'input_prompt': 'What is my name?',\n 'chat_history': ' Gurp initiated a conversation by introducing himself and asking for the sum',\n 'text': ' It seems there may have been a misunderstanding. In our current conversation, you'}\n\n\n\n# Check whether it has summarized everything thus far \nllm_chain.invoke({\"input_prompt\": \"What was the first question I asked?\"})\n\nllama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading \"&lt;s&gt;\" in prompt, this will likely reduce response quality, consider removing it...\n  inputs = [input]\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     840.73 ms\nllama_print_timings:      sample time =       3.82 ms /    16 runs   (    0.24 ms per token,  4187.39 tokens per second)\nllama_print_timings: prompt eval time =     274.15 ms /    31 tokens (    8.84 ms per token,   113.08 tokens per second)\nllama_print_timings:        eval time =    1342.18 ms /    15 runs   (   89.48 ms per token,    11.18 tokens per second)\nllama_print_timings:       total time =    1625.96 ms /    46 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     840.73 ms\nllama_print_timings:      sample time =       1.95 ms /    16 runs   (    0.12 ms per token,  8200.92 tokens per second)\nllama_print_timings: prompt eval time =     300.16 ms /    84 tokens (    3.57 ms per token,   279.85 tokens per second)\nllama_print_timings:        eval time =    1455.62 ms /    15 runs   (   97.04 ms per token,    10.30 tokens per second)\nllama_print_timings:       total time =    1761.25 ms /    99 tokens\n\n\n{'input_prompt': 'What was the first question I asked?',\n 'chat_history': ' Gurp introduced himself to the human and inquired about their name. The',\n 'text': ' The first question you asked could be, \"Nice to meet you, G'}\n\n\n\n# Check what the summary is thus far \nmemory.load_memory_variables({})\n\n{'chat_history': ' Gurp introduced himself to the human and inquired about their name, while'}\n\n\nThe conversation summary approach reduces the tokens required, but it does risk losing information depending on the quality of the summary."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter7/lesson.html#comparison-of-memory-approaches",
    "href": "posts/ml/hands_on_llm/chapter7/lesson.html#comparison-of-memory-approaches",
    "title": "Hands-On LLMs: Advanced Text Generation Techniques and Tools",
    "section": "3.3. Comparison of Memory Approaches",
    "text": "3.3. Comparison of Memory Approaches\nConversation buffer\nPros:\n\nEasiest to implement\nEnsures no loss of info (as long as conversation fits in congtext window)\n\nCons:\n\nSlower generation (more tokens needed)\nOnly suitable for LLMs with large context windows\nHandles larger chat histories poorly\n\nWindowed conversation buffer\nPros:\n\nCan use LLMs with smaller context windows\nGood for shorter chats; no information loss over the last k interactions\n\nCons:\n\nOnly captures k interactions\nNo compression, so can still require a large context window if k is large\n\nConversation summary\nPros:\n\nCaptures full history\nEnables long chats\nReduces required tokens\n\nCons:\n\nRequires an additional LLM call per interaction\nQuality of response depends on LLM’s summarisation quality"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter7/lesson.html#react",
    "href": "posts/ml/hands_on_llm/chapter7/lesson.html#react",
    "title": "Hands-On LLMs: Advanced Text Generation Techniques and Tools",
    "section": "4.1. ReAct",
    "text": "4.1. ReAct\nMany agent-based systems rely on the ReAct framework, which standard for Reasoning and Acting.\nWe can give the LLM the ability to use tools, but it can only generate text, so it needs to generate the right text to interact with tools. For example, if we let it use a weather forecasting API, it needs to provide a request in the correct format.\nReAct merges the concepts of reasoning and acting as they are essentially two sides of the same coin: we want reasonong to afect actions and actions to affect reasoning. It does this by iteratively following these three steps:\n\nThought\nAction\nObservation\n\nWe incorporate this into a prompt template like so:\n\n\n\nReAct prompt template\n\n\nWe ask it to create a thought about the prompt, then trigger an action based on the thought, then observe the output, i.e. whatever it retrieved from an external tool.\nAn example of this is using an LLM to use a calculator.\n\nimport os \nfrom langchain_openai import ChatOpenAI \n\n# Load OpenAI's LLMs with LangChain \nos.environ[\"OPENAI_API_KEY\"] = \"MY_KEY\" \nopenai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n\n\n# Create the ReAct template \n\nreact_template = \"\"\"\n    Answer the following questions as best you can. \n    You have access to the following tools: {tools} \n    \n    Use the following format: \n    \n    Question: the input question you must answer \n    Thought: you should always think about what to do \n    Action: the action to take, should be one of [{tool_names}] \n    Action Input: the input to the action \n    Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) \n    Thought: I now know the final answer \n    Final Answer: the final answer to the original input question\n    \n    Begin! \n    \n    Question: {input} \n    Thought:{agent_scratchpad}\n\"\"\" \n    \nprompt = PromptTemplate(\n    template=react_template,\n    input_variables=[\"tools\", \"tool_names\", \"input\", \"agent_scratchpad\"]\n)\n\nNext we need to define the tools it can use to interact with the outside world.\n\nfrom langchain.agents import load_tools, Tool \nfrom langchain.tools import DuckDuckGoSearchResults \n\n# You can create the tool to pass to an agent \nsearch = DuckDuckGoSearchResults() \nsearch_tool = Tool(\n    name=\"duckduck\",\n    description=\"A web search engine. Use this to as a search engine for general queries.\",\n    func=search.run\n)\n\n# Prepare tools \ntools = load_tools([\"llm-math\"], llm=llm)   # Calculator tool is included by default \ntools.append(search_tool)\n\nFinally we can create the ReAct agent and pass it to the AgentExecutor which handles the execution steps.\n\nfrom langchain.agents import AgentExecutor, create_react_agen\n\n# Construct the ReAct agent \nagent = create_react_agent(openai_llm, tools, prompt) \nagent_executor = AgentExecutor(\n    agent=agent,\n    tools=tools,\n    verbose=True,\n    handle_parsing_errors=True\n)\n\nNow we can invoke the LLM to find the price of an item and convert the currency. It will choose the appropriate tools to use for this.\n\n# What is the price of a MacBook Pro? \nagent_executor.invoke({\n    \"input\": \"What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD.\"\n})\n\nThe agent generates intermediate steps during execution which we can use to follow its train of thought.\nThe important thing to consider when using agents is that there is no human in the loop; it will generate an answer but there is no guarantee that it is the correct answer.\nWe can make some tweaks to help ourselves debug this. For exampe, asking the agent to return the website’s URL that it retrieved prices from to make manual verification easier."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter1/lesson.html",
    "href": "posts/ml/hands_on_llm/chapter1/lesson.html",
    "title": "Hands-On LLMs: Intro",
    "section": "",
    "text": "This approach originated in the 1950s but gained popularity in the 2000s.\nIt treats unstructured text as a bag or words, throwing away any information from the position / ordering of the words and any semantic meaning of text.\n\nTokenise the text. A straightforward way to do this is to split on the spaces so we have a list of words.\nCreate a vocabulary of length N, containing every word in our training data.\nWe can then represent any sentence or document as a one-hot encoded N-dimensional vector.\nUse those vectors for downstream tasks, e.g. cosine similarity between vectors to measure the similarity of documents for recommender systems.\n\nFor example, if our vocabulary contains the words: that is a cute dog my cat\nThen we can encode the sentence “that is a cute dog” as:\n\n\n\nthat\nis\na\ncute\ndog\nmy\ncat\n\n\n\n\n1\n1\n1\n1\n1\n0\n0\n\n\n\nAnd another sentence “my cat is cute” as:\n\n\n\nthat\nis\na\ncute\ndog\nmy\ncat\n\n\n\n\n0\n1\n0\n1\n0\n1\n1\n\n\n\nThen to compare how similar the two sentences are, we can compare those vectors, for example using cosine similarity.\n\n\n\nA limitation of bag-of-words is that it makes no attempt to capture meaning from the text, treating each word as an unrelated token. By encoding text as one-hot encoded vectors, it does not capture that the word “cute” might be similar to “adorable” or “scrum-diddly-umptious”; every word is simply an arbitrary element of the vocabulary.\nDense vector embeddings attempt to capture these differences; rather than treating words as discrete elements, we can introduce a continuous scale for each embedding dimension, and learn where each word falls on the scale. Word2Vec was an early, and successful, approach to generating these embeddings.\nThe approach is to:\n\nAssign every word in the vocabulary an (initial random) vector of the embedding dimension, say 50.\nTake pairs of words from the training data, and train a model to predict whether they are likely to be neighbors in a sentence.\nIf two words typically share the same neighbouring words, they are likely to share similar embedding vectors, and vice versa.\n\nIllustrated word2vec provides a deeper dive.\nThese embeddings then have interesting properties. The classic example is that adding/subtracting the vectors for the corresponding words gives: \\[\nking - man + woman \\approx queen\n\\]\n\nThe numbers don’t lie and they spell disaster\n- “Big Poppa Pump” Scott Steiner\n\n\nN-grams are sliding windows of N words sampled from text. These can be used to train a model where the input is N words and the output is the predicted next word.\nContinuous bag or words (CBOW) tries to predict a missing word given the N preceding and following words.\nSkip-grams take a single word and try to predict the surrounding words. The are the “opposite” of CBOW.\n\n\n\n\n\n\n\n\n\n\nArchitecture\nTask\nInputs\nOutput\n\n\n\n\nN-gram\nThe numbers ___\n[The, numbers]\ndon’t\n\n\nCBOW\nThe numbers ___ lie and\n[The, numbers, lie, and]\ndon’t\n\n\nSkip-gram\n___ ___ don’t ___ ___\ndon’t\n[The, numbers, lie, and]\n\n\n\nNegative sampling is used to speed up the next-word prediction process. Instead of predicting the next token (a computationally expensive neural network), we reframe the task as “given two words, what is the probability that they are neighbours?” (a much faster logistic regression problem.)\nBut the issue is, our training dataset only has positive examples of neighbours. So the model could just always output 1 to get 100% accuracy. To avoid this, we introduce negative exmaples by taking random combinations of words in the vocabulary that aren’t neighbours. This idea is called noise-contrastive estimation.\nWord2vec is then just “skip gram with negative sampling” to generate word embeddings.\n\n\n\n\n\n\nTypes of embeddings\n\n\n\nThere are different types of embeddings that indicate different levels of abstraction.\nWe can create an embedding for a sub-word, a word, a sentence or a whole document. In each case, the result is an N-dimensional vector where N is the embedding size.\n\n\n\n\n\nThe embeddings so far have been static: the embedding for “bank” will be the same regardless of whether it’s referring to the bank of a river or a branch of Santander.\nThe next development notes that the embeddings should vary depending on their context, i.e. the surrounding words.\nRecurrent Neural Networks (RNNs) were initially used with attention mechanisms. These would:\n\nTake pre-generated embeddings (say, from word2vec) as inputs\nPass this to an encoder RNN to generate a context embedding\nPass this to a decoder RNN to generate an output, such as the input text translated to another language.\n\n\n\n\n\n\nflowchart LR\n\n  A(Pre-generated embeddings) --&gt; B[[Encoder RNN]] --&gt; C(Context embedding) --&gt; D[[Decoder RNN]] --&gt; E(Output)\n\n\n\n\n\n\n\n\n\nTransformers were introduced in the 2017 paper “Attention is All You Need”, which solely used the attention mechanism and removed the RNNs.\nThe original transformer was an encoder-decoder model for machine translation.\n\n\n\n\n\nflowchart LR\n\n  A(Pre-generated embeddings) --&gt; B[[Transformer Encoder]] --&gt; C[[Transformer Decoder]] --&gt; E(Output)\n\n\n\n\n\n\n\n\n\nBy splitting the encoder-decoder architecture and focusing only on the encoder, we can create models the excel at creating meaningful representations of language.\nThis is the premise behind models like BERT. The classification token is appended to the input, and the encoder alone is trained.\n\n\n\nSimilarly, we can split the encoder-decoder architecture and focusing only on the decoder. These excel at text generation.\nThis is the premise behind models like GPT.\nGenerative LLMs are essentially sequence-to-sequence machines: given some input text, predict the next tokens. The primary use case these days is being fine-tuned for “instruct” or “chat” models that are trained to provide an answer when given a question.\nFoundation models are open-source base models that can be fine-tuned for specific tasks.\n\n\n\nAside from Transformers, Mamba and RWKV perform well.\n\n\n\n\nThis size of a large larnguage model is a moving target as the field develops and model sizes scale.\nConsiderations:\n\nWhat if a new model has the same capabilities as an existing LLM but with a fraction of the parameters. Is this new model still “large”?\nWhat if we train a model the same size as GPT-4 but for text classification instead of generation? Is it still an LLM?\n\nThe creation of LLMs is typically done in two stages:\n\nLanguage modeling: Create a foundation model by (unsuperivsed) training on a vast corpus of text. This step allows the model to learn the grammar, structure and patterns of the language. It is not yet directed at a specific task. This takes the majority of the training time.\nFine-tuning: Using the foundation model for (supervised) training on a specific task.\n\n\n\n\n\nBias and fariness: Training data is seldom shared, so may contain implicit biases\nTransparency and accountability: Unintended consequences when there is no “human in the loop”. Who is accountable for the outcomes of the LLM? The company that trained it? Or the one that used it? Or the patient?\nGenerating harmful content\nIntellectual property: Who owns the output of an LLM? The user? The company that trained it? Or the original creators of the training data?\nRegulation\n\n\n\n\nThe following can be run in Google Colab on a free GPU.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\n# Load model and tokenizer \nmodel = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/Phi-3-mini-4k-instruct\", device_map=\"cuda\", torch_dtype=\"auto\", trust_remote_code=True\n) \ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini4k-instruct\")\n\n# Create a pipeline \ngenerator = pipeline(\n    \"text-generation\", model=model, tokenizer=tokenizer, return_full_text=False, max_new_tokens=500, do_sample=False\n)\n\n# The prompt (user input / query) \nmessages = [{\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"}]\n\n# Generate output \noutput = generator(messages)\nprint(output[0][\"generated_text\"])\n\n&gt;&gt;&gt; Why don't chickens like to go to the gym? Because they can't crack the egg-sistence of it!\n\n\n\n\nChapter 1 of Hands-On Large Language Models by Jay Alammar & Marten Grootendoorst\nhttps://jalammar.github.io/illustrated-word2vec/"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter1/lesson.html#a-brief-history-of-language-ai",
    "href": "posts/ml/hands_on_llm/chapter1/lesson.html#a-brief-history-of-language-ai",
    "title": "Hands-On LLMs: Intro",
    "section": "",
    "text": "This approach originated in the 1950s but gained popularity in the 2000s.\nIt treats unstructured text as a bag or words, throwing away any information from the position / ordering of the words and any semantic meaning of text.\n\nTokenise the text. A straightforward way to do this is to split on the spaces so we have a list of words.\nCreate a vocabulary of length N, containing every word in our training data.\nWe can then represent any sentence or document as a one-hot encoded N-dimensional vector.\nUse those vectors for downstream tasks, e.g. cosine similarity between vectors to measure the similarity of documents for recommender systems.\n\nFor example, if our vocabulary contains the words: that is a cute dog my cat\nThen we can encode the sentence “that is a cute dog” as:\n\n\n\nthat\nis\na\ncute\ndog\nmy\ncat\n\n\n\n\n1\n1\n1\n1\n1\n0\n0\n\n\n\nAnd another sentence “my cat is cute” as:\n\n\n\nthat\nis\na\ncute\ndog\nmy\ncat\n\n\n\n\n0\n1\n0\n1\n0\n1\n1\n\n\n\nThen to compare how similar the two sentences are, we can compare those vectors, for example using cosine similarity.\n\n\n\nA limitation of bag-of-words is that it makes no attempt to capture meaning from the text, treating each word as an unrelated token. By encoding text as one-hot encoded vectors, it does not capture that the word “cute” might be similar to “adorable” or “scrum-diddly-umptious”; every word is simply an arbitrary element of the vocabulary.\nDense vector embeddings attempt to capture these differences; rather than treating words as discrete elements, we can introduce a continuous scale for each embedding dimension, and learn where each word falls on the scale. Word2Vec was an early, and successful, approach to generating these embeddings.\nThe approach is to:\n\nAssign every word in the vocabulary an (initial random) vector of the embedding dimension, say 50.\nTake pairs of words from the training data, and train a model to predict whether they are likely to be neighbors in a sentence.\nIf two words typically share the same neighbouring words, they are likely to share similar embedding vectors, and vice versa.\n\nIllustrated word2vec provides a deeper dive.\nThese embeddings then have interesting properties. The classic example is that adding/subtracting the vectors for the corresponding words gives: \\[\nking - man + woman \\approx queen\n\\]\n\nThe numbers don’t lie and they spell disaster\n- “Big Poppa Pump” Scott Steiner\n\n\nN-grams are sliding windows of N words sampled from text. These can be used to train a model where the input is N words and the output is the predicted next word.\nContinuous bag or words (CBOW) tries to predict a missing word given the N preceding and following words.\nSkip-grams take a single word and try to predict the surrounding words. The are the “opposite” of CBOW.\n\n\n\n\n\n\n\n\n\n\nArchitecture\nTask\nInputs\nOutput\n\n\n\n\nN-gram\nThe numbers ___\n[The, numbers]\ndon’t\n\n\nCBOW\nThe numbers ___ lie and\n[The, numbers, lie, and]\ndon’t\n\n\nSkip-gram\n___ ___ don’t ___ ___\ndon’t\n[The, numbers, lie, and]\n\n\n\nNegative sampling is used to speed up the next-word prediction process. Instead of predicting the next token (a computationally expensive neural network), we reframe the task as “given two words, what is the probability that they are neighbours?” (a much faster logistic regression problem.)\nBut the issue is, our training dataset only has positive examples of neighbours. So the model could just always output 1 to get 100% accuracy. To avoid this, we introduce negative exmaples by taking random combinations of words in the vocabulary that aren’t neighbours. This idea is called noise-contrastive estimation.\nWord2vec is then just “skip gram with negative sampling” to generate word embeddings.\n\n\n\n\n\n\nTypes of embeddings\n\n\n\nThere are different types of embeddings that indicate different levels of abstraction.\nWe can create an embedding for a sub-word, a word, a sentence or a whole document. In each case, the result is an N-dimensional vector where N is the embedding size.\n\n\n\n\n\nThe embeddings so far have been static: the embedding for “bank” will be the same regardless of whether it’s referring to the bank of a river or a branch of Santander.\nThe next development notes that the embeddings should vary depending on their context, i.e. the surrounding words.\nRecurrent Neural Networks (RNNs) were initially used with attention mechanisms. These would:\n\nTake pre-generated embeddings (say, from word2vec) as inputs\nPass this to an encoder RNN to generate a context embedding\nPass this to a decoder RNN to generate an output, such as the input text translated to another language.\n\n\n\n\n\n\nflowchart LR\n\n  A(Pre-generated embeddings) --&gt; B[[Encoder RNN]] --&gt; C(Context embedding) --&gt; D[[Decoder RNN]] --&gt; E(Output)\n\n\n\n\n\n\n\n\n\nTransformers were introduced in the 2017 paper “Attention is All You Need”, which solely used the attention mechanism and removed the RNNs.\nThe original transformer was an encoder-decoder model for machine translation.\n\n\n\n\n\nflowchart LR\n\n  A(Pre-generated embeddings) --&gt; B[[Transformer Encoder]] --&gt; C[[Transformer Decoder]] --&gt; E(Output)\n\n\n\n\n\n\n\n\n\nBy splitting the encoder-decoder architecture and focusing only on the encoder, we can create models the excel at creating meaningful representations of language.\nThis is the premise behind models like BERT. The classification token is appended to the input, and the encoder alone is trained.\n\n\n\nSimilarly, we can split the encoder-decoder architecture and focusing only on the decoder. These excel at text generation.\nThis is the premise behind models like GPT.\nGenerative LLMs are essentially sequence-to-sequence machines: given some input text, predict the next tokens. The primary use case these days is being fine-tuned for “instruct” or “chat” models that are trained to provide an answer when given a question.\nFoundation models are open-source base models that can be fine-tuned for specific tasks.\n\n\n\nAside from Transformers, Mamba and RWKV perform well."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter1/lesson.html#how-large-is-a-large-language-model",
    "href": "posts/ml/hands_on_llm/chapter1/lesson.html#how-large-is-a-large-language-model",
    "title": "Hands-On LLMs: Intro",
    "section": "",
    "text": "This size of a large larnguage model is a moving target as the field develops and model sizes scale.\nConsiderations:\n\nWhat if a new model has the same capabilities as an existing LLM but with a fraction of the parameters. Is this new model still “large”?\nWhat if we train a model the same size as GPT-4 but for text classification instead of generation? Is it still an LLM?\n\nThe creation of LLMs is typically done in two stages:\n\nLanguage modeling: Create a foundation model by (unsuperivsed) training on a vast corpus of text. This step allows the model to learn the grammar, structure and patterns of the language. It is not yet directed at a specific task. This takes the majority of the training time.\nFine-tuning: Using the foundation model for (supervised) training on a specific task."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter1/lesson.html#ethical-considerations-of-llms",
    "href": "posts/ml/hands_on_llm/chapter1/lesson.html#ethical-considerations-of-llms",
    "title": "Hands-On LLMs: Intro",
    "section": "",
    "text": "Bias and fariness: Training data is seldom shared, so may contain implicit biases\nTransparency and accountability: Unintended consequences when there is no “human in the loop”. Who is accountable for the outcomes of the LLM? The company that trained it? Or the one that used it? Or the patient?\nGenerating harmful content\nIntellectual property: Who owns the output of an LLM? The user? The company that trained it? Or the original creators of the training data?\nRegulation"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter1/lesson.html#using-an-llm-locally",
    "href": "posts/ml/hands_on_llm/chapter1/lesson.html#using-an-llm-locally",
    "title": "Hands-On LLMs: Intro",
    "section": "",
    "text": "The following can be run in Google Colab on a free GPU.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\n# Load model and tokenizer \nmodel = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/Phi-3-mini-4k-instruct\", device_map=\"cuda\", torch_dtype=\"auto\", trust_remote_code=True\n) \ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini4k-instruct\")\n\n# Create a pipeline \ngenerator = pipeline(\n    \"text-generation\", model=model, tokenizer=tokenizer, return_full_text=False, max_new_tokens=500, do_sample=False\n)\n\n# The prompt (user input / query) \nmessages = [{\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"}]\n\n# Generate output \noutput = generator(messages)\nprint(output[0][\"generated_text\"])\n\n&gt;&gt;&gt; Why don't chickens like to go to the gym? Because they can't crack the egg-sistence of it!"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter1/lesson.html#references",
    "href": "posts/ml/hands_on_llm/chapter1/lesson.html#references",
    "title": "Hands-On LLMs: Intro",
    "section": "",
    "text": "Chapter 1 of Hands-On Large Language Models by Jay Alammar & Marten Grootendoorst\nhttps://jalammar.github.io/illustrated-word2vec/"
  },
  {
    "objectID": "posts/ml/fastai/lesson6/lesson.html",
    "href": "posts/ml/fastai/lesson6/lesson.html",
    "title": "FastAI Lesson 6: Random Forests",
    "section": "",
    "text": "These are notes from lesson 6 of Fast AI Practical Deep Learning for Coders.\n\n\nIt’s worth starting with a random forest as a baseline model because “it’s very hard to screw up”. Decision trees only consider the ordering of data, so are not sensitive to outliers, skewed distributions, dummy variables etc.\nA random forest is an ensemble of trees. A tree is an ensemble of binary splits.\nbinary split -&gt; tree -&gt; forest\n\n\nPick a column of the data set and split the rows into two groups. Predict the outcome based just on that.\nFor example, in the titanic dataset, pick the Sex column. This splits into male vs female. If we predict that all female passengers survived and all male passengers died, that’s a reasonably accurate prediction.\nA model which iterates through the variables and finds the best binary split is called a “OneR” or one rule model.\nThis was actually the state-of-the-art in the 90s and performs reasonably well across a lot of problems.\n\n\n\nThis extends the “OneR” idea to two or more splits.\nFor example, if we first split by sex, then find the next best variable to split on to create a two layer tree.\nLeaf nodes are the number of terminating nodes in the tree. OneR creates 2 leaf nodes. If we split one side again, we’ll have 3 leaf nodes. If we split the other side, to create a balanced two layer tree, we’ll have 4 leaf nodes.\n\n\n\nThe idea behind bagging is that if we have many unbiased, uncorrelated models, we can average their predictions to get an aggregate model that is better than any of them.\nEach individual model will overfit, so either be too high or too low for a given point, a positive or negative error term respectively. By combining multiple uncorrelated models, the error will average to 0.\nAn easy way to get many uncorrelated models is to only use a random subset of the data each time. Then build a decision tree for each subset.\nThis is the idea behind random forests.\nThe error decreases with the number of trees, with diminishing returns.\n\nJeremy’s rule of thumb: improvements level off after about 30 trees and he doesn’t often use &gt;100.\n\n\n\n\n\n\n\nA nice side effect of using decision trees is that we get feature importance plots for free.\nGini is a measure of inequality, similar to the score used in the notebook to quantify how good a split was.\nIntuitively, this measures the likelihood that if you picked two samples from a group, how likely is it they’d be the same every time. If the group is all the same, the probability is 1. If every item is different, the probability is 0.\nThe idea is for each split of a decision tree, we can track the column used to split and the amount that the gini decreased by. If we loop through the tree and accumulate the “gini reduction” for each column, we have a metric of how important that column was in splitting the dataset.\nThis makes random forests a useful first model when tackling a new big dataset, as it can give an insight into how useful each column is.\n\n\n\nA comment on explainability, regarding feature importance compared to SHAP, LIME etc. These explain the model, so to be useful the model needs to be accurate.\nIf you use feature importance from a bad model then the columns it claims are important might not actually be. So the usefulness of explainability techniques boils down to what models can they explain and how accurate are those models.\n\n\n\nFor each tree, we trained on a subset of the rows. We can then see how each one performed on the held out data; this is the out-of-bag (OOB) error per tree.\nWe can average this over all of the trees to get an overall OOB error for the forest.\n\n\n\nThese are not specific to random forests and can be applied to any ML model including deep neural networks.\nIf we want to know how an independent variable affects the dependent variable, a naive approach would be to just plot them.\nBut this could include a confounding variable. For example, the price of bulldozers increases over time, but driven by the presence of air conditioning which also increased over time.\nA partial dependence plot takes each row in turn and sets the column(s) of interest to the first value, say, year=1950. Then we predict the target variable using our model. Then repeat this for year=1951, 1952, etc.\nWe can then average the target variable per year to get a view of how it depends on the independent variable, all else being equal.\n\n\n\n\nWe make multiple trees, but instead of fitting all to different data subsets, we fit to residuals.\nSo we fit a very small tree (OneR even) to the data to get a first prediction. Then we calculate the error term. Then we fit another tree to predict the error term. Then calculate the second order error term. Then fit a tree to this, etc.\nThen our prediction is the sum of these trees, rather than the average like with a random forest.\nThis is “boosting”; calculating an error term then fitting another model to it.\nContrast this with “bagging” which was when we calculate multiple models to different subsets of data and average them.\n\n\n\n\nCourse lesson page\nComparison of computer vision models for fine-tuning\nBagging predictors"
  },
  {
    "objectID": "posts/ml/fastai/lesson6/lesson.html#background-on-random-forests",
    "href": "posts/ml/fastai/lesson6/lesson.html#background-on-random-forests",
    "title": "FastAI Lesson 6: Random Forests",
    "section": "",
    "text": "It’s worth starting with a random forest as a baseline model because “it’s very hard to screw up”. Decision trees only consider the ordering of data, so are not sensitive to outliers, skewed distributions, dummy variables etc.\nA random forest is an ensemble of trees. A tree is an ensemble of binary splits.\nbinary split -&gt; tree -&gt; forest\n\n\nPick a column of the data set and split the rows into two groups. Predict the outcome based just on that.\nFor example, in the titanic dataset, pick the Sex column. This splits into male vs female. If we predict that all female passengers survived and all male passengers died, that’s a reasonably accurate prediction.\nA model which iterates through the variables and finds the best binary split is called a “OneR” or one rule model.\nThis was actually the state-of-the-art in the 90s and performs reasonably well across a lot of problems.\n\n\n\nThis extends the “OneR” idea to two or more splits.\nFor example, if we first split by sex, then find the next best variable to split on to create a two layer tree.\nLeaf nodes are the number of terminating nodes in the tree. OneR creates 2 leaf nodes. If we split one side again, we’ll have 3 leaf nodes. If we split the other side, to create a balanced two layer tree, we’ll have 4 leaf nodes.\n\n\n\nThe idea behind bagging is that if we have many unbiased, uncorrelated models, we can average their predictions to get an aggregate model that is better than any of them.\nEach individual model will overfit, so either be too high or too low for a given point, a positive or negative error term respectively. By combining multiple uncorrelated models, the error will average to 0.\nAn easy way to get many uncorrelated models is to only use a random subset of the data each time. Then build a decision tree for each subset.\nThis is the idea behind random forests.\nThe error decreases with the number of trees, with diminishing returns.\n\nJeremy’s rule of thumb: improvements level off after about 30 trees and he doesn’t often use &gt;100."
  },
  {
    "objectID": "posts/ml/fastai/lesson6/lesson.html#attributes-of-random-forests",
    "href": "posts/ml/fastai/lesson6/lesson.html#attributes-of-random-forests",
    "title": "FastAI Lesson 6: Random Forests",
    "section": "",
    "text": "A nice side effect of using decision trees is that we get feature importance plots for free.\nGini is a measure of inequality, similar to the score used in the notebook to quantify how good a split was.\nIntuitively, this measures the likelihood that if you picked two samples from a group, how likely is it they’d be the same every time. If the group is all the same, the probability is 1. If every item is different, the probability is 0.\nThe idea is for each split of a decision tree, we can track the column used to split and the amount that the gini decreased by. If we loop through the tree and accumulate the “gini reduction” for each column, we have a metric of how important that column was in splitting the dataset.\nThis makes random forests a useful first model when tackling a new big dataset, as it can give an insight into how useful each column is.\n\n\n\nA comment on explainability, regarding feature importance compared to SHAP, LIME etc. These explain the model, so to be useful the model needs to be accurate.\nIf you use feature importance from a bad model then the columns it claims are important might not actually be. So the usefulness of explainability techniques boils down to what models can they explain and how accurate are those models.\n\n\n\nFor each tree, we trained on a subset of the rows. We can then see how each one performed on the held out data; this is the out-of-bag (OOB) error per tree.\nWe can average this over all of the trees to get an overall OOB error for the forest.\n\n\n\nThese are not specific to random forests and can be applied to any ML model including deep neural networks.\nIf we want to know how an independent variable affects the dependent variable, a naive approach would be to just plot them.\nBut this could include a confounding variable. For example, the price of bulldozers increases over time, but driven by the presence of air conditioning which also increased over time.\nA partial dependence plot takes each row in turn and sets the column(s) of interest to the first value, say, year=1950. Then we predict the target variable using our model. Then repeat this for year=1951, 1952, etc.\nWe can then average the target variable per year to get a view of how it depends on the independent variable, all else being equal."
  },
  {
    "objectID": "posts/ml/fastai/lesson6/lesson.html#gradient-boosting-forests",
    "href": "posts/ml/fastai/lesson6/lesson.html#gradient-boosting-forests",
    "title": "FastAI Lesson 6: Random Forests",
    "section": "",
    "text": "We make multiple trees, but instead of fitting all to different data subsets, we fit to residuals.\nSo we fit a very small tree (OneR even) to the data to get a first prediction. Then we calculate the error term. Then we fit another tree to predict the error term. Then calculate the second order error term. Then fit a tree to this, etc.\nThen our prediction is the sum of these trees, rather than the average like with a random forest.\nThis is “boosting”; calculating an error term then fitting another model to it.\nContrast this with “bagging” which was when we calculate multiple models to different subsets of data and average them."
  },
  {
    "objectID": "posts/ml/fastai/lesson6/lesson.html#references",
    "href": "posts/ml/fastai/lesson6/lesson.html#references",
    "title": "FastAI Lesson 6: Random Forests",
    "section": "",
    "text": "Course lesson page\nComparison of computer vision models for fine-tuning\nBagging predictors"
  },
  {
    "objectID": "posts/ml/fastai/lesson8/lesson.html",
    "href": "posts/ml/fastai/lesson8/lesson.html",
    "title": "FastAI Lesson 8: Convolutions",
    "section": "",
    "text": "These are notes from lesson 8 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\nRecreate the convolutions in a spreadsheet which underpins the discussion below.\n\n\n\n\n\n\nConvolutions slide a window of numbers, say 3x3, across our original image.\nDepending on the values in the filter, it will be able to pick out different features like horizontal or vertical edges. Subsequent layers can combine these into more sophisticated layers, like corners. These can eventually be combined to detect complex features of the image.\nAn interactive example of this to see the intuition behind the sliding window is here.\n\n\n\nThis is a technique to reduce the size of the input tensor.\nA 2x2 max pool layer slides a 2x2 filter over the input and replaces each value with the max of the 4 values in the image.\nNowadays, using the stride length is generally preferred over max pool layers.\n\n\n\nAn alternative technique to reduce the size of the input is to skip pixels when we slide our filter over the image.\nFor example, a stride=2 convolution would apply to every second pixel and therefore halves the image size in each axis, having the same effect as a 2x2 max pool.\n\n\n\nAs a regularisation technique to make sure the model is not overly reliant on any single pixel or region, we can add a dropout layer.\nConceptually, this is the same as initialisng a random tensor the same size as the input and masking the input based on whether the random weight is above a threshold.\n\n\n\nWe eventually want to reduce our input image size to output a tensor with one value per class.\nOne approach is to apply a dense layer when the image has been reduced “enough”, taking the dot product between the reduced image tensor and the dense layer. This is again deprecated in favour of the next approach…\n\n\n\nNowadays, we use stride=2 convolutions until we get a small (7x7) tensor, then apply a single average pool layer to it.\nThis 7x7 tensor (for a bear detector, say) effectively gives a value quantifying “is there a bear in this part of the image?”. It then takes the average of all of these to determine if there is a bear in the overall photo.\nThis works fine if the bear occupies most of the photo, but less well if the bear occupies a small region in the corner of the photo. So the details of the model depend on the use case. If we want ot be able to detect small bears in the corner, max pooling would work better here.\nConcat pool is a hybrid approach which does the max pool AND the average pool and concatentates the two results together.\n\n\n\n\nConvolutions can be reframed in different ways, as matrix multiplications or as systems of linear equations.\nThis article is a a helpful exploration of the topic.\n\n\n\nA summary of questions to end the course.\n\nRead Meta Learning.\nDon’t try to know everything. Pick a bit that you’re interested and dig in. You’ll start to recognise the same ideas cropping up with slight tweaks.\nDoes success in deep learning boil down to more compute power? No, we can be smarter about our approach. Also pick your problems to be ones that you can actually manage with smaller compute resources.\nDragonbox Algebra 5+ can teach little kids algebra.\nTurning a model into a startup. The key to a legitimate business venture is to solve a legitimate problem, and one that people will pay you to solve. Start with the problem, not the prototype. The LEan Startup by Eric Ries: create the MVP as quick as possible (and fake the solution) then gradually make it “less fake” as more people use it (and pay you).\nMake the things you want to do easier, then you’ll want to do them more.\n\n\n\n\n\nCourse lesson page\nMeta Learning"
  },
  {
    "objectID": "posts/ml/fastai/lesson8/lesson.html#the-intuition-behind-cnns",
    "href": "posts/ml/fastai/lesson8/lesson.html#the-intuition-behind-cnns",
    "title": "FastAI Lesson 8: Convolutions",
    "section": "",
    "text": "Convolutions slide a window of numbers, say 3x3, across our original image.\nDepending on the values in the filter, it will be able to pick out different features like horizontal or vertical edges. Subsequent layers can combine these into more sophisticated layers, like corners. These can eventually be combined to detect complex features of the image.\nAn interactive example of this to see the intuition behind the sliding window is here.\n\n\n\nThis is a technique to reduce the size of the input tensor.\nA 2x2 max pool layer slides a 2x2 filter over the input and replaces each value with the max of the 4 values in the image.\nNowadays, using the stride length is generally preferred over max pool layers.\n\n\n\nAn alternative technique to reduce the size of the input is to skip pixels when we slide our filter over the image.\nFor example, a stride=2 convolution would apply to every second pixel and therefore halves the image size in each axis, having the same effect as a 2x2 max pool.\n\n\n\nAs a regularisation technique to make sure the model is not overly reliant on any single pixel or region, we can add a dropout layer.\nConceptually, this is the same as initialisng a random tensor the same size as the input and masking the input based on whether the random weight is above a threshold.\n\n\n\nWe eventually want to reduce our input image size to output a tensor with one value per class.\nOne approach is to apply a dense layer when the image has been reduced “enough”, taking the dot product between the reduced image tensor and the dense layer. This is again deprecated in favour of the next approach…\n\n\n\nNowadays, we use stride=2 convolutions until we get a small (7x7) tensor, then apply a single average pool layer to it.\nThis 7x7 tensor (for a bear detector, say) effectively gives a value quantifying “is there a bear in this part of the image?”. It then takes the average of all of these to determine if there is a bear in the overall photo.\nThis works fine if the bear occupies most of the photo, but less well if the bear occupies a small region in the corner of the photo. So the details of the model depend on the use case. If we want ot be able to detect small bears in the corner, max pooling would work better here.\nConcat pool is a hybrid approach which does the max pool AND the average pool and concatentates the two results together."
  },
  {
    "objectID": "posts/ml/fastai/lesson8/lesson.html#convolutions-from-different-viewpoints",
    "href": "posts/ml/fastai/lesson8/lesson.html#convolutions-from-different-viewpoints",
    "title": "FastAI Lesson 8: Convolutions",
    "section": "",
    "text": "Convolutions can be reframed in different ways, as matrix multiplications or as systems of linear equations.\nThis article is a a helpful exploration of the topic."
  },
  {
    "objectID": "posts/ml/fastai/lesson8/lesson.html#assorted-thoughts-from-jeremy",
    "href": "posts/ml/fastai/lesson8/lesson.html#assorted-thoughts-from-jeremy",
    "title": "FastAI Lesson 8: Convolutions",
    "section": "",
    "text": "A summary of questions to end the course.\n\nRead Meta Learning.\nDon’t try to know everything. Pick a bit that you’re interested and dig in. You’ll start to recognise the same ideas cropping up with slight tweaks.\nDoes success in deep learning boil down to more compute power? No, we can be smarter about our approach. Also pick your problems to be ones that you can actually manage with smaller compute resources.\nDragonbox Algebra 5+ can teach little kids algebra.\nTurning a model into a startup. The key to a legitimate business venture is to solve a legitimate problem, and one that people will pay you to solve. Start with the problem, not the prototype. The LEan Startup by Eric Ries: create the MVP as quick as possible (and fake the solution) then gradually make it “less fake” as more people use it (and pay you).\nMake the things you want to do easier, then you’ll want to do them more."
  },
  {
    "objectID": "posts/ml/fastai/lesson8/lesson.html#references",
    "href": "posts/ml/fastai/lesson8/lesson.html#references",
    "title": "FastAI Lesson 8: Convolutions",
    "section": "",
    "text": "Course lesson page\nMeta Learning"
  },
  {
    "objectID": "posts/ml/fastai/lesson6_1/lesson.html",
    "href": "posts/ml/fastai/lesson6_1/lesson.html",
    "title": "FastAI Lesson 6.5: Road to the Top",
    "section": "",
    "text": "These are notes on the “Road to the Top” notebooks that span lessons 6 and 7 of Fast AI Practical Deep Learning for Coders. I’ve separated these from the main topics of those lectires to keep the posts focused.\n\n\n\n\n\n\nHomework Task\n\n\n\n\nRecreate the softmax and cross-entropy loss spreadsheet example\nRead the “Road to the Top” notebook series - parts 1, 2 and 3\nRead “Things that confused me about cross entropy” by Chris Said.\n\n\n\n\n\nThe focus should be:\n\nCreate an effective validation set\nIterate quickly to find changes which improve the validation set\n\nTrain a simple model straight away, get a result and submit it. You can iterate and improve later.\nData augmentation, in particular test-time augmentation (TTA), can improve results.\n\n\n\nRules of thumb on model selection by problem type:\n\nFor computer vision use CNNs, fine-tune a pre-trained model. See comparison of pre-trained models here.\n\nIf unsure which model to choose, start with (small) convnext.\nDifferent models can have big differences in accuracy so try a few small models from different families.\nOnce you are happy with the model, try a size up in that family.\nResizing images to a square is a good, easy compromise to accomodate many different image sizes, orientations and aspect ratios. A more involved approach that may improve results is to batch images together and resize them to the median rectangle size.\n\nFor tabular data, random forests will give a reasonably good result.\n\nGBMs will give a better result eventually, but with more effort required to get a good model.\nWorth running a hyperparameter grid search for GBM because it’s fiddly.\n\n\nRules of thumb on hardware:\n\nYou generally want ~8 physical CPUs per GPU\nIf model training is CPU bound, it can help to resize images to be smaller. The file I/O on the CPU may be taking too long.\nIf model training is CPU bound and GPU usage is low, you might as well go up to a “better” (i.e. bigger) model with no increase of overall execution time.\n\n\n\n\nThis is a technique that allows you to run larger batch sizes without running out of GPU memory.\nRather than needing to fit a whole batch (say, 64 images) in memory, we can split this up into sub-batches. Let’s say we use an accumulation factor of 4; this means each sub-batch would have 16 images.\nWe calculate the gradient for each sub-batch but don’t immediately subtract it from the weights. We also keep a running count of the number of images we have seen. Once we have seen 64 images, i.e. 4 batches, then apply the total accumulated gradient and reset the count. Remember that in Pytorch, if we calculate another gradient without zeroing in between, it will add the new gradient to the old one, so this is in effect the “default behaviour”.\nGradient accumulation gives an identical result unless using batch normalisation, since the moving averages will be more volatile. Most big models use layer normalisation rather than batch normalisation so often this is not an issue.\nHow do we pick a batch size? Just pick the largest you can without running out of GPU memory.\nWhy not just reduce the batch size? If using a pretrained model, the other parameters such as learning rate work for that batch size and might not work for others. So we’d have to perform a hyperparameter search again to find a good combination for the new batch size. Gradient accumulation sidesteps this issue.\n\n\n\nConsider the case where we want to train a model with two different predicted labels, say plant species and disease type. If we have 10 plant species and 10 diseases, we create a model with 20 output neurons.\nHow does the model know what each should do? Through the loss function. We make one loss function for disease which takes the first 10 columns of the input to the final layer, and a second loss function for variety which takes the last 10 columns of the input to the final layer. The overall loss function is the sum of the two.\nIf you train it for longer, the multi target model can get better at predicting one target, say disease, than a single target model trained on just disease alone.\n\nThe early layers that are useful for variety may also be useful for disease\nKnowing the variety may be useful in predicting disease if there are confounding factors between disease and variety. For example, certain diseases affect certain varieties.\n\n\n\n\n\nCourse lesson page\n“Road to the Top” notebook\n“Things that confused me about cross entropy” by Chris Said."
  },
  {
    "objectID": "posts/ml/fastai/lesson6_1/lesson.html#general-points",
    "href": "posts/ml/fastai/lesson6_1/lesson.html#general-points",
    "title": "FastAI Lesson 6.5: Road to the Top",
    "section": "",
    "text": "The focus should be:\n\nCreate an effective validation set\nIterate quickly to find changes which improve the validation set\n\nTrain a simple model straight away, get a result and submit it. You can iterate and improve later.\nData augmentation, in particular test-time augmentation (TTA), can improve results."
  },
  {
    "objectID": "posts/ml/fastai/lesson6_1/lesson.html#some-rules-of-thumb",
    "href": "posts/ml/fastai/lesson6_1/lesson.html#some-rules-of-thumb",
    "title": "FastAI Lesson 6.5: Road to the Top",
    "section": "",
    "text": "Rules of thumb on model selection by problem type:\n\nFor computer vision use CNNs, fine-tune a pre-trained model. See comparison of pre-trained models here.\n\nIf unsure which model to choose, start with (small) convnext.\nDifferent models can have big differences in accuracy so try a few small models from different families.\nOnce you are happy with the model, try a size up in that family.\nResizing images to a square is a good, easy compromise to accomodate many different image sizes, orientations and aspect ratios. A more involved approach that may improve results is to batch images together and resize them to the median rectangle size.\n\nFor tabular data, random forests will give a reasonably good result.\n\nGBMs will give a better result eventually, but with more effort required to get a good model.\nWorth running a hyperparameter grid search for GBM because it’s fiddly.\n\n\nRules of thumb on hardware:\n\nYou generally want ~8 physical CPUs per GPU\nIf model training is CPU bound, it can help to resize images to be smaller. The file I/O on the CPU may be taking too long.\nIf model training is CPU bound and GPU usage is low, you might as well go up to a “better” (i.e. bigger) model with no increase of overall execution time."
  },
  {
    "objectID": "posts/ml/fastai/lesson6_1/lesson.html#gradient-accumulation",
    "href": "posts/ml/fastai/lesson6_1/lesson.html#gradient-accumulation",
    "title": "FastAI Lesson 6.5: Road to the Top",
    "section": "",
    "text": "This is a technique that allows you to run larger batch sizes without running out of GPU memory.\nRather than needing to fit a whole batch (say, 64 images) in memory, we can split this up into sub-batches. Let’s say we use an accumulation factor of 4; this means each sub-batch would have 16 images.\nWe calculate the gradient for each sub-batch but don’t immediately subtract it from the weights. We also keep a running count of the number of images we have seen. Once we have seen 64 images, i.e. 4 batches, then apply the total accumulated gradient and reset the count. Remember that in Pytorch, if we calculate another gradient without zeroing in between, it will add the new gradient to the old one, so this is in effect the “default behaviour”.\nGradient accumulation gives an identical result unless using batch normalisation, since the moving averages will be more volatile. Most big models use layer normalisation rather than batch normalisation so often this is not an issue.\nHow do we pick a batch size? Just pick the largest you can without running out of GPU memory.\nWhy not just reduce the batch size? If using a pretrained model, the other parameters such as learning rate work for that batch size and might not work for others. So we’d have to perform a hyperparameter search again to find a good combination for the new batch size. Gradient accumulation sidesteps this issue."
  },
  {
    "objectID": "posts/ml/fastai/lesson6_1/lesson.html#multi-target-classification",
    "href": "posts/ml/fastai/lesson6_1/lesson.html#multi-target-classification",
    "title": "FastAI Lesson 6.5: Road to the Top",
    "section": "",
    "text": "Consider the case where we want to train a model with two different predicted labels, say plant species and disease type. If we have 10 plant species and 10 diseases, we create a model with 20 output neurons.\nHow does the model know what each should do? Through the loss function. We make one loss function for disease which takes the first 10 columns of the input to the final layer, and a second loss function for variety which takes the last 10 columns of the input to the final layer. The overall loss function is the sum of the two.\nIf you train it for longer, the multi target model can get better at predicting one target, say disease, than a single target model trained on just disease alone.\n\nThe early layers that are useful for variety may also be useful for disease\nKnowing the variety may be useful in predicting disease if there are confounding factors between disease and variety. For example, certain diseases affect certain varieties."
  },
  {
    "objectID": "posts/ml/fastai/lesson6_1/lesson.html#references",
    "href": "posts/ml/fastai/lesson6_1/lesson.html#references",
    "title": "FastAI Lesson 6.5: Road to the Top",
    "section": "",
    "text": "Course lesson page\n“Road to the Top” notebook\n“Things that confused me about cross entropy” by Chris Said."
  },
  {
    "objectID": "posts/ml/fastai/lesson5/lesson.html",
    "href": "posts/ml/fastai/lesson5/lesson.html",
    "title": "FastAI Lesson 5: Natural Language Processing",
    "section": "",
    "text": "These are notes from lesson 5 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\n\nRecreate the Jupyter notebook to train a linear model and a neural network from scratch - see from scratch notebook\nThen repeat the exercise using the fastai framework (it’s much easier!) - see framework notebook\nRead numpy broadcasting rules\n\n\n\n\n\nTrain a linear model from scratch in Python using on the Titanic dataset. Then train a neural network from scratch in a similar way. This is an extension of the spreadsheet approach to make a neural network from scratch in lesson 3.\n\n\nNever throw away data.\nAn easy way of imputing missing values is to fill them with the mode. This is good enough for a first pass at creating a model.\n\n\n\nNumeric values that can grow exponentially like prices or population sizes often have long-tailed distributions.\nAn easy way to scale is to take log(x+1). The +1 term is just to avoid taking log of 0.\n\n\n\nOne-hot encode any categorical variables.\nWe should include an “other” category for each in case the validation or test set contains a category we didn’t encounter in the training set.\nIf there are categories with ony a small number of observations we can group them into an “other” category too.\n\n\n\nBroadcasting arrays together avoids boilerplate code to make dimensions match.\nFor reference, see numpy’s broadcasting rules and try APL\n\n\n\nFor a binary classification model, the outputs should be between 0 and 1. If you train a linear model, it might result in negative values or values &gt;1.\nThis means we can improve the loss by just clipping to ensure they stay between 0 and 1.\nThis is the idea behind the sigmoid function for output layers: smaller values will tend to 0 and larger values will tend to 1. In general, for any binary classification model, we should always have a sigmoid as the final layer. If the model isn’t training well, it’s worth checking that the final activation function is a sigmoid.\nThe sigmoid function is defined as: \\[ y = \\frac{1}{1+e^{-x}} \\]\n\n\n\nIn general, the middle layers of a neural network are similar between different problems.\nThe input layer will depend on the data for our specific problem. The output will depend on the target for our specific problem.\nSo we spend most of our time thinking about the correct input and output layers, and the hidden layers are less important.\n\n\n\n\nWhen creating the models from scratch, there was a lot of boilerplate code to:\n\nImpute missing values using the “obvious” method (fill with mode)\nNormalise continuous variables to be between 0 and 1\nOne-hot encode categorical variables\nRepeat all of these steps in the same order for the test set\n\nThe benefits of using a framework like fastai:\n\nLess boilerplate, so the obvious things are done automatically unless you say otherwise\nRepeating all of the feature engineering steps on the output is trivial with DataLoaders\n\n\n\n\nCreating independent models and taking the mean of their predictions improves the accuracy. There are a few different approaches to ensembling a categorical variable:\n\nTake the mean of the predictions (binary 0 or 1)\nTake the mean of the probabilities (continuous between 0 and 1) then threshold the result\nTake the mode of the predictions (binary 0 or 1)\n\nIn general the mean approaches work better but there’s no rule as to why, so try all of them.\n\n\n\n\nCourse lesson page\nNumpy broadcasting rules\nAPL"
  },
  {
    "objectID": "posts/ml/fastai/lesson5/lesson.html#training-a-model-from-scratch",
    "href": "posts/ml/fastai/lesson5/lesson.html#training-a-model-from-scratch",
    "title": "FastAI Lesson 5: Natural Language Processing",
    "section": "",
    "text": "Train a linear model from scratch in Python using on the Titanic dataset. Then train a neural network from scratch in a similar way. This is an extension of the spreadsheet approach to make a neural network from scratch in lesson 3.\n\n\nNever throw away data.\nAn easy way of imputing missing values is to fill them with the mode. This is good enough for a first pass at creating a model.\n\n\n\nNumeric values that can grow exponentially like prices or population sizes often have long-tailed distributions.\nAn easy way to scale is to take log(x+1). The +1 term is just to avoid taking log of 0.\n\n\n\nOne-hot encode any categorical variables.\nWe should include an “other” category for each in case the validation or test set contains a category we didn’t encounter in the training set.\nIf there are categories with ony a small number of observations we can group them into an “other” category too.\n\n\n\nBroadcasting arrays together avoids boilerplate code to make dimensions match.\nFor reference, see numpy’s broadcasting rules and try APL\n\n\n\nFor a binary classification model, the outputs should be between 0 and 1. If you train a linear model, it might result in negative values or values &gt;1.\nThis means we can improve the loss by just clipping to ensure they stay between 0 and 1.\nThis is the idea behind the sigmoid function for output layers: smaller values will tend to 0 and larger values will tend to 1. In general, for any binary classification model, we should always have a sigmoid as the final layer. If the model isn’t training well, it’s worth checking that the final activation function is a sigmoid.\nThe sigmoid function is defined as: \\[ y = \\frac{1}{1+e^{-x}} \\]\n\n\n\nIn general, the middle layers of a neural network are similar between different problems.\nThe input layer will depend on the data for our specific problem. The output will depend on the target for our specific problem.\nSo we spend most of our time thinking about the correct input and output layers, and the hidden layers are less important."
  },
  {
    "objectID": "posts/ml/fastai/lesson5/lesson.html#using-a-framework",
    "href": "posts/ml/fastai/lesson5/lesson.html#using-a-framework",
    "title": "FastAI Lesson 5: Natural Language Processing",
    "section": "",
    "text": "When creating the models from scratch, there was a lot of boilerplate code to:\n\nImpute missing values using the “obvious” method (fill with mode)\nNormalise continuous variables to be between 0 and 1\nOne-hot encode categorical variables\nRepeat all of these steps in the same order for the test set\n\nThe benefits of using a framework like fastai:\n\nLess boilerplate, so the obvious things are done automatically unless you say otherwise\nRepeating all of the feature engineering steps on the output is trivial with DataLoaders"
  },
  {
    "objectID": "posts/ml/fastai/lesson5/lesson.html#ensembles",
    "href": "posts/ml/fastai/lesson5/lesson.html#ensembles",
    "title": "FastAI Lesson 5: Natural Language Processing",
    "section": "",
    "text": "Creating independent models and taking the mean of their predictions improves the accuracy. There are a few different approaches to ensembling a categorical variable:\n\nTake the mean of the predictions (binary 0 or 1)\nTake the mean of the probabilities (continuous between 0 and 1) then threshold the result\nTake the mode of the predictions (binary 0 or 1)\n\nIn general the mean approaches work better but there’s no rule as to why, so try all of them."
  },
  {
    "objectID": "posts/ml/fastai/lesson5/lesson.html#references",
    "href": "posts/ml/fastai/lesson5/lesson.html#references",
    "title": "FastAI Lesson 5: Natural Language Processing",
    "section": "",
    "text": "Course lesson page\nNumpy broadcasting rules\nAPL"
  },
  {
    "objectID": "posts/ml/fastai/lesson3/lesson.html",
    "href": "posts/ml/fastai/lesson3/lesson.html",
    "title": "FastAI Lesson 3: How Does a Neural Network Learn?",
    "section": "",
    "text": "These are notes from lesson 3 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\nRecreate the spreadsheet to train a linear model and a neural network from scratch: see spreadsheet\n\n\n\n\nSome options for cloud environments: Kaggle, Colab, Paperspace.\nA comparison of performance vs training time for different image models useful for model selection is provided here. Resnet and convnext are generally good to start with.\nThe best practice is to start with a “good enough” model with a quick training time, so you can iterate quickly. Then as you get further on with your research and need a better model, you can move to a slower, more accurate model.\nThe criteria we generally care about are:\n\nHow fast are they\nHow much memory do they use\nHow accurate are they\n\nA learner object contains the pre-processing steps and the model itself.\n\n\n\nHow do we fit a function to data? An ML model is just a function fitted to data. Deep learning is just fitting an infinitely flexible model to data.\nIn this notebook there is an interactive cell to fit a quadratic function to some noisy data: y = ax^2 +bx + c\nWe can vary a, b and c to get a better fit by eye. We can make this more rigorous by defining a loss function to quantify how good the fit is.\nIn this case, use the mean absolute error: mae = mean(abs(actual - preds)) We can then use stochastic gradient descent to autome the tweaking of a, b and c to minimise the loss.\nWe can store the parameters as a tensor, then pytorch will calculate gradients of the loss function based on that tensor.\nabc = torch.tensor([1.1,1.1,1.1]) \nabc.requires_grad_()  # Modifies abc in place so that it will include gradient calculations on anything which uses abc downstream\nloss = quad_mae(abc)  # `loss` uses abc so pytorch will give the gradient of loss too\nloss.backward()  # Back-propagate the gradients\nabc.grad  # Returns a tensor of the loss gradients\nWe can then take a “small step” in the direction that will decrease the gradient. The size of the step should be proportional to the size of the gradient. We define a learning rate hyperparameter to determine how much to scale the gradients by.\nlearning_rate = 0.01\nabc -= abc.grad * learning_rate\nloss = quad_mae(abc)  # The loss should now be smaller\nWe can repeat this process to take multiple steps to minimise the gradient.\nfor step in range(10):\n    loss = quad_mae(abc)\n    loss.backward()\n    with torch.no_grad():\n        abc -= abc.grad * learning_rate\n    print(f'step={step}; loss={loss:.2f}')\nThe learning rate should decrease as we get closer to the minimum to ensure we don’t overshoot the minimum and increase the loss. A learning rate schedule can be specified to do this.\n\n\n\nFor deep learning, the premise is the same but instead of quadratic functions, we fit ReLUs and other non-linear functions.\nUniversal approximation theorem states that this is infinitely expressive if enough ReLUs (or other non-linear units) are combined. This means we can learn any computable function.\nA ReLU is essentially a linear function with the negative values clipped to 0.\ndef relu(m,c,x):\n    y = m * x + c\n    return np.clip(y, 0.)\nThis is all that deep learning is! All we need is:\n\nA model - a bunch of ReLUs combined will be flexible\nA loss function - mean absolute error between the actual data values and the values predicted by the model\nAn optimiser - stochastic gradient descent can start from random weights to incrementally improve the loss until we get a “good enough” fit\n\nWe just need enough time and data. There are a few hacks to decrease the time and data required:\n\nData augmentation\nRunning on GPUs to parallelise matrix multiplications\nConvolutions to skip over values to reduce the number of matrix multiplications required\nTransfer learning - initialise with parameters from another pre-trained model instead of random weights.\n\nThis spreadsheet from the homework task is a worked example of manually training a multivariate linear model, then extending that to a neural network summing two ReLUs.\n\n\n\n\nCourse lesson page\nComparison of computer vision models\n“How does a neural network really work?” notebook"
  },
  {
    "objectID": "posts/ml/fastai/lesson3/lesson.html#model-selection",
    "href": "posts/ml/fastai/lesson3/lesson.html#model-selection",
    "title": "FastAI Lesson 3: How Does a Neural Network Learn?",
    "section": "",
    "text": "Some options for cloud environments: Kaggle, Colab, Paperspace.\nA comparison of performance vs training time for different image models useful for model selection is provided here. Resnet and convnext are generally good to start with.\nThe best practice is to start with a “good enough” model with a quick training time, so you can iterate quickly. Then as you get further on with your research and need a better model, you can move to a slower, more accurate model.\nThe criteria we generally care about are:\n\nHow fast are they\nHow much memory do they use\nHow accurate are they\n\nA learner object contains the pre-processing steps and the model itself."
  },
  {
    "objectID": "posts/ml/fastai/lesson3/lesson.html#fitting-a-quadratic-function",
    "href": "posts/ml/fastai/lesson3/lesson.html#fitting-a-quadratic-function",
    "title": "FastAI Lesson 3: How Does a Neural Network Learn?",
    "section": "",
    "text": "How do we fit a function to data? An ML model is just a function fitted to data. Deep learning is just fitting an infinitely flexible model to data.\nIn this notebook there is an interactive cell to fit a quadratic function to some noisy data: y = ax^2 +bx + c\nWe can vary a, b and c to get a better fit by eye. We can make this more rigorous by defining a loss function to quantify how good the fit is.\nIn this case, use the mean absolute error: mae = mean(abs(actual - preds)) We can then use stochastic gradient descent to autome the tweaking of a, b and c to minimise the loss.\nWe can store the parameters as a tensor, then pytorch will calculate gradients of the loss function based on that tensor.\nabc = torch.tensor([1.1,1.1,1.1]) \nabc.requires_grad_()  # Modifies abc in place so that it will include gradient calculations on anything which uses abc downstream\nloss = quad_mae(abc)  # `loss` uses abc so pytorch will give the gradient of loss too\nloss.backward()  # Back-propagate the gradients\nabc.grad  # Returns a tensor of the loss gradients\nWe can then take a “small step” in the direction that will decrease the gradient. The size of the step should be proportional to the size of the gradient. We define a learning rate hyperparameter to determine how much to scale the gradients by.\nlearning_rate = 0.01\nabc -= abc.grad * learning_rate\nloss = quad_mae(abc)  # The loss should now be smaller\nWe can repeat this process to take multiple steps to minimise the gradient.\nfor step in range(10):\n    loss = quad_mae(abc)\n    loss.backward()\n    with torch.no_grad():\n        abc -= abc.grad * learning_rate\n    print(f'step={step}; loss={loss:.2f}')\nThe learning rate should decrease as we get closer to the minimum to ensure we don’t overshoot the minimum and increase the loss. A learning rate schedule can be specified to do this."
  },
  {
    "objectID": "posts/ml/fastai/lesson3/lesson.html#fit-a-deep-learning-model",
    "href": "posts/ml/fastai/lesson3/lesson.html#fit-a-deep-learning-model",
    "title": "FastAI Lesson 3: How Does a Neural Network Learn?",
    "section": "",
    "text": "For deep learning, the premise is the same but instead of quadratic functions, we fit ReLUs and other non-linear functions.\nUniversal approximation theorem states that this is infinitely expressive if enough ReLUs (or other non-linear units) are combined. This means we can learn any computable function.\nA ReLU is essentially a linear function with the negative values clipped to 0.\ndef relu(m,c,x):\n    y = m * x + c\n    return np.clip(y, 0.)\nThis is all that deep learning is! All we need is:\n\nA model - a bunch of ReLUs combined will be flexible\nA loss function - mean absolute error between the actual data values and the values predicted by the model\nAn optimiser - stochastic gradient descent can start from random weights to incrementally improve the loss until we get a “good enough” fit\n\nWe just need enough time and data. There are a few hacks to decrease the time and data required:\n\nData augmentation\nRunning on GPUs to parallelise matrix multiplications\nConvolutions to skip over values to reduce the number of matrix multiplications required\nTransfer learning - initialise with parameters from another pre-trained model instead of random weights.\n\nThis spreadsheet from the homework task is a worked example of manually training a multivariate linear model, then extending that to a neural network summing two ReLUs."
  },
  {
    "objectID": "posts/ml/fastai/lesson3/lesson.html#references",
    "href": "posts/ml/fastai/lesson3/lesson.html#references",
    "title": "FastAI Lesson 3: How Does a Neural Network Learn?",
    "section": "",
    "text": "Course lesson page\nComparison of computer vision models\n“How does a neural network really work?” notebook"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html",
    "href": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html",
    "title": "Generative AI: Deep Learning Foundations",
    "section": "",
    "text": "Notes on deep learning concepts in the context of generative AI.\n\n\n\nDeep learning is a class of ML algorithms that use multiple stacked layers of processing units to learn high-level representations from unstructured data.\n\nStructured data is naturally arranged into columns of features. Think of relational database tables.\nBy contrast, unstructured data refers to any data that is not structured in tabular format. E.g. images, audio, text. Individual pixels – or frequencies or words – are not informative alone, but higher-level representations are, so we aim to learn these.\n\n\n\nMost deep learning systems in practice are ANNs,so deep learning has become synonymous with deep deural networks, vut there are other forms such as deep belief networks.\nA neural network where all adjacent layers are fully connected is called a Multilayer Perceptron (MLP) for historical reasons.\nBatches of inputs (e.g. images) are passed through and the predicted outputs are compared to the ground truth. This gives a loss function.\nThe prediction error is then back propagated through the network to adjust the weights incrementally.\nThis allows the model to learn features without manual feature engineering. Earlier layers learn low-level features (like edges) and these are combined in intermediate leayers to learn features like teeth, through to later layers which learning high-level representations like smiling.\n\n\n\nWe will implement an MLP for a supervised (read: not generative) learning problem to classify images.\nThe following uses the CIFAR-10 dataset.\n\n\nLoad the dataset, then scale pixel values to be in the 0 to 1 range and one-hot encode the labels.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras import layers, models, optimizers, utils, datasets\n\n\n# Load the data set\n(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n\n# Scale pixel values to be between 0 and 1.\nx_train = x_train.astype(\"float32\") / 255.\nx_test = x_test.astype(\"float32\") / 255.\n\n# One-hot encode the labels. The CIFAR-10 dataset contains 10 categories.\nNUM_CLASSES = 10\ny_train = utils.to_categorical(y_train, NUM_CLASSES)\ny_test = utils.to_categorical(y_test, NUM_CLASSES)\n\n\nDownloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n170498071/170498071 [==============================] - 64s 0us/step\n\n\n\nplt.imshow(x_train[1])\n\n\n\n\n\n\n\n\n\nprint(y_train[:5])\n\n[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n\n\n\n\n\nThe model is a series of fully connected dense layers.\n\n\nCode\nINPUT_SHAPE = (32, 32, 3,)\n\ninput_layer = layers.Input(INPUT_SHAPE)\nx = layers.Flatten()(input_layer)\nx = layers.Dense(units=200, activation='relu')(x)\nx = layers.Dense(units=150, activation='relu')(x)\noutput_layer = layers.Dense(units=10, activation='softmax')(x)\n\nmodel = models.Model(input_layer, output_layer)\n\n\nThe nonlinear activation function is critical, as this ensures the model is able to learn complex functions of the input rather than simply a linear combination of inputs.\nEach unit within a given layer has a bias term that alwways output 1, which ensures that the output of a unit can be non-zero even if the inputs were all 0. This helps prevent vanishing gradients.\nThe number of parameters in the 200-unit Dense layer is thus: \\(200 * (3072 + 1) = 614600\\)\n\nmodel.summary()\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n                                                                 \n flatten (Flatten)           (None, 3072)              0         \n                                                                 \n dense (Dense)               (None, 200)               614600    \n                                                                 \n dense_1 (Dense)             (None, 150)               30150     \n                                                                 \n dense_2 (Dense)             (None, 10)                1510      \n                                                                 \n=================================================================\nTotal params: 646260 (2.47 MB)\nTrainable params: 646260 (2.47 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\nTo train the model, we require an optimizer and a loss function.\n\n\nThis is used by the network to compare predicted outputs \\(p_i\\) with ground truth labels \\(y_i\\).\nFor regression tasks use mean-squared error: \\[\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - p_i)^2\n\\]\nFor classification tasks with exclusive categories uses categorical cross-entropy: \\[\nCCE = - \\sum_{i=1}^{n} y_i \\log(p_i)\n\\]\nFor classification tasks with binary one output unit or if an observation can belong to multiple classes simultaneously use binary cross-entropy: \\[\nBCE = - \\frac{1}{n} \\sum_{i=1}^{n} (y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) )\n\\]\n\n\n\nThis is the algorithm that updates the weights of the model based on the loss function.\nAdaptive Moment Estimation (ADAM) or Root Mean Squared Propagation (RMSProp) are commonly used.\nThe learning rate is the hyperparameter that is most likely to need tweaking.\n\n\n\nThis determines gow many images are shown to the model in each training step.\nThe larger the batch size, the more stable the gradient calculation but at the expense of a slower training step.\n\nIt is recommend to increase the batch size as training progresses\n\n\n\nCode\n# WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n# opt = optimizers.Adam(learning_rate=0.0005)\nopt = optimizers.legacy.Adam(learning_rate=0.0005)\n\nmodel.compile(\n    loss=\"categorical_crossentropy\",\n    optimizer=opt,\n    metrics=[\"accuracy\"]\n)\n\n\n\n\nCode\nmodel.fit(x_train, y_train, batch_size=32, epochs=10, shuffle=True)\n\n\nEpoch 1/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.8519 - accuracy: 0.3320\nEpoch 2/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.6692 - accuracy: 0.4044\nEpoch 3/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.5840 - accuracy: 0.4360\nEpoch 4/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.5353 - accuracy: 0.4532\nEpoch 5/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4925 - accuracy: 0.4711\nEpoch 6/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4675 - accuracy: 0.4764\nEpoch 7/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4386 - accuracy: 0.4878\nEpoch 8/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4146 - accuracy: 0.4971\nEpoch 9/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.3917 - accuracy: 0.5055\nEpoch 10/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.3691 - accuracy: 0.5124\n\n\n&lt;keras.src.callbacks.History at 0x149e41610&gt;\n\n\n\n\n\n\nUse the model to predict the classification of unseen images from the test set.\nEvaluate the model loss and accuracy over the test set:\n\n\nCode\nmodel.evaluate(x_test, y_test)\n\n\n313/313 [==============================] - 0s 560us/step - loss: 1.4571 - accuracy: 0.4813\n\n\n[1.4570728540420532, 0.4812999963760376]\n\n\n\n\nCode\n# Look at the predictions and ground truth labels from the test set\nCLASSES = np.array(\n    [\n        \"airplane\",\n        \"automobile\",\n        \"bird\",\n        \"cat\",\n        \"deer\",\n        \"dog\",\n        \"frog\",\n        \"horse\",\n        \"ship\",\n        \"truck\",\n    ]\n)\n\ny_pred = model.predict(x_test)\ny_pred_single = CLASSES[np.argmax(y_pred, axis=1)]\ny_test_single = CLASSES[np.argmax(y_test, axis=1)]\n\n\n313/313 [==============================] - 0s 493us/step\n\n\nPlot a comparison of a random selection of test images.\n\n\nCode\nN_IMAGES = 10\nindices = np.random.choice(range(len(x_test)), N_IMAGES)\n\nfig = plt.figure(figsize=(15, 3))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i, idx in enumerate(indices):\n    img = x_test[idx]\n    ax = fig.add_subplot(1, N_IMAGES, i + 1)\n    ax.axis(\"off\")\n    ax.text(\n        0.5,\n        -0.35,\n        \"pred = \" + str(y_pred_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.text(\n        0.5,\n        -0.7,\n        \"act = \" + str(y_test_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.imshow(img)\n\n\n\n\n\n\n\n\n\n\n\n\n\nA simple MLP does not take into account the spatial structure of images, so we can improve performance by using a convolutional layer.\nThis slides a kernel or filter (say, a 3x3 window) over the image and multiply the values elementwise.\nA convolutional layer is a collection of kernels where the (initially random) weights are learned through training.\n\n\nThis is the step size used when moving the kernel window across the image.\nThis determines the output size of the layer. For example, a stride=2 layer will half the height and width of the input image.\n\n\n\nWhen stride=1, we may still end up with a smaller images as the kernel does not overhang the edges of the image.\nUsing padding='same' adds 0s around the border to ensure that the output size is the same as the input size in this case (i.e. it lets the kernel overhang the edges of the original image). This helps to keep track of the size of the tensor.\n\n\n\nThe output of a convolutional layer is another 4-dimensional tensor with shape (batch_size, height, width, num_kernels).\nSo we can stack convolutional layers in series to learn higher-level representations.\n\n\n\nThe exploding gradient problem is common in deep learning: when errors are back-propagated, the gradient values in earlier layers can grow exponentially large.\nIn practice, this can cause overflow errors resulting in the loss function returning NaN.\nThe input layer is scaled, so we may naively expect the activations of all future layers to be well-scaled too. This may initially be true, but as training moves the weights further from their initial values the assumption of outputs being well-scaled breaks down. This is called covariate shift.\nBatch normalization z-scores each input channel, i.e. it subtract the mean and divides by the standard deviation of each mini-batch \\(B\\): \\[\n\\hat{x_i} = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_{B}^2 + \\epsilon}}\n\\]\nWe then learn two parameters, scale \\(\\gamma\\) and shift \\(\\beta\\). Then the output of the BatchNormalization layer is: \\[\ny_i = \\gamma \\hat{x_i} + \\beta\n\\]\nAt test time we may predict only a single image, so the batch normalization value would not be meaningful. Instead, the BatchNormalization layer also stores the moving average of \\(\\gamma\\) and \\(\\beta\\) as tow additional (derived therefore non-trainable) parameters. These moving average values are used at test time.\nBatch normalization has the added benefit of reducing overfitting in practice.\nUsually, batch normalization layers are placed before activation layers, although this is a matter of preference. The acronym BAD can be helpful to remember the general order: Batchnorm, Activation, Dropout.\n\n\n\nWe want to ensure that the model generalises well to unseen data, so it cannot rely on any single layer or neuron too heavily. Dropout acts as a regularization technique to prevent overfitting.\nEach Dropout layer chooses a random set of units from the previous layer and sets their value to 0\nAt test time the Dropout layer does not drop any connections, utilising the full network.\n\n\n\n\n\n\nThis is the same data set as the MLP case so we can re-use the same steps.\n\n\n\nThis is a sequence of stacks containing Conv2D layers with nonlinear acitvation functions.\n\n\nCode\ninput_layer = layers.Input((32, 32, 3))\n\n# Stack 1\nx = layers.Conv2D(filters=32, kernel_size=3, strides=1, padding=\"same\")(\n    input_layer\n)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Stack 2\nx = layers.Conv2D(filters=32, kernel_size=3, strides=2, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Stack 3\nx = layers.Conv2D(filters=64, kernel_size=3, strides=1, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Stack 4\nx = layers.Conv2D(filters=64, kernel_size=3, strides=2, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Dense final layers\nx = layers.Flatten()(x)\nx = layers.Dense(128)(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\nx = layers.Dropout(rate=0.5)(x)\nx = layers.Dense(NUM_CLASSES)(x)\n\n# Output\noutput_layer = layers.Activation(\"softmax\")(x)\nmodel = models.Model(input_layer, output_layer)\n\n\n\nmodel.summary()\n\nModel: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_4 (InputLayer)        [(None, 32, 32, 3)]       0         \n                                                                 \n conv2d_4 (Conv2D)           (None, 32, 32, 32)        896       \n                                                                 \n batch_normalization_5 (Bat  (None, 32, 32, 32)        128       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_5 (LeakyReLU)   (None, 32, 32, 32)        0         \n                                                                 \n conv2d_5 (Conv2D)           (None, 16, 16, 32)        9248      \n                                                                 \n batch_normalization_6 (Bat  (None, 16, 16, 32)        128       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_6 (LeakyReLU)   (None, 16, 16, 32)        0         \n                                                                 \n conv2d_6 (Conv2D)           (None, 16, 16, 64)        18496     \n                                                                 \n batch_normalization_7 (Bat  (None, 16, 16, 64)        256       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_7 (LeakyReLU)   (None, 16, 16, 64)        0         \n                                                                 \n conv2d_7 (Conv2D)           (None, 8, 8, 64)          36928     \n                                                                 \n batch_normalization_8 (Bat  (None, 8, 8, 64)          256       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_8 (LeakyReLU)   (None, 8, 8, 64)          0         \n                                                                 \n flatten_2 (Flatten)         (None, 4096)              0         \n                                                                 \n dense_5 (Dense)             (None, 128)               524416    \n                                                                 \n batch_normalization_9 (Bat  (None, 128)               512       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_9 (LeakyReLU)   (None, 128)               0         \n                                                                 \n dropout_1 (Dropout)         (None, 128)               0         \n                                                                 \n dense_6 (Dense)             (None, 10)                1290      \n                                                                 \n activation_1 (Activation)   (None, 10)                0         \n                                                                 \n=================================================================\nTotal params: 592554 (2.26 MB)\nTrainable params: 591914 (2.26 MB)\nNon-trainable params: 640 (2.50 KB)\n_________________________________________________________________\n\n\n\n\n\nThis is identical to the previous MLP model\n\n\nCode\n# WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n# opt = optimizers.Adam(learning_rate=0.0005)\nopt = optimizers.legacy.Adam(learning_rate=0.0005)\n\nmodel.compile(\n    loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"]\n)\n\n\n\n\nCode\nmodel.fit(\n    x_train,\n    y_train,\n    batch_size=32,\n    epochs=10,\n    shuffle=True,\n    validation_data=(x_test, y_test),\n)\n\n\nEpoch 1/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 1.5393 - accuracy: 0.4599 - val_loss: 1.5656 - val_accuracy: 0.4825\nEpoch 2/10\n1563/1563 [==============================] - 31s 20ms/step - loss: 1.1390 - accuracy: 0.5980 - val_loss: 1.2575 - val_accuracy: 0.5584\nEpoch 3/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 0.9980 - accuracy: 0.6515 - val_loss: 0.9970 - val_accuracy: 0.6513\nEpoch 4/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 0.9146 - accuracy: 0.6798 - val_loss: 0.9783 - val_accuracy: 0.6613\nEpoch 5/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.8515 - accuracy: 0.7044 - val_loss: 0.8269 - val_accuracy: 0.7094\nEpoch 6/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 0.7940 - accuracy: 0.7230 - val_loss: 0.8417 - val_accuracy: 0.7102\nEpoch 7/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.7511 - accuracy: 0.7362 - val_loss: 0.8542 - val_accuracy: 0.7044\nEpoch 8/10\n1563/1563 [==============================] - 29s 18ms/step - loss: 0.7167 - accuracy: 0.7495 - val_loss: 0.8554 - val_accuracy: 0.7083\nEpoch 9/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.6833 - accuracy: 0.7622 - val_loss: 1.0474 - val_accuracy: 0.6651\nEpoch 10/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.6510 - accuracy: 0.7726 - val_loss: 0.8449 - val_accuracy: 0.7020\n\n\n&lt;keras.src.callbacks.History at 0x14aa81d50&gt;\n\n\n\n\n\nAs before, we use the model to predict the classification of unseen images from the test set.\nEvaluate the model loss and accuracy over the test set:\n\n\nCode\nmodel.evaluate(x_test, y_test)\n\n\n313/313 [==============================] - 2s 5ms/step - loss: 0.8449 - accuracy: 0.7020\n\n\n[0.8448793888092041, 0.7020000219345093]\n\n\n\n\nCode\ny_pred = model.predict(x_test)\ny_pred_single = CLASSES[np.argmax(y_pred, axis=1)]\ny_test_single = CLASSES[np.argmax(y_test, axis=1)]\n\n\n313/313 [==============================] - 2s 5ms/step\n\n\nPlot a comparison of a random selection of test images.\n\n\nCode\nN_IMAGES = 10\nindices = np.random.choice(range(len(x_test)), N_IMAGES)\n\nfig = plt.figure(figsize=(15, 3))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i, idx in enumerate(indices):\n    img = x_test[idx]\n    ax = fig.add_subplot(1, N_IMAGES, i + 1)\n    ax.axis(\"off\")\n    ax.text(\n        0.5,\n        -0.35,\n        \"pred = \" + str(y_pred_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.text(\n        0.5,\n        -0.7,\n        \"act = \" + str(y_test_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.imshow(img)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 2 of Generative Deep Learning by David Foster."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#what-is-deep-learning",
    "href": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#what-is-deep-learning",
    "title": "Generative AI: Deep Learning Foundations",
    "section": "",
    "text": "Deep learning is a class of ML algorithms that use multiple stacked layers of processing units to learn high-level representations from unstructured data.\n\nStructured data is naturally arranged into columns of features. Think of relational database tables.\nBy contrast, unstructured data refers to any data that is not structured in tabular format. E.g. images, audio, text. Individual pixels – or frequencies or words – are not informative alone, but higher-level representations are, so we aim to learn these."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#deep-neural-networks",
    "href": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#deep-neural-networks",
    "title": "Generative AI: Deep Learning Foundations",
    "section": "",
    "text": "Most deep learning systems in practice are ANNs,so deep learning has become synonymous with deep deural networks, vut there are other forms such as deep belief networks.\nA neural network where all adjacent layers are fully connected is called a Multilayer Perceptron (MLP) for historical reasons.\nBatches of inputs (e.g. images) are passed through and the predicted outputs are compared to the ground truth. This gives a loss function.\nThe prediction error is then back propagated through the network to adjust the weights incrementally.\nThis allows the model to learn features without manual feature engineering. Earlier layers learn low-level features (like edges) and these are combined in intermediate leayers to learn features like teeth, through to later layers which learning high-level representations like smiling."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#building-a-multilayer-perceptron",
    "href": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#building-a-multilayer-perceptron",
    "title": "Generative AI: Deep Learning Foundations",
    "section": "",
    "text": "We will implement an MLP for a supervised (read: not generative) learning problem to classify images.\nThe following uses the CIFAR-10 dataset.\n\n\nLoad the dataset, then scale pixel values to be in the 0 to 1 range and one-hot encode the labels.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras import layers, models, optimizers, utils, datasets\n\n\n# Load the data set\n(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n\n# Scale pixel values to be between 0 and 1.\nx_train = x_train.astype(\"float32\") / 255.\nx_test = x_test.astype(\"float32\") / 255.\n\n# One-hot encode the labels. The CIFAR-10 dataset contains 10 categories.\nNUM_CLASSES = 10\ny_train = utils.to_categorical(y_train, NUM_CLASSES)\ny_test = utils.to_categorical(y_test, NUM_CLASSES)\n\n\nDownloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n170498071/170498071 [==============================] - 64s 0us/step\n\n\n\nplt.imshow(x_train[1])\n\n\n\n\n\n\n\n\n\nprint(y_train[:5])\n\n[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n\n\n\n\n\nThe model is a series of fully connected dense layers.\n\n\nCode\nINPUT_SHAPE = (32, 32, 3,)\n\ninput_layer = layers.Input(INPUT_SHAPE)\nx = layers.Flatten()(input_layer)\nx = layers.Dense(units=200, activation='relu')(x)\nx = layers.Dense(units=150, activation='relu')(x)\noutput_layer = layers.Dense(units=10, activation='softmax')(x)\n\nmodel = models.Model(input_layer, output_layer)\n\n\nThe nonlinear activation function is critical, as this ensures the model is able to learn complex functions of the input rather than simply a linear combination of inputs.\nEach unit within a given layer has a bias term that alwways output 1, which ensures that the output of a unit can be non-zero even if the inputs were all 0. This helps prevent vanishing gradients.\nThe number of parameters in the 200-unit Dense layer is thus: \\(200 * (3072 + 1) = 614600\\)\n\nmodel.summary()\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n                                                                 \n flatten (Flatten)           (None, 3072)              0         \n                                                                 \n dense (Dense)               (None, 200)               614600    \n                                                                 \n dense_1 (Dense)             (None, 150)               30150     \n                                                                 \n dense_2 (Dense)             (None, 10)                1510      \n                                                                 \n=================================================================\nTotal params: 646260 (2.47 MB)\nTrainable params: 646260 (2.47 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\nTo train the model, we require an optimizer and a loss function.\n\n\nThis is used by the network to compare predicted outputs \\(p_i\\) with ground truth labels \\(y_i\\).\nFor regression tasks use mean-squared error: \\[\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - p_i)^2\n\\]\nFor classification tasks with exclusive categories uses categorical cross-entropy: \\[\nCCE = - \\sum_{i=1}^{n} y_i \\log(p_i)\n\\]\nFor classification tasks with binary one output unit or if an observation can belong to multiple classes simultaneously use binary cross-entropy: \\[\nBCE = - \\frac{1}{n} \\sum_{i=1}^{n} (y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) )\n\\]\n\n\n\nThis is the algorithm that updates the weights of the model based on the loss function.\nAdaptive Moment Estimation (ADAM) or Root Mean Squared Propagation (RMSProp) are commonly used.\nThe learning rate is the hyperparameter that is most likely to need tweaking.\n\n\n\nThis determines gow many images are shown to the model in each training step.\nThe larger the batch size, the more stable the gradient calculation but at the expense of a slower training step.\n\nIt is recommend to increase the batch size as training progresses\n\n\n\nCode\n# WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n# opt = optimizers.Adam(learning_rate=0.0005)\nopt = optimizers.legacy.Adam(learning_rate=0.0005)\n\nmodel.compile(\n    loss=\"categorical_crossentropy\",\n    optimizer=opt,\n    metrics=[\"accuracy\"]\n)\n\n\n\n\nCode\nmodel.fit(x_train, y_train, batch_size=32, epochs=10, shuffle=True)\n\n\nEpoch 1/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.8519 - accuracy: 0.3320\nEpoch 2/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.6692 - accuracy: 0.4044\nEpoch 3/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.5840 - accuracy: 0.4360\nEpoch 4/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.5353 - accuracy: 0.4532\nEpoch 5/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4925 - accuracy: 0.4711\nEpoch 6/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4675 - accuracy: 0.4764\nEpoch 7/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4386 - accuracy: 0.4878\nEpoch 8/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4146 - accuracy: 0.4971\nEpoch 9/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.3917 - accuracy: 0.5055\nEpoch 10/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.3691 - accuracy: 0.5124\n\n\n&lt;keras.src.callbacks.History at 0x149e41610&gt;\n\n\n\n\n\n\nUse the model to predict the classification of unseen images from the test set.\nEvaluate the model loss and accuracy over the test set:\n\n\nCode\nmodel.evaluate(x_test, y_test)\n\n\n313/313 [==============================] - 0s 560us/step - loss: 1.4571 - accuracy: 0.4813\n\n\n[1.4570728540420532, 0.4812999963760376]\n\n\n\n\nCode\n# Look at the predictions and ground truth labels from the test set\nCLASSES = np.array(\n    [\n        \"airplane\",\n        \"automobile\",\n        \"bird\",\n        \"cat\",\n        \"deer\",\n        \"dog\",\n        \"frog\",\n        \"horse\",\n        \"ship\",\n        \"truck\",\n    ]\n)\n\ny_pred = model.predict(x_test)\ny_pred_single = CLASSES[np.argmax(y_pred, axis=1)]\ny_test_single = CLASSES[np.argmax(y_test, axis=1)]\n\n\n313/313 [==============================] - 0s 493us/step\n\n\nPlot a comparison of a random selection of test images.\n\n\nCode\nN_IMAGES = 10\nindices = np.random.choice(range(len(x_test)), N_IMAGES)\n\nfig = plt.figure(figsize=(15, 3))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i, idx in enumerate(indices):\n    img = x_test[idx]\n    ax = fig.add_subplot(1, N_IMAGES, i + 1)\n    ax.axis(\"off\")\n    ax.text(\n        0.5,\n        -0.35,\n        \"pred = \" + str(y_pred_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.text(\n        0.5,\n        -0.7,\n        \"act = \" + str(y_test_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.imshow(img)"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#cnn-concepts-and-layers",
    "href": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#cnn-concepts-and-layers",
    "title": "Generative AI: Deep Learning Foundations",
    "section": "",
    "text": "A simple MLP does not take into account the spatial structure of images, so we can improve performance by using a convolutional layer.\nThis slides a kernel or filter (say, a 3x3 window) over the image and multiply the values elementwise.\nA convolutional layer is a collection of kernels where the (initially random) weights are learned through training.\n\n\nThis is the step size used when moving the kernel window across the image.\nThis determines the output size of the layer. For example, a stride=2 layer will half the height and width of the input image.\n\n\n\nWhen stride=1, we may still end up with a smaller images as the kernel does not overhang the edges of the image.\nUsing padding='same' adds 0s around the border to ensure that the output size is the same as the input size in this case (i.e. it lets the kernel overhang the edges of the original image). This helps to keep track of the size of the tensor.\n\n\n\nThe output of a convolutional layer is another 4-dimensional tensor with shape (batch_size, height, width, num_kernels).\nSo we can stack convolutional layers in series to learn higher-level representations.\n\n\n\nThe exploding gradient problem is common in deep learning: when errors are back-propagated, the gradient values in earlier layers can grow exponentially large.\nIn practice, this can cause overflow errors resulting in the loss function returning NaN.\nThe input layer is scaled, so we may naively expect the activations of all future layers to be well-scaled too. This may initially be true, but as training moves the weights further from their initial values the assumption of outputs being well-scaled breaks down. This is called covariate shift.\nBatch normalization z-scores each input channel, i.e. it subtract the mean and divides by the standard deviation of each mini-batch \\(B\\): \\[\n\\hat{x_i} = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_{B}^2 + \\epsilon}}\n\\]\nWe then learn two parameters, scale \\(\\gamma\\) and shift \\(\\beta\\). Then the output of the BatchNormalization layer is: \\[\ny_i = \\gamma \\hat{x_i} + \\beta\n\\]\nAt test time we may predict only a single image, so the batch normalization value would not be meaningful. Instead, the BatchNormalization layer also stores the moving average of \\(\\gamma\\) and \\(\\beta\\) as tow additional (derived therefore non-trainable) parameters. These moving average values are used at test time.\nBatch normalization has the added benefit of reducing overfitting in practice.\nUsually, batch normalization layers are placed before activation layers, although this is a matter of preference. The acronym BAD can be helpful to remember the general order: Batchnorm, Activation, Dropout.\n\n\n\nWe want to ensure that the model generalises well to unseen data, so it cannot rely on any single layer or neuron too heavily. Dropout acts as a regularization technique to prevent overfitting.\nEach Dropout layer chooses a random set of units from the previous layer and sets their value to 0\nAt test time the Dropout layer does not drop any connections, utilising the full network."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#building-a-convolutional-neural-network",
    "href": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#building-a-convolutional-neural-network",
    "title": "Generative AI: Deep Learning Foundations",
    "section": "",
    "text": "This is the same data set as the MLP case so we can re-use the same steps.\n\n\n\nThis is a sequence of stacks containing Conv2D layers with nonlinear acitvation functions.\n\n\nCode\ninput_layer = layers.Input((32, 32, 3))\n\n# Stack 1\nx = layers.Conv2D(filters=32, kernel_size=3, strides=1, padding=\"same\")(\n    input_layer\n)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Stack 2\nx = layers.Conv2D(filters=32, kernel_size=3, strides=2, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Stack 3\nx = layers.Conv2D(filters=64, kernel_size=3, strides=1, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Stack 4\nx = layers.Conv2D(filters=64, kernel_size=3, strides=2, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Dense final layers\nx = layers.Flatten()(x)\nx = layers.Dense(128)(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\nx = layers.Dropout(rate=0.5)(x)\nx = layers.Dense(NUM_CLASSES)(x)\n\n# Output\noutput_layer = layers.Activation(\"softmax\")(x)\nmodel = models.Model(input_layer, output_layer)\n\n\n\nmodel.summary()\n\nModel: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_4 (InputLayer)        [(None, 32, 32, 3)]       0         \n                                                                 \n conv2d_4 (Conv2D)           (None, 32, 32, 32)        896       \n                                                                 \n batch_normalization_5 (Bat  (None, 32, 32, 32)        128       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_5 (LeakyReLU)   (None, 32, 32, 32)        0         \n                                                                 \n conv2d_5 (Conv2D)           (None, 16, 16, 32)        9248      \n                                                                 \n batch_normalization_6 (Bat  (None, 16, 16, 32)        128       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_6 (LeakyReLU)   (None, 16, 16, 32)        0         \n                                                                 \n conv2d_6 (Conv2D)           (None, 16, 16, 64)        18496     \n                                                                 \n batch_normalization_7 (Bat  (None, 16, 16, 64)        256       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_7 (LeakyReLU)   (None, 16, 16, 64)        0         \n                                                                 \n conv2d_7 (Conv2D)           (None, 8, 8, 64)          36928     \n                                                                 \n batch_normalization_8 (Bat  (None, 8, 8, 64)          256       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_8 (LeakyReLU)   (None, 8, 8, 64)          0         \n                                                                 \n flatten_2 (Flatten)         (None, 4096)              0         \n                                                                 \n dense_5 (Dense)             (None, 128)               524416    \n                                                                 \n batch_normalization_9 (Bat  (None, 128)               512       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_9 (LeakyReLU)   (None, 128)               0         \n                                                                 \n dropout_1 (Dropout)         (None, 128)               0         \n                                                                 \n dense_6 (Dense)             (None, 10)                1290      \n                                                                 \n activation_1 (Activation)   (None, 10)                0         \n                                                                 \n=================================================================\nTotal params: 592554 (2.26 MB)\nTrainable params: 591914 (2.26 MB)\nNon-trainable params: 640 (2.50 KB)\n_________________________________________________________________\n\n\n\n\n\nThis is identical to the previous MLP model\n\n\nCode\n# WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n# opt = optimizers.Adam(learning_rate=0.0005)\nopt = optimizers.legacy.Adam(learning_rate=0.0005)\n\nmodel.compile(\n    loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"]\n)\n\n\n\n\nCode\nmodel.fit(\n    x_train,\n    y_train,\n    batch_size=32,\n    epochs=10,\n    shuffle=True,\n    validation_data=(x_test, y_test),\n)\n\n\nEpoch 1/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 1.5393 - accuracy: 0.4599 - val_loss: 1.5656 - val_accuracy: 0.4825\nEpoch 2/10\n1563/1563 [==============================] - 31s 20ms/step - loss: 1.1390 - accuracy: 0.5980 - val_loss: 1.2575 - val_accuracy: 0.5584\nEpoch 3/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 0.9980 - accuracy: 0.6515 - val_loss: 0.9970 - val_accuracy: 0.6513\nEpoch 4/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 0.9146 - accuracy: 0.6798 - val_loss: 0.9783 - val_accuracy: 0.6613\nEpoch 5/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.8515 - accuracy: 0.7044 - val_loss: 0.8269 - val_accuracy: 0.7094\nEpoch 6/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 0.7940 - accuracy: 0.7230 - val_loss: 0.8417 - val_accuracy: 0.7102\nEpoch 7/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.7511 - accuracy: 0.7362 - val_loss: 0.8542 - val_accuracy: 0.7044\nEpoch 8/10\n1563/1563 [==============================] - 29s 18ms/step - loss: 0.7167 - accuracy: 0.7495 - val_loss: 0.8554 - val_accuracy: 0.7083\nEpoch 9/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.6833 - accuracy: 0.7622 - val_loss: 1.0474 - val_accuracy: 0.6651\nEpoch 10/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.6510 - accuracy: 0.7726 - val_loss: 0.8449 - val_accuracy: 0.7020\n\n\n&lt;keras.src.callbacks.History at 0x14aa81d50&gt;\n\n\n\n\n\nAs before, we use the model to predict the classification of unseen images from the test set.\nEvaluate the model loss and accuracy over the test set:\n\n\nCode\nmodel.evaluate(x_test, y_test)\n\n\n313/313 [==============================] - 2s 5ms/step - loss: 0.8449 - accuracy: 0.7020\n\n\n[0.8448793888092041, 0.7020000219345093]\n\n\n\n\nCode\ny_pred = model.predict(x_test)\ny_pred_single = CLASSES[np.argmax(y_pred, axis=1)]\ny_test_single = CLASSES[np.argmax(y_test, axis=1)]\n\n\n313/313 [==============================] - 2s 5ms/step\n\n\nPlot a comparison of a random selection of test images.\n\n\nCode\nN_IMAGES = 10\nindices = np.random.choice(range(len(x_test)), N_IMAGES)\n\nfig = plt.figure(figsize=(15, 3))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i, idx in enumerate(indices):\n    img = x_test[idx]\n    ax = fig.add_subplot(1, N_IMAGES, i + 1)\n    ax.axis(\"off\")\n    ax.text(\n        0.5,\n        -0.35,\n        \"pred = \" + str(y_pred_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.text(\n        0.5,\n        -0.7,\n        \"act = \" + str(y_test_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.imshow(img)"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#references",
    "href": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#references",
    "title": "Generative AI: Deep Learning Foundations",
    "section": "",
    "text": "Chapter 2 of Generative Deep Learning by David Foster."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson4/chapter4.html",
    "href": "posts/ml/generative_deep_learning/lesson4/chapter4.html",
    "title": "Generative AI: GANs",
    "section": "",
    "text": "Notes on Generative Adversarial Networks (GANs).\n\n\n\n\n\n\nStory Time\n\n\n\nImagine a forger trying to forge £20 notes and the police trying to stop them.\nThe police learn to spot the fakes. But then the forger learns to improve their forging skills to make better fakes.\nThis goes back and forth. With each iteration, the forger keeps getting better but then the police learn to spot these more sophisticated fakes.\nThe results in a forger (generator) learning to create convincing fakes and the popo (discriminator) learning to spot fakes.\n\n\n\n\nThe idea of GANs is that we can train two competing models:\n\nThe generator tries to convert random noise into convincing observations.\nThe discriminator tries to predict whether an observation came from the original training dataset or is a “fake”.\n\nWe initialise both as random models; the generator outputs noise and the discriminator predicts randomly. We then alternate the training of the two networks so that the generator gets incrementally better at fooling the discriminator, then the discriminator gets incrementally better at spotting fakes.\n\n\n\n\n\nflowchart LR\n\n  A([Random noise]) --&gt; B[Generator] --&gt; C([Generated image]) \n\n  D([Image]) --&gt; E[Discriminator] --&gt; F([Prediction of realness probability])\n\n\n\n\n\n\n\n\n\nWe will implement a GAN to generate pictures of bricks.\n\n\nLoad image data of lego bricks. We will train a model that can generate novel lego brick images.\n\n\nThe original data is scaled from [0, 255].\nOften we will rescale this to [0, 1] so that we can use sigmoid activation functions.\nIn this case we will scale to [-1, 1] so that we can use tanh activation functions, which tend to give stronger gradients than sigmoid.\n\nfrom pathlib import Path\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras import (\n    layers,\n    models,\n    callbacks,\n    losses,\n    utils,\n    metrics,\n    optimizers,\n)\n\n\ndef sample_batch(dataset):\n    batch = dataset.take(1).get_single_element()\n    if isinstance(batch, tuple):\n        batch = batch[0]\n    return batch.numpy()\n\n\ndef display_images(\n    images, n=10, size=(20, 3), cmap=\"gray_r\", as_type=\"float32\", save_to=None\n):\n    \"\"\"Displays n random images from each one of the supplied arrays.\"\"\"\n    if images.max() &gt; 1.0:\n        images = images / 255.0\n    elif images.min() &lt; 0.0:\n        images = (images + 1.0) / 2.0\n\n    plt.figure(figsize=size)\n    for i in range(n):\n        _ = plt.subplot(1, n, i + 1)\n        plt.imshow(images[i].astype(as_type), cmap=cmap)\n        plt.axis(\"off\")\n\n    if save_to:\n        plt.savefig(save_to)\n        print(f\"\\nSaved to {save_to}\")\n\n    plt.show()\n\nModel and data parameters:\n\nDATA_DIR = Path(\"/Users/gurpreetjohl/workspace/datasets/lego-brick-images\")\n\nIMAGE_SIZE = 64\nCHANNELS = 1\nBATCH_SIZE = 128\nZ_DIM = 100\nEPOCHS = 100\nLOAD_MODEL = False\nADAM_BETA_1 = 0.5\nADAM_BETA_2 = 0.999\nLEARNING_RATE = 0.0002\nNOISE_PARAM = 0.1\n\nLoad and pre-process the training data:\n\ndef preprocess(img):\n    \"\"\"Normalize and reshape the images.\"\"\"\n    img = (tf.cast(img, \"float32\") - 127.5) / 127.5\n    return img\n\n\ntraining_data = utils.image_dataset_from_directory(\n    DATA_DIR / \"dataset\",\n    labels=None,\n    color_mode=\"grayscale\",\n    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    seed=42,\n    interpolation=\"bilinear\",\n)\ntrain = training_data.map(lambda x: preprocess(x))\n\nFound 40000 files belonging to 1 classes.\n\n\nSome sample input images:\n\ndisplay_images(sample_batch(train))\n\n\n\n\n\n\n\n\n\n\n\n\nThe goal of the discriminator is to predict whether an image is real or fake.\nThis is a supervised binary classification problem, so we can use CNN architecture with a single output node. We stack Conv2D layers with BatchNormalization, LeakyReLU and Dropout layers sandwiched between.\n\ndiscriminator_input = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS))\n\nx = layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(discriminator_input)\nx = layers.LeakyReLU(0.2)(x)\nx = layers.Dropout(0.3)(x)\n\nx = layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\nx = layers.Dropout(0.3)(x)\n\nx = layers.Conv2D(256, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\nx = layers.Dropout(0.3)(x)\n\nx = layers.Conv2D(512, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\nx = layers.Dropout(0.3)(x)\n\nx = layers.Conv2D(1, kernel_size=4, strides=1, padding=\"valid\", use_bias=False, activation=\"sigmoid\")(x)\ndiscriminator_output = layers.Flatten()(x)  # The shape is already 1x1 so no need for a Dense layer after this\n\ndiscriminator = models.Model(discriminator_input, discriminator_output)\ndiscriminator.summary()\n\nModel: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 64, 64, 1)]       0         \n                                                                 \n conv2d_5 (Conv2D)           (None, 32, 32, 64)        1024      \n                                                                 \n leaky_re_lu_4 (LeakyReLU)   (None, 32, 32, 64)        0         \n                                                                 \n dropout_4 (Dropout)         (None, 32, 32, 64)        0         \n                                                                 \n conv2d_6 (Conv2D)           (None, 16, 16, 128)       131072    \n                                                                 \n batch_normalization_3 (Bat  (None, 16, 16, 128)       512       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_5 (LeakyReLU)   (None, 16, 16, 128)       0         \n                                                                 \n dropout_5 (Dropout)         (None, 16, 16, 128)       0         \n                                                                 \n conv2d_7 (Conv2D)           (None, 8, 8, 256)         524288    \n                                                                 \n batch_normalization_4 (Bat  (None, 8, 8, 256)         1024      \n chNormalization)                                                \n                                                                 \n leaky_re_lu_6 (LeakyReLU)   (None, 8, 8, 256)         0         \n                                                                 \n dropout_6 (Dropout)         (None, 8, 8, 256)         0         \n                                                                 \n conv2d_8 (Conv2D)           (None, 4, 4, 512)         2097152   \n                                                                 \n batch_normalization_5 (Bat  (None, 4, 4, 512)         2048      \n chNormalization)                                                \n                                                                 \n leaky_re_lu_7 (LeakyReLU)   (None, 4, 4, 512)         0         \n                                                                 \n dropout_7 (Dropout)         (None, 4, 4, 512)         0         \n                                                                 \n conv2d_9 (Conv2D)           (None, 1, 1, 1)           8192      \n                                                                 \n flatten_1 (Flatten)         (None, 1)                 0         \n                                                                 \n=================================================================\nTotal params: 2765312 (10.55 MB)\nTrainable params: 2763520 (10.54 MB)\nNon-trainable params: 1792 (7.00 KB)\n_________________________________________________________________\n\n\n\n\n\nThe purpose of the generator is to turn random noise into convincing images.\nThe input is a vector sampled from a multivariate Normal distribution, and the output is an image of the same size as the training data.\nThe discriminator-generator relationship in a GAN is similar to that of the encoder-decoder relations in a VAE.\nThe architecture of the discriminator is similar to the discriminator but in reverse (like a decoder). We pass stack Conv2DTranspose layers with BatchNormalization and LeakyReLU layers sandwiched in between.\n\n\nWe use Conv2DTranspose layers to scale the image size up.\nAn alternative would be to use stacks of Upsampling2D and Conv2D layers, i.e. the following serves the same purpose as a Conv2DTranspose layer:\nx = layers.Upsampling2D(size=2)(x)\nx = layers.Conv2D(256, kernel_size=4, strides=1, padding=\"same\")(x)\nThe Upsampling2D layer simply repeats each row and column to double its size, then Conv2D applies a convolution.\nThe idea is similar with Conv2DTranspose, but the extra rows and columns are filled with zeros rather than repeated existing values.\nConv2DTranspose layers can result in checkerboard pattern artifacts. Both options are used in practice, so it is often helpful to experiment and see which gives better results.\n\ngenerator_input = layers.Input(shape=(Z_DIM,))\n\nx = layers.Reshape((1, 1, Z_DIM))(generator_input)  # Reshape the input vector so we can apply conv transpose operations to it\n\nx = layers.Conv2DTranspose(512, kernel_size=4, strides=1, padding=\"valid\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\n\nx = layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\n\nx = layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\n\nx = layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\n\ngenerator_output = layers.Conv2DTranspose(\n    CHANNELS,\n    kernel_size=4,\n    strides=2,\n    padding=\"same\",\n    use_bias=False,\n    activation=\"tanh\",\n)(x)\n\ngenerator = models.Model(generator_input, generator_output)\ngenerator.summary()\n\nModel: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_3 (InputLayer)        [(None, 100)]             0         \n                                                                 \n reshape (Reshape)           (None, 1, 1, 100)         0         \n                                                                 \n conv2d_transpose (Conv2DTr  (None, 4, 4, 512)         819200    \n anspose)                                                        \n                                                                 \n batch_normalization_6 (Bat  (None, 4, 4, 512)         2048      \n chNormalization)                                                \n                                                                 \n leaky_re_lu_8 (LeakyReLU)   (None, 4, 4, 512)         0         \n                                                                 \n conv2d_transpose_1 (Conv2D  (None, 8, 8, 256)         2097152   \n Transpose)                                                      \n                                                                 \n batch_normalization_7 (Bat  (None, 8, 8, 256)         1024      \n chNormalization)                                                \n                                                                 \n leaky_re_lu_9 (LeakyReLU)   (None, 8, 8, 256)         0         \n                                                                 \n conv2d_transpose_2 (Conv2D  (None, 16, 16, 128)       524288    \n Transpose)                                                      \n                                                                 \n batch_normalization_8 (Bat  (None, 16, 16, 128)       512       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_10 (LeakyReLU)  (None, 16, 16, 128)       0         \n                                                                 \n conv2d_transpose_3 (Conv2D  (None, 32, 32, 64)        131072    \n Transpose)                                                      \n                                                                 \n batch_normalization_9 (Bat  (None, 32, 32, 64)        256       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_11 (LeakyReLU)  (None, 32, 32, 64)        0         \n                                                                 \n conv2d_transpose_4 (Conv2D  (None, 64, 64, 1)         1024      \n Transpose)                                                      \n                                                                 \n=================================================================\nTotal params: 3576576 (13.64 MB)\nTrainable params: 3574656 (13.64 MB)\nNon-trainable params: 1920 (7.50 KB)\n_________________________________________________________________\n\n\n\n\n\n\nWe alternate between training the discriminator and generator. They are not trained simultaneously. We want the generated images to be predicted close to 1 because the generator is good, not because the discriminator is weak.\nFor the discriminator, we create a training set where some images are real images from the training data and some are outputs from the generator. This is then a supervised binary classification problem.\nFor the generator, we want a way of scoring each generated image on its realness so that we can optimise this. The discriminator provides exactly this. We pass the generated images through the discriminator to get probabilities. The generator wants to fool the discriminator, so ideally this would be a vector of 1s. So the loss function is the binary crossentropy between these probabilities and a vector of 1s.\n\nclass DCGAN(models.Model):\n    def __init__(self, discriminator, generator, latent_dim):\n        super(DCGAN, self).__init__()\n        self.discriminator = discriminator\n        self.generator = generator\n        self.latent_dim = latent_dim\n\n    def compile(self, d_optimizer, g_optimizer):\n        super(DCGAN, self).compile()\n        self.loss_fn = losses.BinaryCrossentropy()\n        self.d_optimizer = d_optimizer\n        self.g_optimizer = g_optimizer\n        self.d_loss_metric = metrics.Mean(name=\"d_loss\")\n        self.d_real_acc_metric = metrics.BinaryAccuracy(name=\"d_real_acc\")\n        self.d_fake_acc_metric = metrics.BinaryAccuracy(name=\"d_fake_acc\")\n        self.d_acc_metric = metrics.BinaryAccuracy(name=\"d_acc\")\n        self.g_loss_metric = metrics.Mean(name=\"g_loss\")\n        self.g_acc_metric = metrics.BinaryAccuracy(name=\"g_acc\")\n\n    @property\n    def metrics(self):\n        return [\n            self.d_loss_metric,\n            self.d_real_acc_metric,\n            self.d_fake_acc_metric,\n            self.d_acc_metric,\n            self.g_loss_metric,\n            self.g_acc_metric,\n        ]\n\n    def train_step(self, real_images):\n        # Sample random points in the latent space\n        batch_size = tf.shape(real_images)[0]\n        random_latent_vectors = tf.random.normal(\n            shape=(batch_size, self.latent_dim)\n        )\n\n        # Train the discriminator on fake images\n        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n            generated_images = self.generator(\n                random_latent_vectors, training=True\n            )\n\n            # Evaluate the discriminator on the real and fake images\n            real_predictions = self.discriminator(real_images, training=True)\n            fake_predictions = self.discriminator(generated_images, training=True)\n\n            real_labels = tf.ones_like(real_predictions)\n            real_noisy_labels = real_labels + NOISE_PARAM * tf.random.uniform(\n                tf.shape(real_predictions)\n            )\n            fake_labels = tf.zeros_like(fake_predictions)\n            fake_noisy_labels = fake_labels - NOISE_PARAM * tf.random.uniform(\n                tf.shape(fake_predictions)\n            )\n\n            # Calculate the losses\n            d_real_loss = self.loss_fn(real_noisy_labels, real_predictions)\n            d_fake_loss = self.loss_fn(fake_noisy_labels, fake_predictions)\n            d_loss = (d_real_loss + d_fake_loss) / 2.0\n\n            g_loss = self.loss_fn(real_labels, fake_predictions)\n\n        # Update gradients\n        gradients_of_discriminator = disc_tape.gradient(\n            d_loss, self.discriminator.trainable_variables\n        )\n        gradients_of_generator = gen_tape.gradient(\n            g_loss, self.generator.trainable_variables\n        )\n\n        self.d_optimizer.apply_gradients(\n            zip(gradients_of_discriminator, discriminator.trainable_variables)\n        )\n        self.g_optimizer.apply_gradients(\n            zip(gradients_of_generator, generator.trainable_variables)\n        )\n\n        # Update metrics\n        self.d_loss_metric.update_state(d_loss)\n        self.d_real_acc_metric.update_state(real_labels, real_predictions)\n        self.d_fake_acc_metric.update_state(fake_labels, fake_predictions)\n        self.d_acc_metric.update_state(\n            [real_labels, fake_labels], [real_predictions, fake_predictions]\n        )\n        self.g_loss_metric.update_state(g_loss)\n        self.g_acc_metric.update_state(real_labels, fake_predictions)\n\n        return {m.name: m.result() for m in self.metrics}\n\n\n# Create a DCGAN\ndcgan = DCGAN(\n    discriminator=discriminator, generator=generator, latent_dim=Z_DIM\n)\n\n\ndcgan.compile(\n    d_optimizer=optimizers.legacy.Adam(\n        learning_rate=LEARNING_RATE, beta_1=ADAM_BETA_1, beta_2=ADAM_BETA_2\n    ),\n    g_optimizer=optimizers.legacy.Adam(\n        learning_rate=LEARNING_RATE, beta_1=ADAM_BETA_1, beta_2=ADAM_BETA_2\n    ),\n)\ndcgan.fit(train,  epochs=EPOCHS)\n\nEpoch 1/100\n313/313 [==============================] - 459s 1s/step - d_loss: 0.0252 - d_real_acc: 0.9011 - d_fake_acc: 0.9013 - d_acc: 0.9012 - g_loss: 5.3464 - g_acc: 0.0987\nEpoch 2/100\n313/313 [==============================] - 459s 1s/step - d_loss: 0.0454 - d_real_acc: 0.8986 - d_fake_acc: 0.8997 - d_acc: 0.8992 - g_loss: 5.2642 - g_acc: 0.1002\nEpoch 3/100\n313/313 [==============================] - 458s 1s/step - d_loss: 0.0556 - d_real_acc: 0.8958 - d_fake_acc: 0.8975 - d_acc: 0.8967 - g_loss: 4.9679 - g_acc: 0.1025\nEpoch 4/100\n313/313 [==============================] - 467s 1s/step - d_loss: 0.0246 - d_real_acc: 0.9065 - d_fake_acc: 0.9091 - d_acc: 0.9078 - g_loss: 5.1611 - g_acc: 0.0909\nEpoch 5/100\n313/313 [==============================] - 470s 1s/step - d_loss: 0.0178 - d_real_acc: 0.9067 - d_fake_acc: 0.9088 - d_acc: 0.9078 - g_loss: 5.1731 - g_acc: 0.0912\nEpoch 6/100\n313/313 [==============================] - 463s 1s/step - d_loss: 0.0314 - d_real_acc: 0.9116 - d_fake_acc: 0.9105 - d_acc: 0.9110 - g_loss: 5.2774 - g_acc: 0.0895\nEpoch 7/100\n313/313 [==============================] - 559s 2s/step - d_loss: 0.0229 - d_real_acc: 0.9085 - d_fake_acc: 0.9079 - d_acc: 0.9082 - g_loss: 5.3445 - g_acc: 0.0921\nEpoch 8/100\n313/313 [==============================] - 467s 1s/step - d_loss: -0.0155 - d_real_acc: 0.9161 - d_fake_acc: 0.9161 - d_acc: 0.9161 - g_loss: 5.7091 - g_acc: 0.0839\nEpoch 9/100\n313/313 [==============================] - 438s 1s/step - d_loss: -0.0077 - d_real_acc: 0.9220 - d_fake_acc: 0.9224 - d_acc: 0.9222 - g_loss: 5.8731 - g_acc: 0.0776\nEpoch 10/100\n313/313 [==============================] - 468s 1s/step - d_loss: -0.0472 - d_real_acc: 0.9228 - d_fake_acc: 0.9241 - d_acc: 0.9234 - g_loss: 5.9693 - g_acc: 0.0759\nEpoch 11/100\n313/313 [==============================] - 430s 1s/step - d_loss: -0.0839 - d_real_acc: 0.9404 - d_fake_acc: 0.9424 - d_acc: 0.9414 - g_loss: 6.1212 - g_acc: 0.0576\nEpoch 12/100\n313/313 [==============================] - 457s 1s/step - d_loss: 0.0431 - d_real_acc: 0.9053 - d_fake_acc: 0.9046 - d_acc: 0.9049 - g_loss: 6.0708 - g_acc: 0.0954\nEpoch 13/100\n313/313 [==============================] - 448s 1s/step - d_loss: -0.0154 - d_real_acc: 0.9236 - d_fake_acc: 0.9244 - d_acc: 0.9240 - g_loss: 6.3106 - g_acc: 0.0756\nEpoch 14/100\n313/313 [==============================] - 432s 1s/step - d_loss: -0.0720 - d_real_acc: 0.9320 - d_fake_acc: 0.9342 - d_acc: 0.9331 - g_loss: 6.6509 - g_acc: 0.0658\nEpoch 15/100\n313/313 [==============================] - 443s 1s/step - d_loss: 0.0057 - d_real_acc: 0.9072 - d_fake_acc: 0.9097 - d_acc: 0.9085 - g_loss: 6.1399 - g_acc: 0.0903\nEpoch 16/100\n313/313 [==============================] - 427s 1s/step - d_loss: 0.0069 - d_real_acc: 0.9185 - d_fake_acc: 0.9167 - d_acc: 0.9176 - g_loss: 6.3255 - g_acc: 0.0833\nEpoch 17/100\n313/313 [==============================] - 439s 1s/step - d_loss: -0.0709 - d_real_acc: 0.9220 - d_fake_acc: 0.9239 - d_acc: 0.9230 - g_loss: 6.8108 - g_acc: 0.0761\nEpoch 18/100\n313/313 [==============================] - 437s 1s/step - d_loss: 0.0373 - d_real_acc: 0.9288 - d_fake_acc: 0.9512 - d_acc: 0.9400 - g_loss: 8.1066 - g_acc: 0.0488\nEpoch 19/100\n313/313 [==============================] - 1029s 3s/step - d_loss: -0.1154 - d_real_acc: 0.9408 - d_fake_acc: 0.9420 - d_acc: 0.9414 - g_loss: 7.6274 - g_acc: 0.0580\nEpoch 20/100\n313/313 [==============================] - 5781s 19s/step - d_loss: -0.0431 - d_real_acc: 0.9222 - d_fake_acc: 0.9231 - d_acc: 0.9227 - g_loss: 7.1953 - g_acc: 0.0769\nEpoch 21/100\n313/313 [==============================] - 2696s 9s/step - d_loss: -0.0542 - d_real_acc: 0.9176 - d_fake_acc: 0.9205 - d_acc: 0.9191 - g_loss: 7.1675 - g_acc: 0.0794\nEpoch 22/100\n313/313 [==============================] - 1481s 5s/step - d_loss: -0.1424 - d_real_acc: 0.9398 - d_fake_acc: 0.9400 - d_acc: 0.9399 - g_loss: 7.7399 - g_acc: 0.0600\nEpoch 23/100\n313/313 [==============================] - 439s 1s/step - d_loss: -0.0263 - d_real_acc: 0.9154 - d_fake_acc: 0.9148 - d_acc: 0.9151 - g_loss: 7.3322 - g_acc: 0.0852\nEpoch 24/100\n313/313 [==============================] - 443s 1s/step - d_loss: -0.1420 - d_real_acc: 0.9406 - d_fake_acc: 0.9430 - d_acc: 0.9418 - g_loss: 8.1502 - g_acc: 0.0570\nEpoch 25/100\n313/313 [==============================] - 475s 2s/step - d_loss: -0.1650 - d_real_acc: 0.9393 - d_fake_acc: 0.9395 - d_acc: 0.9394 - g_loss: 7.9602 - g_acc: 0.0605\nEpoch 26/100\n313/313 [==============================] - 447s 1s/step - d_loss: -0.1247 - d_real_acc: 0.9419 - d_fake_acc: 0.9437 - d_acc: 0.9428 - g_loss: 7.7939 - g_acc: 0.0563\nEpoch 27/100\n313/313 [==============================] - 1356s 4s/step - d_loss: -0.0182 - d_real_acc: 0.9160 - d_fake_acc: 0.9212 - d_acc: 0.9186 - g_loss: 6.9180 - g_acc: 0.0787\nEpoch 28/100\n313/313 [==============================] - 2219s 7s/step - d_loss: -0.2227 - d_real_acc: 0.9511 - d_fake_acc: 0.9519 - d_acc: 0.9515 - g_loss: 8.1970 - g_acc: 0.0481\nEpoch 29/100\n313/313 [==============================] - 5807s 19s/step - d_loss: -0.1091 - d_real_acc: 0.9318 - d_fake_acc: 0.9320 - d_acc: 0.9319 - g_loss: 7.5829 - g_acc: 0.0680\nEpoch 30/100\n313/313 [==============================] - 2511s 8s/step - d_loss: -0.3131 - d_real_acc: 0.9571 - d_fake_acc: 0.9604 - d_acc: 0.9588 - g_loss: 9.8839 - g_acc: 0.0395\nEpoch 31/100\n313/313 [==============================] - 2768s 9s/step - d_loss: -0.0996 - d_real_acc: 0.9269 - d_fake_acc: 0.9286 - d_acc: 0.9277 - g_loss: 8.3337 - g_acc: 0.0714\nEpoch 32/100\n313/313 [==============================] - 3046s 10s/step - d_loss: -0.1619 - d_real_acc: 0.9423 - d_fake_acc: 0.9482 - d_acc: 0.9453 - g_loss: 8.2435 - g_acc: 0.0518\nEpoch 33/100\n313/313 [==============================] - 3478s 11s/step - d_loss: -0.1182 - d_real_acc: 0.9284 - d_fake_acc: 0.9304 - d_acc: 0.9294 - g_loss: 8.1681 - g_acc: 0.0696\nEpoch 34/100\n313/313 [==============================] - 2776s 9s/step - d_loss: -0.2214 - d_real_acc: 0.9459 - d_fake_acc: 0.9582 - d_acc: 0.9520 - g_loss: 9.9168 - g_acc: 0.0417\nEpoch 35/100\n313/313 [==============================] - 2724s 9s/step - d_loss: -0.3101 - d_real_acc: 0.9421 - d_fake_acc: 0.9293 - d_acc: 0.9357 - g_loss: 13.2857 - g_acc: 0.0707\nEpoch 36/100\n313/313 [==============================] - 2648s 8s/step - d_loss: -0.0441 - d_real_acc: 0.8963 - d_fake_acc: 0.8961 - d_acc: 0.8962 - g_loss: 7.5664 - g_acc: 0.1038\nEpoch 37/100\n313/313 [==============================] - 3262s 10s/step - d_loss: -0.0859 - d_real_acc: 0.9314 - d_fake_acc: 0.9402 - d_acc: 0.9358 - g_loss: 8.3591 - g_acc: 0.0598\nEpoch 38/100\n313/313 [==============================] - 2612s 8s/step - d_loss: -0.2979 - d_real_acc: 0.9554 - d_fake_acc: 0.9577 - d_acc: 0.9566 - g_loss: 9.2534 - g_acc: 0.0423\nEpoch 39/100\n313/313 [==============================] - 2235s 7s/step - d_loss: -0.3387 - d_real_acc: 0.9607 - d_fake_acc: 0.9622 - d_acc: 0.9615 - g_loss: 9.9397 - g_acc: 0.0378\nEpoch 40/100\n313/313 [==============================] - 3453s 11s/step - d_loss: -0.1056 - d_real_acc: 0.9279 - d_fake_acc: 0.9310 - d_acc: 0.9294 - g_loss: 8.9394 - g_acc: 0.0690\nEpoch 41/100\n313/313 [==============================] - 2316s 7s/step - d_loss: -0.2147 - d_real_acc: 0.9318 - d_fake_acc: 0.9327 - d_acc: 0.9323 - g_loss: 9.0337 - g_acc: 0.0673\nEpoch 42/100\n313/313 [==============================] - 3134s 10s/step - d_loss: -0.2554 - d_real_acc: 0.9511 - d_fake_acc: 0.9540 - d_acc: 0.9526 - g_loss: 9.5571 - g_acc: 0.0460\nEpoch 43/100\n313/313 [==============================] - 3933s 13s/step - d_loss: -0.2871 - d_real_acc: 0.9490 - d_fake_acc: 0.9526 - d_acc: 0.9508 - g_loss: 10.3316 - g_acc: 0.0474\nEpoch 44/100\n313/313 [==============================] - 3248s 10s/step - d_loss: -0.3456 - d_real_acc: 0.9635 - d_fake_acc: 0.9635 - d_acc: 0.9635 - g_loss: 9.8675 - g_acc: 0.0364\nEpoch 45/100\n313/313 [==============================] - 3043s 10s/step - d_loss: -0.3274 - d_real_acc: 0.9603 - d_fake_acc: 0.9618 - d_acc: 0.9610 - g_loss: 10.4185 - g_acc: 0.0382\nEpoch 46/100\n313/313 [==============================] - 2706s 9s/step - d_loss: -0.6160 - d_real_acc: 0.9902 - d_fake_acc: 0.9908 - d_acc: 0.9905 - g_loss: 13.0574 - g_acc: 0.0092\nEpoch 47/100\n313/313 [==============================] - 2453s 8s/step - d_loss: 0.3413 - d_real_acc: 0.8073 - d_fake_acc: 0.8054 - d_acc: 0.8064 - g_loss: 6.5391 - g_acc: 0.1946\nEpoch 48/100\n313/313 [==============================] - 2898s 9s/step - d_loss: -0.4416 - d_real_acc: 0.9764 - d_fake_acc: 0.9784 - d_acc: 0.9774 - g_loss: 10.8318 - g_acc: 0.0216\nEpoch 49/100\n313/313 [==============================] - 3358s 11s/step - d_loss: 6.8776 - d_real_acc: 0.1058 - d_fake_acc: 0.9910 - d_acc: 0.5484 - g_loss: 14.8921 - g_acc: 0.0090\nEpoch 50/100\n313/313 [==============================] - 2940s 9s/step - d_loss: 7.7113 - d_real_acc: 0.0000e+00 - d_fake_acc: 1.0000 - d_acc: 0.5000 - g_loss: 15.4250 - g_acc: 0.0000e+00\nEpoch 51/100\n313/313 [==============================] - 2983s 10s/step - d_loss: 7.7121 - d_real_acc: 0.0000e+00 - d_fake_acc: 1.0000 - d_acc: 0.5000 - g_loss: 15.4250 - g_acc: 0.0000e+00\nEpoch 52/100\n313/313 [==============================] - 3458s 11s/step - d_loss: 7.7149 - d_real_acc: 0.0000e+00 - d_fake_acc: 1.0000 - d_acc: 0.5000 - g_loss: 15.4250 - g_acc: 0.0000e+00\nEpoch 53/100\n313/313 [==============================] - 2404s 8s/step - d_loss: 4.4950 - d_real_acc: 0.3543 - d_fake_acc: 0.8865 - d_acc: 0.6204 - g_loss: 10.6772 - g_acc: 0.1135\nEpoch 54/100\n313/313 [==============================] - 3297s 11s/step - d_loss: -0.0132 - d_real_acc: 0.9068 - d_fake_acc: 0.9010 - d_acc: 0.9039 - g_loss: 7.9660 - g_acc: 0.0990\nEpoch 55/100\n313/313 [==============================] - 2486s 8s/step - d_loss: -0.3508 - d_real_acc: 0.9615 - d_fake_acc: 0.9612 - d_acc: 0.9614 - g_loss: 10.2242 - g_acc: 0.0388\nEpoch 56/100\n313/313 [==============================] - 2995s 10s/step - d_loss: -0.3125 - d_real_acc: 0.9525 - d_fake_acc: 0.9533 - d_acc: 0.9529 - g_loss: 10.4182 - g_acc: 0.0467\nEpoch 57/100\n313/313 [==============================] - 1791s 6s/step - d_loss: -0.3201 - d_real_acc: 0.9532 - d_fake_acc: 0.9560 - d_acc: 0.9546 - g_loss: 10.4752 - g_acc: 0.0441\nEpoch 58/100\n313/313 [==============================] - 2792s 9s/step - d_loss: -0.2649 - d_real_acc: 0.9509 - d_fake_acc: 0.9532 - d_acc: 0.9520 - g_loss: 9.3587 - g_acc: 0.0468\nEpoch 59/100\n313/313 [==============================] - 3665s 12s/step - d_loss: -0.1747 - d_real_acc: 0.9413 - d_fake_acc: 0.9584 - d_acc: 0.9499 - g_loss: 10.1369 - g_acc: 0.0416\nEpoch 60/100\n313/313 [==============================] - 2493s 8s/step - d_loss: -0.2692 - d_real_acc: 0.9499 - d_fake_acc: 0.9534 - d_acc: 0.9517 - g_loss: 9.7124 - g_acc: 0.0466\nEpoch 61/100\n313/313 [==============================] - 2293s 7s/step - d_loss: -0.2869 - d_real_acc: 0.9520 - d_fake_acc: 0.9556 - d_acc: 0.9538 - g_loss: 10.2684 - g_acc: 0.0444\nEpoch 62/100\n313/313 [==============================] - 2865s 9s/step - d_loss: -0.6188 - d_real_acc: 0.9900 - d_fake_acc: 0.9901 - d_acc: 0.9901 - g_loss: 12.9584 - g_acc: 0.0099\nEpoch 63/100\n313/313 [==============================] - 2301s 7s/step - d_loss: -0.7197 - d_real_acc: 0.9984 - d_fake_acc: 0.9985 - d_acc: 0.9985 - g_loss: 14.5762 - g_acc: 0.0015\nEpoch 64/100\n313/313 [==============================] - 2404s 8s/step - d_loss: -0.4320 - d_real_acc: 0.9665 - d_fake_acc: 0.9702 - d_acc: 0.9683 - g_loss: 12.0177 - g_acc: 0.0298\nEpoch 65/100\n313/313 [==============================] - 4723s 15s/step - d_loss: 6.7591 - d_real_acc: 0.9940 - d_fake_acc: 0.1077 - d_acc: 0.5509 - g_loss: 1.4402 - g_acc: 0.8923\nEpoch 66/100\n313/313 [==============================] - 2726s 9s/step - d_loss: 7.6259 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 67/100\n313/313 [==============================] - 3325s 11s/step - d_loss: 7.6250 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 68/100\n313/313 [==============================] - 2513s 8s/step - d_loss: 7.6251 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 7.4387e-12 - g_acc: 1.0000\nEpoch 69/100\n313/313 [==============================] - 3304s 11s/step - d_loss: 3.4076 - d_real_acc: 0.8552 - d_fake_acc: 0.5472 - d_acc: 0.7012 - g_loss: 5.2072 - g_acc: 0.4528\nEpoch 70/100\n313/313 [==============================] - 2276s 7s/step - d_loss: -0.2424 - d_real_acc: 0.9448 - d_fake_acc: 0.9581 - d_acc: 0.9514 - g_loss: 10.5206 - g_acc: 0.0419\nEpoch 71/100\n313/313 [==============================] - 3315s 11s/step - d_loss: -0.3093 - d_real_acc: 0.9563 - d_fake_acc: 0.9640 - d_acc: 0.9602 - g_loss: 10.6076 - g_acc: 0.0360\nEpoch 72/100\n313/313 [==============================] - 2306s 7s/step - d_loss: -0.2440 - d_real_acc: 0.9466 - d_fake_acc: 0.9581 - d_acc: 0.9523 - g_loss: 10.1996 - g_acc: 0.0419\nEpoch 73/100\n313/313 [==============================] - 3218s 10s/step - d_loss: -0.7206 - d_real_acc: 0.9985 - d_fake_acc: 0.9983 - d_acc: 0.9984 - g_loss: 14.6350 - g_acc: 0.0017\nEpoch 74/100\n313/313 [==============================] - 3258s 10s/step - d_loss: -0.6281 - d_real_acc: 0.9828 - d_fake_acc: 0.9841 - d_acc: 0.9834 - g_loss: 14.5219 - g_acc: 0.0159\nEpoch 75/100\n313/313 [==============================] - 2874s 9s/step - d_loss: -0.0555 - d_real_acc: 0.9254 - d_fake_acc: 0.9371 - d_acc: 0.9312 - g_loss: 9.3443 - g_acc: 0.0629\nEpoch 76/100\n313/313 [==============================] - 2559s 8s/step - d_loss: -0.2825 - d_real_acc: 0.9515 - d_fake_acc: 0.9611 - d_acc: 0.9563 - g_loss: 10.7583 - g_acc: 0.0388\nEpoch 77/100\n313/313 [==============================] - 3663s 12s/step - d_loss: -0.3945 - d_real_acc: 0.9667 - d_fake_acc: 0.9691 - d_acc: 0.9679 - g_loss: 10.8566 - g_acc: 0.0309\nEpoch 78/100\n313/313 [==============================] - 2314s 7s/step - d_loss: -0.3953 - d_real_acc: 0.9508 - d_fake_acc: 0.9529 - d_acc: 0.9519 - g_loss: 12.0037 - g_acc: 0.0471\nEpoch 79/100\n313/313 [==============================] - 2816s 9s/step - d_loss: -0.6059 - d_real_acc: 0.9841 - d_fake_acc: 0.9838 - d_acc: 0.9840 - g_loss: 13.3516 - g_acc: 0.0162\nEpoch 80/100\n313/313 [==============================] - 3232s 10s/step - d_loss: -0.3555 - d_real_acc: 0.9587 - d_fake_acc: 0.9649 - d_acc: 0.9618 - g_loss: 11.2453 - g_acc: 0.0351\nEpoch 81/100\n313/313 [==============================] - 3705s 12s/step - d_loss: -0.4501 - d_real_acc: 0.9731 - d_fake_acc: 0.9743 - d_acc: 0.9737 - g_loss: 11.4553 - g_acc: 0.0258\nEpoch 82/100\n313/313 [==============================] - 2364s 8s/step - d_loss: -0.3827 - d_real_acc: 0.9588 - d_fake_acc: 0.9639 - d_acc: 0.9613 - g_loss: 11.6275 - g_acc: 0.0361\nEpoch 83/100\n313/313 [==============================] - 1122s 4s/step - d_loss: -0.4355 - d_real_acc: 0.9642 - d_fake_acc: 0.9674 - d_acc: 0.9658 - g_loss: 12.1025 - g_acc: 0.0326\nEpoch 84/100\n313/313 [==============================] - 4065s 13s/step - d_loss: -0.4456 - d_real_acc: 0.9695 - d_fake_acc: 0.9714 - d_acc: 0.9704 - g_loss: 11.6065 - g_acc: 0.0287\nEpoch 85/100\n313/313 [==============================] - 4461s 14s/step - d_loss: -0.6405 - d_real_acc: 0.9901 - d_fake_acc: 0.9899 - d_acc: 0.9900 - g_loss: 13.4694 - g_acc: 0.0101\nEpoch 86/100\n313/313 [==============================] - 2630s 8s/step - d_loss: -0.6431 - d_real_acc: 0.9857 - d_fake_acc: 0.9856 - d_acc: 0.9857 - g_loss: 14.3623 - g_acc: 0.0144\nEpoch 87/100\n313/313 [==============================] - 2567s 8s/step - d_loss: -0.3870 - d_real_acc: 0.9534 - d_fake_acc: 0.9578 - d_acc: 0.9556 - g_loss: 12.0201 - g_acc: 0.0422\nEpoch 88/100\n313/313 [==============================] - 2597s 8s/step - d_loss: -0.7624 - d_real_acc: 0.9999 - d_fake_acc: 0.9998 - d_acc: 0.9998 - g_loss: 15.3547 - g_acc: 2.5000e-04\nEpoch 89/100\n313/313 [==============================] - 1477s 5s/step - d_loss: -0.5787 - d_real_acc: 0.9764 - d_fake_acc: 0.9759 - d_acc: 0.9762 - g_loss: 13.8500 - g_acc: 0.0241\nEpoch 90/100\n313/313 [==============================] - 522s 2s/step - d_loss: -0.6747 - d_real_acc: 0.9885 - d_fake_acc: 0.9897 - d_acc: 0.9891 - g_loss: 14.5329 - g_acc: 0.0104\nEpoch 91/100\n313/313 [==============================] - 512s 2s/step - d_loss: 6.4703 - d_real_acc: 0.9892 - d_fake_acc: 0.1438 - d_acc: 0.5665 - g_loss: 2.1184 - g_acc: 0.8562\nEpoch 92/100\n313/313 [==============================] - 514s 2s/step - d_loss: 7.6245 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 93/100\n313/313 [==============================] - 533s 2s/step - d_loss: 7.6249 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 94/100\n313/313 [==============================] - 499s 2s/step - d_loss: 7.6236 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 95/100\n313/313 [==============================] - 483s 2s/step - d_loss: 7.6240 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 96/100\n313/313 [==============================] - 481s 2s/step - d_loss: 7.6248 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 97/100\n313/313 [==============================] - 488s 2s/step - d_loss: 7.6247 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 98/100\n313/313 [==============================] - 481s 2s/step - d_loss: 7.6263 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 99/100\n313/313 [==============================] - 459s 1s/step - d_loss: 7.6235 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 100/100\n313/313 [==============================] - 4669s 15s/step - d_loss: 7.6231 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\n\n\n&lt;keras.src.callbacks.History at 0x10f686690&gt;\n\n\n\n# Save the final models\ngenerator.save(\"./models/generator\")\ndiscriminator.save(\"./models/discriminator\")\n\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nINFO:tensorflow:Assets written to: ./models/generator/assets\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nINFO:tensorflow:Assets written to: ./models/discriminator/assets\n\n\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nINFO:tensorflow:Assets written to: ./models/generator/assets\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nINFO:tensorflow:Assets written to: ./models/discriminator/assets\n\n\n\n\n\n\nWe can see some examples of images produced by the GAN.\n(I don’t have a GPU so training is slow and I only trained 100 epochs… they’re a bit crap)\n\n# Sample some points in the latent space, from the standard normal distribution\ngrid_width, grid_height = (10, 3)\nz_sample = np.random.normal(size=(grid_width * grid_height, Z_DIM))\n\n# Decode the sampled points\nreconstructions = generator.predict(z_sample)\n\n# Draw a plot of decoded images\nfig = plt.figure(figsize=(18, 5))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\n# Output the grid of faces\nfor i in range(grid_width * grid_height):\n    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n    ax.axis(\"off\")\n    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")\n\n\n\n\n\n\n\n\nWe also want to make sure a generative model doesn’t simply recreate images that are already in the training set.\nAs a sanity check, we plot some generated images and the closest training images (using the L1 distance). This confirms that the generator is able to understand high-level features, even though we didn’t provide anything other than raw pixels, and it can generate examples distinct from those encountered before.\n\ndef compare_images(img1, img2):\n    return np.mean(np.abs(img1 - img2))\n\nall_data = []\nfor i in train.as_numpy_iterator():\n    all_data.extend(i)\nall_data = np.array(all_data)\n\n# Plot the images\nr, c = 3, 5\nfig, axs = plt.subplots(r, c, figsize=(10, 6))\nfig.suptitle(\"Generated images\", fontsize=20)\n\nnoise = np.random.normal(size=(r * c, Z_DIM))\ngen_imgs = generator.predict(noise)\n\ncnt = 0\nfor i in range(r):\n    for j in range(c):\n        axs[i, j].imshow(gen_imgs[cnt], cmap=\"gray_r\")\n        axs[i, j].axis(\"off\")\n        cnt += 1\n\nplt.show()\n\n1/1 [==============================] - 0s 80ms/step\n\n\n\n\n\n\n\n\n\n\nfig, axs = plt.subplots(r, c, figsize=(10, 6))\nfig.suptitle(\"Closest images in the training set\", fontsize=20)\n\ncnt = 0\nfor i in range(r):\n    for j in range(c):\n        c_diff = 99999\n        c_img = None\n        for k_idx, k in enumerate(all_data):\n            diff = compare_images(gen_imgs[cnt], k)\n            if diff &lt; c_diff:\n                c_img = np.copy(k)\n                c_diff = diff\n        axs[i, j].imshow(c_img, cmap=\"gray_r\")\n        axs[i, j].axis(\"off\")\n        cnt += 1\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nGANs are notoriously difficult to train because there is a balancing act between the generator and discriminator; neither should grow so strong that it overpowers the other.\n\n\nThe discriminator can always spot the fakes, so the signal from the loss function becomes too weak to cause any meaningful improvements in the generator.\nIn the extreme case, the discriminator distinguishes fakes perfectly, so gradients vanish and no training takes place.\nWe need to weaken the discriminator in this case. Some possible options are:\n\nMore dropout\nLower learning rate\nSimplify the discriminator architecture - use fewer layers\nAdd noise to the labels when training the discriminator\nAdd intentional labelling errors - randomly flip the labels of some images when training the discriminator\n\n\n\n\nIf the discriminator is too weak, the generator will learn that it can trick the discriminator using a small sample of nearly identical images. This is known as mode collapse. The generator would map every point in the latent space to this image, so the gradients of the loss function would vanish and it would not be able to recover.\nStrengthening the discriminator would not help because the generator would just learn to find a different mode that fools the discriminator with no diversity; it is numb to its input.\nSome possible options are:\n\nStrengthen the discriminator - do the opposite of the previous section\nReduce the learning rate of both generator and discriminator\nIncrease the batch size\n\n\n\n\n\nThe value of the loss is not meaningful as a measure of the generator’s strength when training.\nThe loss function is relative to the discriminator, and since the discriminator is also being trained, the goalposts are constantly shifting. Also, we don’t want the loss function to reach 0 or else we may reach mode collapse as described above.\nThis makes GAN training difficult to monitor.\n\n\n\nThere are a lot of hyperparameters involved with GANs because we are now training two networks.\nThe performance is highly sensitive to these hyperparameter choices, and involves a lot of trial and error.\n\n\n\nThe Wasserstein GAN replaces the binary crossentropy loss function with the Wassserstein lss function in both the discriminator and generator.\nThis results in two desirable properties:\n\nA meaningful loss metric that correlates with generator convergence. This allows for better monitoring of training.\nMore stable optimisation process.\n\n\n\n\n\nFirst, recall the binary cross-entropy loss, which is defined as: \\[\n-\\frac{1}{n} \\sum_{i=1}^{n}{y_i log(p_i) + (1 - y_i)log(1-p_i)}\n\\]\nIn a regular GAN, the discriminator compares the predictions for real images \\(p_i = D(x_i)\\) to the response \\(y_i = 1\\), and it compares the predictions for generated images \\(p_i = D(G(z_i))\\) to the response \\(y_i = 0\\).\nSo the discriminator loss minimisation can be written as: \\[\n\\min_{D} -(E_{x \\sim p_X}[log D(x)] + E_{z \\sim p_Z}[log (1 - D(G(z))] )\n\\]\nThe generator aims to trick the discriminator into believing the images are real, so it compares the discriminator’s predicted response to the desired response of \\(y_i=1\\). So the generator loss minimisation can be written as: \\[\n\\min_{G} -(E_{z \\sim p_Z}[log D(G(z)] )\n\\]\n\n\n\nIn contrast , the Wasserstein loss function is defined as: \\[\n-\\frac{1}{n} \\sum_{i=1}^{n}{y_i p_i}\n\\]\nIt requires that we use \\(y_i=1\\) and \\(y_i=-1\\) as labels rather than 1 and 0. We also remove the final sigmoid activation layer, which constrained the output to the range \\([0, 1]\\), meaning the output can now be any real number in the range \\((-\\infty, \\infty)\\).\nBecause of these changes, rather than referring to a discriminator which outputs a probability, for WGANs we refer to a critic which outputs a score.\nWith these changes of labels to -1 and 1, the critic loss minimisation becomes: \\[\n\\min_{D} -(E_{x \\sim p_X}[D(x)] - E_{z \\sim p_Z}[D(G(z)] )\n\\]\ni.e. it aims to maximise the difference in predictions between real and generated images.\nThe generator is still trying to trick the critic as in a regular GAN, so it wants the critic to score its generated images as highly as possible. To this end, it still compares to the desired critic response of \\(y_i=1\\) corresponding to the critic believing the generated image is real. So the generator loss minimisation remains similar, just without the log: \\[\n\\min_{G} -(E_{z \\sim p_Z}[D(G(z)] )\n\\]\n\n\n\n\nAllowing the critic to use the range \\((-\\infty, \\infty)\\) initially seems a bit counterintuitive - usually we want to avoid large numbers in neural networks otherwise gradients explode!\nThere is an additional constraint placed on the critic requiring it to be a 1-Lipschitz continuous function. This means that for any two input images, \\(x_1\\) and \\(x_2\\), it satisfied the following inequality: \\[\n\\frac{| D(x_1) - D(x_2) |}{| x_1 - x_2 |} \\le 1\n\\]\nRecall that the critic is a function \\(D\\) which converts an image into a scalar prediction. The numerator is the change in predictions, and the denominator is the average pixelwise difference. So this is essentially a limit on how sharply the critic predictions are allowed to change for a given image perturbation.\nAn in-depth exploration of why the Wasserstein loss requires this constraint is given here.\n\n\nOne crude way of enforcing the constraint suggested by the original authors (and described as “a terrible way to enforce a Lipschitz constraint) is to clip the weights to \\([-0.01, 0.01]\\) after each training batch. However, this diminishes the critic’s ability to learn.\nAn improved approach is to introduce a gradient penalty term to penalise the gradient norm when it deviates from 1.\n\n\n\n\nThe gradient penalty (GP) loss term encourages gradients towards 1 to conform to the 1-Lipschitz continuous constraint.\nThe GP loss measures the squared difference between the norm of the gradient of predictions w.r.t. input images and 1, i.e. \\[\nloss_{GP} = (||\\frac{\\delta predictions}{\\delta input}|| - 1)^2\n\\]\nCalculating this everywhere during the training process would be computationally intensive. Instead we evaluate it at a sample of points. To encourage a balanced mix, we use an interpolated image which is a pairwise weighted average of a real image and a generated image.\nThe combined loss function is then a weighted sum of the Wasserstein loss and the GP loss: \\[\nloss = loss_{Wasserstein} + \\lambda_{GP} * loss_{GP}\n\\]\n\n\n\nWhen training WGANs, we train the critic to convergence and then the train the generator. This ensures the gradients used for the generator update are accurate.\nThis is a major benefit over traditional GANs, where we must balance the alternating training of generator and discriminator.\nWe train the critic several times between each generator training step. A typical ratio is 3-5 critic updates for each generator update.\nNote that batch normalisation shouldn’t be used for WGAN-GP because it creates correlation between images of a given batch, which makes the GP term less effective.\n\n\n\nImages produced by VAEs tend to produce softer images that blur colour boundaries, whereas GANs produce sharper, more well-defined.\nHowever, GANs typically take longer to train and can be more sensitive to hyperparameter choices to reach a satisfactory result.\n\n\n\n\nConditional GANs allow us to control the type of image that is generated. For example, should we generate a large or small brick? A male or female face?\n\n\nThe key difference of a CGAN is we pass a one-hot encoded label.\nFor the generator, we append the one-hot encoded label vector to the random noise input to the generator. For the critic, we append the one-hot encoded label vector to the image. If it has multiple channels, as in an RGB image, the label vector is repeated on each of the channels to fit the required shape.\nThe critic can now see the label, which means the generator needs to create images that match the generated labels. If the generated image was inconsistent with the generated label, then the critic could use this to easily distinguish the fakes.\n\nThe only change required to the architecture is to concatenate the label to the inputs of the discriminator (actual training labels) and the generator (initially randomised vectors of the correct shape).\n\nThe images and labels are unpacked during the training step.\n\n\n\nWe can control the type of image generated by passing a particular one-hot encoded label into the input of the generator.\nIf labels are available for your training data, it is generally a good idea to include them in the inputs even if you don’t intend to create a conditional GAN from it. They tend to improve the quality of the output generated, since the labels act as a highly informative extension of the pixel inputs.\n\n\n\n\n\nChapter 4 of Generative Deep Learning by David Foster.\nWasserstein loss and Lipschitz constraint"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson4/chapter4.html#gans-1",
    "href": "posts/ml/generative_deep_learning/lesson4/chapter4.html#gans-1",
    "title": "Generative AI: GANs",
    "section": "",
    "text": "The idea of GANs is that we can train two competing models:\n\nThe generator tries to convert random noise into convincing observations.\nThe discriminator tries to predict whether an observation came from the original training dataset or is a “fake”.\n\nWe initialise both as random models; the generator outputs noise and the discriminator predicts randomly. We then alternate the training of the two networks so that the generator gets incrementally better at fooling the discriminator, then the discriminator gets incrementally better at spotting fakes.\n\n\n\n\n\nflowchart LR\n\n  A([Random noise]) --&gt; B[Generator] --&gt; C([Generated image]) \n\n  D([Image]) --&gt; E[Discriminator] --&gt; F([Prediction of realness probability])"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson4/chapter4.html#building-a-deep-convolutional-gan-dcgan",
    "href": "posts/ml/generative_deep_learning/lesson4/chapter4.html#building-a-deep-convolutional-gan-dcgan",
    "title": "Generative AI: GANs",
    "section": "",
    "text": "We will implement a GAN to generate pictures of bricks.\n\n\nLoad image data of lego bricks. We will train a model that can generate novel lego brick images.\n\n\nThe original data is scaled from [0, 255].\nOften we will rescale this to [0, 1] so that we can use sigmoid activation functions.\nIn this case we will scale to [-1, 1] so that we can use tanh activation functions, which tend to give stronger gradients than sigmoid.\n\nfrom pathlib import Path\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras import (\n    layers,\n    models,\n    callbacks,\n    losses,\n    utils,\n    metrics,\n    optimizers,\n)\n\n\ndef sample_batch(dataset):\n    batch = dataset.take(1).get_single_element()\n    if isinstance(batch, tuple):\n        batch = batch[0]\n    return batch.numpy()\n\n\ndef display_images(\n    images, n=10, size=(20, 3), cmap=\"gray_r\", as_type=\"float32\", save_to=None\n):\n    \"\"\"Displays n random images from each one of the supplied arrays.\"\"\"\n    if images.max() &gt; 1.0:\n        images = images / 255.0\n    elif images.min() &lt; 0.0:\n        images = (images + 1.0) / 2.0\n\n    plt.figure(figsize=size)\n    for i in range(n):\n        _ = plt.subplot(1, n, i + 1)\n        plt.imshow(images[i].astype(as_type), cmap=cmap)\n        plt.axis(\"off\")\n\n    if save_to:\n        plt.savefig(save_to)\n        print(f\"\\nSaved to {save_to}\")\n\n    plt.show()\n\nModel and data parameters:\n\nDATA_DIR = Path(\"/Users/gurpreetjohl/workspace/datasets/lego-brick-images\")\n\nIMAGE_SIZE = 64\nCHANNELS = 1\nBATCH_SIZE = 128\nZ_DIM = 100\nEPOCHS = 100\nLOAD_MODEL = False\nADAM_BETA_1 = 0.5\nADAM_BETA_2 = 0.999\nLEARNING_RATE = 0.0002\nNOISE_PARAM = 0.1\n\nLoad and pre-process the training data:\n\ndef preprocess(img):\n    \"\"\"Normalize and reshape the images.\"\"\"\n    img = (tf.cast(img, \"float32\") - 127.5) / 127.5\n    return img\n\n\ntraining_data = utils.image_dataset_from_directory(\n    DATA_DIR / \"dataset\",\n    labels=None,\n    color_mode=\"grayscale\",\n    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    seed=42,\n    interpolation=\"bilinear\",\n)\ntrain = training_data.map(lambda x: preprocess(x))\n\nFound 40000 files belonging to 1 classes.\n\n\nSome sample input images:\n\ndisplay_images(sample_batch(train))\n\n\n\n\n\n\n\n\n\n\n\n\nThe goal of the discriminator is to predict whether an image is real or fake.\nThis is a supervised binary classification problem, so we can use CNN architecture with a single output node. We stack Conv2D layers with BatchNormalization, LeakyReLU and Dropout layers sandwiched between.\n\ndiscriminator_input = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS))\n\nx = layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(discriminator_input)\nx = layers.LeakyReLU(0.2)(x)\nx = layers.Dropout(0.3)(x)\n\nx = layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\nx = layers.Dropout(0.3)(x)\n\nx = layers.Conv2D(256, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\nx = layers.Dropout(0.3)(x)\n\nx = layers.Conv2D(512, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\nx = layers.Dropout(0.3)(x)\n\nx = layers.Conv2D(1, kernel_size=4, strides=1, padding=\"valid\", use_bias=False, activation=\"sigmoid\")(x)\ndiscriminator_output = layers.Flatten()(x)  # The shape is already 1x1 so no need for a Dense layer after this\n\ndiscriminator = models.Model(discriminator_input, discriminator_output)\ndiscriminator.summary()\n\nModel: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 64, 64, 1)]       0         \n                                                                 \n conv2d_5 (Conv2D)           (None, 32, 32, 64)        1024      \n                                                                 \n leaky_re_lu_4 (LeakyReLU)   (None, 32, 32, 64)        0         \n                                                                 \n dropout_4 (Dropout)         (None, 32, 32, 64)        0         \n                                                                 \n conv2d_6 (Conv2D)           (None, 16, 16, 128)       131072    \n                                                                 \n batch_normalization_3 (Bat  (None, 16, 16, 128)       512       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_5 (LeakyReLU)   (None, 16, 16, 128)       0         \n                                                                 \n dropout_5 (Dropout)         (None, 16, 16, 128)       0         \n                                                                 \n conv2d_7 (Conv2D)           (None, 8, 8, 256)         524288    \n                                                                 \n batch_normalization_4 (Bat  (None, 8, 8, 256)         1024      \n chNormalization)                                                \n                                                                 \n leaky_re_lu_6 (LeakyReLU)   (None, 8, 8, 256)         0         \n                                                                 \n dropout_6 (Dropout)         (None, 8, 8, 256)         0         \n                                                                 \n conv2d_8 (Conv2D)           (None, 4, 4, 512)         2097152   \n                                                                 \n batch_normalization_5 (Bat  (None, 4, 4, 512)         2048      \n chNormalization)                                                \n                                                                 \n leaky_re_lu_7 (LeakyReLU)   (None, 4, 4, 512)         0         \n                                                                 \n dropout_7 (Dropout)         (None, 4, 4, 512)         0         \n                                                                 \n conv2d_9 (Conv2D)           (None, 1, 1, 1)           8192      \n                                                                 \n flatten_1 (Flatten)         (None, 1)                 0         \n                                                                 \n=================================================================\nTotal params: 2765312 (10.55 MB)\nTrainable params: 2763520 (10.54 MB)\nNon-trainable params: 1792 (7.00 KB)\n_________________________________________________________________\n\n\n\n\n\nThe purpose of the generator is to turn random noise into convincing images.\nThe input is a vector sampled from a multivariate Normal distribution, and the output is an image of the same size as the training data.\nThe discriminator-generator relationship in a GAN is similar to that of the encoder-decoder relations in a VAE.\nThe architecture of the discriminator is similar to the discriminator but in reverse (like a decoder). We pass stack Conv2DTranspose layers with BatchNormalization and LeakyReLU layers sandwiched in between.\n\n\nWe use Conv2DTranspose layers to scale the image size up.\nAn alternative would be to use stacks of Upsampling2D and Conv2D layers, i.e. the following serves the same purpose as a Conv2DTranspose layer:\nx = layers.Upsampling2D(size=2)(x)\nx = layers.Conv2D(256, kernel_size=4, strides=1, padding=\"same\")(x)\nThe Upsampling2D layer simply repeats each row and column to double its size, then Conv2D applies a convolution.\nThe idea is similar with Conv2DTranspose, but the extra rows and columns are filled with zeros rather than repeated existing values.\nConv2DTranspose layers can result in checkerboard pattern artifacts. Both options are used in practice, so it is often helpful to experiment and see which gives better results.\n\ngenerator_input = layers.Input(shape=(Z_DIM,))\n\nx = layers.Reshape((1, 1, Z_DIM))(generator_input)  # Reshape the input vector so we can apply conv transpose operations to it\n\nx = layers.Conv2DTranspose(512, kernel_size=4, strides=1, padding=\"valid\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\n\nx = layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\n\nx = layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\n\nx = layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\n\ngenerator_output = layers.Conv2DTranspose(\n    CHANNELS,\n    kernel_size=4,\n    strides=2,\n    padding=\"same\",\n    use_bias=False,\n    activation=\"tanh\",\n)(x)\n\ngenerator = models.Model(generator_input, generator_output)\ngenerator.summary()\n\nModel: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_3 (InputLayer)        [(None, 100)]             0         \n                                                                 \n reshape (Reshape)           (None, 1, 1, 100)         0         \n                                                                 \n conv2d_transpose (Conv2DTr  (None, 4, 4, 512)         819200    \n anspose)                                                        \n                                                                 \n batch_normalization_6 (Bat  (None, 4, 4, 512)         2048      \n chNormalization)                                                \n                                                                 \n leaky_re_lu_8 (LeakyReLU)   (None, 4, 4, 512)         0         \n                                                                 \n conv2d_transpose_1 (Conv2D  (None, 8, 8, 256)         2097152   \n Transpose)                                                      \n                                                                 \n batch_normalization_7 (Bat  (None, 8, 8, 256)         1024      \n chNormalization)                                                \n                                                                 \n leaky_re_lu_9 (LeakyReLU)   (None, 8, 8, 256)         0         \n                                                                 \n conv2d_transpose_2 (Conv2D  (None, 16, 16, 128)       524288    \n Transpose)                                                      \n                                                                 \n batch_normalization_8 (Bat  (None, 16, 16, 128)       512       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_10 (LeakyReLU)  (None, 16, 16, 128)       0         \n                                                                 \n conv2d_transpose_3 (Conv2D  (None, 32, 32, 64)        131072    \n Transpose)                                                      \n                                                                 \n batch_normalization_9 (Bat  (None, 32, 32, 64)        256       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_11 (LeakyReLU)  (None, 32, 32, 64)        0         \n                                                                 \n conv2d_transpose_4 (Conv2D  (None, 64, 64, 1)         1024      \n Transpose)                                                      \n                                                                 \n=================================================================\nTotal params: 3576576 (13.64 MB)\nTrainable params: 3574656 (13.64 MB)\nNon-trainable params: 1920 (7.50 KB)\n_________________________________________________________________\n\n\n\n\n\n\nWe alternate between training the discriminator and generator. They are not trained simultaneously. We want the generated images to be predicted close to 1 because the generator is good, not because the discriminator is weak.\nFor the discriminator, we create a training set where some images are real images from the training data and some are outputs from the generator. This is then a supervised binary classification problem.\nFor the generator, we want a way of scoring each generated image on its realness so that we can optimise this. The discriminator provides exactly this. We pass the generated images through the discriminator to get probabilities. The generator wants to fool the discriminator, so ideally this would be a vector of 1s. So the loss function is the binary crossentropy between these probabilities and a vector of 1s.\n\nclass DCGAN(models.Model):\n    def __init__(self, discriminator, generator, latent_dim):\n        super(DCGAN, self).__init__()\n        self.discriminator = discriminator\n        self.generator = generator\n        self.latent_dim = latent_dim\n\n    def compile(self, d_optimizer, g_optimizer):\n        super(DCGAN, self).compile()\n        self.loss_fn = losses.BinaryCrossentropy()\n        self.d_optimizer = d_optimizer\n        self.g_optimizer = g_optimizer\n        self.d_loss_metric = metrics.Mean(name=\"d_loss\")\n        self.d_real_acc_metric = metrics.BinaryAccuracy(name=\"d_real_acc\")\n        self.d_fake_acc_metric = metrics.BinaryAccuracy(name=\"d_fake_acc\")\n        self.d_acc_metric = metrics.BinaryAccuracy(name=\"d_acc\")\n        self.g_loss_metric = metrics.Mean(name=\"g_loss\")\n        self.g_acc_metric = metrics.BinaryAccuracy(name=\"g_acc\")\n\n    @property\n    def metrics(self):\n        return [\n            self.d_loss_metric,\n            self.d_real_acc_metric,\n            self.d_fake_acc_metric,\n            self.d_acc_metric,\n            self.g_loss_metric,\n            self.g_acc_metric,\n        ]\n\n    def train_step(self, real_images):\n        # Sample random points in the latent space\n        batch_size = tf.shape(real_images)[0]\n        random_latent_vectors = tf.random.normal(\n            shape=(batch_size, self.latent_dim)\n        )\n\n        # Train the discriminator on fake images\n        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n            generated_images = self.generator(\n                random_latent_vectors, training=True\n            )\n\n            # Evaluate the discriminator on the real and fake images\n            real_predictions = self.discriminator(real_images, training=True)\n            fake_predictions = self.discriminator(generated_images, training=True)\n\n            real_labels = tf.ones_like(real_predictions)\n            real_noisy_labels = real_labels + NOISE_PARAM * tf.random.uniform(\n                tf.shape(real_predictions)\n            )\n            fake_labels = tf.zeros_like(fake_predictions)\n            fake_noisy_labels = fake_labels - NOISE_PARAM * tf.random.uniform(\n                tf.shape(fake_predictions)\n            )\n\n            # Calculate the losses\n            d_real_loss = self.loss_fn(real_noisy_labels, real_predictions)\n            d_fake_loss = self.loss_fn(fake_noisy_labels, fake_predictions)\n            d_loss = (d_real_loss + d_fake_loss) / 2.0\n\n            g_loss = self.loss_fn(real_labels, fake_predictions)\n\n        # Update gradients\n        gradients_of_discriminator = disc_tape.gradient(\n            d_loss, self.discriminator.trainable_variables\n        )\n        gradients_of_generator = gen_tape.gradient(\n            g_loss, self.generator.trainable_variables\n        )\n\n        self.d_optimizer.apply_gradients(\n            zip(gradients_of_discriminator, discriminator.trainable_variables)\n        )\n        self.g_optimizer.apply_gradients(\n            zip(gradients_of_generator, generator.trainable_variables)\n        )\n\n        # Update metrics\n        self.d_loss_metric.update_state(d_loss)\n        self.d_real_acc_metric.update_state(real_labels, real_predictions)\n        self.d_fake_acc_metric.update_state(fake_labels, fake_predictions)\n        self.d_acc_metric.update_state(\n            [real_labels, fake_labels], [real_predictions, fake_predictions]\n        )\n        self.g_loss_metric.update_state(g_loss)\n        self.g_acc_metric.update_state(real_labels, fake_predictions)\n\n        return {m.name: m.result() for m in self.metrics}\n\n\n# Create a DCGAN\ndcgan = DCGAN(\n    discriminator=discriminator, generator=generator, latent_dim=Z_DIM\n)\n\n\ndcgan.compile(\n    d_optimizer=optimizers.legacy.Adam(\n        learning_rate=LEARNING_RATE, beta_1=ADAM_BETA_1, beta_2=ADAM_BETA_2\n    ),\n    g_optimizer=optimizers.legacy.Adam(\n        learning_rate=LEARNING_RATE, beta_1=ADAM_BETA_1, beta_2=ADAM_BETA_2\n    ),\n)\ndcgan.fit(train,  epochs=EPOCHS)\n\nEpoch 1/100\n313/313 [==============================] - 459s 1s/step - d_loss: 0.0252 - d_real_acc: 0.9011 - d_fake_acc: 0.9013 - d_acc: 0.9012 - g_loss: 5.3464 - g_acc: 0.0987\nEpoch 2/100\n313/313 [==============================] - 459s 1s/step - d_loss: 0.0454 - d_real_acc: 0.8986 - d_fake_acc: 0.8997 - d_acc: 0.8992 - g_loss: 5.2642 - g_acc: 0.1002\nEpoch 3/100\n313/313 [==============================] - 458s 1s/step - d_loss: 0.0556 - d_real_acc: 0.8958 - d_fake_acc: 0.8975 - d_acc: 0.8967 - g_loss: 4.9679 - g_acc: 0.1025\nEpoch 4/100\n313/313 [==============================] - 467s 1s/step - d_loss: 0.0246 - d_real_acc: 0.9065 - d_fake_acc: 0.9091 - d_acc: 0.9078 - g_loss: 5.1611 - g_acc: 0.0909\nEpoch 5/100\n313/313 [==============================] - 470s 1s/step - d_loss: 0.0178 - d_real_acc: 0.9067 - d_fake_acc: 0.9088 - d_acc: 0.9078 - g_loss: 5.1731 - g_acc: 0.0912\nEpoch 6/100\n313/313 [==============================] - 463s 1s/step - d_loss: 0.0314 - d_real_acc: 0.9116 - d_fake_acc: 0.9105 - d_acc: 0.9110 - g_loss: 5.2774 - g_acc: 0.0895\nEpoch 7/100\n313/313 [==============================] - 559s 2s/step - d_loss: 0.0229 - d_real_acc: 0.9085 - d_fake_acc: 0.9079 - d_acc: 0.9082 - g_loss: 5.3445 - g_acc: 0.0921\nEpoch 8/100\n313/313 [==============================] - 467s 1s/step - d_loss: -0.0155 - d_real_acc: 0.9161 - d_fake_acc: 0.9161 - d_acc: 0.9161 - g_loss: 5.7091 - g_acc: 0.0839\nEpoch 9/100\n313/313 [==============================] - 438s 1s/step - d_loss: -0.0077 - d_real_acc: 0.9220 - d_fake_acc: 0.9224 - d_acc: 0.9222 - g_loss: 5.8731 - g_acc: 0.0776\nEpoch 10/100\n313/313 [==============================] - 468s 1s/step - d_loss: -0.0472 - d_real_acc: 0.9228 - d_fake_acc: 0.9241 - d_acc: 0.9234 - g_loss: 5.9693 - g_acc: 0.0759\nEpoch 11/100\n313/313 [==============================] - 430s 1s/step - d_loss: -0.0839 - d_real_acc: 0.9404 - d_fake_acc: 0.9424 - d_acc: 0.9414 - g_loss: 6.1212 - g_acc: 0.0576\nEpoch 12/100\n313/313 [==============================] - 457s 1s/step - d_loss: 0.0431 - d_real_acc: 0.9053 - d_fake_acc: 0.9046 - d_acc: 0.9049 - g_loss: 6.0708 - g_acc: 0.0954\nEpoch 13/100\n313/313 [==============================] - 448s 1s/step - d_loss: -0.0154 - d_real_acc: 0.9236 - d_fake_acc: 0.9244 - d_acc: 0.9240 - g_loss: 6.3106 - g_acc: 0.0756\nEpoch 14/100\n313/313 [==============================] - 432s 1s/step - d_loss: -0.0720 - d_real_acc: 0.9320 - d_fake_acc: 0.9342 - d_acc: 0.9331 - g_loss: 6.6509 - g_acc: 0.0658\nEpoch 15/100\n313/313 [==============================] - 443s 1s/step - d_loss: 0.0057 - d_real_acc: 0.9072 - d_fake_acc: 0.9097 - d_acc: 0.9085 - g_loss: 6.1399 - g_acc: 0.0903\nEpoch 16/100\n313/313 [==============================] - 427s 1s/step - d_loss: 0.0069 - d_real_acc: 0.9185 - d_fake_acc: 0.9167 - d_acc: 0.9176 - g_loss: 6.3255 - g_acc: 0.0833\nEpoch 17/100\n313/313 [==============================] - 439s 1s/step - d_loss: -0.0709 - d_real_acc: 0.9220 - d_fake_acc: 0.9239 - d_acc: 0.9230 - g_loss: 6.8108 - g_acc: 0.0761\nEpoch 18/100\n313/313 [==============================] - 437s 1s/step - d_loss: 0.0373 - d_real_acc: 0.9288 - d_fake_acc: 0.9512 - d_acc: 0.9400 - g_loss: 8.1066 - g_acc: 0.0488\nEpoch 19/100\n313/313 [==============================] - 1029s 3s/step - d_loss: -0.1154 - d_real_acc: 0.9408 - d_fake_acc: 0.9420 - d_acc: 0.9414 - g_loss: 7.6274 - g_acc: 0.0580\nEpoch 20/100\n313/313 [==============================] - 5781s 19s/step - d_loss: -0.0431 - d_real_acc: 0.9222 - d_fake_acc: 0.9231 - d_acc: 0.9227 - g_loss: 7.1953 - g_acc: 0.0769\nEpoch 21/100\n313/313 [==============================] - 2696s 9s/step - d_loss: -0.0542 - d_real_acc: 0.9176 - d_fake_acc: 0.9205 - d_acc: 0.9191 - g_loss: 7.1675 - g_acc: 0.0794\nEpoch 22/100\n313/313 [==============================] - 1481s 5s/step - d_loss: -0.1424 - d_real_acc: 0.9398 - d_fake_acc: 0.9400 - d_acc: 0.9399 - g_loss: 7.7399 - g_acc: 0.0600\nEpoch 23/100\n313/313 [==============================] - 439s 1s/step - d_loss: -0.0263 - d_real_acc: 0.9154 - d_fake_acc: 0.9148 - d_acc: 0.9151 - g_loss: 7.3322 - g_acc: 0.0852\nEpoch 24/100\n313/313 [==============================] - 443s 1s/step - d_loss: -0.1420 - d_real_acc: 0.9406 - d_fake_acc: 0.9430 - d_acc: 0.9418 - g_loss: 8.1502 - g_acc: 0.0570\nEpoch 25/100\n313/313 [==============================] - 475s 2s/step - d_loss: -0.1650 - d_real_acc: 0.9393 - d_fake_acc: 0.9395 - d_acc: 0.9394 - g_loss: 7.9602 - g_acc: 0.0605\nEpoch 26/100\n313/313 [==============================] - 447s 1s/step - d_loss: -0.1247 - d_real_acc: 0.9419 - d_fake_acc: 0.9437 - d_acc: 0.9428 - g_loss: 7.7939 - g_acc: 0.0563\nEpoch 27/100\n313/313 [==============================] - 1356s 4s/step - d_loss: -0.0182 - d_real_acc: 0.9160 - d_fake_acc: 0.9212 - d_acc: 0.9186 - g_loss: 6.9180 - g_acc: 0.0787\nEpoch 28/100\n313/313 [==============================] - 2219s 7s/step - d_loss: -0.2227 - d_real_acc: 0.9511 - d_fake_acc: 0.9519 - d_acc: 0.9515 - g_loss: 8.1970 - g_acc: 0.0481\nEpoch 29/100\n313/313 [==============================] - 5807s 19s/step - d_loss: -0.1091 - d_real_acc: 0.9318 - d_fake_acc: 0.9320 - d_acc: 0.9319 - g_loss: 7.5829 - g_acc: 0.0680\nEpoch 30/100\n313/313 [==============================] - 2511s 8s/step - d_loss: -0.3131 - d_real_acc: 0.9571 - d_fake_acc: 0.9604 - d_acc: 0.9588 - g_loss: 9.8839 - g_acc: 0.0395\nEpoch 31/100\n313/313 [==============================] - 2768s 9s/step - d_loss: -0.0996 - d_real_acc: 0.9269 - d_fake_acc: 0.9286 - d_acc: 0.9277 - g_loss: 8.3337 - g_acc: 0.0714\nEpoch 32/100\n313/313 [==============================] - 3046s 10s/step - d_loss: -0.1619 - d_real_acc: 0.9423 - d_fake_acc: 0.9482 - d_acc: 0.9453 - g_loss: 8.2435 - g_acc: 0.0518\nEpoch 33/100\n313/313 [==============================] - 3478s 11s/step - d_loss: -0.1182 - d_real_acc: 0.9284 - d_fake_acc: 0.9304 - d_acc: 0.9294 - g_loss: 8.1681 - g_acc: 0.0696\nEpoch 34/100\n313/313 [==============================] - 2776s 9s/step - d_loss: -0.2214 - d_real_acc: 0.9459 - d_fake_acc: 0.9582 - d_acc: 0.9520 - g_loss: 9.9168 - g_acc: 0.0417\nEpoch 35/100\n313/313 [==============================] - 2724s 9s/step - d_loss: -0.3101 - d_real_acc: 0.9421 - d_fake_acc: 0.9293 - d_acc: 0.9357 - g_loss: 13.2857 - g_acc: 0.0707\nEpoch 36/100\n313/313 [==============================] - 2648s 8s/step - d_loss: -0.0441 - d_real_acc: 0.8963 - d_fake_acc: 0.8961 - d_acc: 0.8962 - g_loss: 7.5664 - g_acc: 0.1038\nEpoch 37/100\n313/313 [==============================] - 3262s 10s/step - d_loss: -0.0859 - d_real_acc: 0.9314 - d_fake_acc: 0.9402 - d_acc: 0.9358 - g_loss: 8.3591 - g_acc: 0.0598\nEpoch 38/100\n313/313 [==============================] - 2612s 8s/step - d_loss: -0.2979 - d_real_acc: 0.9554 - d_fake_acc: 0.9577 - d_acc: 0.9566 - g_loss: 9.2534 - g_acc: 0.0423\nEpoch 39/100\n313/313 [==============================] - 2235s 7s/step - d_loss: -0.3387 - d_real_acc: 0.9607 - d_fake_acc: 0.9622 - d_acc: 0.9615 - g_loss: 9.9397 - g_acc: 0.0378\nEpoch 40/100\n313/313 [==============================] - 3453s 11s/step - d_loss: -0.1056 - d_real_acc: 0.9279 - d_fake_acc: 0.9310 - d_acc: 0.9294 - g_loss: 8.9394 - g_acc: 0.0690\nEpoch 41/100\n313/313 [==============================] - 2316s 7s/step - d_loss: -0.2147 - d_real_acc: 0.9318 - d_fake_acc: 0.9327 - d_acc: 0.9323 - g_loss: 9.0337 - g_acc: 0.0673\nEpoch 42/100\n313/313 [==============================] - 3134s 10s/step - d_loss: -0.2554 - d_real_acc: 0.9511 - d_fake_acc: 0.9540 - d_acc: 0.9526 - g_loss: 9.5571 - g_acc: 0.0460\nEpoch 43/100\n313/313 [==============================] - 3933s 13s/step - d_loss: -0.2871 - d_real_acc: 0.9490 - d_fake_acc: 0.9526 - d_acc: 0.9508 - g_loss: 10.3316 - g_acc: 0.0474\nEpoch 44/100\n313/313 [==============================] - 3248s 10s/step - d_loss: -0.3456 - d_real_acc: 0.9635 - d_fake_acc: 0.9635 - d_acc: 0.9635 - g_loss: 9.8675 - g_acc: 0.0364\nEpoch 45/100\n313/313 [==============================] - 3043s 10s/step - d_loss: -0.3274 - d_real_acc: 0.9603 - d_fake_acc: 0.9618 - d_acc: 0.9610 - g_loss: 10.4185 - g_acc: 0.0382\nEpoch 46/100\n313/313 [==============================] - 2706s 9s/step - d_loss: -0.6160 - d_real_acc: 0.9902 - d_fake_acc: 0.9908 - d_acc: 0.9905 - g_loss: 13.0574 - g_acc: 0.0092\nEpoch 47/100\n313/313 [==============================] - 2453s 8s/step - d_loss: 0.3413 - d_real_acc: 0.8073 - d_fake_acc: 0.8054 - d_acc: 0.8064 - g_loss: 6.5391 - g_acc: 0.1946\nEpoch 48/100\n313/313 [==============================] - 2898s 9s/step - d_loss: -0.4416 - d_real_acc: 0.9764 - d_fake_acc: 0.9784 - d_acc: 0.9774 - g_loss: 10.8318 - g_acc: 0.0216\nEpoch 49/100\n313/313 [==============================] - 3358s 11s/step - d_loss: 6.8776 - d_real_acc: 0.1058 - d_fake_acc: 0.9910 - d_acc: 0.5484 - g_loss: 14.8921 - g_acc: 0.0090\nEpoch 50/100\n313/313 [==============================] - 2940s 9s/step - d_loss: 7.7113 - d_real_acc: 0.0000e+00 - d_fake_acc: 1.0000 - d_acc: 0.5000 - g_loss: 15.4250 - g_acc: 0.0000e+00\nEpoch 51/100\n313/313 [==============================] - 2983s 10s/step - d_loss: 7.7121 - d_real_acc: 0.0000e+00 - d_fake_acc: 1.0000 - d_acc: 0.5000 - g_loss: 15.4250 - g_acc: 0.0000e+00\nEpoch 52/100\n313/313 [==============================] - 3458s 11s/step - d_loss: 7.7149 - d_real_acc: 0.0000e+00 - d_fake_acc: 1.0000 - d_acc: 0.5000 - g_loss: 15.4250 - g_acc: 0.0000e+00\nEpoch 53/100\n313/313 [==============================] - 2404s 8s/step - d_loss: 4.4950 - d_real_acc: 0.3543 - d_fake_acc: 0.8865 - d_acc: 0.6204 - g_loss: 10.6772 - g_acc: 0.1135\nEpoch 54/100\n313/313 [==============================] - 3297s 11s/step - d_loss: -0.0132 - d_real_acc: 0.9068 - d_fake_acc: 0.9010 - d_acc: 0.9039 - g_loss: 7.9660 - g_acc: 0.0990\nEpoch 55/100\n313/313 [==============================] - 2486s 8s/step - d_loss: -0.3508 - d_real_acc: 0.9615 - d_fake_acc: 0.9612 - d_acc: 0.9614 - g_loss: 10.2242 - g_acc: 0.0388\nEpoch 56/100\n313/313 [==============================] - 2995s 10s/step - d_loss: -0.3125 - d_real_acc: 0.9525 - d_fake_acc: 0.9533 - d_acc: 0.9529 - g_loss: 10.4182 - g_acc: 0.0467\nEpoch 57/100\n313/313 [==============================] - 1791s 6s/step - d_loss: -0.3201 - d_real_acc: 0.9532 - d_fake_acc: 0.9560 - d_acc: 0.9546 - g_loss: 10.4752 - g_acc: 0.0441\nEpoch 58/100\n313/313 [==============================] - 2792s 9s/step - d_loss: -0.2649 - d_real_acc: 0.9509 - d_fake_acc: 0.9532 - d_acc: 0.9520 - g_loss: 9.3587 - g_acc: 0.0468\nEpoch 59/100\n313/313 [==============================] - 3665s 12s/step - d_loss: -0.1747 - d_real_acc: 0.9413 - d_fake_acc: 0.9584 - d_acc: 0.9499 - g_loss: 10.1369 - g_acc: 0.0416\nEpoch 60/100\n313/313 [==============================] - 2493s 8s/step - d_loss: -0.2692 - d_real_acc: 0.9499 - d_fake_acc: 0.9534 - d_acc: 0.9517 - g_loss: 9.7124 - g_acc: 0.0466\nEpoch 61/100\n313/313 [==============================] - 2293s 7s/step - d_loss: -0.2869 - d_real_acc: 0.9520 - d_fake_acc: 0.9556 - d_acc: 0.9538 - g_loss: 10.2684 - g_acc: 0.0444\nEpoch 62/100\n313/313 [==============================] - 2865s 9s/step - d_loss: -0.6188 - d_real_acc: 0.9900 - d_fake_acc: 0.9901 - d_acc: 0.9901 - g_loss: 12.9584 - g_acc: 0.0099\nEpoch 63/100\n313/313 [==============================] - 2301s 7s/step - d_loss: -0.7197 - d_real_acc: 0.9984 - d_fake_acc: 0.9985 - d_acc: 0.9985 - g_loss: 14.5762 - g_acc: 0.0015\nEpoch 64/100\n313/313 [==============================] - 2404s 8s/step - d_loss: -0.4320 - d_real_acc: 0.9665 - d_fake_acc: 0.9702 - d_acc: 0.9683 - g_loss: 12.0177 - g_acc: 0.0298\nEpoch 65/100\n313/313 [==============================] - 4723s 15s/step - d_loss: 6.7591 - d_real_acc: 0.9940 - d_fake_acc: 0.1077 - d_acc: 0.5509 - g_loss: 1.4402 - g_acc: 0.8923\nEpoch 66/100\n313/313 [==============================] - 2726s 9s/step - d_loss: 7.6259 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 67/100\n313/313 [==============================] - 3325s 11s/step - d_loss: 7.6250 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 68/100\n313/313 [==============================] - 2513s 8s/step - d_loss: 7.6251 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 7.4387e-12 - g_acc: 1.0000\nEpoch 69/100\n313/313 [==============================] - 3304s 11s/step - d_loss: 3.4076 - d_real_acc: 0.8552 - d_fake_acc: 0.5472 - d_acc: 0.7012 - g_loss: 5.2072 - g_acc: 0.4528\nEpoch 70/100\n313/313 [==============================] - 2276s 7s/step - d_loss: -0.2424 - d_real_acc: 0.9448 - d_fake_acc: 0.9581 - d_acc: 0.9514 - g_loss: 10.5206 - g_acc: 0.0419\nEpoch 71/100\n313/313 [==============================] - 3315s 11s/step - d_loss: -0.3093 - d_real_acc: 0.9563 - d_fake_acc: 0.9640 - d_acc: 0.9602 - g_loss: 10.6076 - g_acc: 0.0360\nEpoch 72/100\n313/313 [==============================] - 2306s 7s/step - d_loss: -0.2440 - d_real_acc: 0.9466 - d_fake_acc: 0.9581 - d_acc: 0.9523 - g_loss: 10.1996 - g_acc: 0.0419\nEpoch 73/100\n313/313 [==============================] - 3218s 10s/step - d_loss: -0.7206 - d_real_acc: 0.9985 - d_fake_acc: 0.9983 - d_acc: 0.9984 - g_loss: 14.6350 - g_acc: 0.0017\nEpoch 74/100\n313/313 [==============================] - 3258s 10s/step - d_loss: -0.6281 - d_real_acc: 0.9828 - d_fake_acc: 0.9841 - d_acc: 0.9834 - g_loss: 14.5219 - g_acc: 0.0159\nEpoch 75/100\n313/313 [==============================] - 2874s 9s/step - d_loss: -0.0555 - d_real_acc: 0.9254 - d_fake_acc: 0.9371 - d_acc: 0.9312 - g_loss: 9.3443 - g_acc: 0.0629\nEpoch 76/100\n313/313 [==============================] - 2559s 8s/step - d_loss: -0.2825 - d_real_acc: 0.9515 - d_fake_acc: 0.9611 - d_acc: 0.9563 - g_loss: 10.7583 - g_acc: 0.0388\nEpoch 77/100\n313/313 [==============================] - 3663s 12s/step - d_loss: -0.3945 - d_real_acc: 0.9667 - d_fake_acc: 0.9691 - d_acc: 0.9679 - g_loss: 10.8566 - g_acc: 0.0309\nEpoch 78/100\n313/313 [==============================] - 2314s 7s/step - d_loss: -0.3953 - d_real_acc: 0.9508 - d_fake_acc: 0.9529 - d_acc: 0.9519 - g_loss: 12.0037 - g_acc: 0.0471\nEpoch 79/100\n313/313 [==============================] - 2816s 9s/step - d_loss: -0.6059 - d_real_acc: 0.9841 - d_fake_acc: 0.9838 - d_acc: 0.9840 - g_loss: 13.3516 - g_acc: 0.0162\nEpoch 80/100\n313/313 [==============================] - 3232s 10s/step - d_loss: -0.3555 - d_real_acc: 0.9587 - d_fake_acc: 0.9649 - d_acc: 0.9618 - g_loss: 11.2453 - g_acc: 0.0351\nEpoch 81/100\n313/313 [==============================] - 3705s 12s/step - d_loss: -0.4501 - d_real_acc: 0.9731 - d_fake_acc: 0.9743 - d_acc: 0.9737 - g_loss: 11.4553 - g_acc: 0.0258\nEpoch 82/100\n313/313 [==============================] - 2364s 8s/step - d_loss: -0.3827 - d_real_acc: 0.9588 - d_fake_acc: 0.9639 - d_acc: 0.9613 - g_loss: 11.6275 - g_acc: 0.0361\nEpoch 83/100\n313/313 [==============================] - 1122s 4s/step - d_loss: -0.4355 - d_real_acc: 0.9642 - d_fake_acc: 0.9674 - d_acc: 0.9658 - g_loss: 12.1025 - g_acc: 0.0326\nEpoch 84/100\n313/313 [==============================] - 4065s 13s/step - d_loss: -0.4456 - d_real_acc: 0.9695 - d_fake_acc: 0.9714 - d_acc: 0.9704 - g_loss: 11.6065 - g_acc: 0.0287\nEpoch 85/100\n313/313 [==============================] - 4461s 14s/step - d_loss: -0.6405 - d_real_acc: 0.9901 - d_fake_acc: 0.9899 - d_acc: 0.9900 - g_loss: 13.4694 - g_acc: 0.0101\nEpoch 86/100\n313/313 [==============================] - 2630s 8s/step - d_loss: -0.6431 - d_real_acc: 0.9857 - d_fake_acc: 0.9856 - d_acc: 0.9857 - g_loss: 14.3623 - g_acc: 0.0144\nEpoch 87/100\n313/313 [==============================] - 2567s 8s/step - d_loss: -0.3870 - d_real_acc: 0.9534 - d_fake_acc: 0.9578 - d_acc: 0.9556 - g_loss: 12.0201 - g_acc: 0.0422\nEpoch 88/100\n313/313 [==============================] - 2597s 8s/step - d_loss: -0.7624 - d_real_acc: 0.9999 - d_fake_acc: 0.9998 - d_acc: 0.9998 - g_loss: 15.3547 - g_acc: 2.5000e-04\nEpoch 89/100\n313/313 [==============================] - 1477s 5s/step - d_loss: -0.5787 - d_real_acc: 0.9764 - d_fake_acc: 0.9759 - d_acc: 0.9762 - g_loss: 13.8500 - g_acc: 0.0241\nEpoch 90/100\n313/313 [==============================] - 522s 2s/step - d_loss: -0.6747 - d_real_acc: 0.9885 - d_fake_acc: 0.9897 - d_acc: 0.9891 - g_loss: 14.5329 - g_acc: 0.0104\nEpoch 91/100\n313/313 [==============================] - 512s 2s/step - d_loss: 6.4703 - d_real_acc: 0.9892 - d_fake_acc: 0.1438 - d_acc: 0.5665 - g_loss: 2.1184 - g_acc: 0.8562\nEpoch 92/100\n313/313 [==============================] - 514s 2s/step - d_loss: 7.6245 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 93/100\n313/313 [==============================] - 533s 2s/step - d_loss: 7.6249 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 94/100\n313/313 [==============================] - 499s 2s/step - d_loss: 7.6236 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 95/100\n313/313 [==============================] - 483s 2s/step - d_loss: 7.6240 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 96/100\n313/313 [==============================] - 481s 2s/step - d_loss: 7.6248 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 97/100\n313/313 [==============================] - 488s 2s/step - d_loss: 7.6247 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 98/100\n313/313 [==============================] - 481s 2s/step - d_loss: 7.6263 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 99/100\n313/313 [==============================] - 459s 1s/step - d_loss: 7.6235 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 100/100\n313/313 [==============================] - 4669s 15s/step - d_loss: 7.6231 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\n\n\n&lt;keras.src.callbacks.History at 0x10f686690&gt;\n\n\n\n# Save the final models\ngenerator.save(\"./models/generator\")\ndiscriminator.save(\"./models/discriminator\")\n\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nINFO:tensorflow:Assets written to: ./models/generator/assets\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nINFO:tensorflow:Assets written to: ./models/discriminator/assets\n\n\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nINFO:tensorflow:Assets written to: ./models/generator/assets\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nINFO:tensorflow:Assets written to: ./models/discriminator/assets"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson4/chapter4.html#analysing-the-gan",
    "href": "posts/ml/generative_deep_learning/lesson4/chapter4.html#analysing-the-gan",
    "title": "Generative AI: GANs",
    "section": "",
    "text": "We can see some examples of images produced by the GAN.\n(I don’t have a GPU so training is slow and I only trained 100 epochs… they’re a bit crap)\n\n# Sample some points in the latent space, from the standard normal distribution\ngrid_width, grid_height = (10, 3)\nz_sample = np.random.normal(size=(grid_width * grid_height, Z_DIM))\n\n# Decode the sampled points\nreconstructions = generator.predict(z_sample)\n\n# Draw a plot of decoded images\nfig = plt.figure(figsize=(18, 5))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\n# Output the grid of faces\nfor i in range(grid_width * grid_height):\n    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n    ax.axis(\"off\")\n    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")\n\n\n\n\n\n\n\n\nWe also want to make sure a generative model doesn’t simply recreate images that are already in the training set.\nAs a sanity check, we plot some generated images and the closest training images (using the L1 distance). This confirms that the generator is able to understand high-level features, even though we didn’t provide anything other than raw pixels, and it can generate examples distinct from those encountered before.\n\ndef compare_images(img1, img2):\n    return np.mean(np.abs(img1 - img2))\n\nall_data = []\nfor i in train.as_numpy_iterator():\n    all_data.extend(i)\nall_data = np.array(all_data)\n\n# Plot the images\nr, c = 3, 5\nfig, axs = plt.subplots(r, c, figsize=(10, 6))\nfig.suptitle(\"Generated images\", fontsize=20)\n\nnoise = np.random.normal(size=(r * c, Z_DIM))\ngen_imgs = generator.predict(noise)\n\ncnt = 0\nfor i in range(r):\n    for j in range(c):\n        axs[i, j].imshow(gen_imgs[cnt], cmap=\"gray_r\")\n        axs[i, j].axis(\"off\")\n        cnt += 1\n\nplt.show()\n\n1/1 [==============================] - 0s 80ms/step\n\n\n\n\n\n\n\n\n\n\nfig, axs = plt.subplots(r, c, figsize=(10, 6))\nfig.suptitle(\"Closest images in the training set\", fontsize=20)\n\ncnt = 0\nfor i in range(r):\n    for j in range(c):\n        c_diff = 99999\n        c_img = None\n        for k_idx, k in enumerate(all_data):\n            diff = compare_images(gen_imgs[cnt], k)\n            if diff &lt; c_diff:\n                c_img = np.copy(k)\n                c_diff = diff\n        axs[i, j].imshow(c_img, cmap=\"gray_r\")\n        axs[i, j].axis(\"off\")\n        cnt += 1\n\nplt.show()"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson4/chapter4.html#gan-training-tips",
    "href": "posts/ml/generative_deep_learning/lesson4/chapter4.html#gan-training-tips",
    "title": "Generative AI: GANs",
    "section": "",
    "text": "GANs are notoriously difficult to train because there is a balancing act between the generator and discriminator; neither should grow so strong that it overpowers the other.\n\n\nThe discriminator can always spot the fakes, so the signal from the loss function becomes too weak to cause any meaningful improvements in the generator.\nIn the extreme case, the discriminator distinguishes fakes perfectly, so gradients vanish and no training takes place.\nWe need to weaken the discriminator in this case. Some possible options are:\n\nMore dropout\nLower learning rate\nSimplify the discriminator architecture - use fewer layers\nAdd noise to the labels when training the discriminator\nAdd intentional labelling errors - randomly flip the labels of some images when training the discriminator\n\n\n\n\nIf the discriminator is too weak, the generator will learn that it can trick the discriminator using a small sample of nearly identical images. This is known as mode collapse. The generator would map every point in the latent space to this image, so the gradients of the loss function would vanish and it would not be able to recover.\nStrengthening the discriminator would not help because the generator would just learn to find a different mode that fools the discriminator with no diversity; it is numb to its input.\nSome possible options are:\n\nStrengthen the discriminator - do the opposite of the previous section\nReduce the learning rate of both generator and discriminator\nIncrease the batch size"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson4/chapter4.html#uninformative-loss",
    "href": "posts/ml/generative_deep_learning/lesson4/chapter4.html#uninformative-loss",
    "title": "Generative AI: GANs",
    "section": "",
    "text": "The value of the loss is not meaningful as a measure of the generator’s strength when training.\nThe loss function is relative to the discriminator, and since the discriminator is also being trained, the goalposts are constantly shifting. Also, we don’t want the loss function to reach 0 or else we may reach mode collapse as described above.\nThis makes GAN training difficult to monitor."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson4/chapter4.html#hyperparameters",
    "href": "posts/ml/generative_deep_learning/lesson4/chapter4.html#hyperparameters",
    "title": "Generative AI: GANs",
    "section": "",
    "text": "There are a lot of hyperparameters involved with GANs because we are now training two networks.\nThe performance is highly sensitive to these hyperparameter choices, and involves a lot of trial and error."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson4/chapter4.html#wasserstein-gan-with-gradient-penalty-wgan-gp",
    "href": "posts/ml/generative_deep_learning/lesson4/chapter4.html#wasserstein-gan-with-gradient-penalty-wgan-gp",
    "title": "Generative AI: GANs",
    "section": "",
    "text": "The Wasserstein GAN replaces the binary crossentropy loss function with the Wassserstein lss function in both the discriminator and generator.\nThis results in two desirable properties:\n\nA meaningful loss metric that correlates with generator convergence. This allows for better monitoring of training.\nMore stable optimisation process.\n\n\n\n\n\nFirst, recall the binary cross-entropy loss, which is defined as: \\[\n-\\frac{1}{n} \\sum_{i=1}^{n}{y_i log(p_i) + (1 - y_i)log(1-p_i)}\n\\]\nIn a regular GAN, the discriminator compares the predictions for real images \\(p_i = D(x_i)\\) to the response \\(y_i = 1\\), and it compares the predictions for generated images \\(p_i = D(G(z_i))\\) to the response \\(y_i = 0\\).\nSo the discriminator loss minimisation can be written as: \\[\n\\min_{D} -(E_{x \\sim p_X}[log D(x)] + E_{z \\sim p_Z}[log (1 - D(G(z))] )\n\\]\nThe generator aims to trick the discriminator into believing the images are real, so it compares the discriminator’s predicted response to the desired response of \\(y_i=1\\). So the generator loss minimisation can be written as: \\[\n\\min_{G} -(E_{z \\sim p_Z}[log D(G(z)] )\n\\]\n\n\n\nIn contrast , the Wasserstein loss function is defined as: \\[\n-\\frac{1}{n} \\sum_{i=1}^{n}{y_i p_i}\n\\]\nIt requires that we use \\(y_i=1\\) and \\(y_i=-1\\) as labels rather than 1 and 0. We also remove the final sigmoid activation layer, which constrained the output to the range \\([0, 1]\\), meaning the output can now be any real number in the range \\((-\\infty, \\infty)\\).\nBecause of these changes, rather than referring to a discriminator which outputs a probability, for WGANs we refer to a critic which outputs a score.\nWith these changes of labels to -1 and 1, the critic loss minimisation becomes: \\[\n\\min_{D} -(E_{x \\sim p_X}[D(x)] - E_{z \\sim p_Z}[D(G(z)] )\n\\]\ni.e. it aims to maximise the difference in predictions between real and generated images.\nThe generator is still trying to trick the critic as in a regular GAN, so it wants the critic to score its generated images as highly as possible. To this end, it still compares to the desired critic response of \\(y_i=1\\) corresponding to the critic believing the generated image is real. So the generator loss minimisation remains similar, just without the log: \\[\n\\min_{G} -(E_{z \\sim p_Z}[D(G(z)] )\n\\]\n\n\n\n\nAllowing the critic to use the range \\((-\\infty, \\infty)\\) initially seems a bit counterintuitive - usually we want to avoid large numbers in neural networks otherwise gradients explode!\nThere is an additional constraint placed on the critic requiring it to be a 1-Lipschitz continuous function. This means that for any two input images, \\(x_1\\) and \\(x_2\\), it satisfied the following inequality: \\[\n\\frac{| D(x_1) - D(x_2) |}{| x_1 - x_2 |} \\le 1\n\\]\nRecall that the critic is a function \\(D\\) which converts an image into a scalar prediction. The numerator is the change in predictions, and the denominator is the average pixelwise difference. So this is essentially a limit on how sharply the critic predictions are allowed to change for a given image perturbation.\nAn in-depth exploration of why the Wasserstein loss requires this constraint is given here.\n\n\nOne crude way of enforcing the constraint suggested by the original authors (and described as “a terrible way to enforce a Lipschitz constraint) is to clip the weights to \\([-0.01, 0.01]\\) after each training batch. However, this diminishes the critic’s ability to learn.\nAn improved approach is to introduce a gradient penalty term to penalise the gradient norm when it deviates from 1.\n\n\n\n\nThe gradient penalty (GP) loss term encourages gradients towards 1 to conform to the 1-Lipschitz continuous constraint.\nThe GP loss measures the squared difference between the norm of the gradient of predictions w.r.t. input images and 1, i.e. \\[\nloss_{GP} = (||\\frac{\\delta predictions}{\\delta input}|| - 1)^2\n\\]\nCalculating this everywhere during the training process would be computationally intensive. Instead we evaluate it at a sample of points. To encourage a balanced mix, we use an interpolated image which is a pairwise weighted average of a real image and a generated image.\nThe combined loss function is then a weighted sum of the Wasserstein loss and the GP loss: \\[\nloss = loss_{Wasserstein} + \\lambda_{GP} * loss_{GP}\n\\]\n\n\n\nWhen training WGANs, we train the critic to convergence and then the train the generator. This ensures the gradients used for the generator update are accurate.\nThis is a major benefit over traditional GANs, where we must balance the alternating training of generator and discriminator.\nWe train the critic several times between each generator training step. A typical ratio is 3-5 critic updates for each generator update.\nNote that batch normalisation shouldn’t be used for WGAN-GP because it creates correlation between images of a given batch, which makes the GP term less effective.\n\n\n\nImages produced by VAEs tend to produce softer images that blur colour boundaries, whereas GANs produce sharper, more well-defined.\nHowever, GANs typically take longer to train and can be more sensitive to hyperparameter choices to reach a satisfactory result."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson4/chapter4.html#conditional-gan-cgan",
    "href": "posts/ml/generative_deep_learning/lesson4/chapter4.html#conditional-gan-cgan",
    "title": "Generative AI: GANs",
    "section": "",
    "text": "Conditional GANs allow us to control the type of image that is generated. For example, should we generate a large or small brick? A male or female face?\n\n\nThe key difference of a CGAN is we pass a one-hot encoded label.\nFor the generator, we append the one-hot encoded label vector to the random noise input to the generator. For the critic, we append the one-hot encoded label vector to the image. If it has multiple channels, as in an RGB image, the label vector is repeated on each of the channels to fit the required shape.\nThe critic can now see the label, which means the generator needs to create images that match the generated labels. If the generated image was inconsistent with the generated label, then the critic could use this to easily distinguish the fakes.\n\nThe only change required to the architecture is to concatenate the label to the inputs of the discriminator (actual training labels) and the generator (initially randomised vectors of the correct shape).\n\nThe images and labels are unpacked during the training step.\n\n\n\nWe can control the type of image generated by passing a particular one-hot encoded label into the input of the generator.\nIf labels are available for your training data, it is generally a good idea to include them in the inputs even if you don’t intend to create a conditional GAN from it. They tend to improve the quality of the output generated, since the labels act as a highly informative extension of the pixel inputs."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson4/chapter4.html#references",
    "href": "posts/ml/generative_deep_learning/lesson4/chapter4.html#references",
    "title": "Generative AI: GANs",
    "section": "",
    "text": "Chapter 4 of Generative Deep Learning by David Foster.\nWasserstein loss and Lipschitz constraint"
  },
  {
    "objectID": "posts/ml/graph_ml/part_1/graph_notes.html",
    "href": "posts/ml/graph_ml/part_1/graph_notes.html",
    "title": "Graph ML: What’s a Graph?",
    "section": "",
    "text": "Graphs are a generic way of representing relationships (edges) between entities (nodes).\nThis makes them useful in a wide variety of applications, including modelling biological pathways, social networks, or even images.\nGraphs do not have a fixed structure and the relationships (edges) can be time-varying.\n\n\nEntities are nodes or vertices and their relationships are edges.\nNodes that are connected are adjacent and are called neighbours.\nGraphs are the general case of trees. A graph is broadly something with nodes and edges. A tree is a graph that has no cycles and all nodes are connected.\nA directed graph or digraph shows where relationships can be directional, e.g. social network followers.\nEdges can be weighted to represent quantities like distance or cost.\nA path is the sequence of edges used to get from one vertex to another.\nA simple path does not visit any node more than once.\nA graph is connected if there is a path between every pair of vertices, i.e. there are no isolated nodes or clusters.\nA cycle is a path where the first and last nodes are the same.\n\n\n\n\nTree: An undirected graph with no cycles.\nRooted tree: A tree where one node is designated ads the root.\nDirected Acyclic Graph (DAG): A directed graph with no cycles. This means edges can only be traversed in a particular direction.\nBipartite graphs: A graph where vertices can be divided into two disjoint sets.\nComplete graph: A graph where every pair of vertices is connected by an edge, i.e. it is fully connected.\n\n\n\n\n\nGraphs are an abstract data structure, so there are multiple ways we can represent them in memory.\nSome common options are:\n\nObjects and pointers\nAdjacency lists\nEdge lists\nAdjacency matrix\n\nEach approach has its own trade-offs in terms of memory usage, ease of implementation, and efficiency based on the specific characteristics of the graph you’re dealing with.\nWe’ll implement the following simple graph using each approach:\n\n\n\n\n\nflowchart LR\n\nA[1] &lt;---&gt; B[2]\nB[2] &lt;---&gt; C[3]\n\n\n\n\n\n\n\n\nIn this approach, we represent each node in the graph as an object, and you use pointers to connect these nodes. Each object typically contains a list of pointers to other nodes it’s connected to.\nFor a connected graph, if we have access to one node we can traverse the graph to find all other nodes. This is not true for graphs with unconnected nodes.\nThis approach will be familiar to computer scientists - it bears a resemblance to linked lists. In practice this isn’t used much in practical ML because traversing the graph is more cumbersome.\nPros:\n\nAllows for flexible representation of complex relationships.\nIdeal for graphs where nodes have additional properties beyond just connectivity.\n\nCons:\n\nMemory overhead due to object instantiation and pointer storage.\nCan be complex to implement and manage.\nIf the graph is not connected, an additional data structure is needed to keep track of isolated nodes.\n\nPython Implementation:\n\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.neighbors = []\n\n    def __repr__(self) -&gt; str:\n        return f\"Node {self.value} with neighbors {[k.value for k in self.neighbors]}\"\n\ndef add_edge(node1, node2):\n    node1.neighbors.append(node2)\n    node2.neighbors.append(node1)\n\nnode1 = Node(1)\nnode2 = Node(2)\nnode3 = Node(3)\n\nadd_edge(node1, node2)\nadd_edge(node2, node3)\n\n\nprint(node1)\n\nNode 1 with neighbors [2]\n\n\n\nprint(node2)\n\nNode 2 with neighbors [1, 3]\n\n\n\nprint(node3)\n\nNode 3 with neighbors [2]\n\n\n\n\n\nIn this approach, we store a list of edges per node.\nWe use a list or dictionary where the index or key represents a node, and the value is a list of its adjacent nodes.\nPros:\n\nEfficient memory usage for sparse graphs - \\(O(V + E)\\) space complexity.\nEasy to implement and understand.\nSuitable for graphs with varying degrees of connectivity.\nEasy to iterate through neighbors.\n\nCons:\n\nSlower for dense graphs.\nChecking if nodes are connected is slow - retrieving edge information may require linear search.\n\nPython Implementation:\n\nclass Graph:\n    def __init__(self):\n        self.adj_list = {}\n\n    def __repr__(self) -&gt; str:\n        return str(self.adj_list)\n\n    def add_edge(self, node1, node2):\n        if node1 not in self.adj_list:\n            self.adj_list[node1] = []\n        if node2 not in self.adj_list:\n            self.adj_list[node2] = []\n        self.adj_list[node1].append(node2)\n        self.adj_list[node2].append(node1)\n\n\ngraph = Graph()\ngraph.add_edge(1, 2)\ngraph.add_edge(2, 3)\n\nprint(graph)\n\n{1: [2], 2: [1, 3], 3: [2]}\n\n\n\n\n\nAn even more compact form is the edge list, which lists all of the edges as (source_node, target_node) tuples.\nPros:\n\nEven more efficient memory usage for sparse graphs - \\(O(E)\\) space complexity.\n\nCons:\n\nSlower for dense graphs.\nChecking if nodes are connected is slow - retrieving edge information may require linear search.\n\nPython Implementation:\n\nclass Graph:\n    def __init__(self):\n        self.edge_list = []\n\n    def __repr__(self) -&gt; str:\n        return str(self.edge_list)\n\n    def add_edge(self, node1, node2):\n        node_pair = (node1, node2)\n        if node_pair not in self.edge_list:\n            self.edge_list.append(node_pair)\n\n\ngraph = Graph()\ngraph.add_edge(1, 2)\ngraph.add_edge(2, 3)\n\nprint(graph)\n\n[(1, 2), (2, 3)]\n\n\n\n\n\nIn this approach, we represent the graph as a 2D matrix where rows and columns represent nodes, and matrix cells indicate whether there’s an edge between the nodes.\nFor weighted graphs, the values in the matrix correspond to the weights of the edges.\nPros:\n\nEfficient for dense graphs.\nConstant time access to check edge existence; checking if nodes are connected is \\(O(1)\\).\n\nCons:\n\nNot ideal for graphs with a large number of nodes due to \\(O(V^2)\\) space complexity. This is particularly bad for sparse graphs.\nAdding/removing nodes can be expensive as it requires resizing the matrix. Bad for dynamic graphs.\n\nPython Implementation:\n\nclass Graph:\n    def __init__(self, num_nodes):\n        self.adj_matrix = [[0] * num_nodes for row in range(num_nodes)]\n\n    def __repr__(self) -&gt; str:\n        return str(self.adj_matrix)\n\n    def add_edge(self, node1, node2):\n        self.adj_matrix[node1][node2] = 1\n        self.adj_matrix[node2][node1] = 1\n\n\ngraph = Graph(num_nodes=3)\ngraph.add_edge(0, 1)\ngraph.add_edge(1, 2)\n\nprint(graph)\n\n[[0, 1, 0], [1, 0, 1], [0, 1, 0]]\n\n\nThe adjacency matrix approach is useful in machine learning as it naturally fits into a tensor representation, which most ML libraries (e.g. pytorch, tensorflow) play nicely with. So we’ll stick with tthat going forward.\n\n\n\n\nGrpahs are common enough to work with that we don’t need to implement them from scratch every time we want ot use them (although doing so in the previous section is instructive).\nnetworkx is a useful third-party library for this task\n\nimport networkx as nx\n\n\ngraph = nx.Graph()\ngraph.add_edges_from([\n    ('A', 'B'),\n    ('A', 'C'), \n    ('B', 'D'), \n    ('B', 'E'), \n    ('C', 'F'), \n    ('C', 'G'), \n    ('G', 'G'), \n])\n\nprint(graph)\n\nGraph with 7 nodes and 7 edges\n\n\nFrom this Graph object we can see our familiar edge list:\n\ngraph.adj\n\nAdjacencyView({'A': {'B': {}, 'C': {}}, 'B': {'A': {}, 'D': {}, 'E': {}}, 'C': {'A': {}, 'F': {}, 'G': {}}, 'D': {'B': {}}, 'E': {'B': {}}, 'F': {'C': {}}, 'G': {'C': {}, 'G': {}}})\n\n\nOr the adjacency matrix:\n\nnx.adjacency_matrix(graph).todense()\n\n/var/folders/8k/8jqhnfbd1t99blb07r1hs5440000gn/T/ipykernel_50230/1468791322.py:1: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n  nx.adjacency_matrix(graph).todense()\n\n\nmatrix([[0, 1, 1, 0, 0, 0, 0],\n        [1, 0, 0, 1, 1, 0, 0],\n        [1, 0, 0, 0, 0, 1, 1],\n        [0, 1, 0, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0, 0, 0],\n        [0, 0, 1, 0, 0, 0, 0],\n        [0, 0, 1, 0, 0, 0, 1]])\n\n\n\n\n\n\n\nThis is the number of edges which are incident on a node.\nFor an undirected graph, this is the number of edges connected to the node. Self-loops add 2 to the degree.\nFor a directed graph, we can distinguish the indegree, \\(deg^-(V)\\), as the edges that point towards the node. The outdegree, \\(deg^+(V)\\), denotes the edges that start from the node. Self-loops add 1 to the indegree and 1 to the outdegree.\n\ngraph.degree\n\nDegreeView({'A': 2, 'B': 3, 'C': 3, 'D': 1, 'E': 1, 'F': 1, 'G': 3})\n\n\n\n\n\nCentrality measures the importance of a given node in a network.\nThere a different formulations of centrality:\n\nDegree centrality: This is simply the degree of the node. How many neighbours?\nCloseness centrality: The average length of the shortest path between this node and all others. How fast can I reach my neighbours?\nBetweenness centrality: The number of times a node lies on the shortest path between pairs of other nodes in the graph. How often am I the bottleneck/bridge for others?\n\n\nnx.degree_centrality(graph)\n\n{'A': 0.3333333333333333,\n 'B': 0.5,\n 'C': 0.5,\n 'D': 0.16666666666666666,\n 'E': 0.16666666666666666,\n 'F': 0.16666666666666666,\n 'G': 0.5}\n\n\n\nnx.closeness_centrality(graph)\n\n{'A': 0.6,\n 'B': 0.5454545454545454,\n 'C': 0.5454545454545454,\n 'D': 0.375,\n 'E': 0.375,\n 'F': 0.375,\n 'G': 0.375}\n\n\n\nnx.betweenness_centrality(graph)\n\n{'A': 0.6, 'B': 0.6, 'C': 0.6, 'D': 0.0, 'E': 0.0, 'F': 0.0, 'G': 0.0}\n\n\n\n\n\n\n\nThis is the minimum number of edges that would need to be removed to make the graph disconnected.\nHow easy is it to create isolated nodes/clusters?\nThis is a measure of connectedness.\n\n\n\nThis is the ratio of the number of edges in the grap to the maximum possible number of edges between all nodes.\nFor a directed graph with \\(n\\) nodes, the maximum number of edges is \\(n(n-1)\\).\nFor an undirected graph with \\(n\\) nodes, the maximum number of edges is \\(\\frac{n(n-1)}{2}\\).\nBroadly speaking, a graph is considered dense if \\(density &gt; 0.5\\) and sparse if \\(density &lt; 0.1\\).\n\n\n\n\n\nThere are several categories of learning tasks we may want to perform on a graph:\n\nNode classification: Predict the category of each node. E.g. categorising songs by genre.\nLink prediction: Predict missing links between nodes. E.g. friend recommnedations in a social network.\nGraph classification: Categorise an entire graph.\nGraph generation: Generate a new graph based on desired properties.\n\nThere are several prominent families of graph learning techniques:\n\nGraph signal processing: Apply traditional signal processing techniques like Fourier analysis to graphs to study its connectivity and structure.\nMatrix factorisation: Find low-dimensional representations of large matrices\nRandom walk: Simulate random walks over a graph, e.g. to generate training date for downstream models.\nDeep learning: Encode the graph as tensors and perform deep learning.\n\nTraditional tabular datasets focus on the entities which are represented as rows. But often, the relationships between entities contain vital information. Global features, such as network-wide statistics, may also provide useful information."
  },
  {
    "objectID": "posts/ml/graph_ml/part_1/graph_notes.html#graphs-what-and-why",
    "href": "posts/ml/graph_ml/part_1/graph_notes.html#graphs-what-and-why",
    "title": "Graph ML: What’s a Graph?",
    "section": "",
    "text": "Graphs are a generic way of representing relationships (edges) between entities (nodes).\nThis makes them useful in a wide variety of applications, including modelling biological pathways, social networks, or even images.\nGraphs do not have a fixed structure and the relationships (edges) can be time-varying.\n\n\nEntities are nodes or vertices and their relationships are edges.\nNodes that are connected are adjacent and are called neighbours.\nGraphs are the general case of trees. A graph is broadly something with nodes and edges. A tree is a graph that has no cycles and all nodes are connected.\nA directed graph or digraph shows where relationships can be directional, e.g. social network followers.\nEdges can be weighted to represent quantities like distance or cost.\nA path is the sequence of edges used to get from one vertex to another.\nA simple path does not visit any node more than once.\nA graph is connected if there is a path between every pair of vertices, i.e. there are no isolated nodes or clusters.\nA cycle is a path where the first and last nodes are the same.\n\n\n\n\nTree: An undirected graph with no cycles.\nRooted tree: A tree where one node is designated ads the root.\nDirected Acyclic Graph (DAG): A directed graph with no cycles. This means edges can only be traversed in a particular direction.\nBipartite graphs: A graph where vertices can be divided into two disjoint sets.\nComplete graph: A graph where every pair of vertices is connected by an edge, i.e. it is fully connected."
  },
  {
    "objectID": "posts/ml/graph_ml/part_1/graph_notes.html#implementing-a-graph-from-scratch",
    "href": "posts/ml/graph_ml/part_1/graph_notes.html#implementing-a-graph-from-scratch",
    "title": "Graph ML: What’s a Graph?",
    "section": "",
    "text": "Graphs are an abstract data structure, so there are multiple ways we can represent them in memory.\nSome common options are:\n\nObjects and pointers\nAdjacency lists\nEdge lists\nAdjacency matrix\n\nEach approach has its own trade-offs in terms of memory usage, ease of implementation, and efficiency based on the specific characteristics of the graph you’re dealing with.\nWe’ll implement the following simple graph using each approach:\n\n\n\n\n\nflowchart LR\n\nA[1] &lt;---&gt; B[2]\nB[2] &lt;---&gt; C[3]\n\n\n\n\n\n\n\n\nIn this approach, we represent each node in the graph as an object, and you use pointers to connect these nodes. Each object typically contains a list of pointers to other nodes it’s connected to.\nFor a connected graph, if we have access to one node we can traverse the graph to find all other nodes. This is not true for graphs with unconnected nodes.\nThis approach will be familiar to computer scientists - it bears a resemblance to linked lists. In practice this isn’t used much in practical ML because traversing the graph is more cumbersome.\nPros:\n\nAllows for flexible representation of complex relationships.\nIdeal for graphs where nodes have additional properties beyond just connectivity.\n\nCons:\n\nMemory overhead due to object instantiation and pointer storage.\nCan be complex to implement and manage.\nIf the graph is not connected, an additional data structure is needed to keep track of isolated nodes.\n\nPython Implementation:\n\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.neighbors = []\n\n    def __repr__(self) -&gt; str:\n        return f\"Node {self.value} with neighbors {[k.value for k in self.neighbors]}\"\n\ndef add_edge(node1, node2):\n    node1.neighbors.append(node2)\n    node2.neighbors.append(node1)\n\nnode1 = Node(1)\nnode2 = Node(2)\nnode3 = Node(3)\n\nadd_edge(node1, node2)\nadd_edge(node2, node3)\n\n\nprint(node1)\n\nNode 1 with neighbors [2]\n\n\n\nprint(node2)\n\nNode 2 with neighbors [1, 3]\n\n\n\nprint(node3)\n\nNode 3 with neighbors [2]\n\n\n\n\n\nIn this approach, we store a list of edges per node.\nWe use a list or dictionary where the index or key represents a node, and the value is a list of its adjacent nodes.\nPros:\n\nEfficient memory usage for sparse graphs - \\(O(V + E)\\) space complexity.\nEasy to implement and understand.\nSuitable for graphs with varying degrees of connectivity.\nEasy to iterate through neighbors.\n\nCons:\n\nSlower for dense graphs.\nChecking if nodes are connected is slow - retrieving edge information may require linear search.\n\nPython Implementation:\n\nclass Graph:\n    def __init__(self):\n        self.adj_list = {}\n\n    def __repr__(self) -&gt; str:\n        return str(self.adj_list)\n\n    def add_edge(self, node1, node2):\n        if node1 not in self.adj_list:\n            self.adj_list[node1] = []\n        if node2 not in self.adj_list:\n            self.adj_list[node2] = []\n        self.adj_list[node1].append(node2)\n        self.adj_list[node2].append(node1)\n\n\ngraph = Graph()\ngraph.add_edge(1, 2)\ngraph.add_edge(2, 3)\n\nprint(graph)\n\n{1: [2], 2: [1, 3], 3: [2]}\n\n\n\n\n\nAn even more compact form is the edge list, which lists all of the edges as (source_node, target_node) tuples.\nPros:\n\nEven more efficient memory usage for sparse graphs - \\(O(E)\\) space complexity.\n\nCons:\n\nSlower for dense graphs.\nChecking if nodes are connected is slow - retrieving edge information may require linear search.\n\nPython Implementation:\n\nclass Graph:\n    def __init__(self):\n        self.edge_list = []\n\n    def __repr__(self) -&gt; str:\n        return str(self.edge_list)\n\n    def add_edge(self, node1, node2):\n        node_pair = (node1, node2)\n        if node_pair not in self.edge_list:\n            self.edge_list.append(node_pair)\n\n\ngraph = Graph()\ngraph.add_edge(1, 2)\ngraph.add_edge(2, 3)\n\nprint(graph)\n\n[(1, 2), (2, 3)]\n\n\n\n\n\nIn this approach, we represent the graph as a 2D matrix where rows and columns represent nodes, and matrix cells indicate whether there’s an edge between the nodes.\nFor weighted graphs, the values in the matrix correspond to the weights of the edges.\nPros:\n\nEfficient for dense graphs.\nConstant time access to check edge existence; checking if nodes are connected is \\(O(1)\\).\n\nCons:\n\nNot ideal for graphs with a large number of nodes due to \\(O(V^2)\\) space complexity. This is particularly bad for sparse graphs.\nAdding/removing nodes can be expensive as it requires resizing the matrix. Bad for dynamic graphs.\n\nPython Implementation:\n\nclass Graph:\n    def __init__(self, num_nodes):\n        self.adj_matrix = [[0] * num_nodes for row in range(num_nodes)]\n\n    def __repr__(self) -&gt; str:\n        return str(self.adj_matrix)\n\n    def add_edge(self, node1, node2):\n        self.adj_matrix[node1][node2] = 1\n        self.adj_matrix[node2][node1] = 1\n\n\ngraph = Graph(num_nodes=3)\ngraph.add_edge(0, 1)\ngraph.add_edge(1, 2)\n\nprint(graph)\n\n[[0, 1, 0], [1, 0, 1], [0, 1, 0]]\n\n\nThe adjacency matrix approach is useful in machine learning as it naturally fits into a tensor representation, which most ML libraries (e.g. pytorch, tensorflow) play nicely with. So we’ll stick with tthat going forward."
  },
  {
    "objectID": "posts/ml/graph_ml/part_1/graph_notes.html#implementing-a-graph-with-networkx",
    "href": "posts/ml/graph_ml/part_1/graph_notes.html#implementing-a-graph-with-networkx",
    "title": "Graph ML: What’s a Graph?",
    "section": "",
    "text": "Grpahs are common enough to work with that we don’t need to implement them from scratch every time we want ot use them (although doing so in the previous section is instructive).\nnetworkx is a useful third-party library for this task\n\nimport networkx as nx\n\n\ngraph = nx.Graph()\ngraph.add_edges_from([\n    ('A', 'B'),\n    ('A', 'C'), \n    ('B', 'D'), \n    ('B', 'E'), \n    ('C', 'F'), \n    ('C', 'G'), \n    ('G', 'G'), \n])\n\nprint(graph)\n\nGraph with 7 nodes and 7 edges\n\n\nFrom this Graph object we can see our familiar edge list:\n\ngraph.adj\n\nAdjacencyView({'A': {'B': {}, 'C': {}}, 'B': {'A': {}, 'D': {}, 'E': {}}, 'C': {'A': {}, 'F': {}, 'G': {}}, 'D': {'B': {}}, 'E': {'B': {}}, 'F': {'C': {}}, 'G': {'C': {}, 'G': {}}})\n\n\nOr the adjacency matrix:\n\nnx.adjacency_matrix(graph).todense()\n\n/var/folders/8k/8jqhnfbd1t99blb07r1hs5440000gn/T/ipykernel_50230/1468791322.py:1: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n  nx.adjacency_matrix(graph).todense()\n\n\nmatrix([[0, 1, 1, 0, 0, 0, 0],\n        [1, 0, 0, 1, 1, 0, 0],\n        [1, 0, 0, 0, 0, 1, 1],\n        [0, 1, 0, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0, 0, 0],\n        [0, 0, 1, 0, 0, 0, 0],\n        [0, 0, 1, 0, 0, 0, 1]])"
  },
  {
    "objectID": "posts/ml/graph_ml/part_1/graph_notes.html#graph-metrics",
    "href": "posts/ml/graph_ml/part_1/graph_notes.html#graph-metrics",
    "title": "Graph ML: What’s a Graph?",
    "section": "",
    "text": "This is the number of edges which are incident on a node.\nFor an undirected graph, this is the number of edges connected to the node. Self-loops add 2 to the degree.\nFor a directed graph, we can distinguish the indegree, \\(deg^-(V)\\), as the edges that point towards the node. The outdegree, \\(deg^+(V)\\), denotes the edges that start from the node. Self-loops add 1 to the indegree and 1 to the outdegree.\n\ngraph.degree\n\nDegreeView({'A': 2, 'B': 3, 'C': 3, 'D': 1, 'E': 1, 'F': 1, 'G': 3})\n\n\n\n\n\nCentrality measures the importance of a given node in a network.\nThere a different formulations of centrality:\n\nDegree centrality: This is simply the degree of the node. How many neighbours?\nCloseness centrality: The average length of the shortest path between this node and all others. How fast can I reach my neighbours?\nBetweenness centrality: The number of times a node lies on the shortest path between pairs of other nodes in the graph. How often am I the bottleneck/bridge for others?\n\n\nnx.degree_centrality(graph)\n\n{'A': 0.3333333333333333,\n 'B': 0.5,\n 'C': 0.5,\n 'D': 0.16666666666666666,\n 'E': 0.16666666666666666,\n 'F': 0.16666666666666666,\n 'G': 0.5}\n\n\n\nnx.closeness_centrality(graph)\n\n{'A': 0.6,\n 'B': 0.5454545454545454,\n 'C': 0.5454545454545454,\n 'D': 0.375,\n 'E': 0.375,\n 'F': 0.375,\n 'G': 0.375}\n\n\n\nnx.betweenness_centrality(graph)\n\n{'A': 0.6, 'B': 0.6, 'C': 0.6, 'D': 0.0, 'E': 0.0, 'F': 0.0, 'G': 0.0}\n\n\n\n\n\n\n\nThis is the minimum number of edges that would need to be removed to make the graph disconnected.\nHow easy is it to create isolated nodes/clusters?\nThis is a measure of connectedness.\n\n\n\nThis is the ratio of the number of edges in the grap to the maximum possible number of edges between all nodes.\nFor a directed graph with \\(n\\) nodes, the maximum number of edges is \\(n(n-1)\\).\nFor an undirected graph with \\(n\\) nodes, the maximum number of edges is \\(\\frac{n(n-1)}{2}\\).\nBroadly speaking, a graph is considered dense if \\(density &gt; 0.5\\) and sparse if \\(density &lt; 0.1\\)."
  },
  {
    "objectID": "posts/ml/graph_ml/part_1/graph_notes.html#why-use-graph-learning",
    "href": "posts/ml/graph_ml/part_1/graph_notes.html#why-use-graph-learning",
    "title": "Graph ML: What’s a Graph?",
    "section": "",
    "text": "There are several categories of learning tasks we may want to perform on a graph:\n\nNode classification: Predict the category of each node. E.g. categorising songs by genre.\nLink prediction: Predict missing links between nodes. E.g. friend recommnedations in a social network.\nGraph classification: Categorise an entire graph.\nGraph generation: Generate a new graph based on desired properties.\n\nThere are several prominent families of graph learning techniques:\n\nGraph signal processing: Apply traditional signal processing techniques like Fourier analysis to graphs to study its connectivity and structure.\nMatrix factorisation: Find low-dimensional representations of large matrices\nRandom walk: Simulate random walks over a graph, e.g. to generate training date for downstream models.\nDeep learning: Encode the graph as tensors and perform deep learning.\n\nTraditional tabular datasets focus on the entities which are represented as rows. But often, the relationships between entities contain vital information. Global features, such as network-wide statistics, may also provide useful information."
  },
  {
    "objectID": "posts/software/react/23_deployment/post.html",
    "href": "posts/software/react/23_deployment/post.html",
    "title": "React: Deployment",
    "section": "",
    "text": "We should think about the following when we’re ready to deploy our app:\n\nTest code\nOptimise code\nBuild app\nUpload app\nConfigure server\n\nTesting is handled in a separate post.\nThis post contains some optimisation, build and configuration considerations.\n\n\nLoad code only when it’s needed.\nWhen we import files into other files, they are immediately resolved. This means that we need to load everything before any part of the site loads.\nWith lazy loading, we only load each file as it is needed by the site.\nInstead of:\nimport BlogPage from './pages/Blog';\nWe can use:\nimport { lazy, Suspense } from 'react';\n\nconst BlogPage = lazy(() =&gt; import('./pages/Blog'));\n\n// Then wherever the BlogPage component is used, wrap it with a Suspense component\n&lt;Suspense fallback={&lt;p&gt;Loading...&lt;/p&gt;}&gt;\n  &lt;BlogPost /&gt;\n&lt;/Suspense&gt;\nYou can verify how lazy loading works by looking at the network tab of the browser while navigating the website.\nIt should only load pages as required while you navigate.\n\n\n\nRunning npm run build will create an optimised build which transforms React code to Javascript, CSS and HTML, which are supported natively by browsers.\nThe contents of this build directory should be uploaded to the hosting server.\nA React Single-Page Application (SPA) is a static website. It does not require code to be executed by the server.\nThere are many static site hosts, e.g. Github Pages, Firebase.\n\n\n\nClient-side routing is commonly used in smaller apps to keep it as an SPA.\nThere are multiple “pages” which are handled by react-router. So the server only ever actually returns a single page, index.html, regardless of the URL and the page routing is handled in JavaScript on the client side.\nThis is in contrast to server-side routing, where each page is a separate html file, so different URLs return different HTML pages.\nIf creating the website as an SPA, the deployment job should be configured for this so that the server correctly resolves the different URLs internally rather than trying to serve different HTML files.\n\n\n\n\nSection 23 of “React: The Complete Guide” Udemy course"
  },
  {
    "objectID": "posts/software/react/23_deployment/post.html#lazy-loading",
    "href": "posts/software/react/23_deployment/post.html#lazy-loading",
    "title": "React: Deployment",
    "section": "",
    "text": "Load code only when it’s needed.\nWhen we import files into other files, they are immediately resolved. This means that we need to load everything before any part of the site loads.\nWith lazy loading, we only load each file as it is needed by the site.\nInstead of:\nimport BlogPage from './pages/Blog';\nWe can use:\nimport { lazy, Suspense } from 'react';\n\nconst BlogPage = lazy(() =&gt; import('./pages/Blog'));\n\n// Then wherever the BlogPage component is used, wrap it with a Suspense component\n&lt;Suspense fallback={&lt;p&gt;Loading...&lt;/p&gt;}&gt;\n  &lt;BlogPost /&gt;\n&lt;/Suspense&gt;\nYou can verify how lazy loading works by looking at the network tab of the browser while navigating the website.\nIt should only load pages as required while you navigate."
  },
  {
    "objectID": "posts/software/react/23_deployment/post.html#building-code-for-production",
    "href": "posts/software/react/23_deployment/post.html#building-code-for-production",
    "title": "React: Deployment",
    "section": "",
    "text": "Running npm run build will create an optimised build which transforms React code to Javascript, CSS and HTML, which are supported natively by browsers.\nThe contents of this build directory should be uploaded to the hosting server.\nA React Single-Page Application (SPA) is a static website. It does not require code to be executed by the server.\nThere are many static site hosts, e.g. Github Pages, Firebase."
  },
  {
    "objectID": "posts/software/react/23_deployment/post.html#server-side-routing",
    "href": "posts/software/react/23_deployment/post.html#server-side-routing",
    "title": "React: Deployment",
    "section": "",
    "text": "Client-side routing is commonly used in smaller apps to keep it as an SPA.\nThere are multiple “pages” which are handled by react-router. So the server only ever actually returns a single page, index.html, regardless of the URL and the page routing is handled in JavaScript on the client side.\nThis is in contrast to server-side routing, where each page is a separate html file, so different URLs return different HTML pages.\nIf creating the website as an SPA, the deployment job should be configured for this so that the server correctly resolves the different URLs internally rather than trying to serve different HTML files."
  },
  {
    "objectID": "posts/software/react/23_deployment/post.html#references",
    "href": "posts/software/react/23_deployment/post.html#references",
    "title": "React: Deployment",
    "section": "",
    "text": "Section 23 of “React: The Complete Guide” Udemy course"
  },
  {
    "objectID": "posts/software/react/30_typescript/post.html",
    "href": "posts/software/react/30_typescript/post.html",
    "title": "React: TypeScript Essentials",
    "section": "",
    "text": "TypeScript (TS) is a superset of JavaScript (JS) which adds static typing.\nVanilla JS is dynamically typed. Types do not have to be specified ahead of time. This can result in funky errors like the classic “1” + “2”= “12”.\n\n\n\nIt can be installed like any other package and managed in the project dependencies package.json\nnpm install typescript \nTypescript is compiled. It does not run in the browser directly. Instead, there is a compilation step which converts TS to JS which can run in the browser.\nThis compile step is where we will find type errors, before they hit production.\nnpx tsc\n\n\n\n\n\nThe primitives are number, string, boolean. We also have null and undefined.\nNote that primitive types are lowercase, e.g. number. The object itself, e.g. Number, is not what we want here.\nThere is also an any type which is a catch all. We generally avoid this, as it defeats the purpose of using TS.\n\n\n\nWe have built-in complex types: objects and arrays.\nDefine an array of strings like:\nlet myArray: string[] = [“this”, “is”, “an”, “array”];\nDefine an object type by specifying the keys and their types:\nlet person: {name: string, age: number} = {name: “Gurp”, age: 30}\n\n\n\nUse a pipe to denote where multiple types are allowed, e.g.\nlet val: string|number = 69;\n\n\n\nDefine a type with the type keyword.\nThis allows the type to be reusable if it’s used in multiple places.\n\n\n\nTS infers the output type based on the arguments.\nIf this is correct, it’s common practice to not override this, let it infer. But you may want to override if you want it to output a union of types which it hasn’t inferred.\nFunctions also have a special void return type if they do not return anything.\n\n\n\nIf you have a utility function that can accept any input type, but the output should be the same type as the input, you can denote this using generic types with angled brackets.\nconst myFunc&lt;T&gt;(inputArray: T[], inputValue: T) {\n    return [inputValue, …inputArray]\n}\nThis can be called with strings and would return an array of strings. Or called with numbers and return an array of numbers. Rather than having to use any, we can use the generic (T is arbitrary and just stands for Type) then when we call the function TypeScript will infer the output type correctly.\n\n\n\nThese are often interchangeable. The key difference is that interfaces can be extended, whereas type cannot.\nSee here\n\n\n\nWhen defining a class, you can specify the types of each attribute.\nclass Todo {\n    id: string;\n    text: string;\n\n    constructor(inputText: string) {\n        this.text = inputText;\n        this.id = new Date().toISOString();\n    }\n}\nWhen instances of this class are used, you can simply use the class itself as the type.\nThis is useful for defining data models.\n\n\n\n\nCreating a react project using TypeScript is largely the same, but you will have .tsx files rather than .jsx.\nCertain packages that you install may have additional type annotations packages if they were written in JS, to make them they play nicely with TS. Some packages don’t need it if they were written in TS to begin with.\n\n\nWhen passing props in React, it automatically passes certain default props like children. It would be cumbersome if we had to manually define the types of those default props on every component.\nInstead, we can set the output type of our component as React.FC (Functional Component) and this will handle the default props.\nIf we then want to define our custom prop types, we can do so in angle brackets after:\nReact.FC&lt;{prop1: string, prop2: number}&gt;\nHere we are using a generic type, React.FC. The angled brackets are defining what types are being used in this particular case for this generic type.\nProps are marked as optional by adding a ? after the variable name, i.e. the key in the object.\n\n\n\nThe form submit outputs an event object which can be used by other functions.\nThe type of this can be encapsulated by the React.FormEvent type. Similarly, there is a React.MouseEvent for the onClick listener.\n\n\n\nWe create a ref with useRef then attach it to a component (can be built-in or custom).\nTypeScript doesn’t know which component you intend to attach the ref to, so you need to specify this when creating the ref.\nBy default, useRef returns a generic type, so we need to set the specific type when we call it. We also need to provide a starting value (null) to convince the TypeScript compiler that the ref isn’t already assigned to something else.\nconst inputRef = useRef&lt;HTMLInputElement&gt;(null);\nThen use this in an input element\n&lt;input ref={inputRef} /&gt;\nWhen working with ref.current TypeScript will often demand a ? to indicate that this is possibly null. The resulting value’s type will then be, for example, string or null. If you know it will never be null, you can replace with the ! operator. This means the resulting value will have type string only.\n\n\n\nWhere we pass a function as a prop we define its type as an arrow function specifying inputs and outputs.\nmyFunc: (text: string) =&gt; void\nNote that this is similar to how object types look like an object but don’t actually create an object. Function types look like a function but don’t actually create a function.\nThe .bind method is similar to partial in Python. This is useful when we are passing a function down a prop chain and it will always have a certain argument. Bind saves us having to pass the value and declare its type and every stage of the prop chain.\n\n\n\nThe compilerOptions.target value defines which version of JavaScript the TypeScript compiler will transform the code to.\nThe compilerOptions.lib value defines which TypeScript default types are included out of the box. For example, “DOM” gives support for built-in html types like HTMLInputType.\nIf we want to allow plain JavaScript files in the project alongside TypeScript, we can set compilerOptions.allowJs to True. If False, everything must strictly be TypeScript.\nWe can set a strict compile with compilerOptions.strict. This will forbid implicit any types etc.\n\n\n\nWhen we create a state, we often initialise it with an empty value, e.g. null or an empty array. But then TypeScript does not know what type is going to go in that state later.\nThe useState function returns a generic type so we can overwrite it with our type.\nconst [myState, setMyState] = useState&lt;string[]&gt;([])\n\n\n\n\n\nSection 30 of “React: The Complete Guide” Udemy course\nTypes vs Interfaces"
  },
  {
    "objectID": "posts/software/react/30_typescript/post.html#wtf-is-ts",
    "href": "posts/software/react/30_typescript/post.html#wtf-is-ts",
    "title": "React: TypeScript Essentials",
    "section": "",
    "text": "TypeScript (TS) is a superset of JavaScript (JS) which adds static typing.\nVanilla JS is dynamically typed. Types do not have to be specified ahead of time. This can result in funky errors like the classic “1” + “2”= “12”."
  },
  {
    "objectID": "posts/software/react/30_typescript/post.html#installing-and-using-ts.",
    "href": "posts/software/react/30_typescript/post.html#installing-and-using-ts.",
    "title": "React: TypeScript Essentials",
    "section": "",
    "text": "It can be installed like any other package and managed in the project dependencies package.json\nnpm install typescript \nTypescript is compiled. It does not run in the browser directly. Instead, there is a compilation step which converts TS to JS which can run in the browser.\nThis compile step is where we will find type errors, before they hit production.\nnpx tsc"
  },
  {
    "objectID": "posts/software/react/30_typescript/post.html#types",
    "href": "posts/software/react/30_typescript/post.html#types",
    "title": "React: TypeScript Essentials",
    "section": "",
    "text": "The primitives are number, string, boolean. We also have null and undefined.\nNote that primitive types are lowercase, e.g. number. The object itself, e.g. Number, is not what we want here.\nThere is also an any type which is a catch all. We generally avoid this, as it defeats the purpose of using TS.\n\n\n\nWe have built-in complex types: objects and arrays.\nDefine an array of strings like:\nlet myArray: string[] = [“this”, “is”, “an”, “array”];\nDefine an object type by specifying the keys and their types:\nlet person: {name: string, age: number} = {name: “Gurp”, age: 30}\n\n\n\nUse a pipe to denote where multiple types are allowed, e.g.\nlet val: string|number = 69;\n\n\n\nDefine a type with the type keyword.\nThis allows the type to be reusable if it’s used in multiple places.\n\n\n\nTS infers the output type based on the arguments.\nIf this is correct, it’s common practice to not override this, let it infer. But you may want to override if you want it to output a union of types which it hasn’t inferred.\nFunctions also have a special void return type if they do not return anything.\n\n\n\nIf you have a utility function that can accept any input type, but the output should be the same type as the input, you can denote this using generic types with angled brackets.\nconst myFunc&lt;T&gt;(inputArray: T[], inputValue: T) {\n    return [inputValue, …inputArray]\n}\nThis can be called with strings and would return an array of strings. Or called with numbers and return an array of numbers. Rather than having to use any, we can use the generic (T is arbitrary and just stands for Type) then when we call the function TypeScript will infer the output type correctly.\n\n\n\nThese are often interchangeable. The key difference is that interfaces can be extended, whereas type cannot.\nSee here\n\n\n\nWhen defining a class, you can specify the types of each attribute.\nclass Todo {\n    id: string;\n    text: string;\n\n    constructor(inputText: string) {\n        this.text = inputText;\n        this.id = new Date().toISOString();\n    }\n}\nWhen instances of this class are used, you can simply use the class itself as the type.\nThis is useful for defining data models."
  },
  {
    "objectID": "posts/software/react/30_typescript/post.html#typescript-with-react",
    "href": "posts/software/react/30_typescript/post.html#typescript-with-react",
    "title": "React: TypeScript Essentials",
    "section": "",
    "text": "Creating a react project using TypeScript is largely the same, but you will have .tsx files rather than .jsx.\nCertain packages that you install may have additional type annotations packages if they were written in JS, to make them they play nicely with TS. Some packages don’t need it if they were written in TS to begin with.\n\n\nWhen passing props in React, it automatically passes certain default props like children. It would be cumbersome if we had to manually define the types of those default props on every component.\nInstead, we can set the output type of our component as React.FC (Functional Component) and this will handle the default props.\nIf we then want to define our custom prop types, we can do so in angle brackets after:\nReact.FC&lt;{prop1: string, prop2: number}&gt;\nHere we are using a generic type, React.FC. The angled brackets are defining what types are being used in this particular case for this generic type.\nProps are marked as optional by adding a ? after the variable name, i.e. the key in the object.\n\n\n\nThe form submit outputs an event object which can be used by other functions.\nThe type of this can be encapsulated by the React.FormEvent type. Similarly, there is a React.MouseEvent for the onClick listener.\n\n\n\nWe create a ref with useRef then attach it to a component (can be built-in or custom).\nTypeScript doesn’t know which component you intend to attach the ref to, so you need to specify this when creating the ref.\nBy default, useRef returns a generic type, so we need to set the specific type when we call it. We also need to provide a starting value (null) to convince the TypeScript compiler that the ref isn’t already assigned to something else.\nconst inputRef = useRef&lt;HTMLInputElement&gt;(null);\nThen use this in an input element\n&lt;input ref={inputRef} /&gt;\nWhen working with ref.current TypeScript will often demand a ? to indicate that this is possibly null. The resulting value’s type will then be, for example, string or null. If you know it will never be null, you can replace with the ! operator. This means the resulting value will have type string only.\n\n\n\nWhere we pass a function as a prop we define its type as an arrow function specifying inputs and outputs.\nmyFunc: (text: string) =&gt; void\nNote that this is similar to how object types look like an object but don’t actually create an object. Function types look like a function but don’t actually create a function.\nThe .bind method is similar to partial in Python. This is useful when we are passing a function down a prop chain and it will always have a certain argument. Bind saves us having to pass the value and declare its type and every stage of the prop chain.\n\n\n\nThe compilerOptions.target value defines which version of JavaScript the TypeScript compiler will transform the code to.\nThe compilerOptions.lib value defines which TypeScript default types are included out of the box. For example, “DOM” gives support for built-in html types like HTMLInputType.\nIf we want to allow plain JavaScript files in the project alongside TypeScript, we can set compilerOptions.allowJs to True. If False, everything must strictly be TypeScript.\nWe can set a strict compile with compilerOptions.strict. This will forbid implicit any types etc.\n\n\n\nWhen we create a state, we often initialise it with an empty value, e.g. null or an empty array. But then TypeScript does not know what type is going to go in that state later.\nThe useState function returns a generic type so we can overwrite it with our type.\nconst [myState, setMyState] = useState&lt;string[]&gt;([])"
  },
  {
    "objectID": "posts/software/react/30_typescript/post.html#references",
    "href": "posts/software/react/30_typescript/post.html#references",
    "title": "React: TypeScript Essentials",
    "section": "",
    "text": "Section 30 of “React: The Complete Guide” Udemy course\nTypes vs Interfaces"
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html",
    "href": "posts/software/react/2_js_essentials/post.html",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "These notes serve as a JavaScript refresher, and for me a gentle introduction to JavaScript coming from Python.\n\n\n\n\n\nJavascript is supported natively by the browser, so we can add it directly to the html of a webpage.\nWe can add it either in the body of the &lt;script&gt; tag, or preferably as a separate .js file that’s then called as the src parameter of the script tag.\nThe defer parameter means the script won’t be called until the rest of the body is loaded.\nThe type=module parameter means the JavaScript file will be treated as a module rather than executed as a script.\n\n\n\nJSX is not natively supported by the browser, so a build tool transforms it to regular JavaScript.\nIt also minifies the project to optimise the size and loading times.\n\n\n\nWe need to use the export keyword to make a function or variable available outside of that file. Each file can have at most one default export.\nThe import keyword then lets us use this.\nUse curly braces for the import unless it is a default export. If it is a default export, you assign your own name to the imported variable. The path to import from is in single or double quotes, with the file extension in plain JS. In React, some build tools automatically populate the file extension so you don’t need it.\nWe can group the imports if there are many by using starred imports.\nimport * as utils from “./utils.js”;\nThen use utils.blah to use those imported values.\nWe can also alias individual imported variables with the as keyword.\n\n\n\nThe primitives in JavaScript are: string, number, boolean, null, undefined.\nThe are also complex types built in: object, array.\nVariables are defined with the let keyword. Camel case is most common in JS.\nConstants are defined with the const keyword. They cannot be reassigned. Prefer const where it is appropriate, to be clear about your intentions that this should not be reassigned.\nOlder versions of JavaScript did not make this distinction and used var in all cases. This is discouraged now.\n\n\n\nThese include add, subtract, divide, multiply.\nThese can be defined on any types, not just numbers.\nTriple equals === is used to compare values.\n\n\n\nFunctions can be defined using “regular” syntax or “arrow” syntax.\nRegular syntax:\nfunction sum(a, b) {\n    return a + b;\n}\nArrow function syntax:\nconst sum = (a, b) =&gt; {a + b};\nThe function can then be invoked as\nsum(1, 3)\nWe can set default values of variables as\nconst sum = (a, b = 1) =&gt; {a + b};\nFunctions can be passed as props to other functions. This is helpful when defining components which we want to pass state setters or other handler functions to (functional components are ultimately just functions themselves).\nWe can also define functions inside of other functions. This is helpful when we want the function to be scoped only to the outer function, not defined globally. This is again used a lot in React since we may want to define functions with our (functional) components.\n\n\n\nObjects are key-value pairs. The value for a given key can be accessed with ., for example:\nobj.key1\nObjects can also have methods. These are functions defined inside the object.\nconst obj = {\n    name: “Gurp”,\n    method1: greet() {return “Hello “ + this.name}\n}\nobj.greet()\nClass instances are essentially objects like above. If we want to create a reusable class, we can formally define a class.\nclass User (\n    constructor(name, age) {\n        this.name = name;\n        this.age = age;\n    }\n\n    greet() {\n        return “Hello “ + this.name;\n    }\n)\n\nconst user1 = new User(\"Gurp\", 30);\nObjects (and, by extension, arrays) are passed by reference. So when we modify the object, it does not create a new object, it mutates the original. The memory address is stored as a constant, not the value. So if we create an object as a const, we can modify it without reassigning it.\n\n\n\nArrays are technically a special case of object.\nconst array1 = [1, 2, 3, 4];\nThere are some built-in utility methods of arrays that are particularly helpful/common:\n\n\n\nMethod\nExample\nDocs\n\n\n\n\npush\narray1.push(5);\npush docs\n\n\nmap\nconst squares = array1.map((item) =&gt; item * 2);\nmap docs\n\n\nfind\nconst found = array1.find((element) =&gt; element &gt; 3);\nfind docs\n\n\nfindIndex\narr.findIndex((item) =&gt; item===2)\nfindIndex docs\n\n\nfilter\nconst result = array1.filter((item) =&gt; item &gt; 2);\nfilter docs\n\n\nreduce\nconst summedArray = array1.reduce((accumulator, currentValue) =&gt; accumulator + currentValue);\nreduce docs\n\n\nconcat\nconst array3 = array1.concat(array2);\nconcat docs\n\n\nslice\narray1.slice(1,3)  // returns [2, 3]\nslice docs\n\n\nsplice\nmonths.splice(4, 1, 'May');  // Replaces 1 element at index 4\nsplice docs\n\n\n\n\n\n\nArray destructuring allows us to pick out the values of an array rather than assigning them one-by-one.\nconst [firstName, lastName] = [\"Gurp\", \"Johl\"]\ninstead of\nconst nameArray = [\"Gurp\", \"Johl\"]\nconst firstName = nameArray[0]\nconst lastName =  nameArray[1]\nSimilarly, we can destructure objects too. We can also alias the keys with a :, as in the example below where the name key is aliased to userName.\nconst {name: userName, age} = {name: 'Gurp', age: 30}\ninstead of\nconst userObj = {name: 'Gurp', age: 30}\nconst userName = userObj.name\nconst age = userObj.age\n\n\n\nThe spread operator pulls out values of arrays and objects. This is useful for merging multiple arrays, e.g.\nconst arrayA = [1, 2, 3]\nconst arrayB = [4, 5, 6]\nconst mergedArray = [...arrayA, ...arrayB]\nThe same applies to merging objects.\n\n\n\nIf-else clauses work similarly to other languages:\nif userName === \"Gurp\" {\n    // Do something\n} else {\n    // Do something else\n}\nFor loops again are similar, although the syntax looks a bit janky at first compared to Python:\nconst hobbies = [\"Sports\", \"Music\"];\n\nfor (const hobby of hobbies) {\n    console.log(hobby);\n}\n\n\n\n\nMDN docs\nSection 2 of “React: The Complete Guide” Udemy course"
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#adding-javascript-to-a-page",
    "href": "posts/software/react/2_js_essentials/post.html#adding-javascript-to-a-page",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "Javascript is supported natively by the browser, so we can add it directly to the html of a webpage.\nWe can add it either in the body of the &lt;script&gt; tag, or preferably as a separate .js file that’s then called as the src parameter of the script tag.\nThe defer parameter means the script won’t be called until the rest of the body is loaded.\nThe type=module parameter means the JavaScript file will be treated as a module rather than executed as a script."
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#react-projects-use-a-build-process.",
    "href": "posts/software/react/2_js_essentials/post.html#react-projects-use-a-build-process.",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "JSX is not natively supported by the browser, so a build tool transforms it to regular JavaScript.\nIt also minifies the project to optimise the size and loading times."
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#imports-and-exports",
    "href": "posts/software/react/2_js_essentials/post.html#imports-and-exports",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "We need to use the export keyword to make a function or variable available outside of that file. Each file can have at most one default export.\nThe import keyword then lets us use this.\nUse curly braces for the import unless it is a default export. If it is a default export, you assign your own name to the imported variable. The path to import from is in single or double quotes, with the file extension in plain JS. In React, some build tools automatically populate the file extension so you don’t need it.\nWe can group the imports if there are many by using starred imports.\nimport * as utils from “./utils.js”;\nThen use utils.blah to use those imported values.\nWe can also alias individual imported variables with the as keyword."
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#variables",
    "href": "posts/software/react/2_js_essentials/post.html#variables",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "The primitives in JavaScript are: string, number, boolean, null, undefined.\nThe are also complex types built in: object, array.\nVariables are defined with the let keyword. Camel case is most common in JS.\nConstants are defined with the const keyword. They cannot be reassigned. Prefer const where it is appropriate, to be clear about your intentions that this should not be reassigned.\nOlder versions of JavaScript did not make this distinction and used var in all cases. This is discouraged now."
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#operators",
    "href": "posts/software/react/2_js_essentials/post.html#operators",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "These include add, subtract, divide, multiply.\nThese can be defined on any types, not just numbers.\nTriple equals === is used to compare values."
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#functions",
    "href": "posts/software/react/2_js_essentials/post.html#functions",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "Functions can be defined using “regular” syntax or “arrow” syntax.\nRegular syntax:\nfunction sum(a, b) {\n    return a + b;\n}\nArrow function syntax:\nconst sum = (a, b) =&gt; {a + b};\nThe function can then be invoked as\nsum(1, 3)\nWe can set default values of variables as\nconst sum = (a, b = 1) =&gt; {a + b};\nFunctions can be passed as props to other functions. This is helpful when defining components which we want to pass state setters or other handler functions to (functional components are ultimately just functions themselves).\nWe can also define functions inside of other functions. This is helpful when we want the function to be scoped only to the outer function, not defined globally. This is again used a lot in React since we may want to define functions with our (functional) components."
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#objects",
    "href": "posts/software/react/2_js_essentials/post.html#objects",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "Objects are key-value pairs. The value for a given key can be accessed with ., for example:\nobj.key1\nObjects can also have methods. These are functions defined inside the object.\nconst obj = {\n    name: “Gurp”,\n    method1: greet() {return “Hello “ + this.name}\n}\nobj.greet()\nClass instances are essentially objects like above. If we want to create a reusable class, we can formally define a class.\nclass User (\n    constructor(name, age) {\n        this.name = name;\n        this.age = age;\n    }\n\n    greet() {\n        return “Hello “ + this.name;\n    }\n)\n\nconst user1 = new User(\"Gurp\", 30);\nObjects (and, by extension, arrays) are passed by reference. So when we modify the object, it does not create a new object, it mutates the original. The memory address is stored as a constant, not the value. So if we create an object as a const, we can modify it without reassigning it."
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#arrays",
    "href": "posts/software/react/2_js_essentials/post.html#arrays",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "Arrays are technically a special case of object.\nconst array1 = [1, 2, 3, 4];\nThere are some built-in utility methods of arrays that are particularly helpful/common:\n\n\n\nMethod\nExample\nDocs\n\n\n\n\npush\narray1.push(5);\npush docs\n\n\nmap\nconst squares = array1.map((item) =&gt; item * 2);\nmap docs\n\n\nfind\nconst found = array1.find((element) =&gt; element &gt; 3);\nfind docs\n\n\nfindIndex\narr.findIndex((item) =&gt; item===2)\nfindIndex docs\n\n\nfilter\nconst result = array1.filter((item) =&gt; item &gt; 2);\nfilter docs\n\n\nreduce\nconst summedArray = array1.reduce((accumulator, currentValue) =&gt; accumulator + currentValue);\nreduce docs\n\n\nconcat\nconst array3 = array1.concat(array2);\nconcat docs\n\n\nslice\narray1.slice(1,3)  // returns [2, 3]\nslice docs\n\n\nsplice\nmonths.splice(4, 1, 'May');  // Replaces 1 element at index 4\nsplice docs"
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#destructuring",
    "href": "posts/software/react/2_js_essentials/post.html#destructuring",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "Array destructuring allows us to pick out the values of an array rather than assigning them one-by-one.\nconst [firstName, lastName] = [\"Gurp\", \"Johl\"]\ninstead of\nconst nameArray = [\"Gurp\", \"Johl\"]\nconst firstName = nameArray[0]\nconst lastName =  nameArray[1]\nSimilarly, we can destructure objects too. We can also alias the keys with a :, as in the example below where the name key is aliased to userName.\nconst {name: userName, age} = {name: 'Gurp', age: 30}\ninstead of\nconst userObj = {name: 'Gurp', age: 30}\nconst userName = userObj.name\nconst age = userObj.age"
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#spread-operator",
    "href": "posts/software/react/2_js_essentials/post.html#spread-operator",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "The spread operator pulls out values of arrays and objects. This is useful for merging multiple arrays, e.g.\nconst arrayA = [1, 2, 3]\nconst arrayB = [4, 5, 6]\nconst mergedArray = [...arrayA, ...arrayB]\nThe same applies to merging objects."
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#control-structures",
    "href": "posts/software/react/2_js_essentials/post.html#control-structures",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "If-else clauses work similarly to other languages:\nif userName === \"Gurp\" {\n    // Do something\n} else {\n    // Do something else\n}\nFor loops again are similar, although the syntax looks a bit janky at first compared to Python:\nconst hobbies = [\"Sports\", \"Music\"];\n\nfor (const hobby of hobbies) {\n    console.log(hobby);\n}"
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#references",
    "href": "posts/software/react/2_js_essentials/post.html#references",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "MDN docs\nSection 2 of “React: The Complete Guide” Udemy course"
  },
  {
    "objectID": "posts/software/react/6_styling/post.html",
    "href": "posts/software/react/6_styling/post.html",
    "title": "React: Styling",
    "section": "",
    "text": "First stylin’, then profilin’. Woo!\n\n\nThere are several options of styling in React:\n\nVanilla CSS with separate modules\nIn-line CSS styles\nScoped CSS with modules\nCSS-in-React with styled components\nTailwind CSS\n\nWe will look at each in turn, along with their relative merits.\nIn all cases, styling can be static or dynamic.\n\n\n\nIn plain CSS we can use:\n\nElement selectors - header h1\nID selectors - #auth-inputs\nClass selectors - .controls\n\nImport the .css file into the JSX file you want to style. You can have multiple css files, placing each next to the component JSX file it relates to.\nAdvantages:\n\nDecouples styling from JSX.\nCSS can be modified independently of the code logic, when working with multiple developers .\nMore developers will be familiar with plain CSS.\n\nDisadvantages:\n\nStyles are not scoped to components, which can lead to clashes and unexpected behaviour. The CSS just gets injected into the styles part of the page by the build process, so they apply globally.\n\nDynamic styling can be achieved by having two different class names and conditionally switching between them with a ternary expression.\n\n\n\nYou can set the style prop of each component directly.\nAdvantages:\n\nQuick to add\nStyles are scoped to the component\nDynamic styling is easy\n\nDisadvantages:\n\nYou have to style each component individually\nNo separation between CSS and JSX code\n\n\n\n\nFile-specific scoping for CSS classes.\nUsing .module in the css file name, e.g. Header.module.css will scope the styles to the file that it’s imported into.\nThe import is done slightly differently as it now returns a JavaScript object.\nimport { styles } from ‘Header.module.css’\nThis module approach is not supported natively by browsers. Instead, the build tool takes each of your classes and renames it to ensure it is unique per file. These transformed styles are what you see in the rendered DOM.\nConditional switching for dynamic styles works the same as vanilla CSS.\nAdvantages:\n\nDecouple CSS from JS\nCSS classes are scoped to file\n\nDisadvantages:\n\nBigger projects end up with many small CSS module files\n\n\n\n\nInstall this with\nnpm install styled-components\nThis keeps components and styles linked as a combined object that can be reused in multiple places.\nUse backticks to define a style template :\nconst Container = styled.div`\n    Styling goes here\n`\nThis creates a regular div under the hood. Any props passed to it will get forwarded to the underlying component so we can use it like normal.\nWe can pass functions in between the backticks for dynamic styling. Any props passed to the component are forwarded, so we can use these in functions to set styles.\nconst Label = styled.label`\n    display: block;\n    margin-bottom: 0.5rem;\n    color: ${({invalid}) =&gt; invalid ? 'red' : 'green'}\n`\nA common convention is to name any props used only for styling with a $ at the start to ensure they don’t clash with any other built-in props of the component.\n\n\n\nThis is another 3rd-party framework. The idea is you don’t need to know CSS. Instead you apply small, pre-defined utility classes to each component to achieve a style, and these abstract away a lot of the CSS styles.\nUse a VSCode tailwind plugin to get autocomplete suggestions.\nMedia queries can be used to apply different styles depending on the screen size. E.g. md:, hover:, etc\nAdvantages:\n\nDon’t need to know CSS.\nStyles are scoped to the component.\nConfigurable and extensible.\nRapid development (once you know the built-in class names).\n\nDisadvantages:\n\nLong class names.\nChanging styling requires modifying JSX.\nYou end up with lots of small wrapper components or lots of copy paste.\nYou need to learn the built-in class names.\n\n\n\n\n\nSection 6 of “React: The Complete Guide” Udemy course"
  },
  {
    "objectID": "posts/software/react/6_styling/post.html#styling-options-in-react",
    "href": "posts/software/react/6_styling/post.html#styling-options-in-react",
    "title": "React: Styling",
    "section": "",
    "text": "There are several options of styling in React:\n\nVanilla CSS with separate modules\nIn-line CSS styles\nScoped CSS with modules\nCSS-in-React with styled components\nTailwind CSS\n\nWe will look at each in turn, along with their relative merits.\nIn all cases, styling can be static or dynamic."
  },
  {
    "objectID": "posts/software/react/6_styling/post.html#vanilla-css",
    "href": "posts/software/react/6_styling/post.html#vanilla-css",
    "title": "React: Styling",
    "section": "",
    "text": "In plain CSS we can use:\n\nElement selectors - header h1\nID selectors - #auth-inputs\nClass selectors - .controls\n\nImport the .css file into the JSX file you want to style. You can have multiple css files, placing each next to the component JSX file it relates to.\nAdvantages:\n\nDecouples styling from JSX.\nCSS can be modified independently of the code logic, when working with multiple developers .\nMore developers will be familiar with plain CSS.\n\nDisadvantages:\n\nStyles are not scoped to components, which can lead to clashes and unexpected behaviour. The CSS just gets injected into the styles part of the page by the build process, so they apply globally.\n\nDynamic styling can be achieved by having two different class names and conditionally switching between them with a ternary expression."
  },
  {
    "objectID": "posts/software/react/6_styling/post.html#inline-styles",
    "href": "posts/software/react/6_styling/post.html#inline-styles",
    "title": "React: Styling",
    "section": "",
    "text": "You can set the style prop of each component directly.\nAdvantages:\n\nQuick to add\nStyles are scoped to the component\nDynamic styling is easy\n\nDisadvantages:\n\nYou have to style each component individually\nNo separation between CSS and JSX code"
  },
  {
    "objectID": "posts/software/react/6_styling/post.html#css-modules",
    "href": "posts/software/react/6_styling/post.html#css-modules",
    "title": "React: Styling",
    "section": "",
    "text": "File-specific scoping for CSS classes.\nUsing .module in the css file name, e.g. Header.module.css will scope the styles to the file that it’s imported into.\nThe import is done slightly differently as it now returns a JavaScript object.\nimport { styles } from ‘Header.module.css’\nThis module approach is not supported natively by browsers. Instead, the build tool takes each of your classes and renames it to ensure it is unique per file. These transformed styles are what you see in the rendered DOM.\nConditional switching for dynamic styles works the same as vanilla CSS.\nAdvantages:\n\nDecouple CSS from JS\nCSS classes are scoped to file\n\nDisadvantages:\n\nBigger projects end up with many small CSS module files"
  },
  {
    "objectID": "posts/software/react/6_styling/post.html#styled-components-third-party-library",
    "href": "posts/software/react/6_styling/post.html#styled-components-third-party-library",
    "title": "React: Styling",
    "section": "",
    "text": "Install this with\nnpm install styled-components\nThis keeps components and styles linked as a combined object that can be reused in multiple places.\nUse backticks to define a style template :\nconst Container = styled.div`\n    Styling goes here\n`\nThis creates a regular div under the hood. Any props passed to it will get forwarded to the underlying component so we can use it like normal.\nWe can pass functions in between the backticks for dynamic styling. Any props passed to the component are forwarded, so we can use these in functions to set styles.\nconst Label = styled.label`\n    display: block;\n    margin-bottom: 0.5rem;\n    color: ${({invalid}) =&gt; invalid ? 'red' : 'green'}\n`\nA common convention is to name any props used only for styling with a $ at the start to ensure they don’t clash with any other built-in props of the component."
  },
  {
    "objectID": "posts/software/react/6_styling/post.html#tailwind-css",
    "href": "posts/software/react/6_styling/post.html#tailwind-css",
    "title": "React: Styling",
    "section": "",
    "text": "This is another 3rd-party framework. The idea is you don’t need to know CSS. Instead you apply small, pre-defined utility classes to each component to achieve a style, and these abstract away a lot of the CSS styles.\nUse a VSCode tailwind plugin to get autocomplete suggestions.\nMedia queries can be used to apply different styles depending on the screen size. E.g. md:, hover:, etc\nAdvantages:\n\nDon’t need to know CSS.\nStyles are scoped to the component.\nConfigurable and extensible.\nRapid development (once you know the built-in class names).\n\nDisadvantages:\n\nLong class names.\nChanging styling requires modifying JSX.\nYou end up with lots of small wrapper components or lots of copy paste.\nYou need to learn the built-in class names."
  },
  {
    "objectID": "posts/software/react/6_styling/post.html#references",
    "href": "posts/software/react/6_styling/post.html#references",
    "title": "React: Styling",
    "section": "",
    "text": "Section 6 of “React: The Complete Guide” Udemy course"
  },
  {
    "objectID": "posts/software/data_structures_algos/sorting/sorting.html",
    "href": "posts/software/data_structures_algos/sorting/sorting.html",
    "title": "Sorting Algorithms",
    "section": "",
    "text": "Sorting algorithms come up so frequently that they deserve their own section.\nIn general, we want to solve the problem:\n\nGiven an array of unsorted values, how can we sort them in ascending order?\n\n\n\n\nAlgorithm\nBest Case\nAverage Case\nWorst Case\n\n\n\n\nBubble sort\n\\(N^2\\)\n\\(N^2\\)\n\\(N^2\\)\n\n\nSelection sort\n\\(\\frac{N^2}{2}\\)\n\\(\\frac{N^2}{2}\\)\n\\(\\frac{N^2}{2}\\)\n\n\nInsertion sort\n\\(N\\)\n\\(\\frac{N^2}{2}\\)\n\\(N^2\\)\n\n\nQuicksort\n\\(N log N\\)\n\\(N log N\\)\n\\(N^2\\)\n\n\nMerge sort\n\\(N log N\\)\n\\(N log N\\)\n\\(N log N\\)\n\n\nHeap sort\n\\(N log N\\)\n\\(N log N\\)\n\\(N log N\\)\n\n\nCounting sort\n\\(N + K\\)\n\\(N + K\\)\n\\(N + K\\)\n\n\nRadix sort\n\\(N * D\\)\n\\(N * D\\)\n\\(N * D\\)\n\n\n\nSorting is often a pre-requisite for other algorithms, so often as a pre-processing step we want to presort the array. This means the efficiency of the sort is important to the overall efficiency of many other problem types.\n\n\n\n\nWe “bubble up” the next highest unsorted number on each pass-through.\n\nStart with 2 pointers pointing at the first two values in the array.\nCompare the left and right values. If left_value &gt; right_value, swap them. Otherwise, do nothing.\nMove both pointers one cell rightwards.\nRepeat Steps 2-3 until we reach values that have already been sorted. This completes the first “pass-through” and means we have “bubbled up” the biggest number to the end of the array.\nStart over. repeat Steps 1-4 to bubble up the second biggest number into the second to last position. Repeat this until we perform a pass through with no swaps.\n\n\n\n\n\ndef bubble_sort(array):\n    \"\"\"Bubble sort algorithm to sort an array into ascending order.\n\n    Note we ignore edge cases for the sake of clarity.\n    These are left as an exercise for the reader:\n    - array is empty\n    - array has only 1 element\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n\n    Returns\n    -------\n    array: list\n        The input array sorted in-place.\n    \"\"\"\n    # Initially the entire array is unsorted\n    last_unsorted_index = len(array) - 1\n    is_sorted = False\n\n    while not is_sorted:\n        # Set this to True before we pass through the elements, \n        # then if we need to perform a swap the array is not sorted so we set it to False\n        is_sorted = True\n\n        # Perform a pass-through\n        for left_pointer in range(last_unsorted_index):\n            right_pointer = left_pointer + 1\n\n            if array[left_pointer] &gt; array[right_pointer]:\n                # Swap the values\n                array[left_pointer], array[right_pointer] = array[right_pointer], array[left_pointer]\n                is_sorted = False\n\n        # The pass-through is finished so the next highest value has been \"bubbled up\".        \n        last_unsorted_index -= 1\n\n    return array\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\nbubble_sort(input_array)\n\n[0, 1, 3, 5, 7, 8, 8, 9, 12, 23, 69, 420]\n\n\n\n\n\nEach pass through loops through one fewer element:\n\n\n\nPass-through number\nNumber of operations\n\n\n\n\n1\nN-1\n\n\n2\nN-2\n\n\n3\nN-3\n\n\n4\nN-4\n\n\n5\nN-5\n\n\n…\n…\n\n\nk\nN-k\n\n\n\nSo in total, there are \\((N-1) + (N-2) + (N-3) + ... + 1\\) comparisons. This is the sum of an arithmetic progression, which we can calculate as \\(\\frac{N^2}{2}\\).\nAlso worth noting that in the worst case - an input array in descending order - each comparison will also result in a swap. This does not affect the Big-O complexity but would slow down the run time in practice. There are \\(\\frac{N^2}{2}\\) comparisons and up to \\(\\frac{N^2}{2}\\) swaps, resulting in \\(O(N^2)\\) total operations.\nThe complexity is therefore \\(O(N^2)\\).\nIn general, any nested loop should be a hint at quadratic time complexity.\n\n\n\n\n\n\nFind the smallest value from the unsorted part of the array and put it at the beginning.\n\nCheck each cell of the array from left to right to find the lowest value. Store the index of the running minimum value.\nAt the end of pass-through \\(j\\) (starting at 0), swap the minimum value with the one at index \\(j\\).\nRepeat steps 1-2 until we reach a pass-through that would start at the end of the array, i.e. \\(j = N-1\\)\n\n\n\n\n\ndef selection_sort(array):\n    \"\"\"Selection sort algorithm to sort an array into ascending order.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n\n    Returns\n    -------\n    array: list\n        The input array sorted in-place.\n    \"\"\"\n    array_length = len(array)\n\n    # Loop through all array elements\n    for pass_through_number in range(array_length):\n\n        # Find the minimum element in the remaining unsorted array\n        min_index = pass_through_number\n        for idx_unsorted_section in range(pass_through_number + 1, array_length):\n            if array[idx_unsorted_section] &lt; array[min_index]:\n                min_index = idx_unsorted_section\n\n        # Swap the found minimum element with the first element\n        array[pass_through_number], array[min_index] = array[min_index], array[pass_through_number]\n    \n    return array\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\nselection_sort(input_array)\n\n[0, 1, 3, 5, 7, 8, 8, 9, 12, 23, 69, 420]\n\n\n\n\n\nAs with bubble sort, on each pass-through we loop through one fewer element, so there are \\(\\frac{N^2}{2}\\) comparisons. But each pass-through only performs 1 swap, so the total number of operations is \\(\\frac{N^2}{2} + N\\).\nThis is still therefore \\(O(N^2)\\) complexity, but it should be about twice as fast as bubble sort.\n\n\n\n\n\n\nRemove a value to create a gap, shift values along to move the gap leftwards, then fill the gap.\n\nCreate a gap. For the first pass-through, we temporarily remove the second cell (i.e. index 1) and store it as a temporary variable. This leaves a gap at that index.\nShifting phase. Take each value to the left of the gap and compare it to the temporary variable. if left_val &gt; temp_val, move left value to the right. This has the same effect as moving the gap leftwards. As soon as we encounter a value where left_val &lt; temp_val the shifting phase is complete.\nFill the gap. Insert the temporary value into the current gap.\nRepeat pass-throughs. Steps 1-3 constitute a single pass-through. Repeat this until the pass through begins at the final index of the array. At this point the array is sorted.\n\n\n\n\n\ndef insertion_sort(array):\n    \"\"\"Insertion sort algorithm to sort an array into ascending order.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n\n    Returns\n    -------\n    array: list\n        The input array sorted in-place.\n    \"\"\"\n    array_len = len(array)\n\n    for pass_thru_number in range(1, array_len):\n        # 1. Create a gap\n        temp_val = array[pass_thru_number]\n        gap_idx = pass_thru_number\n\n        # 2. Shifting phase\n        # Move leftwards from the gap and keep shifting elements right if they are greater than temp_val\n        while (gap_idx &gt; 0) and (array[gap_idx - 1] &gt; temp_val):\n            array[gap_idx] = array[gap_idx - 1]\n            gap_idx -= 1\n\n        # 3. Fill the gap\n        array[gap_idx] = temp_val\n\n    return array\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\ninsertion_sort(input_array)\n\n[0, 1, 3, 5, 7, 8, 8, 9, 12, 23, 69, 420]\n\n\n\n\n\nThe worst case is when the input array is sorted in descending order.\nThere are 4 types of operations that occur:\n\n\nIf we compare values to the left of the gap on each step, there will be 1 comparison on the first pass-through, 2 on the second, 3 on the third, etc.\nSo there are \\(1 + 2 + ... + N-1 = \\frac{N^2}{2}\\) comparisons in the worst case.\n\n\n\nEach comparison could result in a shift, so there are the same number of shifts as comparisons in the worst case.\n\n\n\nWe remove 1 temp value per pass-through, so there are \\(N-1\\) removals.\nWe insert that value at the end of each pass-through, so there are also \\(N-1\\) insertions.\n\n\n\n\n\n\nOperation\nNumber\n\n\n\n\nRemovals\n\\(N-1\\)\n\n\nComparisons\n\\(\\frac{N^2}{2}\\)\n\n\nShifts\n\\(\\frac{N^2}{2}\\)\n\n\nInsertions\n\\(N-1\\)\n\n\n\nOverall there are \\(N^2 + 2N - 2\\) operations, so the complexity is \\(O(N^2)\\).\n\n\n\n\nThe complexity generally considers the worst case, but insertion sort varies greatly based on the input.\nWe saw in the worst case the number of steps is \\(N^2 + 2N - 2\\).\nIn the best case the data is already sorted, so we do a remove and an insert on each pass-through, for a totla of \\(2N - 2\\) steps.\nIn the average case, let’s assume about half of the data is already sorted. So we’ll still need to do \\(N - 1\\) removes and \\(N - 1\\) inserts in total. We’ll also need to compare about half the data, so \\(\\frac{N^2}{4}\\) comparisons and the same number on swaps. This gives a total of \\(\\frac{N^2}{2} + 2N - 2\\) steps.\nDepending on the state of the input data, the speed can vary considerably. Compare this to selection sort, which will always take \\(\\frac{N^2}{2}\\) steps regardless of the input data.\n\n\n\nAlgorithm\nBest Case\nAverage Case\nWorst Case\n\n\n\n\nSelection sort\n\\(\\frac{N^2}{2}\\)\n\\(\\frac{N^2}{2}\\)\n\\(\\frac{N^2}{2}\\)\n\n\nInsertion sort\n\\(N\\)\n\\(\\frac{N^2}{2}\\)\n\\(N^2\\)\n\n\n\nSo the choice of which algorithm is “best” depends on the state of your input data. Both have the same \\(O(N^2)\\) time complexity.\n\n\n\n\nQuicksort is a sorting algorithm that relies on the concept of partitioning.\n\n\nThe idea behind partitioning is we take a random value from the array which we call the pivot.\nWe then want to ensure any smaller numbers are left of the pivot and any larger numbers to its right.\n\nIf we take the rightmost value as the pivot, we create a left pointer pointing at the leftmost value and a right pointer at the second from the right (i.e. not the pivot but the next rightmost).\nThe left pointer moves rightwards until it reaches a value &gt;= the pivot value.\nThe right pointer moves leftwards until it reaches a value &lt;= the pivot value.\nIf at this pointer the left pointer has crossed past the right pointer, move straight on to the next step. Otherwise swap the values that the left and right pointers are pointing at.\nSwap the pivot value with the left pointer’s value.\n\n\ndef partition(array):\n    \"\"\"Partition an array around the rightmost value.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to partition.\n\n    Returns\n    -------\n    array: list\n        The input array partitioned in-place.\n    \"\"\"\n    # 1. Initialise the pivot and pointers\n    pivot_idx = len(array) - 1\n    left_pointer_idx = 0\n    right_pointer_idx = pivot_idx - 1\n\n    # Loop until the left and right pointers cross \n    while left_pointer_idx &lt; right_pointer_idx:\n        # 2. Move the left pointer\n        while (left_pointer_idx &lt; pivot_idx) and (array[left_pointer_idx] &lt; array[pivot_idx]):\n            left_pointer_idx += 1\n\n        # 3. Move the right pointer\n        while (right_pointer_idx &gt; 0) and (array[right_pointer_idx] &gt; array[pivot_idx]):\n            right_pointer_idx -= 1\n\n        # 4. Swap values if the pointers haven't crossed yet, otherwise end the loop\n        if left_pointer_idx &lt; right_pointer_idx:\n            array[left_pointer_idx], array[right_pointer_idx] = array[right_pointer_idx], array[left_pointer_idx]\n\n    # 5. Swap the pivot and left_pointer values\n    array[left_pointer_idx], array[pivot_idx] = array[pivot_idx], array[left_pointer_idx]\n\n    return array\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\npartition(input_array)\n\n([8, 7, 5, 3, 1, 9, 0, 8, 12, 69, 420, 23], 8)\n\n\nPartitioning isn’t the same as sorting the array, but it’s “sorted-ish”.\n\n\n\nQuicksort is a combination of partitions and recursion.\n\nPartition the array. The pivot is now at the correct position in the sorted array.\nConsider the subarrays to the left and the right of the pivot as their own arrays. We’ll partition these recursively.\nIf a subarray has 0 or 1 elements, that is the base case and we do nothing; it is already sorted.\n\nWe’ll make a few tweaks to the partition function to take a start index and an end index, and to also return the pivot index. These extra parameters will allow us to call it recursively in quicksort.\n\n\n\n\ndef partition(array, start_idx, end_idx):\n    \"\"\"Partition an array around the rightmost value.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to partition.\n    start_idx: int\n        The start index of the array to consider.\n    end_index: int\n        The end index of the array to consider.\n\n    Returns\n    -------\n    array: list\n        The input array partitioned in-place.\n    final_pivot_idx: int\n        The index position of the pivot point in the partitioned array.\n    \"\"\"\n    # 1. Initialise the pivot and pointers\n    pivot_idx = end_idx\n    left_pointer_idx = start_idx\n    right_pointer_idx = pivot_idx - 1\n\n    # Loop until the left and right pointers cross \n    while left_pointer_idx &lt; right_pointer_idx:\n        # 2. Move the left pointer\n        while (left_pointer_idx &lt; pivot_idx) and (array[left_pointer_idx] &lt; array[pivot_idx]):\n            left_pointer_idx += 1\n\n        # 3. Move the right pointer\n        while (right_pointer_idx &gt; 0) and (array[right_pointer_idx] &gt; array[pivot_idx]):\n            right_pointer_idx -= 1\n\n        # 4. Swap values if the pointers haven't crossed yet, otherwise end the loop\n        if left_pointer_idx &lt; right_pointer_idx:\n            array[left_pointer_idx], array[right_pointer_idx] = array[right_pointer_idx], array[left_pointer_idx]\n\n    # 5. Swap the pivot and left_pointer values\n    array[left_pointer_idx], array[pivot_idx] = array[pivot_idx], array[left_pointer_idx]\n    final_pivot_idx = left_pointer_idx\n\n    return array, final_pivot_idx\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\npartition(input_array, 0, 11)\n\n([8, 7, 5, 3, 1, 9, 0, 8, 12, 69, 420, 23], 8)\n\n\n\ndef quicksort(array, start_idx=0, end_idx=None):\n    \"\"\"Quicksort algorithm to sort an array into ascending order.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n    start_idx: int\n        The start index of the array to consider.\n    end_index: int\n        The end index of the array to consider.\n\n    Returns\n    -------\n    array: list\n        The input array sorted in-place.\n    \"\"\"\n    end_idx = end_idx or len(array) - 1\n\n    # 3. Base case - an array of 0 or 1 elements is already sorted\n    if len(array[start_idx: end_idx]) &lt;= 1:\n        return array\n    \n    # 1. Partition the array\n    array, pivot_idx = partition(array, start_idx, end_idx)\n\n    # 2. Recursively partition the left and right subarrays\n    quicksort(array, start_idx=start_idx, end_idx=pivot_idx - 1)\n    quicksort(array, start_idx=pivot_idx + 1, end_idx=end_idx)\n\n    return array\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\nquicksort(input_array)\n\n[0, 1, 3, 5, 7, 8, 9, 8, 12, 23, 69, 420]\n\n\n\n\n\n\n\nPartitioning involves two steps:\n\n\n\nOperation\nNumber\n\n\n\n\nSwaps\n\\(N\\)\n\n\nComparisons\n\\(\\frac{N^2}{2}\\)\n\n\n\nEach element is compared to the pivot point at least N times, because the left and right pointers move until they reach each other. So there are \\(N\\) comparisons.\nEach swap handles two values, so in the worst case if we swapped every value there would be \\(\\frac{N}{2}\\) swaps. On average, there’d be about half that, so \\(\\frac{N}{4}\\).\nEither way, this means a single partition operation is \\(O(N)\\).\n\n\n\nQuicksort performs multiple partitions. How many?\nIn the best case, each partition would place the pivot directly in the middle of the subarray, spltting them clean in half each time. So there would be \\(log_2 N\\) splits.\nIn the average case this is “close enough”. Not every level will split equally so there’ll be a few extra but it will still be broadly symmetric, therefore a logarithmic function of \\(N\\).\nIn the worst case, every partition ends up on the left side, so with each partition we are effectively placing each element one-by-one from the left. This means we will partition \\(N\\) times, and each has \\(O(N)\\) complexity, resulting in a total compleixty of \\(O(N^2)\\).\n\n\n\nAlgorithm\nBest Case\nAverage Case\nWorst Case\n\n\n\n\nQuicksort\n\\(N log N\\)\n\\(N log N\\)\n\\(N^2\\)\n\n\n\n\n\n\n\nQuickselect is an algorithm with a similar approach. It’s a hybrid of Quicksort and a binary search.\n\nGiven an unsorted array, find the fifth-highest value in the array.\n\nOne approach would be to sort the array then pick the fifth element from the right. But sorting is \\(O(N log N)\\) on average for the better algorithms. We can do better without having to sort the whole array.\nRecall that after a partition, the pivot ends up in the correct place in the array. This is crucial. If we want to find the fifth highest value, we want to find the value that ends up in the fifth position from the right. So we can:\n\nPerform a partition and see where our pivot ends up.\nIf our pivot is too far left, partition the right subarray. If the pivot is too dar right, partition the left subarray.\nKeep going until we find the pivot which ends up in our target spot.\n\n\ndef quickselect(array, target_position, start_idx=0, end_idx=None):\n    \"\"\"Quickselect algorithm to find the target position (k-th lowest) element in an unsorted array.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n    target_position: int\n        The kth lowest value that we want to find.\n    start_idx: int\n        The start index of the array to consider.\n    end_index: int\n        The end index of the array to consider.\n\n    Returns\n    -------\n    array: list\n        The input array sorted in-place.\n    \"\"\"\n    end_idx = end_idx or len(array) - 1\n\n    # Base case: If the start index is greater than the end index, or if the target position is out of range\n    if (start_idx &gt; end_idx) or (target_position &lt; 0) or (target_position &gt; end_idx):\n        return None\n\n    # 1. Partition the array\n    array, pivot_idx = partition(array, start_idx, end_idx)\n\n    # 2. Recursively process the subarrays\n    if pivot_idx &lt; target_position:\n        # The pivot is too small, so check the subarray to the right\n        return quickselect(array, target_position, start_idx=pivot_idx + 1, end_idx=end_idx)\n    elif pivot_idx &gt; target_position:\n        # The pivot is too big, so check the subarray to the left\n        return quickselect(array, target_position, start_idx=start_idx, end_idx=pivot_idx - 1)\n    else:\n        # The pivot is at the target position - we have found our target!\n        return array[pivot_idx]\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\nquickselect(input_array, 5)\n\n8\n\n\n\n\n\nIn the average case, we are splitting the subarrays (roughly) in half each time.\nEach split is operating on a subarray of half the size, so in total there are \\(N + \\frac{N}{2} +  \\frac{N}{4} + \\frac{N}{8} + ... + 2 = 2N\\) steps.\nThus, overall the complexity is \\(O(N)\\) in the average case.\n\n\n\n\nMerge sort is like organizing a messy pile of papers. You divide the pile into smaller piles, sort each smaller pile, and then merge them back together in the correct order.\n\n\n\nDivide. Recursively split the array into two halves until each subarray contains only one element (and is therefore sorted).\nSort. Recursively mergesort each half.\nMerge. Recursively merge the sorted subarrays back together in the correct order.\n\n\n\n\n\ndef merge_sort(array):\n    \"\"\"Mergesort sort algorithm to sort an array into ascending order.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n\n    Returns\n    -------\n    array: list\n        The input array sorted in-place.\n    \"\"\"\n    # Base case - the array has 0 or 1 elements so is already sorted\n    if len(array) &lt;= 1:\n        return array\n    \n    # 1. Split the array\n    idx_mid = len(array) // 2\n    left_half = array[:idx_mid]\n    right_half = array[idx_mid:]\n    \n    # 2. Sort each half recursively\n    left_half = merge_sort(left_half)\n    right_half = merge_sort(right_half)\n    \n    # 3. Merge the subarrays\n    return merge(left_half, right_half)\n\ndef merge(left_array, right_array):\n    \"\"\"Merge two subarrays together which are each sorted.\n    \n    Parameters\n    ----------\n    left_array: list\n        The first subarray to merge.\n    right_array: list\n        The second subarray to merge.\n\n    Returns\n    -------\n    result: list\n        The merged, sorted array.\n    \"\"\"\n    # Initialise pointers at the start of each subarray\n    result = []\n    left_idx = 0\n    right_idx = 0\n    \n    # Loop until we reach the end of one of the subarrays\n    while left_idx &lt; len(left_array) and right_idx &lt; len(right_array):\n        # Compare the values of the left and right array, then insert the smaller of the two into the result\n        if left_array[left_idx] &lt; right_array[right_idx]:\n            # The left value is smaller so insert it into the result and increment the pointer\n            result.append(left_array[left_idx])\n            left_idx += 1\n        else:\n            # The right value is smaller so insert it into the result and increment the pointer\n            result.append(right_array[right_idx])\n            right_idx += 1\n    \n    # The array that reaches the end first will be empty, but there will still be elements in the other.\n    # So insert any remaining elements at the end of the result\n    result.extend(left_array[left_idx:])\n    result.extend(right_array[right_idx:])\n    \n    return result\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\nmerge_sort(input_array)\n\n[0, 1, 3, 5, 7, 8, 8, 9, 12, 23, 69, 420]\n\n\n\n\n\nThe time complexity of merge sort is the same for the best case, average case, and worst case.\nIn every case, each divide step splits the array in half, meaning there are \\(log_2(N)\\) splits.\nFor the merge step, the elements subarrays are looped through one-by-one, so all \\(N\\) elements are touched.\nTherefore, the overall complexity is \\(O(NlogN)\\).\nThe space complexity is O(n).\nThis is because merge sort requires additional space to store temporary arrays during the merging process. In the worst case, when merging two halves of the array, an additional array of size equal to the original array is needed to store the merged result temporarily.\n\n\n\n\n\n\nA max-heap is “weakly sorted”; the maximum value is the root of the heap, and nodes are greater than all of their descendants.\nWe can use this property to sort data elements by creating a heap from the data, then popping the maximum value one-by-one to populate an array in order.\n\nBuild a heap: Create a max-heap, which ensures the maximum value is at the root of the heap.\nPop the root: Remove the root and place it at the end of the result array.\nHeapify: Rearrange the remaining blocks to form a new heap.\nRepeat: Iterate through the entire heap until all elements are sorted.\n\n\n\n\n\ndef heapify(array, heap_size, current_idx):\n    \"\"\"Heapify function to maintain the max-heap property.\n    \n    Parameters\n    ----------\n    array: list\n        List of elements\n    heap_size: int\n        Size of heap\n    current_idx: int\n        Index of current node\n\n    Returns\n    -------\n    None\n        The input heap `array` is heapified in-place.\n    \"\"\"\n    # Track the largest node, initially assumed to be the root\n    largest = current_idx  \n    left_child = 2 * current_idx + 1\n    right_child = 2 * current_idx + 2\n\n    # Check if the node's children, if it has any, are larger than the current node\n    if left_child &lt; heap_size and array[left_child] &gt; array[largest]:\n        largest = left_child\n    if right_child &lt; heap_size and array[right_child] &gt; array[largest]:\n        largest = right_child\n\n\n    if largest != current_idx:\n        # If largest is not root, swap it with root to maintain the heap condition\n        array[current_idx], array[largest] = array[largest], array[current_idx]\n        # Recursively heapify the affected sub-tree\n        heapify(array, heap_size, largest)\n\n\ndef heap_sort(array):\n    \"\"\"Heap sort algorithm to sort an array into ascending order.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n\n    Returns\n    -------\n    array: list\n        The sorted array sorted in-place.\n    \"\"\"\n    # 1. Build a max heap.\n    # Start from the last non-leaf node, and heapify each node\n    array_len = len(array)\n    idx_last_non_leaf = array_len // 2 - 1\n    idx_last_node = array_len - 1\n\n    for i in range(idx_last_non_leaf, -1, -1):\n        heapify(array, array_len, i)\n\n    # 2. Extract elements one-by-one starting from the end\n    for i in range(idx_last_node, 0, -1):\n        array[i], array[0] = array[0], array[i]\n        # 3. Maintain the heap condition for the unsorted portion of the array\n        heapify(array, i, 0)\n\n    return array\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\nheap_sort(input_array)\n\n[0, 1, 3, 5, 7, 8, 8, 9, 12, 23, 69, 420]\n\n\n\n\n\nThe pre-processing step is to build the heap. This operation takes O(n) time.\nOnce the heap is created, we perform the following operations:\n\nPop the max value: This is accessible as it is the root, so \\(O(1)\\) time.\nInsert at the end of the result array: This is also \\(O(1)\\) time.\nHeapifying: This takes \\(O(log N)\\) time.\n\nThese are performed for each of the \\(N\\) data elements, so the overall time complexity of heap sort is \\(O(N log N)\\).\nThis is the same regardless of the arrangement of the input, so the time complexity of heap sort is the same in the best case, average case, and worst case.\n\n\n\n\nWe count how many of each element is in the array, i.e. how many 0s, how many 1s, how many 2s.\nFrom this we can then determine what the starting position of each element in the output will be. E.g. if there are three 0s then these will take the first three slots in the sorted output, so 1s will start at the fourth position.\nCounting sort works best when the number of unique items is small, i.e. there are lots of duplicates in the list.\nIt is a stable sorting algorithm, meaning items with identical values in the input will retain their original ordering in the output.\n\n\n\nSay we want to sort the following array:\n[0, 1, 2, 0, 0, 1, 2, 1, 0, 0, 2, 1, 2]\n\nCounting: Count the number of occurrences of each unique element in the input list. How many 0s? How many 1s? How many 2s? Etc.\nCumulative count: Then calculate the cumulative count of the elements. This step determines the starting position of each element in the sorted output.\nPlacement: Finally, you place each element in its correct position in the sorted output based on its cumulative count.\n\n\n\n\n\ndef counting_sort(array):\n    \"\"\"Counting sort algorithm to sort an array into ascending order.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n\n    Returns\n    -------\n    sorted_array: list\n        The sorted array.\n    \"\"\"\n    # 1. Count the elements in the input. \n    max_val = max(array)\n    count_store = [0] * (max_val + 1)\n\n    for num in array:\n        count_store[num] += 1\n    \n    # 2. Cumulative count\n    for i in range(1, len(count_store)):\n        count_store[i] += count_store[i - 1]\n        \n    # 3. Place each element in its correct position in the sorted array\n    sorted_array = [0] * len(array)\n\n    # for num in reversed(array):\n    #     sorted_array[count_store[num] - 1] = num\n    #     count_store[num] -= 1\n\n    for num in array:\n        sorted_array[count_store[num] - 1] = num\n        count_store[num] -= 1\n    \n    return sorted_array\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\ncounting_sort(input_array)\n\n[0, 1, 3, 5, 7, 8, 8, 9, 12, 23, 69, 420]\n\n\n\n\n\nThe complexity of counting sort depends on the number of elements, \\(N\\), and the number of unique elements, \\(K\\).\nFor each each of the steps:\n\nCounting: We loop through all \\(N\\) elements. We also create a count_store array which takes \\(K\\) elements of auxiliary space.\nCumulative count: We loop through the \\(K\\) elements in the count_store array.\nPlacement: We initialise an output_array of size \\(N\\), which therefore takes \\(N\\) elements of auxiliary space. We then loop through the \\(N\\) elements in the original array, using count_store to determine the correct placement.\n\nSo there are \\(N + K + N\\) steps and \\(N + K\\) elements of auxiliary data.\nTime complexity: \\(O(N + K)\\)\nAuxiliary space: \\(O(N + K)\\)\n\n\n\n\nRadix sort is a sorting algorithm that sorts numbers by processing individual digits.\nIt sorts numbers by first grouping the individual digits of the same place value together and sorting them. It starts sorting from the least significant digit (rightmost digit) to the most significant digit (leftmost digit).\n\n\n\nStart from the rightmost digit: Look at the least significant digit of each number. Group the numbers based on this digit.\nSort each group: After grouping, the numbers are rearranged based on their digit value. So, all the numbers with the same rightmost digit are now together, and they are sorted within this group.\nMove to the next digit: Now, look at the second rightmost digit and repeat the process. Group the numbers based on this digit and sort each group.\nContinue until all digits are considered: Keep repeating this process until you’ve looked at all digits, moving towards the leftmost digit.\n\nBy the end of this process, the numbers will be sorted because each time you look at a digit, the numbers are sorted according to that digit.\n\n\n\n\ndef counting_sort(arr, exp):\n    n = len(arr)\n    output = [0] * n\n    count = [0] * 10\n\n    # Count occurrences of digits\n    for i in range(n):\n        index = arr[i] // exp\n        count[index % 10] += 1\n\n    # Cumulative count\n    for i in range(1, 10):\n        count[i] += count[i - 1]\n\n    # Build the output array\n    i = n - 1\n    while i &gt;= 0:\n        index = arr[i] // exp\n        output[count[index % 10] - 1] = arr[i]\n        count[index % 10] -= 1\n        i -= 1\n\n    # Copy the output array to arr, to be ready for the next iteration\n    for i in range(n):\n        arr[i] = output[i]\n\ndef radix_sort(arr):\n    # Find the maximum number to determine the number of digits\n    max_num = max(arr)\n\n    exp = 1\n    while max_num // exp &gt; 0:\n        counting_sort(arr, exp)\n        exp *= 10\n\n    return arr\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\nradix_sort(input_array)\n\n[0, 1, 3, 5, 7, 8, 8, 9, 12, 23, 69, 420]\n\n\n\n\n\nThe time complexity of radix sort depends on the number of digits or the range of the input numbers. Let’s denote:\n\nn as the number of elements in the array to be sorted.\nd as the maximum number of digits in the input numbers.\nb as the base of the number system being used (usually 10 for decimal numbers).\n\nBest Case: The best-case scenario occurs when all the numbers have the same number of digits. In this case, the algorithm still has to iterate through each digit of each number. So, the time complexity in the best case is O(d⋅n)\nAverage Case: The average case time complexity is also O(d⋅n). This is because, on average, each number requires � d passes through the counting and distribution steps of the radix sort. Worst Case: The worst-case scenario happens when the numbers have significantly different numbers of digits, resulting in more passes through the counting and distribution steps. In the worst case, the time complexity is O(d⋅n).\nIn practice, however, the value of d is often considered to be a constant because it is bounded by the word size of the machine (e.g., 32 or 64 bits for integers). Therefore, the time complexity is often simplified to O(n), making radix sort very efficient, especially when the range of input numbers is relatively small.\nIn radix sort, the time complexity is the same in the best case, average case, and worst case. This is because the algorithm always needs to iterate through each digit of each number, regardless of the distribution of digits among the input numbers.\n\n\n\n\n\nCounting sort: https://www.youtube.com/watch?v=OKd534EWcdk\nRadix sort: https://www.youtube.com/watch?v=XiuSW_mEn7g"
  },
  {
    "objectID": "posts/software/data_structures_algos/sorting/sorting.html#bubble-sort",
    "href": "posts/software/data_structures_algos/sorting/sorting.html#bubble-sort",
    "title": "Sorting Algorithms",
    "section": "",
    "text": "We “bubble up” the next highest unsorted number on each pass-through.\n\nStart with 2 pointers pointing at the first two values in the array.\nCompare the left and right values. If left_value &gt; right_value, swap them. Otherwise, do nothing.\nMove both pointers one cell rightwards.\nRepeat Steps 2-3 until we reach values that have already been sorted. This completes the first “pass-through” and means we have “bubbled up” the biggest number to the end of the array.\nStart over. repeat Steps 1-4 to bubble up the second biggest number into the second to last position. Repeat this until we perform a pass through with no swaps.\n\n\n\n\n\ndef bubble_sort(array):\n    \"\"\"Bubble sort algorithm to sort an array into ascending order.\n\n    Note we ignore edge cases for the sake of clarity.\n    These are left as an exercise for the reader:\n    - array is empty\n    - array has only 1 element\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n\n    Returns\n    -------\n    array: list\n        The input array sorted in-place.\n    \"\"\"\n    # Initially the entire array is unsorted\n    last_unsorted_index = len(array) - 1\n    is_sorted = False\n\n    while not is_sorted:\n        # Set this to True before we pass through the elements, \n        # then if we need to perform a swap the array is not sorted so we set it to False\n        is_sorted = True\n\n        # Perform a pass-through\n        for left_pointer in range(last_unsorted_index):\n            right_pointer = left_pointer + 1\n\n            if array[left_pointer] &gt; array[right_pointer]:\n                # Swap the values\n                array[left_pointer], array[right_pointer] = array[right_pointer], array[left_pointer]\n                is_sorted = False\n\n        # The pass-through is finished so the next highest value has been \"bubbled up\".        \n        last_unsorted_index -= 1\n\n    return array\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\nbubble_sort(input_array)\n\n[0, 1, 3, 5, 7, 8, 8, 9, 12, 23, 69, 420]\n\n\n\n\n\nEach pass through loops through one fewer element:\n\n\n\nPass-through number\nNumber of operations\n\n\n\n\n1\nN-1\n\n\n2\nN-2\n\n\n3\nN-3\n\n\n4\nN-4\n\n\n5\nN-5\n\n\n…\n…\n\n\nk\nN-k\n\n\n\nSo in total, there are \\((N-1) + (N-2) + (N-3) + ... + 1\\) comparisons. This is the sum of an arithmetic progression, which we can calculate as \\(\\frac{N^2}{2}\\).\nAlso worth noting that in the worst case - an input array in descending order - each comparison will also result in a swap. This does not affect the Big-O complexity but would slow down the run time in practice. There are \\(\\frac{N^2}{2}\\) comparisons and up to \\(\\frac{N^2}{2}\\) swaps, resulting in \\(O(N^2)\\) total operations.\nThe complexity is therefore \\(O(N^2)\\).\nIn general, any nested loop should be a hint at quadratic time complexity."
  },
  {
    "objectID": "posts/software/data_structures_algos/sorting/sorting.html#selection-sort",
    "href": "posts/software/data_structures_algos/sorting/sorting.html#selection-sort",
    "title": "Sorting Algorithms",
    "section": "",
    "text": "Find the smallest value from the unsorted part of the array and put it at the beginning.\n\nCheck each cell of the array from left to right to find the lowest value. Store the index of the running minimum value.\nAt the end of pass-through \\(j\\) (starting at 0), swap the minimum value with the one at index \\(j\\).\nRepeat steps 1-2 until we reach a pass-through that would start at the end of the array, i.e. \\(j = N-1\\)\n\n\n\n\n\ndef selection_sort(array):\n    \"\"\"Selection sort algorithm to sort an array into ascending order.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n\n    Returns\n    -------\n    array: list\n        The input array sorted in-place.\n    \"\"\"\n    array_length = len(array)\n\n    # Loop through all array elements\n    for pass_through_number in range(array_length):\n\n        # Find the minimum element in the remaining unsorted array\n        min_index = pass_through_number\n        for idx_unsorted_section in range(pass_through_number + 1, array_length):\n            if array[idx_unsorted_section] &lt; array[min_index]:\n                min_index = idx_unsorted_section\n\n        # Swap the found minimum element with the first element\n        array[pass_through_number], array[min_index] = array[min_index], array[pass_through_number]\n    \n    return array\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\nselection_sort(input_array)\n\n[0, 1, 3, 5, 7, 8, 8, 9, 12, 23, 69, 420]\n\n\n\n\n\nAs with bubble sort, on each pass-through we loop through one fewer element, so there are \\(\\frac{N^2}{2}\\) comparisons. But each pass-through only performs 1 swap, so the total number of operations is \\(\\frac{N^2}{2} + N\\).\nThis is still therefore \\(O(N^2)\\) complexity, but it should be about twice as fast as bubble sort."
  },
  {
    "objectID": "posts/software/data_structures_algos/sorting/sorting.html#insertion-sort",
    "href": "posts/software/data_structures_algos/sorting/sorting.html#insertion-sort",
    "title": "Sorting Algorithms",
    "section": "",
    "text": "Remove a value to create a gap, shift values along to move the gap leftwards, then fill the gap.\n\nCreate a gap. For the first pass-through, we temporarily remove the second cell (i.e. index 1) and store it as a temporary variable. This leaves a gap at that index.\nShifting phase. Take each value to the left of the gap and compare it to the temporary variable. if left_val &gt; temp_val, move left value to the right. This has the same effect as moving the gap leftwards. As soon as we encounter a value where left_val &lt; temp_val the shifting phase is complete.\nFill the gap. Insert the temporary value into the current gap.\nRepeat pass-throughs. Steps 1-3 constitute a single pass-through. Repeat this until the pass through begins at the final index of the array. At this point the array is sorted.\n\n\n\n\n\ndef insertion_sort(array):\n    \"\"\"Insertion sort algorithm to sort an array into ascending order.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n\n    Returns\n    -------\n    array: list\n        The input array sorted in-place.\n    \"\"\"\n    array_len = len(array)\n\n    for pass_thru_number in range(1, array_len):\n        # 1. Create a gap\n        temp_val = array[pass_thru_number]\n        gap_idx = pass_thru_number\n\n        # 2. Shifting phase\n        # Move leftwards from the gap and keep shifting elements right if they are greater than temp_val\n        while (gap_idx &gt; 0) and (array[gap_idx - 1] &gt; temp_val):\n            array[gap_idx] = array[gap_idx - 1]\n            gap_idx -= 1\n\n        # 3. Fill the gap\n        array[gap_idx] = temp_val\n\n    return array\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\ninsertion_sort(input_array)\n\n[0, 1, 3, 5, 7, 8, 8, 9, 12, 23, 69, 420]\n\n\n\n\n\nThe worst case is when the input array is sorted in descending order.\nThere are 4 types of operations that occur:\n\n\nIf we compare values to the left of the gap on each step, there will be 1 comparison on the first pass-through, 2 on the second, 3 on the third, etc.\nSo there are \\(1 + 2 + ... + N-1 = \\frac{N^2}{2}\\) comparisons in the worst case.\n\n\n\nEach comparison could result in a shift, so there are the same number of shifts as comparisons in the worst case.\n\n\n\nWe remove 1 temp value per pass-through, so there are \\(N-1\\) removals.\nWe insert that value at the end of each pass-through, so there are also \\(N-1\\) insertions.\n\n\n\n\n\n\nOperation\nNumber\n\n\n\n\nRemovals\n\\(N-1\\)\n\n\nComparisons\n\\(\\frac{N^2}{2}\\)\n\n\nShifts\n\\(\\frac{N^2}{2}\\)\n\n\nInsertions\n\\(N-1\\)\n\n\n\nOverall there are \\(N^2 + 2N - 2\\) operations, so the complexity is \\(O(N^2)\\).\n\n\n\n\nThe complexity generally considers the worst case, but insertion sort varies greatly based on the input.\nWe saw in the worst case the number of steps is \\(N^2 + 2N - 2\\).\nIn the best case the data is already sorted, so we do a remove and an insert on each pass-through, for a totla of \\(2N - 2\\) steps.\nIn the average case, let’s assume about half of the data is already sorted. So we’ll still need to do \\(N - 1\\) removes and \\(N - 1\\) inserts in total. We’ll also need to compare about half the data, so \\(\\frac{N^2}{4}\\) comparisons and the same number on swaps. This gives a total of \\(\\frac{N^2}{2} + 2N - 2\\) steps.\nDepending on the state of the input data, the speed can vary considerably. Compare this to selection sort, which will always take \\(\\frac{N^2}{2}\\) steps regardless of the input data.\n\n\n\nAlgorithm\nBest Case\nAverage Case\nWorst Case\n\n\n\n\nSelection sort\n\\(\\frac{N^2}{2}\\)\n\\(\\frac{N^2}{2}\\)\n\\(\\frac{N^2}{2}\\)\n\n\nInsertion sort\n\\(N\\)\n\\(\\frac{N^2}{2}\\)\n\\(N^2\\)\n\n\n\nSo the choice of which algorithm is “best” depends on the state of your input data. Both have the same \\(O(N^2)\\) time complexity."
  },
  {
    "objectID": "posts/software/data_structures_algos/sorting/sorting.html#quicksort",
    "href": "posts/software/data_structures_algos/sorting/sorting.html#quicksort",
    "title": "Sorting Algorithms",
    "section": "",
    "text": "Quicksort is a sorting algorithm that relies on the concept of partitioning.\n\n\nThe idea behind partitioning is we take a random value from the array which we call the pivot.\nWe then want to ensure any smaller numbers are left of the pivot and any larger numbers to its right.\n\nIf we take the rightmost value as the pivot, we create a left pointer pointing at the leftmost value and a right pointer at the second from the right (i.e. not the pivot but the next rightmost).\nThe left pointer moves rightwards until it reaches a value &gt;= the pivot value.\nThe right pointer moves leftwards until it reaches a value &lt;= the pivot value.\nIf at this pointer the left pointer has crossed past the right pointer, move straight on to the next step. Otherwise swap the values that the left and right pointers are pointing at.\nSwap the pivot value with the left pointer’s value.\n\n\ndef partition(array):\n    \"\"\"Partition an array around the rightmost value.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to partition.\n\n    Returns\n    -------\n    array: list\n        The input array partitioned in-place.\n    \"\"\"\n    # 1. Initialise the pivot and pointers\n    pivot_idx = len(array) - 1\n    left_pointer_idx = 0\n    right_pointer_idx = pivot_idx - 1\n\n    # Loop until the left and right pointers cross \n    while left_pointer_idx &lt; right_pointer_idx:\n        # 2. Move the left pointer\n        while (left_pointer_idx &lt; pivot_idx) and (array[left_pointer_idx] &lt; array[pivot_idx]):\n            left_pointer_idx += 1\n\n        # 3. Move the right pointer\n        while (right_pointer_idx &gt; 0) and (array[right_pointer_idx] &gt; array[pivot_idx]):\n            right_pointer_idx -= 1\n\n        # 4. Swap values if the pointers haven't crossed yet, otherwise end the loop\n        if left_pointer_idx &lt; right_pointer_idx:\n            array[left_pointer_idx], array[right_pointer_idx] = array[right_pointer_idx], array[left_pointer_idx]\n\n    # 5. Swap the pivot and left_pointer values\n    array[left_pointer_idx], array[pivot_idx] = array[pivot_idx], array[left_pointer_idx]\n\n    return array\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\npartition(input_array)\n\n([8, 7, 5, 3, 1, 9, 0, 8, 12, 69, 420, 23], 8)\n\n\nPartitioning isn’t the same as sorting the array, but it’s “sorted-ish”.\n\n\n\nQuicksort is a combination of partitions and recursion.\n\nPartition the array. The pivot is now at the correct position in the sorted array.\nConsider the subarrays to the left and the right of the pivot as their own arrays. We’ll partition these recursively.\nIf a subarray has 0 or 1 elements, that is the base case and we do nothing; it is already sorted.\n\nWe’ll make a few tweaks to the partition function to take a start index and an end index, and to also return the pivot index. These extra parameters will allow us to call it recursively in quicksort.\n\n\n\n\ndef partition(array, start_idx, end_idx):\n    \"\"\"Partition an array around the rightmost value.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to partition.\n    start_idx: int\n        The start index of the array to consider.\n    end_index: int\n        The end index of the array to consider.\n\n    Returns\n    -------\n    array: list\n        The input array partitioned in-place.\n    final_pivot_idx: int\n        The index position of the pivot point in the partitioned array.\n    \"\"\"\n    # 1. Initialise the pivot and pointers\n    pivot_idx = end_idx\n    left_pointer_idx = start_idx\n    right_pointer_idx = pivot_idx - 1\n\n    # Loop until the left and right pointers cross \n    while left_pointer_idx &lt; right_pointer_idx:\n        # 2. Move the left pointer\n        while (left_pointer_idx &lt; pivot_idx) and (array[left_pointer_idx] &lt; array[pivot_idx]):\n            left_pointer_idx += 1\n\n        # 3. Move the right pointer\n        while (right_pointer_idx &gt; 0) and (array[right_pointer_idx] &gt; array[pivot_idx]):\n            right_pointer_idx -= 1\n\n        # 4. Swap values if the pointers haven't crossed yet, otherwise end the loop\n        if left_pointer_idx &lt; right_pointer_idx:\n            array[left_pointer_idx], array[right_pointer_idx] = array[right_pointer_idx], array[left_pointer_idx]\n\n    # 5. Swap the pivot and left_pointer values\n    array[left_pointer_idx], array[pivot_idx] = array[pivot_idx], array[left_pointer_idx]\n    final_pivot_idx = left_pointer_idx\n\n    return array, final_pivot_idx\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\npartition(input_array, 0, 11)\n\n([8, 7, 5, 3, 1, 9, 0, 8, 12, 69, 420, 23], 8)\n\n\n\ndef quicksort(array, start_idx=0, end_idx=None):\n    \"\"\"Quicksort algorithm to sort an array into ascending order.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n    start_idx: int\n        The start index of the array to consider.\n    end_index: int\n        The end index of the array to consider.\n\n    Returns\n    -------\n    array: list\n        The input array sorted in-place.\n    \"\"\"\n    end_idx = end_idx or len(array) - 1\n\n    # 3. Base case - an array of 0 or 1 elements is already sorted\n    if len(array[start_idx: end_idx]) &lt;= 1:\n        return array\n    \n    # 1. Partition the array\n    array, pivot_idx = partition(array, start_idx, end_idx)\n\n    # 2. Recursively partition the left and right subarrays\n    quicksort(array, start_idx=start_idx, end_idx=pivot_idx - 1)\n    quicksort(array, start_idx=pivot_idx + 1, end_idx=end_idx)\n\n    return array\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\nquicksort(input_array)\n\n[0, 1, 3, 5, 7, 8, 9, 8, 12, 23, 69, 420]\n\n\n\n\n\n\n\nPartitioning involves two steps:\n\n\n\nOperation\nNumber\n\n\n\n\nSwaps\n\\(N\\)\n\n\nComparisons\n\\(\\frac{N^2}{2}\\)\n\n\n\nEach element is compared to the pivot point at least N times, because the left and right pointers move until they reach each other. So there are \\(N\\) comparisons.\nEach swap handles two values, so in the worst case if we swapped every value there would be \\(\\frac{N}{2}\\) swaps. On average, there’d be about half that, so \\(\\frac{N}{4}\\).\nEither way, this means a single partition operation is \\(O(N)\\).\n\n\n\nQuicksort performs multiple partitions. How many?\nIn the best case, each partition would place the pivot directly in the middle of the subarray, spltting them clean in half each time. So there would be \\(log_2 N\\) splits.\nIn the average case this is “close enough”. Not every level will split equally so there’ll be a few extra but it will still be broadly symmetric, therefore a logarithmic function of \\(N\\).\nIn the worst case, every partition ends up on the left side, so with each partition we are effectively placing each element one-by-one from the left. This means we will partition \\(N\\) times, and each has \\(O(N)\\) complexity, resulting in a total compleixty of \\(O(N^2)\\).\n\n\n\nAlgorithm\nBest Case\nAverage Case\nWorst Case\n\n\n\n\nQuicksort\n\\(N log N\\)\n\\(N log N\\)\n\\(N^2\\)\n\n\n\n\n\n\n\nQuickselect is an algorithm with a similar approach. It’s a hybrid of Quicksort and a binary search.\n\nGiven an unsorted array, find the fifth-highest value in the array.\n\nOne approach would be to sort the array then pick the fifth element from the right. But sorting is \\(O(N log N)\\) on average for the better algorithms. We can do better without having to sort the whole array.\nRecall that after a partition, the pivot ends up in the correct place in the array. This is crucial. If we want to find the fifth highest value, we want to find the value that ends up in the fifth position from the right. So we can:\n\nPerform a partition and see where our pivot ends up.\nIf our pivot is too far left, partition the right subarray. If the pivot is too dar right, partition the left subarray.\nKeep going until we find the pivot which ends up in our target spot.\n\n\ndef quickselect(array, target_position, start_idx=0, end_idx=None):\n    \"\"\"Quickselect algorithm to find the target position (k-th lowest) element in an unsorted array.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n    target_position: int\n        The kth lowest value that we want to find.\n    start_idx: int\n        The start index of the array to consider.\n    end_index: int\n        The end index of the array to consider.\n\n    Returns\n    -------\n    array: list\n        The input array sorted in-place.\n    \"\"\"\n    end_idx = end_idx or len(array) - 1\n\n    # Base case: If the start index is greater than the end index, or if the target position is out of range\n    if (start_idx &gt; end_idx) or (target_position &lt; 0) or (target_position &gt; end_idx):\n        return None\n\n    # 1. Partition the array\n    array, pivot_idx = partition(array, start_idx, end_idx)\n\n    # 2. Recursively process the subarrays\n    if pivot_idx &lt; target_position:\n        # The pivot is too small, so check the subarray to the right\n        return quickselect(array, target_position, start_idx=pivot_idx + 1, end_idx=end_idx)\n    elif pivot_idx &gt; target_position:\n        # The pivot is too big, so check the subarray to the left\n        return quickselect(array, target_position, start_idx=start_idx, end_idx=pivot_idx - 1)\n    else:\n        # The pivot is at the target position - we have found our target!\n        return array[pivot_idx]\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\nquickselect(input_array, 5)\n\n8\n\n\n\n\n\nIn the average case, we are splitting the subarrays (roughly) in half each time.\nEach split is operating on a subarray of half the size, so in total there are \\(N + \\frac{N}{2} +  \\frac{N}{4} + \\frac{N}{8} + ... + 2 = 2N\\) steps.\nThus, overall the complexity is \\(O(N)\\) in the average case."
  },
  {
    "objectID": "posts/software/data_structures_algos/sorting/sorting.html#merge-sort",
    "href": "posts/software/data_structures_algos/sorting/sorting.html#merge-sort",
    "title": "Sorting Algorithms",
    "section": "",
    "text": "Merge sort is like organizing a messy pile of papers. You divide the pile into smaller piles, sort each smaller pile, and then merge them back together in the correct order.\n\n\n\nDivide. Recursively split the array into two halves until each subarray contains only one element (and is therefore sorted).\nSort. Recursively mergesort each half.\nMerge. Recursively merge the sorted subarrays back together in the correct order.\n\n\n\n\n\ndef merge_sort(array):\n    \"\"\"Mergesort sort algorithm to sort an array into ascending order.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n\n    Returns\n    -------\n    array: list\n        The input array sorted in-place.\n    \"\"\"\n    # Base case - the array has 0 or 1 elements so is already sorted\n    if len(array) &lt;= 1:\n        return array\n    \n    # 1. Split the array\n    idx_mid = len(array) // 2\n    left_half = array[:idx_mid]\n    right_half = array[idx_mid:]\n    \n    # 2. Sort each half recursively\n    left_half = merge_sort(left_half)\n    right_half = merge_sort(right_half)\n    \n    # 3. Merge the subarrays\n    return merge(left_half, right_half)\n\ndef merge(left_array, right_array):\n    \"\"\"Merge two subarrays together which are each sorted.\n    \n    Parameters\n    ----------\n    left_array: list\n        The first subarray to merge.\n    right_array: list\n        The second subarray to merge.\n\n    Returns\n    -------\n    result: list\n        The merged, sorted array.\n    \"\"\"\n    # Initialise pointers at the start of each subarray\n    result = []\n    left_idx = 0\n    right_idx = 0\n    \n    # Loop until we reach the end of one of the subarrays\n    while left_idx &lt; len(left_array) and right_idx &lt; len(right_array):\n        # Compare the values of the left and right array, then insert the smaller of the two into the result\n        if left_array[left_idx] &lt; right_array[right_idx]:\n            # The left value is smaller so insert it into the result and increment the pointer\n            result.append(left_array[left_idx])\n            left_idx += 1\n        else:\n            # The right value is smaller so insert it into the result and increment the pointer\n            result.append(right_array[right_idx])\n            right_idx += 1\n    \n    # The array that reaches the end first will be empty, but there will still be elements in the other.\n    # So insert any remaining elements at the end of the result\n    result.extend(left_array[left_idx:])\n    result.extend(right_array[right_idx:])\n    \n    return result\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\nmerge_sort(input_array)\n\n[0, 1, 3, 5, 7, 8, 8, 9, 12, 23, 69, 420]\n\n\n\n\n\nThe time complexity of merge sort is the same for the best case, average case, and worst case.\nIn every case, each divide step splits the array in half, meaning there are \\(log_2(N)\\) splits.\nFor the merge step, the elements subarrays are looped through one-by-one, so all \\(N\\) elements are touched.\nTherefore, the overall complexity is \\(O(NlogN)\\).\nThe space complexity is O(n).\nThis is because merge sort requires additional space to store temporary arrays during the merging process. In the worst case, when merging two halves of the array, an additional array of size equal to the original array is needed to store the merged result temporarily."
  },
  {
    "objectID": "posts/software/data_structures_algos/sorting/sorting.html#heap-sort",
    "href": "posts/software/data_structures_algos/sorting/sorting.html#heap-sort",
    "title": "Sorting Algorithms",
    "section": "",
    "text": "A max-heap is “weakly sorted”; the maximum value is the root of the heap, and nodes are greater than all of their descendants.\nWe can use this property to sort data elements by creating a heap from the data, then popping the maximum value one-by-one to populate an array in order.\n\nBuild a heap: Create a max-heap, which ensures the maximum value is at the root of the heap.\nPop the root: Remove the root and place it at the end of the result array.\nHeapify: Rearrange the remaining blocks to form a new heap.\nRepeat: Iterate through the entire heap until all elements are sorted.\n\n\n\n\n\ndef heapify(array, heap_size, current_idx):\n    \"\"\"Heapify function to maintain the max-heap property.\n    \n    Parameters\n    ----------\n    array: list\n        List of elements\n    heap_size: int\n        Size of heap\n    current_idx: int\n        Index of current node\n\n    Returns\n    -------\n    None\n        The input heap `array` is heapified in-place.\n    \"\"\"\n    # Track the largest node, initially assumed to be the root\n    largest = current_idx  \n    left_child = 2 * current_idx + 1\n    right_child = 2 * current_idx + 2\n\n    # Check if the node's children, if it has any, are larger than the current node\n    if left_child &lt; heap_size and array[left_child] &gt; array[largest]:\n        largest = left_child\n    if right_child &lt; heap_size and array[right_child] &gt; array[largest]:\n        largest = right_child\n\n\n    if largest != current_idx:\n        # If largest is not root, swap it with root to maintain the heap condition\n        array[current_idx], array[largest] = array[largest], array[current_idx]\n        # Recursively heapify the affected sub-tree\n        heapify(array, heap_size, largest)\n\n\ndef heap_sort(array):\n    \"\"\"Heap sort algorithm to sort an array into ascending order.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n\n    Returns\n    -------\n    array: list\n        The sorted array sorted in-place.\n    \"\"\"\n    # 1. Build a max heap.\n    # Start from the last non-leaf node, and heapify each node\n    array_len = len(array)\n    idx_last_non_leaf = array_len // 2 - 1\n    idx_last_node = array_len - 1\n\n    for i in range(idx_last_non_leaf, -1, -1):\n        heapify(array, array_len, i)\n\n    # 2. Extract elements one-by-one starting from the end\n    for i in range(idx_last_node, 0, -1):\n        array[i], array[0] = array[0], array[i]\n        # 3. Maintain the heap condition for the unsorted portion of the array\n        heapify(array, i, 0)\n\n    return array\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\nheap_sort(input_array)\n\n[0, 1, 3, 5, 7, 8, 8, 9, 12, 23, 69, 420]\n\n\n\n\n\nThe pre-processing step is to build the heap. This operation takes O(n) time.\nOnce the heap is created, we perform the following operations:\n\nPop the max value: This is accessible as it is the root, so \\(O(1)\\) time.\nInsert at the end of the result array: This is also \\(O(1)\\) time.\nHeapifying: This takes \\(O(log N)\\) time.\n\nThese are performed for each of the \\(N\\) data elements, so the overall time complexity of heap sort is \\(O(N log N)\\).\nThis is the same regardless of the arrangement of the input, so the time complexity of heap sort is the same in the best case, average case, and worst case."
  },
  {
    "objectID": "posts/software/data_structures_algos/sorting/sorting.html#counting-sort",
    "href": "posts/software/data_structures_algos/sorting/sorting.html#counting-sort",
    "title": "Sorting Algorithms",
    "section": "",
    "text": "We count how many of each element is in the array, i.e. how many 0s, how many 1s, how many 2s.\nFrom this we can then determine what the starting position of each element in the output will be. E.g. if there are three 0s then these will take the first three slots in the sorted output, so 1s will start at the fourth position.\nCounting sort works best when the number of unique items is small, i.e. there are lots of duplicates in the list.\nIt is a stable sorting algorithm, meaning items with identical values in the input will retain their original ordering in the output.\n\n\n\nSay we want to sort the following array:\n[0, 1, 2, 0, 0, 1, 2, 1, 0, 0, 2, 1, 2]\n\nCounting: Count the number of occurrences of each unique element in the input list. How many 0s? How many 1s? How many 2s? Etc.\nCumulative count: Then calculate the cumulative count of the elements. This step determines the starting position of each element in the sorted output.\nPlacement: Finally, you place each element in its correct position in the sorted output based on its cumulative count.\n\n\n\n\n\ndef counting_sort(array):\n    \"\"\"Counting sort algorithm to sort an array into ascending order.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n\n    Returns\n    -------\n    sorted_array: list\n        The sorted array.\n    \"\"\"\n    # 1. Count the elements in the input. \n    max_val = max(array)\n    count_store = [0] * (max_val + 1)\n\n    for num in array:\n        count_store[num] += 1\n    \n    # 2. Cumulative count\n    for i in range(1, len(count_store)):\n        count_store[i] += count_store[i - 1]\n        \n    # 3. Place each element in its correct position in the sorted array\n    sorted_array = [0] * len(array)\n\n    # for num in reversed(array):\n    #     sorted_array[count_store[num] - 1] = num\n    #     count_store[num] -= 1\n\n    for num in array:\n        sorted_array[count_store[num] - 1] = num\n        count_store[num] -= 1\n    \n    return sorted_array\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\ncounting_sort(input_array)\n\n[0, 1, 3, 5, 7, 8, 8, 9, 12, 23, 69, 420]\n\n\n\n\n\nThe complexity of counting sort depends on the number of elements, \\(N\\), and the number of unique elements, \\(K\\).\nFor each each of the steps:\n\nCounting: We loop through all \\(N\\) elements. We also create a count_store array which takes \\(K\\) elements of auxiliary space.\nCumulative count: We loop through the \\(K\\) elements in the count_store array.\nPlacement: We initialise an output_array of size \\(N\\), which therefore takes \\(N\\) elements of auxiliary space. We then loop through the \\(N\\) elements in the original array, using count_store to determine the correct placement.\n\nSo there are \\(N + K + N\\) steps and \\(N + K\\) elements of auxiliary data.\nTime complexity: \\(O(N + K)\\)\nAuxiliary space: \\(O(N + K)\\)"
  },
  {
    "objectID": "posts/software/data_structures_algos/sorting/sorting.html#radix-sort",
    "href": "posts/software/data_structures_algos/sorting/sorting.html#radix-sort",
    "title": "Sorting Algorithms",
    "section": "",
    "text": "Radix sort is a sorting algorithm that sorts numbers by processing individual digits.\nIt sorts numbers by first grouping the individual digits of the same place value together and sorting them. It starts sorting from the least significant digit (rightmost digit) to the most significant digit (leftmost digit).\n\n\n\nStart from the rightmost digit: Look at the least significant digit of each number. Group the numbers based on this digit.\nSort each group: After grouping, the numbers are rearranged based on their digit value. So, all the numbers with the same rightmost digit are now together, and they are sorted within this group.\nMove to the next digit: Now, look at the second rightmost digit and repeat the process. Group the numbers based on this digit and sort each group.\nContinue until all digits are considered: Keep repeating this process until you’ve looked at all digits, moving towards the leftmost digit.\n\nBy the end of this process, the numbers will be sorted because each time you look at a digit, the numbers are sorted according to that digit.\n\n\n\n\ndef counting_sort(arr, exp):\n    n = len(arr)\n    output = [0] * n\n    count = [0] * 10\n\n    # Count occurrences of digits\n    for i in range(n):\n        index = arr[i] // exp\n        count[index % 10] += 1\n\n    # Cumulative count\n    for i in range(1, 10):\n        count[i] += count[i - 1]\n\n    # Build the output array\n    i = n - 1\n    while i &gt;= 0:\n        index = arr[i] // exp\n        output[count[index % 10] - 1] = arr[i]\n        count[index % 10] -= 1\n        i -= 1\n\n    # Copy the output array to arr, to be ready for the next iteration\n    for i in range(n):\n        arr[i] = output[i]\n\ndef radix_sort(arr):\n    # Find the maximum number to determine the number of digits\n    max_num = max(arr)\n\n    exp = 1\n    while max_num // exp &gt; 0:\n        counting_sort(arr, exp)\n        exp *= 10\n\n    return arr\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\nradix_sort(input_array)\n\n[0, 1, 3, 5, 7, 8, 8, 9, 12, 23, 69, 420]\n\n\n\n\n\nThe time complexity of radix sort depends on the number of digits or the range of the input numbers. Let’s denote:\n\nn as the number of elements in the array to be sorted.\nd as the maximum number of digits in the input numbers.\nb as the base of the number system being used (usually 10 for decimal numbers).\n\nBest Case: The best-case scenario occurs when all the numbers have the same number of digits. In this case, the algorithm still has to iterate through each digit of each number. So, the time complexity in the best case is O(d⋅n)\nAverage Case: The average case time complexity is also O(d⋅n). This is because, on average, each number requires � d passes through the counting and distribution steps of the radix sort. Worst Case: The worst-case scenario happens when the numbers have significantly different numbers of digits, resulting in more passes through the counting and distribution steps. In the worst case, the time complexity is O(d⋅n).\nIn practice, however, the value of d is often considered to be a constant because it is bounded by the word size of the machine (e.g., 32 or 64 bits for integers). Therefore, the time complexity is often simplified to O(n), making radix sort very efficient, especially when the range of input numbers is relatively small.\nIn radix sort, the time complexity is the same in the best case, average case, and worst case. This is because the algorithm always needs to iterate through each digit of each number, regardless of the distribution of digits among the input numbers."
  },
  {
    "objectID": "posts/software/data_structures_algos/sorting/sorting.html#references",
    "href": "posts/software/data_structures_algos/sorting/sorting.html#references",
    "title": "Sorting Algorithms",
    "section": "",
    "text": "Counting sort: https://www.youtube.com/watch?v=OKd534EWcdk\nRadix sort: https://www.youtube.com/watch?v=XiuSW_mEn7g"
  },
  {
    "objectID": "posts/software/data_structures_algos/data_structures/data_structures.html",
    "href": "posts/software/data_structures_algos/data_structures/data_structures.html",
    "title": "Data Structures",
    "section": "",
    "text": "There are common ways that we can interact with different data structures.\nIt is useful to frame the appropriateness of a data structure for a given based on the speed of the operations that are required most for that task.\n\nRead: Look up the value at a particular location in the data structure\nSearch: Look for a particular value in the data structure.\nInsert: Add a new value to the data structure.\nDelete: Remove a value from the data structure.\n\nReading is “find by key”, searching is “find by value”.\n\n\n\nAn array is a list of elements.\nIt is stored in memory as a block of contiguous memory addresses. When the array is declared, the head of the array is stored, i.e. the memory address of the first elelment.\nThe size of an array is the number of elements in the array. The index denotes where a particular piece of data resides in that array.\n\n\n\nOperation\nComplexity (Worst)\nComplexity (Best)\n\n\n\n\nRead\nO(1)\nO(1)\n\n\nSearch\nO(N)\nO(1)\n\n\nInsertion\nO(N)\nO(1)\n\n\nDeletion\nO(N)\nO(1)\n\n\n\n\n\nA computer can look up a given memory address in constant time.\nWe know the head of the array and the index to look up. So we can read in O(1) time.\nExample:\n\nHead of the array is memory address 1063.\nWe want to look up index 5.\nRead memory address 1068 (because 1063 + 5 = 1068).\n\n\nIf you were asked to raise your right pinky finger, you wouldn’t need to search through all of your fingers to find it\n\n\n\n\nA computer has immediate access to all of its memory addresses but does not know ahead of time their contents.\nSo to find a particular value, we will potentially have to search through every element.\nSearching an array is therefore O(N).\n\n\n\nThe efficiency of inserting into an array depends on where in the array you are inserting.\nIf inserting an element at the end, we simply place the new value at that memory address (assuming the memory address is empty). This is a constant time operation O(1).\nBut if we insert at any other position, we need to:\n\nShift each of the existing elements to the right of the insert index 1 position rightwards\nInsert the new value in the gap created.\n\nSo to insert at index \\(i\\), there are \\(N - i\\) elements to shift (Step 1), then 1 more operation to insert the new value (Step 2).\nIn the worst case - inserting at the start of an array - insertion is O(N).\n\n\n\nSimilarly, the efficiency of deletion depends on the index being deleted.\nIf deleting the last element, there is simply 1 operation to clear the memory address, so this is a constant time operation O(1).\nBut if we delete an element in any other position, we need to:\n\nDelete the element. This leaves a gap in the middle of the array.\nShift each of the elements to the right of the gap leftwards, to fill the gap.\n\nSo to delete at index \\(i\\), we do 1 operation to delete that element (Step 1), then shift the next \\(N-i\\) elements leftwards (Step 2).\nIn the worst case - deleting the first element of the array - deletion is O(N).\n\n\n\n\nA set is a collection of unique elements, i.e. duplicate values are not allowed.\nThere are different ways of implementing sets: array-based sets and hash-sets are discussed here.\nNote that Python already has sets, but we’ll give outline implementations for clarity.\n\n\nAn array is used to store elements. As with standard arrays, elements are stored in contiguous memory locations, and each element has a unique index.\nExample in Python:\n\nclass ArraySet:\n    \n    def __init__(self):\n        self.elements = []\n        \n    def __repr__(self):\n        return str(self.elements)\n\n    def add(self, element):\n        # Search the array for `element`, then append it if it is not a duplicate.\n        if element not in self.elements:\n            self.elements.append(element)\n\n    def remove(self, element):\n        # Search the array for the value, then remove it.\n        if element in self.elements:\n            self.elements.remove(element)\n\n    def contains(self, element):\n        return element in self.elements\n\n\nset_arr = ArraySet()\nset_arr.add(1)\nset_arr.add(2)\nset_arr.add(3)\nprint(set_arr)\n\n[1, 2, 3]\n\n\nIf we try to add a duplicate value it does not get added to the array:\n\nset_arr.add(2)\nprint(set_arr)\n\n[1, 2, 3]\n\n\nThe read, search and delete operations for an array-based set are identical to the standard array.\nInsert operations are where array-based sets diverge. We always insert at the end of a set, which is constant time. But we need to search the array every time to ensure the new value is not a duplicate.\nSo we always need to do a search of all N elements, and then 1 insert operation at the end.\nThis means even in the best case, insertion into an array-based set is O(N) compared to O(1) when inserting at the end of a standard array.\nThe reason for using a set is because the use case requires no duplicates, not because it is inherently “quicker” than a standard array.\n\n\n\nOperation\nComplexity (Worst)\nComplexity (Best)\n\n\n\n\nRead\nO(1)\nO(1)\n\n\nSearch\nO(N)\nO(1)\n\n\nInsertion\nO(N)\nO(N)\n\n\nDeletion\nO(N)\nO(1)\n\n\n\n\n\n\nA hash-based set computes the hash of each element and uses this to store elements.\nAn example implementation implements the set as key-value pairs where keys are the hash of the elements and values are a placeholder value like True, or an array to handle hash collisions.\nWhen there is a hash collision between mutliple elements, a typical approach is to insert all of these elements as an array under the same hash key.\nThe worst case scenario is caused by the extreme edge case where hash collisions are so prominent that every element has the same hash, essentially reducing the hash set to an array. This is generally avoided as long as the hash algorithm is decent.\nFor this reason, the average complexity is more meaningful in the table below. (Note that best has been replace with average in the table headings.)\nHash-based sets do not support reading by index, unlike array-based sets. But all other operations are typically constant time.\n\n\n\nOperation\nComplexity (Worst)\nComplexity (Average)\n\n\n\n\nRead\nN/A\nN/A\n\n\nSearch\nO(N)\nO(1)\n\n\nInsertion\nO(N)\nO(1)\n\n\nDeletion\nO(N)\nO(1)\n\n\n\nExample in Python:\n\nclass HashSet:\n    def __init__(self):\n        # Use a dict to represent the hash set.\n        self.elements = {}\n        \n    def __repr__(self):\n        return str(self.elements)\n\n    def add(self, element):\n        # The key is the element and the value is arbitrary.\n        # There are two extensions we could add here:\n        #   1. The key should really be the *hash* of the element, not just the element itself.\n        #      Essentially, this is using an implicit hash function which is just a pass-through:\n        #      hash_func = lambda x: x\n        #   2. Handle hash collisions by making the value an array which is appended to in the case of collisions.\n        self.elements[element] = True\n\n    def remove(self, element):\n        if element in self.elements:\n            del self.elements[element]\n\n    def contains(self, element):\n        return element in self.elements\n\n\nset_hash = HashSet()\nset_hash.add(1)\nset_hash.add(2)\nset_hash.add(3)\nprint(set_hash)\n\n{1: True, 2: True, 3: True}\n\n\nIf we try to add a duplicate value it simply overwrites the previous value:\n\nset_hash.add(2)\nprint(set_hash)\n\n{1: True, 2: True, 3: True}\n\n\n\n\n\n\nThese are identical to regular arrays with the additional condition that elements are always ordered.\nThis obviously relies heavily on efficient sorting. This is a topic unto itself; see notes on sorting for more info.\nWhen inserting into an ordered array, we need to:\n\nSearch for the correct position - Look at each element in turn and compare if the insert element is greater than it\nInsert into the array\n\nThese two terms increase in opposite directions depending on the insert position. The further into the array we need to search (Step 1), the fewer elements we need to shift for the insertion (Step 2).\n\n\nIn a typical (unordered) array, the only option for searching is a linear search: we loop through each element in turn until we find our target.\nFor an ordered array, we can improve on this using a binary search.\n\nPick the middle element.\nIf the target value is greater than this, search the right half, otherwise search the left half.\nRepeat this recursively until we find our target.\n\nThis approach splits the search region in half for every constant time comparison operation.\nOr put another way, if we doubled the number of elements in the array, the binary search would only have to perform 1 extra step. For \\(N\\) elements we need \\(log_2(N)\\) binary splits.\nTherefore, the time complexity is O(log(N)).\n\ndef binary_search(ordered_array, target):\n    \"\"\"Perform a binary search for the target value on the given ordered array.\n\n    Parameters\n    ----------\n    ordered_array: list\n        The array to search in.\n    target: int\n        The target value we are searching for.\n\n    Returns\n    -------\n    target_index: int\n        The index of the target value.\n        Returns None if the value does not exist in the array.\n    \"\"\"\n    # Establish the lower and upper bounds of our search.\n    # Initially, this is the entire array\n    idx_lower = 0\n    idx_upper = len(ordered_array) - 1\n\n    while idx_lower &lt;= idx_upper:\n        # Find the midpoint between our bounds\n        idx_midpoint = (idx_upper + idx_lower) // 2\n        value_at_midpoint = ordered_array[idx_midpoint]\n\n        # Compare to our target value and narrow the upper or lower bound accordingly\n        if value_at_midpoint == target:\n            # We have found the target!\n            return idx_midpoint\n        elif value_at_midpoint &lt; target:\n            # The target is bigger so must be to the right side\n            idx_lower = idx_midpoint + 1\n        elif value_at_midpoint &gt; target:\n            # The target is smaller so must be on the left side\n            idx_upper = idx_midpoint - 1\n\n    # If the lower and upper bounds meet we have exhausted the whole array, so the target is not in the array\n    return None\n\nLet’s try this on a few examples.\n\nordered_array = [1, 2, 4, 5, 7, 8, 9, 10, 13, 14]\n\n\nbinary_search(ordered_array, 7)\n\n4\n\n\n\nbinary_search(ordered_array, 14)\n\n9\n\n\nNow a value that’s not in the array:\n\nbinary_search(ordered_array, 14)\n\n9\n\n\nCompare this with a linear search\n\ndef linear_search(array, target):\n    \"\"\"Perform a linear search for the target value on the given array.\n\n    Parameters\n    ----------\n    array: list\n        The array to search in.\n    target: int\n        The target value we are searching for.\n\n    Returns\n    -------\n    target_index: int\n        The index of the target value.\n        Returns None if the value does not exist in the array.\n    \"\"\"\n    # Loop through every element in the array.\n    # Note: we should really use enumerate() rather than range(len()) but I wanted to keep this generic \n    # without too many python-specific helpers\n    for idx in range(len(array)):\n        if array[idx] == target:\n            return idx\n    \n    # If we reach the end of the array without returning a value, then the target does not exist in the array.\n    return None\n\nLet’s compare how they perform for a reasonably big array with 1 million elements.\n\narray = [k for k in range(1000000)]\n\n\n%%timeit\nbinary_search(array, 987654)\n\n1.13 µs ± 56.9 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\n\n%%timeit\nlinear_search(array, 987654)\n\n15.9 ms ± 320 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nThe binary search is ~14000x faster than the linear search!\n\n\n\n\nHash tables are key:value pairs. We can look up a the value for a given key in \\(O(1)\\) time.\nAlso known as hash maps, dictionaries, maps, associative arrays.\n\n\nThe process of taking characters and converting them to numbers is called hashing.\nThe code that performs this conversion is the hash function.\nA hash function requires one condition:\n\nConsistency: A hash function must convert the same string to the same number every time it’s applied.\n\nIn practice, for the hash function to be useful, it should also be collision resistant:\n\nCollision resistant: Different inputs should hash to different outputs.\n\nAs an extreme example, the following hash function is consistent but not collision resistant:\ndef crappy_hash(input_str: str) -&gt; int:\n    \"\"\"This is the world's worst hash function.\"\"\"\n    return 1\n\n\n\nWe want to insert the following key:value pair into our hash table:\nkey = \"Name\"\nvalue = \"Gurp\"\nLet’s say we have a hash function that is actually good, and in this particular case hash_function(key) returns 12.\nThe hash table will then insert value at memory address 12 (or more specifically, the memory address of the head of the dictionary + 12).\nThis means if we ever want to look up the key \"Name\", we hash it and immediately know to access memory address 12 and return the value \"Gurp\".\nSo hash table lookups are \\(O(1)\\).\nMore specifically, looking up by key is \\(O(1)\\). Searching by value is essentially searching through an array, so is \\(O(N)\\).\n\n\n\nA collision occurs when we try to add data to a cell that is already occupied.\nOne approach to handling this is called separate chaining.\nInstead of placing a single value in a cell, we place a pointer to an array. This array contains length-2 subarrays where the first element is the key and the second element is the value.\nIf there are no collisions, a hash table look up is \\(O(1)\\). In the worst case, ALL keys collide and so we essentially have to search through an array which is \\(O(N)\\).\n\n\n\nA hash tables efficiency depends on:\n\nHow much data we’re storing in it\nHow many cells are available\nWhich hash function we use\n\nA good hash function (3) should distribute the data (1) evenly across all cells (2).\nThis ensures the memory is used efficiently while avoiding collisions.\nThe load factor is the ratio of data to cells, and ideally should be around 0.7, i.e. 7 elements for every 10 cells.\n\n\n\nOperation\nComplexity (Worst)\nComplexity (Average)\n\n\n\n\nRead\nO(N)\nO(1)\n\n\nSearch\nO(N)\nKeys: O(1) Values: O(N)\n\n\nInsertion\nO(N)\nO(1)\n\n\nDeletion\nO(N)\nO(1)\n\n\n\nThe worst case corresponds to when all keys collide, reducing the hash table to an array effectively.\n\n\n\n\nA stack is stored in memory the same as an array, but it has 3 constraints:\n\nData can only be inserted at the end (push)\nData can only be deleted from the end (pop)\nOnly the last element can be read (peek)\n\n\nRead, insert and delete can only happen at the end.\n\nThis makes them useful as Last-In First-Out (LIFO) data stores: the last item pushed the the stack is the first to be popped.\nExample in Python:\n\nclass Stack:\n\n    def __init__(self, initial_elements: list = []):\n        # We can pass a list to initialise the Stack\n        self.data = initial_elements\n\n    def __repr__(self):\n        return str(self.data)\n\n    def push(self, element):\n        self.data.append(element)\n\n    def pop(self):\n        return self.data.pop()\n\n    def peek(self):\n        if self.data:\n            return self.data[-1]\n\n\nstack = Stack([1, 2, 3, 4])\nstack\n\n[1, 2, 3, 4]\n\n\n\nstack.push(5)\nprint(stack)\n\n[1, 2, 3, 4, 5]\n\n\n\nstack.peek()\n\n5\n\n\n\nprint(stack)\n\n[1, 2, 3, 4, 5]\n\n\n\nstack.pop()\n\n5\n\n\n\nprint(stack)\n\n[1, 2, 3, 4]\n\n\nThe benefits of stacks, and other constrained data structures, are:\n\nPrevent potential bugs when using certain algorithms. For example, an algorithm that relies on stacks may break if removing elements from the middle of the array, so using a standard array is more error-prone.\nA new mental model for tackling problems. In the case of stacks, this is the LIFO approach.\n\nThe concept of stacks is a useful precursor to recursion, as we push to and pop from the end of a stack.\n\n\n\nA queue is conceptually similar to a stack - it is a constrained array. This time, it is First-In First-Out (FIFO) like a queue of people; the first person to arrive is the first to leave.\nQueue restrictions:\n\nData can only be inserted at the end (enqueue)\nData can only be deleted from the front (dequeue)\nData can only be read from the front (peek)\n\nPoints (2) and (3) are the opposite of the stack.\n\nclass Queue:\n\n    def __init__(self, initial_elements: list = []):\n        # We can pass a list to initialise the Stack\n        self.data = initial_elements\n\n    def __repr__(self):\n        return str(self.data)\n\n    def enqueue(self, element):\n        self.data.append(element)\n\n    def dequeue(self):\n        return self.data.pop(0)\n\n    def peek(self):\n        if self.data:\n            return self.data[0]\n\n\nq = Queue([1, 2, 3, 4])\nq\n\n[1, 2, 3, 4]\n\n\n\nq.enqueue(5)\nprint(q)\n\n[1, 2, 3, 4, 5]\n\n\n\nq.dequeue()\n\n1\n\n\n\nprint(q)\n\n[2, 3, 4, 5]\n\n\n\nq.peek()\n\n2\n\n\n\nprint(q)\n\n[2, 3, 4, 5]\n\n\n\n\n\nA linked list represents a list of items as non-contiguous blocks of memory.\nIt is a list of items, similar to an array. But an array occupies a continuous block of memory.\n\n\n\nOperation\nComplexity (Worst)\nComplexity (Best)*\n\n\n\n\nRead\nO(N)\nO(1)\n\n\nSearch\nO(N)\nO(1)\n\n\nInsertion\nO(N)\nO(1)\n\n\nDeletion\nO(N)\nO(1)\n\n\n\nThe best case corresponds to operating on the head node.\nIn a linked list, each element is contained in a node that can be in scattered positions in memory. The node contains the data element and a “link” which is a pointer to the memory address of the next element.\nBenefits of a linked list over an array:\n\nMemory efficient: we don’t need a continuous block of memory\n\\(O(1)\\) inserts and deletes from the beginning of the list\nUseful when we want to traverse through a data structure while making inserts and deletes, because we do not have to shift the entire data structure each time as we would have to with an array\n\nA node contains two pieces of information:\n\n\n\nData\nLink\n\n\n\n\n“a”\n1666\n\n\n\nThese nodes can then be linked together in a list… a linked list!\n\n\n\n\n\nflowchart LR\n\n  A(\"'a'|1666\") --&gt; B(\"'b'|1984\") --&gt; C(\"'c'|1066\") --&gt; D(\"...\") --&gt; E(\"'z'|null\")\n\n\n\n\n\n\n\nThe link of the last node is null to indicate the end of the list.\n\n\nWe first need a node data structure, which will hold our data and a link to the next node.\nWe’ll point to the next node itself, rather than its memory address. This still has the same effect as nodes are scattered throughout different memory locations.\n\nclass Node:\n\n    def __init__(self, data, link=None):\n        self.data = data\n        self.link = link\n    \n    def __repr__(self) -&gt; str:\n        return f\"Data: {self.data}\\tLink: \\n{self.link}\"\n\nCreate some nodes and link them together\n\nnode1 = Node(\"a\")\nnode2 = Node(\"b\")\nnode3 = Node(\"c\")\nnode4 = Node(\"d\")\n\nThis is what a single node looks like:\n\nprint(node1)\n\nData: a Link: \nNone\n\n\nNow we link them\n\nnode1.link = node2\nnode2.link = node3\nnode3.link = node4\n\n\nnode1\n\nData: a Link: \nData: b Link: \nData: c Link: \nData: d Link: \nNone\n\n\n\n\n\nThe linked list simply keeps track of the head, i.e. the first node in the list.\nWhen using linked lists, we only have immediate access to this first node. For any other values, we need to start at the head node and traverse the list.\n\nclass LinkedList:\n\n    def __init__(self, head):\n        self.head = head\n\n    def __repr__(self) -&gt; str:\n        return str(self.head)\n\n\nll = LinkedList(node1)\nll\n\nData: a Link: \nData: b Link: \nData: c Link: \nData: d Link: \nNone\n\n\n\n\n\nWe start at the head an traverse the list until we reach the desired index.\nThis means they ar \\(O(N)\\) in the worst case.\n\nclass LinkedList:\n\n    def __init__(self, head):\n        self.head = head\n\n    def __repr__(self) -&gt; str:\n        return str(self.head)\n    \n    def read(self, index):\n        \"\"\"Read the node at the given index.\"\"\"\n        current_idx = 0\n        current_node = self.head\n\n        while (index &gt; current_idx):\n            if current_node.link is None:\n                # The index does not exist in the linked list, we have reached the end\n                return None\n            \n            current_node = current_node.link\n            current_idx += 1          \n\n        return current_node\n\n\nll = LinkedList(node1)\nll.read(2)\n\nData: c Link: \nData: d Link: \nNone\n\n\n\nll.read(10)\n\n\n\n\nTo search for a value, again we have to traverse the whole list.\nThis means the worst case complexity is \\(O(N)\\).\nThe mechanics of searching are the same as reading - we traverse the graph. The difference is we keep going until we find the value or reach the end of the list, rather than stopping at a predetermined index with read.\n\nclass LinkedList:\n\n    def __init__(self, head):\n        self.head = head\n\n    def __repr__(self) -&gt; str:\n        return str(self.head)\n    \n    def read(self, index):\n        \"\"\"Read the node at the given index.\"\"\"\n        # Start at the head\n        current_idx = 0\n        current_node = self.head\n\n        # Traverse the list until we find the desired index\n        while (index &gt; current_idx):\n            if current_node.link is None:\n                # The index does not exist in the linked list, we have reached the end\n                return None\n            \n            current_node = current_node.link\n            current_idx += 1          \n\n        return current_node\n    \n    def search(self, value):\n        \"\"\"Find the index of the given value.\"\"\"\n        # Start at the head\n        current_idx = 0\n        current_node = self.head\n\n        # Loop until we reach the None value which denotes the end of the list\n        while current_node:\n            if current_node.data == value:\n                # We've found our target value\n                return current_idx\n            # Try the next node\n            current_node = current_node.link\n            current_idx += 1\n\n        # We have traversed the whole list without finding a matching value\n        return None\n\n\nll = LinkedList(node1)\nll.search('c')\n\n2\n\n\n\n\n\nInserting a node into a linked list where we already have the current node is an \\(O(1)\\) operation.\n\nPoint to the next node. new_node.link = current_node.link\nLink from the previous node. current_node.link = new_node\n\nWith a linked list, we only have the head node, so we can insert at the start in \\(O(1)\\) time.\nBut to insert at any other point, we have to traverse there first (an \\(O(N)\\) operation) and then do the insert.\nThis is the key point of linked lists: insertion at the beginning is \\(O(1)\\) but at the end is \\(O(N)\\). This is the opposite of arrays, meaning linked lists are useful in cases where insertions are mostly at the beginning.\n\nclass LinkedList:\n\n    def __init__(self, head):\n        self.head = head\n\n    def __repr__(self) -&gt; str:\n        return str(self.head)\n    \n    def read(self, index):\n        \"\"\"Read the node at the given index.\"\"\"\n        # Start at the head\n        current_idx = 0\n        current_node = self.head\n\n        # Traverse the list until we find the desired index\n        while (index &gt; current_idx):\n            if current_node.link is None:\n                # The index does not exist in the linked list, we have reached the end\n                return None\n            \n            current_node = current_node.link\n            current_idx += 1          \n\n        return current_node\n    \n    def search(self, value):\n        \"\"\"Find the index of the given value.\"\"\"\n        # Start at the head\n        current_idx = 0\n        current_node = self.head\n\n        # Loop until we reach the None value which denotes the end of the list\n        while current_node:\n            if current_node.data == value:\n                # We've found our target value\n                return current_idx\n            # Try the next node\n            current_node = current_node.link\n            current_idx += 1\n\n        # We have traversed the whole list without finding a matching value\n        return None\n    \n\n    def insert(self, value, index):\n        \"\"\"Insert the value at the given index.\"\"\"\n        new_node = Node(value)\n\n        if index == 0:\n            # Link to the old head and update the linked lists head\n            new_node.link = self.head\n            self.head = new_node\n            return\n        \n        # Traverse the linked list until we find our node\n        current_node = self.head\n        current_idx = 0\n        while current_idx &lt; index - 1:\n            current_node = current_node.link\n            current_idx += 1\n\n        # Update the links to insert the new node\n        new_node.link = current_node.link\n        current_node.link = new_node\n        return \n\n\n        \n\nInsert a new head of our linked list\n\nll = LinkedList(node1)\nll\n\nData: a Link: \nData: b Link: \nData: c Link: \nData: d Link: \nNone\n\n\n\nll.insert('new_head', 0)\n\n\nll\n\nData: new_head  Link: \nData: a Link: \nData: b Link: \nData: c Link: \nData: d Link: \nNone\n\n\nInsert in the middle\n\nll.insert(\"I'm new here\", 3)\n\n\nll\n\nData: new_head  Link: \nData: a Link: \nData: b Link: \nData: I'm new here  Link: \nData: c Link: \nData: d Link: \nNone\n\n\n\n\n\nIt is quick to delete from the beginning of a linked list for the same reasons as insertion.\n\nMake the previous node point to the next next node\n\n\nclass LinkedList:\n\n    def __init__(self, head):\n        self.head = head\n\n    def __repr__(self) -&gt; str:\n        return str(self.head)\n    \n    def read(self, index):\n        \"\"\"Read the node at the given index.\"\"\"\n        # Start at the head\n        current_idx = 0\n        current_node = self.head\n\n        # Traverse the list until we find the desired index\n        while (index &gt; current_idx):\n            if current_node.link is None:\n                # The index does not exist in the linked list, we have reached the end\n                return None\n            \n            current_node = current_node.link\n            current_idx += 1          \n\n        return current_node\n    \n    def search(self, value):\n        \"\"\"Find the index of the given value.\"\"\"\n        # Start at the head\n        current_idx = 0\n        current_node = self.head\n\n        # Loop until we reach the None value which denotes the end of the list\n        while current_node:\n            if current_node.data == value:\n                # We've found our target value\n                return current_idx\n            # Try the next node\n            current_node = current_node.link\n            current_idx += 1\n\n        # We have traversed the whole list without finding a matching value\n        return None\n    \n\n    def insert(self, value, index):\n        \"\"\"Insert the value at the given index.\"\"\"\n        new_node = Node(value)\n\n        if index == 0:\n            # Link to the old head and update the linked lists head\n            new_node.link = self.head\n            self.head = new_node\n            return\n        \n        # Traverse the linked list until we find our node\n        current_node = self.head\n        current_idx = 0\n        while current_idx &lt; index - 1:\n            current_node = current_node.link\n            current_idx += 1\n\n        # Update the links to insert the new node\n        new_node.link = current_node.link\n        current_node.link = new_node\n        return \n    \n    def delete(self, index):\n        \"\"\"Delete the value at the given index.\"\"\"\n        if index == 0:\n            # We are deleting the head node, so point at the second node instead\n            self.head = self.head.link\n            return\n    \n        # Traverse the linked list until we find our node\n        current_node = self.head\n        current_idx = 0\n        while current_idx &lt; index - 1:\n            current_node = current_node.link\n            current_idx += 1\n\n        # Skip the next node (which we are deleting) and point ot its link instead\n        current_node.link = current_node.link.link\n        return       \n\n\nll = LinkedList(node1)\nll\n\nData: a Link: \nData: b Link: \nData: I'm new here  Link: \nData: c Link: \nData: d Link: \nNone\n\n\nDelete the head node\n\nll.delete(0)\nprint(ll)\n\nData: b Link: \nData: I'm new here  Link: \nData: c Link: \nData: d Link: \nNone\n\n\nDelete a middle node\n\nll.delete(1)\nprint(ll)\n\nData: b Link: \nData: c Link: \nData: d Link: \nNone\n\n\n\n\n\nA doubly linked list is a variant where each node contains pointers to the previous node and the next node.\n\n\n\nData\nPrevious\nNext\n\n\n\n\n“a”\nnull\n1666\n\n\n\nThe linked list tracks the head and tail.\nThis makes it quicker to read/insert/delete from either the beginning or end. We can also traverse backwards or forwards through the list.\n\n\n\n\n\nflowchart LR\n\n  A(\"'a'|null|1666\") &lt;---&gt; B(\"'b'|1234|1984\") &lt;--&gt; C(\"'c'|1666|1066\") &lt;--&gt; D(\"...\") &lt;--&gt; E(\"'z'|1993|null\")\n\n\n\n\n\n\n\nDoubly linked lists are a good data structure to use for queues, since we can insert/delete at either end.\n\n\n\n\nWe can have some use cases where we want to keep our data sorted.\nSorting is expensive, \\(O(N log N)\\) at the best of times, so we want to avoid sorting often. Ideally we would keep our data sorted at all times. An ordered array could do the job, but insertions and deletions are slow as we have to shift a chunk over the array every time.\nWe want a data structure that:\n\nMaintains order\nHas fast inserts, deletes and search\n\nThis is where a binary search tree comes in.\n\n\n\nOperation\nComplexity (Worst)*\nComplexity (Best)\n\n\n\n\nRead\nN/A\nN/A\n\n\nSearch\nO(N)\nO(log N)\n\n\nInsertion\nO(N)\nO(log N)\n\n\nDeletion\nO(N)\nO(log N)\n\n\n\n*The worst case corresponds to an imbalanced tree that is essentailly a linked list (a straight line). The best case is a perfectly balanced tree.\n\n\nTrees are another node-based data structure when each node can point to multiple other nodes.\n\n\n\n\n\nflowchart TD\n\n\n  A(a) --&gt; B(b)\n  A(a) --&gt; C(c)\n\n  B(b) --&gt; D(d)\n  B(b) --&gt; E(e)\n\n  C(c) --&gt; F(f)\n\n\n\n\n\n\n\nThe root is the uppermost node.\na is the parent of b and c; b and c are children of a.\nThe descendants of a node are all of its children and its children’s children’s children etc. The ancestors of anode are its parents and its parent’s parent’s parents etc.\nEach horizontal layer ofthe tree is a level.\nA tree is balanced if all of its subtrees have the same number of nodes.\n\n\n\n\nA binary tree is one in which each node can have at most 2 children.\nA binary search treemust abide by the following rules:\n\nEach node can have at most one “left” child and one “right” child\nA node’s left descendants are all smaller than the node. It’s right descendants are all larger.\n\n\n\n\n\nclass TreeNode:\n\n    def __init__(self, value, left=None, right=None):\n        self.value = value\n        self.left = left\n        self.right = right\n\n    def __repr__(self):\n        return f\"TreeNode with value: {self.value}\"\n\n\nclass Tree: \n\n    def __init__(self, root_node):\n        self.root_node = root_node\n\n    def __repr__(self) -&gt; str:\n        return f\"Tree with root: {self.root_node}\"\n\n\ntree_node2 = TreeNode('b')\ntree_node3 = TreeNode('c')\ntree_node1 = TreeNode('a', tree_node2, tree_node3)\n\ntree = Tree(tree_node1)\n\n\ntree_node1\n\nTreeNode with value: a\n\n\n\ntree_node1.left\n\nTreeNode with value: b\n\n\n\ntree_node1.right\n\nTreeNode with value: c\n\n\n\ntree\n\nTree with root: TreeNode with value: a\n\n\n\n\n\n\nDesignate a current node (start with the root) and inspect its value.\nIf the current node is our target, success! Stop here.\nIf the current node is smaller than our target, search the left subtree. If it’s larger, search the right subtree.\nRepeat until we find our target. If we reach the end of the subtree without finding the target, then the target is not in the tree.\n\n\n\n\nSearching a binary search tree is \\(O(log N)\\) in the best case (a perfectly balanced tree) since we narrow our search area by half on each step.\nThe worst case is a horribly imbalanced tree. Imagine a tree that only has left descendants. This is essentially a linked list, so searching through it means inspecting every element. Therefore the worst case complexity is \\(O(N)\\).\nSo searching a binary search tree is the same complexity as a binary search performed on an ordered array. Where trees differentiate themselves is on insertions and deletes.\n\n\n\n\nCompare our new_node to each value in the tree starting from the root. Like with search, follow the path to the left if new_node is lower or right if higher.\nTraverse until we find a node where the appropriate child (left or right) does not exist yet. Make new_node the child.\n\nThe order of insertion is important to maintain balance. Binary search trees work best when seeded with randomly sorted data, as this will typically end up quite well-balanced. Seeding the tree with sorted data leads to imbalanced trees.\nInsertion is \\(O(log N)\\) for balanced trees because we search for the correct place to insert and then perform an \\(O(1)\\) operation to insert it.\n\n\n\nDeleting a node is more complicated because it depends if the target node has 0, 1 or 2 children.\n\n\n\nNumber of Children\nAction\n\n\n\n\n0\nSimply delete the node\n\n\n1\nReplace the deleted node with its child\n\n\n2\nReplace the deleted node with the successor node\n\n\n\nThe successor node is the next largest descendant of the deleted node. More formally: the child node whose value is the least of all values that are greater than the deleted node.\nFinding the successor:\n\nVisit the right child of the deleted node.\nKeep visiting left children until there are no more elft children. This is the successor node.\nIf the successor node has a right child, that child takes the original place of the successor node. The successor node gets moved to replace the deleted node.\n\nDeletion is \\(O(log N)\\) for balanced trees because we search for the correct place to insert and then perform (potentially several) \\(O(1)\\) operations to delete the node and replace it with a child / successor.\n\n\n\nThe point of a binary search tree is to maintain order. We can traverse the elements in order with the following recursive algorithm.\n\nCall itself recursively on the node’s left child. This will repeat until we hit a node without a left child.\nVisit this node.\nCall itself recursviely on the node’s right child.\n\nSince we are visiting every element of the tree, this is necessarily \\(O(N)\\).\n\ndef traverse(tree_node):\n    \"\"\"Inorder traversal of a binary search tree.\"\"\"\n    if tree_node is None:\n        return\n    traverse(tree_node.left)\n    print(tree_node.value)\n    traverse(tree_node.right)\n\n\n\n\n\n\n\nA queue is a FIFO list that means data is inserted at the end but accessed/removed from the front.\nA priority queue is a list where deletions and access are the same as a regular queue, but insertions are like an ordered array. So the priority queue is always ordered.\nAn example use case is a triage system in a hospital: patients are seen based on severity, not just when they arrived.\nThis is an abstract data types that we could implement in multiple ways using different fundamental data types. E.g. we could implement using an ordered array, but insertions would be O(N).\nHeaps are another data structure that fit this use case well.\n\n\n\nThere a multiple kinds of heaps. As a starting point, we consider the binary max-heap, which is a special kind of binary tree.\nBinary max-heap conditions:\n\nHeap condition: Each node is greater than its children.\nThe tree must be complete.\n\nBecause the heap condition is true for each node, we can recursively reason that a node is greater than its children, and they are too, then a node is greater than all of its descendants.\nThe complete condition essentially means values are filled left-to-right, top-down with no holes. There are no empty positions anywhere except the bottom row, and the bottom row is filled from left to right. The last node is the rightmost node at its bottom level.\nThere is also a min-heap variant. The difference is trivial, aside from the heap condition inequality being reversed, the logic is the same.\n\n\n\n\nHeaps are weakly ordered.\nThe root node is the maximum value.\n\nWith a binary search tree, we know precisely whether a value is the left or right descendant of a node. E.g. 3 will be a left child of 100.\nBut in a max-heap, we don’t know whether 3 is to the left or right, only that it is a descendant rather than an ancestor.\nSearching would require inspecting every node, \\(O(N)\\). In practice, searching a heap is not typically done in the use cases it is used for.\nReading a heap typically refers to accessing the root node, which is \\(O(1)\\).\n\n\n\n\nLast node: Insert the new node as the heap’s last node\nTrickle up the new node: Compare the new node to its parent. If it’s greater than its parent, swap them.\n\nThe number of steps to trickle is proportional to the depth of the tree. Therefore, insertion is a \\(O(log N)\\) operation.\n\n\n\nWe only ever delete the root node from a heap.\n\nNew root: The last node becomes the new root node.\nTrickle down the root node: Trickle the root down to its proper place.\n\nTrickling down is more complicated than trickling up because there are two possible directions to swap in. We compare to both children and swap with the larger of the two. This ensures the largest value ends up as the root node, which is the key property we want to preserve.\nDeletion is also \\(O(log N)\\) since the number of steps to trickle is again proprtional to the depth of the tree.\n\n\n\nAn alternative way of implementing a priority queue would be using an ordered array rather than a heap.\n\n\n\nOperation\nHeap Complexity\nArray Complexity\n\n\n\n\nRead*\n\\(O(1)\\)\n\\(O(1)\\)\n\n\nSearch^\n\\(O(N)\\)\n\\(O(N)\\)\n\n\nInsertion\n\\(O(log N)\\)\n\\(O(N)\\)\n\n\nDeletion\n\\(O(log N)\\)\n\\(O(1)\\)\n\n\n\n* Reading the root node\n^ Searching is not typically done on a heap\nComparing the complexities, heaps are fast, \\(O(log N)\\), for both insertions and deletions, wheres ordered arrays are faster for deletions but slower for insertions.\nAs this is in log space and priority queues typically perform similar numbers of inserts and deletes, on average this makes heap much faster.\n\n\n\nMany operations with heaps, such as insertion, rely on knowing where the last node is.\nThis is important because inserting/deleting the last node ensures the heap is always complete, and completeness is important to ensure the graph remains well-balanced.\nBeing well-balanced ensures the heap remains efficient. As in the case of binary search tree, if a tree is severely unbalanced it effectively becomes a linked list, so actions that should search through the depth of the tree in \\(O(log N)\\) time take \\(O(N)\\) in the worst case.\n\n\nHeaps are often implemented as arrays because the problem of the last node is so crucial, and accessing the last element of an array is \\(O(1)\\).\nThe tree below shows the values and their index in the corresponding array as value|index:\n\n\n\n\n\nflowchart TD\n\n    A[\"100|0\"] ---&gt; B[\"88|1\"]\n    A[\"100|0\"] ---&gt; C[\"25|2\"]\n\n    B[\"88|1\"] ---&gt; B1[\"87|3\"]\n    B[\"88|1\"] ---&gt; B2[\"16|4\"]\n\n    C[\"25|2\"] ---&gt; C1[\"8|5\"]\n    C[\"25|2\"] ---&gt; C2[\"12|6\"]\n\n\n\n\n\n\nThe array representation is then:\n\n\n\n0\n1\n2\n3\n4\n5\n6\n\n\n\n\n100\n88\n25\n87\n16\n8\n12\n\n\n\nThe indices are then deterministic. We can find the child index of any given node as:\nleft_child_idx = (parent_index * 2) + 1\nright_child_idx = (parent_index * 2) + 2\nTo find a given node’s parent index:\nparent_idx = (child_idx - 1) // 2\n\n\n\nIt is also possible to implement heaps as linked nodes.\nIn this case, a different trick is required to solve the problem of the last node using binary numbers.\n\nAssign Binary Numbers: Each level of the heap is assigned a unique binary number. The root is 0, then each left child is concatenates the parent’s number with 0 at the end, and each right child concatenates the parent’s index with 1 at the end.\nInsertion using Binary Representation: To insert a new node into the heap, you convert the index of the node into binary form. Starting from the most significant bit (MSB), traverse the heap according to the binary digits:\n\nIf the bit is 0, move to the left child.\nIf the bit is 1, move to the right child.\n\nInsertion at the Last Available Position: As you traverse the heap, you’ll eventually reach a node that doesn’t exist yet. This node represents the last available position in the heap, where the new node can be inserted while maintaining the complete binary tree property.\n\n\n\n\n\n\nflowchart TD\n\nA[\"100|0\"] ---&gt; B[\"88|00\"]\nA[\"100|0\"] ---&gt; C[\"25|01\"]\n\nB[\"88|00\"] ---&gt; B1[\"87|000\"]\nB[\"88|00\"] ---&gt; B2[\"16|001\"]\n\nC[\"25|01\"] ---&gt; C1[\"8|010\"]\nC[\"25|01\"] ---&gt; C2[\"12|011\"]\n\n\n\n\n\n\n\n\n\n\n\nThis is a kind of tree which is particularly useful for text-based features.\nEach trie node can have any number of children. Each word is split into characters which are stored as a series of nested child nodes. A terminal character is used to denote the end of a word.\n\n\n\n\n\nflowchart TD\n\nG(G) ---&gt; U(U) ---&gt; R(R) ---&gt; P(P) ---&gt; X(*)\n\n\n\n\n\n\n\n\nThere are two flavours of search: (1) checking if a substring is a valid prefix, and (2) checking if a substring is a valid whole word.\nThe algorithm below is for the more general case of searching for a prefix. Searching for a whole word then becomes trivial because we have a terminal character, so we can search for “Implement*” to find the whole word.\nIterate through the trie one character at a time:\n\nInitialise current_node at the root.\nIterate through each character of search_string and see if current_node has a child with that key.\nRepeat Step 3 for the whole search_string. If the character is not a valid key, the word does not exist.\n\nThe efficiency of a trie search depends on the length of the search term, \\(K\\), not the number of elements stored in the trie, \\(N\\). Therefore, it has complexity \\(O(K)\\).\n\n\n\nInserting a new word into a trie follows similar steps to searching for an existing word: we traverse the trie and insert new nodes where they do not already exist.\n\nInitialise current_node at the root.\nIterate through each character of search_string and see if current_node has a child with that key.\n\nIf the key exists, update current_node to move to that next node.\nOtherwise, create a new child node and update current_node to move to it.\n\nInsert the terminal character of the word at the end.\n\nThis is again \\(O(K)\\) since it depends on the length of the input, not the data stored in the trie.\n\n\n\nWe can implement a trie as dictionaries nested within dictionaries.\n\nclass TrieNode:\n    \n    def __init__(self):\n        self.children = {}\n        \n        \nclass Trie:\n    \n    def __init__(self):\n        self.root = TrieNode()\n        self._TERMINAL_CHAR = \"*\"\n        \n    def search(self, word):\n        \"\"\"Search for a given word in the Trie.\"\"\"\n        current_node = self.root\n        \n        for char in word:\n            if char in current_node.keys():\n                # Keep traversing as long as we find valid children\n                current_node = current_node[char]\n            else: \n                # The search_string is not a valid prefix, so return None\n                return None\n        \n        return current_node\n    \n    def insert(self, new_word):\n        \"\"\"Insert a new word into the Trie.\"\"\"\n        current_node = self.root\n\n        # Traverse the word + terminal character\n        for char in new_word + self._TERMINAL_CHAR:\n            if char in current_node.keys():\n                # Keep traversing as long as we find valid children\n                current_node = current_node[char]\n            else:\n                current_node[char] = {}\n                current_node = current_node[char]\n\n\n\n\nA good use case of tries is for autocomplete.\nWe use a trie to store our dictionary of possible words.\nThen for a given user input, we can recursively list all of the words with that prefix.\nWe can improve autocomplete further by storing an integer popularity value as the terminal value rather than an empty dictionary or null value. This can be a 1-10 score of how commonly used that word is, so that we can prioritise showing more common words as autocompletion suggestions."
  },
  {
    "objectID": "posts/software/data_structures_algos/data_structures/data_structures.html#data-structure-operations",
    "href": "posts/software/data_structures_algos/data_structures/data_structures.html#data-structure-operations",
    "title": "Data Structures",
    "section": "",
    "text": "There are common ways that we can interact with different data structures.\nIt is useful to frame the appropriateness of a data structure for a given based on the speed of the operations that are required most for that task.\n\nRead: Look up the value at a particular location in the data structure\nSearch: Look for a particular value in the data structure.\nInsert: Add a new value to the data structure.\nDelete: Remove a value from the data structure.\n\nReading is “find by key”, searching is “find by value”."
  },
  {
    "objectID": "posts/software/data_structures_algos/data_structures/data_structures.html#arrays",
    "href": "posts/software/data_structures_algos/data_structures/data_structures.html#arrays",
    "title": "Data Structures",
    "section": "",
    "text": "An array is a list of elements.\nIt is stored in memory as a block of contiguous memory addresses. When the array is declared, the head of the array is stored, i.e. the memory address of the first elelment.\nThe size of an array is the number of elements in the array. The index denotes where a particular piece of data resides in that array.\n\n\n\nOperation\nComplexity (Worst)\nComplexity (Best)\n\n\n\n\nRead\nO(1)\nO(1)\n\n\nSearch\nO(N)\nO(1)\n\n\nInsertion\nO(N)\nO(1)\n\n\nDeletion\nO(N)\nO(1)\n\n\n\n\n\nA computer can look up a given memory address in constant time.\nWe know the head of the array and the index to look up. So we can read in O(1) time.\nExample:\n\nHead of the array is memory address 1063.\nWe want to look up index 5.\nRead memory address 1068 (because 1063 + 5 = 1068).\n\n\nIf you were asked to raise your right pinky finger, you wouldn’t need to search through all of your fingers to find it\n\n\n\n\nA computer has immediate access to all of its memory addresses but does not know ahead of time their contents.\nSo to find a particular value, we will potentially have to search through every element.\nSearching an array is therefore O(N).\n\n\n\nThe efficiency of inserting into an array depends on where in the array you are inserting.\nIf inserting an element at the end, we simply place the new value at that memory address (assuming the memory address is empty). This is a constant time operation O(1).\nBut if we insert at any other position, we need to:\n\nShift each of the existing elements to the right of the insert index 1 position rightwards\nInsert the new value in the gap created.\n\nSo to insert at index \\(i\\), there are \\(N - i\\) elements to shift (Step 1), then 1 more operation to insert the new value (Step 2).\nIn the worst case - inserting at the start of an array - insertion is O(N).\n\n\n\nSimilarly, the efficiency of deletion depends on the index being deleted.\nIf deleting the last element, there is simply 1 operation to clear the memory address, so this is a constant time operation O(1).\nBut if we delete an element in any other position, we need to:\n\nDelete the element. This leaves a gap in the middle of the array.\nShift each of the elements to the right of the gap leftwards, to fill the gap.\n\nSo to delete at index \\(i\\), we do 1 operation to delete that element (Step 1), then shift the next \\(N-i\\) elements leftwards (Step 2).\nIn the worst case - deleting the first element of the array - deletion is O(N)."
  },
  {
    "objectID": "posts/software/data_structures_algos/data_structures/data_structures.html#sets",
    "href": "posts/software/data_structures_algos/data_structures/data_structures.html#sets",
    "title": "Data Structures",
    "section": "",
    "text": "A set is a collection of unique elements, i.e. duplicate values are not allowed.\nThere are different ways of implementing sets: array-based sets and hash-sets are discussed here.\nNote that Python already has sets, but we’ll give outline implementations for clarity.\n\n\nAn array is used to store elements. As with standard arrays, elements are stored in contiguous memory locations, and each element has a unique index.\nExample in Python:\n\nclass ArraySet:\n    \n    def __init__(self):\n        self.elements = []\n        \n    def __repr__(self):\n        return str(self.elements)\n\n    def add(self, element):\n        # Search the array for `element`, then append it if it is not a duplicate.\n        if element not in self.elements:\n            self.elements.append(element)\n\n    def remove(self, element):\n        # Search the array for the value, then remove it.\n        if element in self.elements:\n            self.elements.remove(element)\n\n    def contains(self, element):\n        return element in self.elements\n\n\nset_arr = ArraySet()\nset_arr.add(1)\nset_arr.add(2)\nset_arr.add(3)\nprint(set_arr)\n\n[1, 2, 3]\n\n\nIf we try to add a duplicate value it does not get added to the array:\n\nset_arr.add(2)\nprint(set_arr)\n\n[1, 2, 3]\n\n\nThe read, search and delete operations for an array-based set are identical to the standard array.\nInsert operations are where array-based sets diverge. We always insert at the end of a set, which is constant time. But we need to search the array every time to ensure the new value is not a duplicate.\nSo we always need to do a search of all N elements, and then 1 insert operation at the end.\nThis means even in the best case, insertion into an array-based set is O(N) compared to O(1) when inserting at the end of a standard array.\nThe reason for using a set is because the use case requires no duplicates, not because it is inherently “quicker” than a standard array.\n\n\n\nOperation\nComplexity (Worst)\nComplexity (Best)\n\n\n\n\nRead\nO(1)\nO(1)\n\n\nSearch\nO(N)\nO(1)\n\n\nInsertion\nO(N)\nO(N)\n\n\nDeletion\nO(N)\nO(1)\n\n\n\n\n\n\nA hash-based set computes the hash of each element and uses this to store elements.\nAn example implementation implements the set as key-value pairs where keys are the hash of the elements and values are a placeholder value like True, or an array to handle hash collisions.\nWhen there is a hash collision between mutliple elements, a typical approach is to insert all of these elements as an array under the same hash key.\nThe worst case scenario is caused by the extreme edge case where hash collisions are so prominent that every element has the same hash, essentially reducing the hash set to an array. This is generally avoided as long as the hash algorithm is decent.\nFor this reason, the average complexity is more meaningful in the table below. (Note that best has been replace with average in the table headings.)\nHash-based sets do not support reading by index, unlike array-based sets. But all other operations are typically constant time.\n\n\n\nOperation\nComplexity (Worst)\nComplexity (Average)\n\n\n\n\nRead\nN/A\nN/A\n\n\nSearch\nO(N)\nO(1)\n\n\nInsertion\nO(N)\nO(1)\n\n\nDeletion\nO(N)\nO(1)\n\n\n\nExample in Python:\n\nclass HashSet:\n    def __init__(self):\n        # Use a dict to represent the hash set.\n        self.elements = {}\n        \n    def __repr__(self):\n        return str(self.elements)\n\n    def add(self, element):\n        # The key is the element and the value is arbitrary.\n        # There are two extensions we could add here:\n        #   1. The key should really be the *hash* of the element, not just the element itself.\n        #      Essentially, this is using an implicit hash function which is just a pass-through:\n        #      hash_func = lambda x: x\n        #   2. Handle hash collisions by making the value an array which is appended to in the case of collisions.\n        self.elements[element] = True\n\n    def remove(self, element):\n        if element in self.elements:\n            del self.elements[element]\n\n    def contains(self, element):\n        return element in self.elements\n\n\nset_hash = HashSet()\nset_hash.add(1)\nset_hash.add(2)\nset_hash.add(3)\nprint(set_hash)\n\n{1: True, 2: True, 3: True}\n\n\nIf we try to add a duplicate value it simply overwrites the previous value:\n\nset_hash.add(2)\nprint(set_hash)\n\n{1: True, 2: True, 3: True}"
  },
  {
    "objectID": "posts/software/data_structures_algos/data_structures/data_structures.html#ordered-arrays",
    "href": "posts/software/data_structures_algos/data_structures/data_structures.html#ordered-arrays",
    "title": "Data Structures",
    "section": "",
    "text": "These are identical to regular arrays with the additional condition that elements are always ordered.\nThis obviously relies heavily on efficient sorting. This is a topic unto itself; see notes on sorting for more info.\nWhen inserting into an ordered array, we need to:\n\nSearch for the correct position - Look at each element in turn and compare if the insert element is greater than it\nInsert into the array\n\nThese two terms increase in opposite directions depending on the insert position. The further into the array we need to search (Step 1), the fewer elements we need to shift for the insertion (Step 2).\n\n\nIn a typical (unordered) array, the only option for searching is a linear search: we loop through each element in turn until we find our target.\nFor an ordered array, we can improve on this using a binary search.\n\nPick the middle element.\nIf the target value is greater than this, search the right half, otherwise search the left half.\nRepeat this recursively until we find our target.\n\nThis approach splits the search region in half for every constant time comparison operation.\nOr put another way, if we doubled the number of elements in the array, the binary search would only have to perform 1 extra step. For \\(N\\) elements we need \\(log_2(N)\\) binary splits.\nTherefore, the time complexity is O(log(N)).\n\ndef binary_search(ordered_array, target):\n    \"\"\"Perform a binary search for the target value on the given ordered array.\n\n    Parameters\n    ----------\n    ordered_array: list\n        The array to search in.\n    target: int\n        The target value we are searching for.\n\n    Returns\n    -------\n    target_index: int\n        The index of the target value.\n        Returns None if the value does not exist in the array.\n    \"\"\"\n    # Establish the lower and upper bounds of our search.\n    # Initially, this is the entire array\n    idx_lower = 0\n    idx_upper = len(ordered_array) - 1\n\n    while idx_lower &lt;= idx_upper:\n        # Find the midpoint between our bounds\n        idx_midpoint = (idx_upper + idx_lower) // 2\n        value_at_midpoint = ordered_array[idx_midpoint]\n\n        # Compare to our target value and narrow the upper or lower bound accordingly\n        if value_at_midpoint == target:\n            # We have found the target!\n            return idx_midpoint\n        elif value_at_midpoint &lt; target:\n            # The target is bigger so must be to the right side\n            idx_lower = idx_midpoint + 1\n        elif value_at_midpoint &gt; target:\n            # The target is smaller so must be on the left side\n            idx_upper = idx_midpoint - 1\n\n    # If the lower and upper bounds meet we have exhausted the whole array, so the target is not in the array\n    return None\n\nLet’s try this on a few examples.\n\nordered_array = [1, 2, 4, 5, 7, 8, 9, 10, 13, 14]\n\n\nbinary_search(ordered_array, 7)\n\n4\n\n\n\nbinary_search(ordered_array, 14)\n\n9\n\n\nNow a value that’s not in the array:\n\nbinary_search(ordered_array, 14)\n\n9\n\n\nCompare this with a linear search\n\ndef linear_search(array, target):\n    \"\"\"Perform a linear search for the target value on the given array.\n\n    Parameters\n    ----------\n    array: list\n        The array to search in.\n    target: int\n        The target value we are searching for.\n\n    Returns\n    -------\n    target_index: int\n        The index of the target value.\n        Returns None if the value does not exist in the array.\n    \"\"\"\n    # Loop through every element in the array.\n    # Note: we should really use enumerate() rather than range(len()) but I wanted to keep this generic \n    # without too many python-specific helpers\n    for idx in range(len(array)):\n        if array[idx] == target:\n            return idx\n    \n    # If we reach the end of the array without returning a value, then the target does not exist in the array.\n    return None\n\nLet’s compare how they perform for a reasonably big array with 1 million elements.\n\narray = [k for k in range(1000000)]\n\n\n%%timeit\nbinary_search(array, 987654)\n\n1.13 µs ± 56.9 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\n\n%%timeit\nlinear_search(array, 987654)\n\n15.9 ms ± 320 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nThe binary search is ~14000x faster than the linear search!"
  },
  {
    "objectID": "posts/software/data_structures_algos/data_structures/data_structures.html#hash-tables",
    "href": "posts/software/data_structures_algos/data_structures/data_structures.html#hash-tables",
    "title": "Data Structures",
    "section": "",
    "text": "Hash tables are key:value pairs. We can look up a the value for a given key in \\(O(1)\\) time.\nAlso known as hash maps, dictionaries, maps, associative arrays.\n\n\nThe process of taking characters and converting them to numbers is called hashing.\nThe code that performs this conversion is the hash function.\nA hash function requires one condition:\n\nConsistency: A hash function must convert the same string to the same number every time it’s applied.\n\nIn practice, for the hash function to be useful, it should also be collision resistant:\n\nCollision resistant: Different inputs should hash to different outputs.\n\nAs an extreme example, the following hash function is consistent but not collision resistant:\ndef crappy_hash(input_str: str) -&gt; int:\n    \"\"\"This is the world's worst hash function.\"\"\"\n    return 1\n\n\n\nWe want to insert the following key:value pair into our hash table:\nkey = \"Name\"\nvalue = \"Gurp\"\nLet’s say we have a hash function that is actually good, and in this particular case hash_function(key) returns 12.\nThe hash table will then insert value at memory address 12 (or more specifically, the memory address of the head of the dictionary + 12).\nThis means if we ever want to look up the key \"Name\", we hash it and immediately know to access memory address 12 and return the value \"Gurp\".\nSo hash table lookups are \\(O(1)\\).\nMore specifically, looking up by key is \\(O(1)\\). Searching by value is essentially searching through an array, so is \\(O(N)\\).\n\n\n\nA collision occurs when we try to add data to a cell that is already occupied.\nOne approach to handling this is called separate chaining.\nInstead of placing a single value in a cell, we place a pointer to an array. This array contains length-2 subarrays where the first element is the key and the second element is the value.\nIf there are no collisions, a hash table look up is \\(O(1)\\). In the worst case, ALL keys collide and so we essentially have to search through an array which is \\(O(N)\\).\n\n\n\nA hash tables efficiency depends on:\n\nHow much data we’re storing in it\nHow many cells are available\nWhich hash function we use\n\nA good hash function (3) should distribute the data (1) evenly across all cells (2).\nThis ensures the memory is used efficiently while avoiding collisions.\nThe load factor is the ratio of data to cells, and ideally should be around 0.7, i.e. 7 elements for every 10 cells.\n\n\n\nOperation\nComplexity (Worst)\nComplexity (Average)\n\n\n\n\nRead\nO(N)\nO(1)\n\n\nSearch\nO(N)\nKeys: O(1) Values: O(N)\n\n\nInsertion\nO(N)\nO(1)\n\n\nDeletion\nO(N)\nO(1)\n\n\n\nThe worst case corresponds to when all keys collide, reducing the hash table to an array effectively."
  },
  {
    "objectID": "posts/software/data_structures_algos/data_structures/data_structures.html#stacks",
    "href": "posts/software/data_structures_algos/data_structures/data_structures.html#stacks",
    "title": "Data Structures",
    "section": "",
    "text": "A stack is stored in memory the same as an array, but it has 3 constraints:\n\nData can only be inserted at the end (push)\nData can only be deleted from the end (pop)\nOnly the last element can be read (peek)\n\n\nRead, insert and delete can only happen at the end.\n\nThis makes them useful as Last-In First-Out (LIFO) data stores: the last item pushed the the stack is the first to be popped.\nExample in Python:\n\nclass Stack:\n\n    def __init__(self, initial_elements: list = []):\n        # We can pass a list to initialise the Stack\n        self.data = initial_elements\n\n    def __repr__(self):\n        return str(self.data)\n\n    def push(self, element):\n        self.data.append(element)\n\n    def pop(self):\n        return self.data.pop()\n\n    def peek(self):\n        if self.data:\n            return self.data[-1]\n\n\nstack = Stack([1, 2, 3, 4])\nstack\n\n[1, 2, 3, 4]\n\n\n\nstack.push(5)\nprint(stack)\n\n[1, 2, 3, 4, 5]\n\n\n\nstack.peek()\n\n5\n\n\n\nprint(stack)\n\n[1, 2, 3, 4, 5]\n\n\n\nstack.pop()\n\n5\n\n\n\nprint(stack)\n\n[1, 2, 3, 4]\n\n\nThe benefits of stacks, and other constrained data structures, are:\n\nPrevent potential bugs when using certain algorithms. For example, an algorithm that relies on stacks may break if removing elements from the middle of the array, so using a standard array is more error-prone.\nA new mental model for tackling problems. In the case of stacks, this is the LIFO approach.\n\nThe concept of stacks is a useful precursor to recursion, as we push to and pop from the end of a stack."
  },
  {
    "objectID": "posts/software/data_structures_algos/data_structures/data_structures.html#queues",
    "href": "posts/software/data_structures_algos/data_structures/data_structures.html#queues",
    "title": "Data Structures",
    "section": "",
    "text": "A queue is conceptually similar to a stack - it is a constrained array. This time, it is First-In First-Out (FIFO) like a queue of people; the first person to arrive is the first to leave.\nQueue restrictions:\n\nData can only be inserted at the end (enqueue)\nData can only be deleted from the front (dequeue)\nData can only be read from the front (peek)\n\nPoints (2) and (3) are the opposite of the stack.\n\nclass Queue:\n\n    def __init__(self, initial_elements: list = []):\n        # We can pass a list to initialise the Stack\n        self.data = initial_elements\n\n    def __repr__(self):\n        return str(self.data)\n\n    def enqueue(self, element):\n        self.data.append(element)\n\n    def dequeue(self):\n        return self.data.pop(0)\n\n    def peek(self):\n        if self.data:\n            return self.data[0]\n\n\nq = Queue([1, 2, 3, 4])\nq\n\n[1, 2, 3, 4]\n\n\n\nq.enqueue(5)\nprint(q)\n\n[1, 2, 3, 4, 5]\n\n\n\nq.dequeue()\n\n1\n\n\n\nprint(q)\n\n[2, 3, 4, 5]\n\n\n\nq.peek()\n\n2\n\n\n\nprint(q)\n\n[2, 3, 4, 5]"
  },
  {
    "objectID": "posts/software/data_structures_algos/data_structures/data_structures.html#linked-lists",
    "href": "posts/software/data_structures_algos/data_structures/data_structures.html#linked-lists",
    "title": "Data Structures",
    "section": "",
    "text": "A linked list represents a list of items as non-contiguous blocks of memory.\nIt is a list of items, similar to an array. But an array occupies a continuous block of memory.\n\n\n\nOperation\nComplexity (Worst)\nComplexity (Best)*\n\n\n\n\nRead\nO(N)\nO(1)\n\n\nSearch\nO(N)\nO(1)\n\n\nInsertion\nO(N)\nO(1)\n\n\nDeletion\nO(N)\nO(1)\n\n\n\nThe best case corresponds to operating on the head node.\nIn a linked list, each element is contained in a node that can be in scattered positions in memory. The node contains the data element and a “link” which is a pointer to the memory address of the next element.\nBenefits of a linked list over an array:\n\nMemory efficient: we don’t need a continuous block of memory\n\\(O(1)\\) inserts and deletes from the beginning of the list\nUseful when we want to traverse through a data structure while making inserts and deletes, because we do not have to shift the entire data structure each time as we would have to with an array\n\nA node contains two pieces of information:\n\n\n\nData\nLink\n\n\n\n\n“a”\n1666\n\n\n\nThese nodes can then be linked together in a list… a linked list!\n\n\n\n\n\nflowchart LR\n\n  A(\"'a'|1666\") --&gt; B(\"'b'|1984\") --&gt; C(\"'c'|1066\") --&gt; D(\"...\") --&gt; E(\"'z'|null\")\n\n\n\n\n\n\n\nThe link of the last node is null to indicate the end of the list.\n\n\nWe first need a node data structure, which will hold our data and a link to the next node.\nWe’ll point to the next node itself, rather than its memory address. This still has the same effect as nodes are scattered throughout different memory locations.\n\nclass Node:\n\n    def __init__(self, data, link=None):\n        self.data = data\n        self.link = link\n    \n    def __repr__(self) -&gt; str:\n        return f\"Data: {self.data}\\tLink: \\n{self.link}\"\n\nCreate some nodes and link them together\n\nnode1 = Node(\"a\")\nnode2 = Node(\"b\")\nnode3 = Node(\"c\")\nnode4 = Node(\"d\")\n\nThis is what a single node looks like:\n\nprint(node1)\n\nData: a Link: \nNone\n\n\nNow we link them\n\nnode1.link = node2\nnode2.link = node3\nnode3.link = node4\n\n\nnode1\n\nData: a Link: \nData: b Link: \nData: c Link: \nData: d Link: \nNone\n\n\n\n\n\nThe linked list simply keeps track of the head, i.e. the first node in the list.\nWhen using linked lists, we only have immediate access to this first node. For any other values, we need to start at the head node and traverse the list.\n\nclass LinkedList:\n\n    def __init__(self, head):\n        self.head = head\n\n    def __repr__(self) -&gt; str:\n        return str(self.head)\n\n\nll = LinkedList(node1)\nll\n\nData: a Link: \nData: b Link: \nData: c Link: \nData: d Link: \nNone\n\n\n\n\n\nWe start at the head an traverse the list until we reach the desired index.\nThis means they ar \\(O(N)\\) in the worst case.\n\nclass LinkedList:\n\n    def __init__(self, head):\n        self.head = head\n\n    def __repr__(self) -&gt; str:\n        return str(self.head)\n    \n    def read(self, index):\n        \"\"\"Read the node at the given index.\"\"\"\n        current_idx = 0\n        current_node = self.head\n\n        while (index &gt; current_idx):\n            if current_node.link is None:\n                # The index does not exist in the linked list, we have reached the end\n                return None\n            \n            current_node = current_node.link\n            current_idx += 1          \n\n        return current_node\n\n\nll = LinkedList(node1)\nll.read(2)\n\nData: c Link: \nData: d Link: \nNone\n\n\n\nll.read(10)\n\n\n\n\nTo search for a value, again we have to traverse the whole list.\nThis means the worst case complexity is \\(O(N)\\).\nThe mechanics of searching are the same as reading - we traverse the graph. The difference is we keep going until we find the value or reach the end of the list, rather than stopping at a predetermined index with read.\n\nclass LinkedList:\n\n    def __init__(self, head):\n        self.head = head\n\n    def __repr__(self) -&gt; str:\n        return str(self.head)\n    \n    def read(self, index):\n        \"\"\"Read the node at the given index.\"\"\"\n        # Start at the head\n        current_idx = 0\n        current_node = self.head\n\n        # Traverse the list until we find the desired index\n        while (index &gt; current_idx):\n            if current_node.link is None:\n                # The index does not exist in the linked list, we have reached the end\n                return None\n            \n            current_node = current_node.link\n            current_idx += 1          \n\n        return current_node\n    \n    def search(self, value):\n        \"\"\"Find the index of the given value.\"\"\"\n        # Start at the head\n        current_idx = 0\n        current_node = self.head\n\n        # Loop until we reach the None value which denotes the end of the list\n        while current_node:\n            if current_node.data == value:\n                # We've found our target value\n                return current_idx\n            # Try the next node\n            current_node = current_node.link\n            current_idx += 1\n\n        # We have traversed the whole list without finding a matching value\n        return None\n\n\nll = LinkedList(node1)\nll.search('c')\n\n2\n\n\n\n\n\nInserting a node into a linked list where we already have the current node is an \\(O(1)\\) operation.\n\nPoint to the next node. new_node.link = current_node.link\nLink from the previous node. current_node.link = new_node\n\nWith a linked list, we only have the head node, so we can insert at the start in \\(O(1)\\) time.\nBut to insert at any other point, we have to traverse there first (an \\(O(N)\\) operation) and then do the insert.\nThis is the key point of linked lists: insertion at the beginning is \\(O(1)\\) but at the end is \\(O(N)\\). This is the opposite of arrays, meaning linked lists are useful in cases where insertions are mostly at the beginning.\n\nclass LinkedList:\n\n    def __init__(self, head):\n        self.head = head\n\n    def __repr__(self) -&gt; str:\n        return str(self.head)\n    \n    def read(self, index):\n        \"\"\"Read the node at the given index.\"\"\"\n        # Start at the head\n        current_idx = 0\n        current_node = self.head\n\n        # Traverse the list until we find the desired index\n        while (index &gt; current_idx):\n            if current_node.link is None:\n                # The index does not exist in the linked list, we have reached the end\n                return None\n            \n            current_node = current_node.link\n            current_idx += 1          \n\n        return current_node\n    \n    def search(self, value):\n        \"\"\"Find the index of the given value.\"\"\"\n        # Start at the head\n        current_idx = 0\n        current_node = self.head\n\n        # Loop until we reach the None value which denotes the end of the list\n        while current_node:\n            if current_node.data == value:\n                # We've found our target value\n                return current_idx\n            # Try the next node\n            current_node = current_node.link\n            current_idx += 1\n\n        # We have traversed the whole list without finding a matching value\n        return None\n    \n\n    def insert(self, value, index):\n        \"\"\"Insert the value at the given index.\"\"\"\n        new_node = Node(value)\n\n        if index == 0:\n            # Link to the old head and update the linked lists head\n            new_node.link = self.head\n            self.head = new_node\n            return\n        \n        # Traverse the linked list until we find our node\n        current_node = self.head\n        current_idx = 0\n        while current_idx &lt; index - 1:\n            current_node = current_node.link\n            current_idx += 1\n\n        # Update the links to insert the new node\n        new_node.link = current_node.link\n        current_node.link = new_node\n        return \n\n\n        \n\nInsert a new head of our linked list\n\nll = LinkedList(node1)\nll\n\nData: a Link: \nData: b Link: \nData: c Link: \nData: d Link: \nNone\n\n\n\nll.insert('new_head', 0)\n\n\nll\n\nData: new_head  Link: \nData: a Link: \nData: b Link: \nData: c Link: \nData: d Link: \nNone\n\n\nInsert in the middle\n\nll.insert(\"I'm new here\", 3)\n\n\nll\n\nData: new_head  Link: \nData: a Link: \nData: b Link: \nData: I'm new here  Link: \nData: c Link: \nData: d Link: \nNone\n\n\n\n\n\nIt is quick to delete from the beginning of a linked list for the same reasons as insertion.\n\nMake the previous node point to the next next node\n\n\nclass LinkedList:\n\n    def __init__(self, head):\n        self.head = head\n\n    def __repr__(self) -&gt; str:\n        return str(self.head)\n    \n    def read(self, index):\n        \"\"\"Read the node at the given index.\"\"\"\n        # Start at the head\n        current_idx = 0\n        current_node = self.head\n\n        # Traverse the list until we find the desired index\n        while (index &gt; current_idx):\n            if current_node.link is None:\n                # The index does not exist in the linked list, we have reached the end\n                return None\n            \n            current_node = current_node.link\n            current_idx += 1          \n\n        return current_node\n    \n    def search(self, value):\n        \"\"\"Find the index of the given value.\"\"\"\n        # Start at the head\n        current_idx = 0\n        current_node = self.head\n\n        # Loop until we reach the None value which denotes the end of the list\n        while current_node:\n            if current_node.data == value:\n                # We've found our target value\n                return current_idx\n            # Try the next node\n            current_node = current_node.link\n            current_idx += 1\n\n        # We have traversed the whole list without finding a matching value\n        return None\n    \n\n    def insert(self, value, index):\n        \"\"\"Insert the value at the given index.\"\"\"\n        new_node = Node(value)\n\n        if index == 0:\n            # Link to the old head and update the linked lists head\n            new_node.link = self.head\n            self.head = new_node\n            return\n        \n        # Traverse the linked list until we find our node\n        current_node = self.head\n        current_idx = 0\n        while current_idx &lt; index - 1:\n            current_node = current_node.link\n            current_idx += 1\n\n        # Update the links to insert the new node\n        new_node.link = current_node.link\n        current_node.link = new_node\n        return \n    \n    def delete(self, index):\n        \"\"\"Delete the value at the given index.\"\"\"\n        if index == 0:\n            # We are deleting the head node, so point at the second node instead\n            self.head = self.head.link\n            return\n    \n        # Traverse the linked list until we find our node\n        current_node = self.head\n        current_idx = 0\n        while current_idx &lt; index - 1:\n            current_node = current_node.link\n            current_idx += 1\n\n        # Skip the next node (which we are deleting) and point ot its link instead\n        current_node.link = current_node.link.link\n        return       \n\n\nll = LinkedList(node1)\nll\n\nData: a Link: \nData: b Link: \nData: I'm new here  Link: \nData: c Link: \nData: d Link: \nNone\n\n\nDelete the head node\n\nll.delete(0)\nprint(ll)\n\nData: b Link: \nData: I'm new here  Link: \nData: c Link: \nData: d Link: \nNone\n\n\nDelete a middle node\n\nll.delete(1)\nprint(ll)\n\nData: b Link: \nData: c Link: \nData: d Link: \nNone\n\n\n\n\n\nA doubly linked list is a variant where each node contains pointers to the previous node and the next node.\n\n\n\nData\nPrevious\nNext\n\n\n\n\n“a”\nnull\n1666\n\n\n\nThe linked list tracks the head and tail.\nThis makes it quicker to read/insert/delete from either the beginning or end. We can also traverse backwards or forwards through the list.\n\n\n\n\n\nflowchart LR\n\n  A(\"'a'|null|1666\") &lt;---&gt; B(\"'b'|1234|1984\") &lt;--&gt; C(\"'c'|1666|1066\") &lt;--&gt; D(\"...\") &lt;--&gt; E(\"'z'|1993|null\")\n\n\n\n\n\n\n\nDoubly linked lists are a good data structure to use for queues, since we can insert/delete at either end."
  },
  {
    "objectID": "posts/software/data_structures_algos/data_structures/data_structures.html#binary-search-trees",
    "href": "posts/software/data_structures_algos/data_structures/data_structures.html#binary-search-trees",
    "title": "Data Structures",
    "section": "",
    "text": "We can have some use cases where we want to keep our data sorted.\nSorting is expensive, \\(O(N log N)\\) at the best of times, so we want to avoid sorting often. Ideally we would keep our data sorted at all times. An ordered array could do the job, but insertions and deletions are slow as we have to shift a chunk over the array every time.\nWe want a data structure that:\n\nMaintains order\nHas fast inserts, deletes and search\n\nThis is where a binary search tree comes in.\n\n\n\nOperation\nComplexity (Worst)*\nComplexity (Best)\n\n\n\n\nRead\nN/A\nN/A\n\n\nSearch\nO(N)\nO(log N)\n\n\nInsertion\nO(N)\nO(log N)\n\n\nDeletion\nO(N)\nO(log N)\n\n\n\n*The worst case corresponds to an imbalanced tree that is essentailly a linked list (a straight line). The best case is a perfectly balanced tree.\n\n\nTrees are another node-based data structure when each node can point to multiple other nodes.\n\n\n\n\n\nflowchart TD\n\n\n  A(a) --&gt; B(b)\n  A(a) --&gt; C(c)\n\n  B(b) --&gt; D(d)\n  B(b) --&gt; E(e)\n\n  C(c) --&gt; F(f)\n\n\n\n\n\n\n\nThe root is the uppermost node.\na is the parent of b and c; b and c are children of a.\nThe descendants of a node are all of its children and its children’s children’s children etc. The ancestors of anode are its parents and its parent’s parent’s parents etc.\nEach horizontal layer ofthe tree is a level.\nA tree is balanced if all of its subtrees have the same number of nodes.\n\n\n\n\nA binary tree is one in which each node can have at most 2 children.\nA binary search treemust abide by the following rules:\n\nEach node can have at most one “left” child and one “right” child\nA node’s left descendants are all smaller than the node. It’s right descendants are all larger.\n\n\n\n\n\nclass TreeNode:\n\n    def __init__(self, value, left=None, right=None):\n        self.value = value\n        self.left = left\n        self.right = right\n\n    def __repr__(self):\n        return f\"TreeNode with value: {self.value}\"\n\n\nclass Tree: \n\n    def __init__(self, root_node):\n        self.root_node = root_node\n\n    def __repr__(self) -&gt; str:\n        return f\"Tree with root: {self.root_node}\"\n\n\ntree_node2 = TreeNode('b')\ntree_node3 = TreeNode('c')\ntree_node1 = TreeNode('a', tree_node2, tree_node3)\n\ntree = Tree(tree_node1)\n\n\ntree_node1\n\nTreeNode with value: a\n\n\n\ntree_node1.left\n\nTreeNode with value: b\n\n\n\ntree_node1.right\n\nTreeNode with value: c\n\n\n\ntree\n\nTree with root: TreeNode with value: a\n\n\n\n\n\n\nDesignate a current node (start with the root) and inspect its value.\nIf the current node is our target, success! Stop here.\nIf the current node is smaller than our target, search the left subtree. If it’s larger, search the right subtree.\nRepeat until we find our target. If we reach the end of the subtree without finding the target, then the target is not in the tree.\n\n\n\n\nSearching a binary search tree is \\(O(log N)\\) in the best case (a perfectly balanced tree) since we narrow our search area by half on each step.\nThe worst case is a horribly imbalanced tree. Imagine a tree that only has left descendants. This is essentially a linked list, so searching through it means inspecting every element. Therefore the worst case complexity is \\(O(N)\\).\nSo searching a binary search tree is the same complexity as a binary search performed on an ordered array. Where trees differentiate themselves is on insertions and deletes.\n\n\n\n\nCompare our new_node to each value in the tree starting from the root. Like with search, follow the path to the left if new_node is lower or right if higher.\nTraverse until we find a node where the appropriate child (left or right) does not exist yet. Make new_node the child.\n\nThe order of insertion is important to maintain balance. Binary search trees work best when seeded with randomly sorted data, as this will typically end up quite well-balanced. Seeding the tree with sorted data leads to imbalanced trees.\nInsertion is \\(O(log N)\\) for balanced trees because we search for the correct place to insert and then perform an \\(O(1)\\) operation to insert it.\n\n\n\nDeleting a node is more complicated because it depends if the target node has 0, 1 or 2 children.\n\n\n\nNumber of Children\nAction\n\n\n\n\n0\nSimply delete the node\n\n\n1\nReplace the deleted node with its child\n\n\n2\nReplace the deleted node with the successor node\n\n\n\nThe successor node is the next largest descendant of the deleted node. More formally: the child node whose value is the least of all values that are greater than the deleted node.\nFinding the successor:\n\nVisit the right child of the deleted node.\nKeep visiting left children until there are no more elft children. This is the successor node.\nIf the successor node has a right child, that child takes the original place of the successor node. The successor node gets moved to replace the deleted node.\n\nDeletion is \\(O(log N)\\) for balanced trees because we search for the correct place to insert and then perform (potentially several) \\(O(1)\\) operations to delete the node and replace it with a child / successor.\n\n\n\nThe point of a binary search tree is to maintain order. We can traverse the elements in order with the following recursive algorithm.\n\nCall itself recursively on the node’s left child. This will repeat until we hit a node without a left child.\nVisit this node.\nCall itself recursviely on the node’s right child.\n\nSince we are visiting every element of the tree, this is necessarily \\(O(N)\\).\n\ndef traverse(tree_node):\n    \"\"\"Inorder traversal of a binary search tree.\"\"\"\n    if tree_node is None:\n        return\n    traverse(tree_node.left)\n    print(tree_node.value)\n    traverse(tree_node.right)"
  },
  {
    "objectID": "posts/software/data_structures_algos/data_structures/data_structures.html#heaps",
    "href": "posts/software/data_structures_algos/data_structures/data_structures.html#heaps",
    "title": "Data Structures",
    "section": "",
    "text": "A queue is a FIFO list that means data is inserted at the end but accessed/removed from the front.\nA priority queue is a list where deletions and access are the same as a regular queue, but insertions are like an ordered array. So the priority queue is always ordered.\nAn example use case is a triage system in a hospital: patients are seen based on severity, not just when they arrived.\nThis is an abstract data types that we could implement in multiple ways using different fundamental data types. E.g. we could implement using an ordered array, but insertions would be O(N).\nHeaps are another data structure that fit this use case well.\n\n\n\nThere a multiple kinds of heaps. As a starting point, we consider the binary max-heap, which is a special kind of binary tree.\nBinary max-heap conditions:\n\nHeap condition: Each node is greater than its children.\nThe tree must be complete.\n\nBecause the heap condition is true for each node, we can recursively reason that a node is greater than its children, and they are too, then a node is greater than all of its descendants.\nThe complete condition essentially means values are filled left-to-right, top-down with no holes. There are no empty positions anywhere except the bottom row, and the bottom row is filled from left to right. The last node is the rightmost node at its bottom level.\nThere is also a min-heap variant. The difference is trivial, aside from the heap condition inequality being reversed, the logic is the same.\n\n\n\n\nHeaps are weakly ordered.\nThe root node is the maximum value.\n\nWith a binary search tree, we know precisely whether a value is the left or right descendant of a node. E.g. 3 will be a left child of 100.\nBut in a max-heap, we don’t know whether 3 is to the left or right, only that it is a descendant rather than an ancestor.\nSearching would require inspecting every node, \\(O(N)\\). In practice, searching a heap is not typically done in the use cases it is used for.\nReading a heap typically refers to accessing the root node, which is \\(O(1)\\).\n\n\n\n\nLast node: Insert the new node as the heap’s last node\nTrickle up the new node: Compare the new node to its parent. If it’s greater than its parent, swap them.\n\nThe number of steps to trickle is proportional to the depth of the tree. Therefore, insertion is a \\(O(log N)\\) operation.\n\n\n\nWe only ever delete the root node from a heap.\n\nNew root: The last node becomes the new root node.\nTrickle down the root node: Trickle the root down to its proper place.\n\nTrickling down is more complicated than trickling up because there are two possible directions to swap in. We compare to both children and swap with the larger of the two. This ensures the largest value ends up as the root node, which is the key property we want to preserve.\nDeletion is also \\(O(log N)\\) since the number of steps to trickle is again proprtional to the depth of the tree.\n\n\n\nAn alternative way of implementing a priority queue would be using an ordered array rather than a heap.\n\n\n\nOperation\nHeap Complexity\nArray Complexity\n\n\n\n\nRead*\n\\(O(1)\\)\n\\(O(1)\\)\n\n\nSearch^\n\\(O(N)\\)\n\\(O(N)\\)\n\n\nInsertion\n\\(O(log N)\\)\n\\(O(N)\\)\n\n\nDeletion\n\\(O(log N)\\)\n\\(O(1)\\)\n\n\n\n* Reading the root node\n^ Searching is not typically done on a heap\nComparing the complexities, heaps are fast, \\(O(log N)\\), for both insertions and deletions, wheres ordered arrays are faster for deletions but slower for insertions.\nAs this is in log space and priority queues typically perform similar numbers of inserts and deletes, on average this makes heap much faster.\n\n\n\nMany operations with heaps, such as insertion, rely on knowing where the last node is.\nThis is important because inserting/deleting the last node ensures the heap is always complete, and completeness is important to ensure the graph remains well-balanced.\nBeing well-balanced ensures the heap remains efficient. As in the case of binary search tree, if a tree is severely unbalanced it effectively becomes a linked list, so actions that should search through the depth of the tree in \\(O(log N)\\) time take \\(O(N)\\) in the worst case.\n\n\nHeaps are often implemented as arrays because the problem of the last node is so crucial, and accessing the last element of an array is \\(O(1)\\).\nThe tree below shows the values and their index in the corresponding array as value|index:\n\n\n\n\n\nflowchart TD\n\n    A[\"100|0\"] ---&gt; B[\"88|1\"]\n    A[\"100|0\"] ---&gt; C[\"25|2\"]\n\n    B[\"88|1\"] ---&gt; B1[\"87|3\"]\n    B[\"88|1\"] ---&gt; B2[\"16|4\"]\n\n    C[\"25|2\"] ---&gt; C1[\"8|5\"]\n    C[\"25|2\"] ---&gt; C2[\"12|6\"]\n\n\n\n\n\n\nThe array representation is then:\n\n\n\n0\n1\n2\n3\n4\n5\n6\n\n\n\n\n100\n88\n25\n87\n16\n8\n12\n\n\n\nThe indices are then deterministic. We can find the child index of any given node as:\nleft_child_idx = (parent_index * 2) + 1\nright_child_idx = (parent_index * 2) + 2\nTo find a given node’s parent index:\nparent_idx = (child_idx - 1) // 2\n\n\n\nIt is also possible to implement heaps as linked nodes.\nIn this case, a different trick is required to solve the problem of the last node using binary numbers.\n\nAssign Binary Numbers: Each level of the heap is assigned a unique binary number. The root is 0, then each left child is concatenates the parent’s number with 0 at the end, and each right child concatenates the parent’s index with 1 at the end.\nInsertion using Binary Representation: To insert a new node into the heap, you convert the index of the node into binary form. Starting from the most significant bit (MSB), traverse the heap according to the binary digits:\n\nIf the bit is 0, move to the left child.\nIf the bit is 1, move to the right child.\n\nInsertion at the Last Available Position: As you traverse the heap, you’ll eventually reach a node that doesn’t exist yet. This node represents the last available position in the heap, where the new node can be inserted while maintaining the complete binary tree property.\n\n\n\n\n\n\nflowchart TD\n\nA[\"100|0\"] ---&gt; B[\"88|00\"]\nA[\"100|0\"] ---&gt; C[\"25|01\"]\n\nB[\"88|00\"] ---&gt; B1[\"87|000\"]\nB[\"88|00\"] ---&gt; B2[\"16|001\"]\n\nC[\"25|01\"] ---&gt; C1[\"8|010\"]\nC[\"25|01\"] ---&gt; C2[\"12|011\"]"
  },
  {
    "objectID": "posts/software/data_structures_algos/data_structures/data_structures.html#tries",
    "href": "posts/software/data_structures_algos/data_structures/data_structures.html#tries",
    "title": "Data Structures",
    "section": "",
    "text": "This is a kind of tree which is particularly useful for text-based features.\nEach trie node can have any number of children. Each word is split into characters which are stored as a series of nested child nodes. A terminal character is used to denote the end of a word.\n\n\n\n\n\nflowchart TD\n\nG(G) ---&gt; U(U) ---&gt; R(R) ---&gt; P(P) ---&gt; X(*)\n\n\n\n\n\n\n\n\nThere are two flavours of search: (1) checking if a substring is a valid prefix, and (2) checking if a substring is a valid whole word.\nThe algorithm below is for the more general case of searching for a prefix. Searching for a whole word then becomes trivial because we have a terminal character, so we can search for “Implement*” to find the whole word.\nIterate through the trie one character at a time:\n\nInitialise current_node at the root.\nIterate through each character of search_string and see if current_node has a child with that key.\nRepeat Step 3 for the whole search_string. If the character is not a valid key, the word does not exist.\n\nThe efficiency of a trie search depends on the length of the search term, \\(K\\), not the number of elements stored in the trie, \\(N\\). Therefore, it has complexity \\(O(K)\\).\n\n\n\nInserting a new word into a trie follows similar steps to searching for an existing word: we traverse the trie and insert new nodes where they do not already exist.\n\nInitialise current_node at the root.\nIterate through each character of search_string and see if current_node has a child with that key.\n\nIf the key exists, update current_node to move to that next node.\nOtherwise, create a new child node and update current_node to move to it.\n\nInsert the terminal character of the word at the end.\n\nThis is again \\(O(K)\\) since it depends on the length of the input, not the data stored in the trie.\n\n\n\nWe can implement a trie as dictionaries nested within dictionaries.\n\nclass TrieNode:\n    \n    def __init__(self):\n        self.children = {}\n        \n        \nclass Trie:\n    \n    def __init__(self):\n        self.root = TrieNode()\n        self._TERMINAL_CHAR = \"*\"\n        \n    def search(self, word):\n        \"\"\"Search for a given word in the Trie.\"\"\"\n        current_node = self.root\n        \n        for char in word:\n            if char in current_node.keys():\n                # Keep traversing as long as we find valid children\n                current_node = current_node[char]\n            else: \n                # The search_string is not a valid prefix, so return None\n                return None\n        \n        return current_node\n    \n    def insert(self, new_word):\n        \"\"\"Insert a new word into the Trie.\"\"\"\n        current_node = self.root\n\n        # Traverse the word + terminal character\n        for char in new_word + self._TERMINAL_CHAR:\n            if char in current_node.keys():\n                # Keep traversing as long as we find valid children\n                current_node = current_node[char]\n            else:\n                current_node[char] = {}\n                current_node = current_node[char]\n\n\n\n\nA good use case of tries is for autocomplete.\nWe use a trie to store our dictionary of possible words.\nThen for a given user input, we can recursively list all of the words with that prefix.\nWe can improve autocomplete further by storing an integer popularity value as the terminal value rather than an empty dictionary or null value. This can be a 1-10 score of how commonly used that word is, so that we can prioritise showing more common words as autocompletion suggestions."
  },
  {
    "objectID": "posts/software/ddia/chapter1/lesson.html#describe-load",
    "href": "posts/software/ddia/chapter1/lesson.html#describe-load",
    "title": "Designing Data Intensive Applications: Part 1",
    "section": "3.1. Describe Load",
    "text": "3.1. Describe Load\nWe first need to describe the load. This means summarising the system’s load with load parameters which depend on the system itself. This may be requests per second for a web server, read/write ratio for a database, number of concurrent users, cache hit rate etc.\n\nTwitter Example\nScaling can be made difficult by fan out. For exmaple, Twitter’s scaling challenge is not primarily due to tweet volume, but due to fan-out, whereby each user follows many people, and each user is followed by many people. When a tweet is published it needs to be updated on the tweeter’s timeline and update on each of their follower’s home screen.\nApproach (1) would be to store all data in a relational database and join the follows, tweets and users table on each tweet and timeline update.\nApproach (2) is to keep a cache for each user’s timeline, and insert new tweets into the relevant users’ caches when someone they follow tweets. This generally works well because there are far more reads than writes - more lurkers than tweeters.\nTwitter started with approach (1) then moved to (2). But for celebrities who have millions of followers, the fan out of approach (2) is prohibitively slow, so they use a hybrid of both where users with lots of followers use approach 1."
  },
  {
    "objectID": "posts/software/ddia/chapter1/lesson.html#describe-performance",
    "href": "posts/software/ddia/chapter1/lesson.html#describe-performance",
    "title": "Designing Data Intensive Applications: Part 1",
    "section": "3.2. Describe Performance",
    "text": "3.2. Describe Performance"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html",
    "href": "posts/software/system_design/system_design_notes.html",
    "title": "System Design Notes",
    "section": "",
    "text": "Steps:\n\nRequirements engineering\nCapacity estimation\nData modeling\nAPI design\nSystem design\nDesign discussion\n\n\n\nFunctional and non-functional requirements. Scale of the system.\n\n\nWhat the system should do.\n\nWhich problem does the system solve\nWhich features are essential to solve these problems\n\nCore features: bare minimum required to solve the user’s problem. Support features: features that help the user solve their problem more conveniently.\n\n\n\nWhat the system should handle.\n\n\n\n\n\n\nInterview Tips\n\n\n\n\nIdentify the non-functional requirements the interviewer cares most about\nShow awareness of system attributes, trade-offs and user experience\n\n\n\nSystem analysis:\n\nRead or write heavy?\n\nImpacts requirements for database scaling, redundancy of services, effectiveness of caching.\nSystem priorities - is it worse if the system fails to read or to write?\n\nMonolithic or distributed architecture?\n\nScale is the key consideration. Large scale applications must be distributed, smaller scale can be single server.\n\nData availability vs consistency\n\nCAP trilemma - A system can have only two of: Consistency, Availability, Partition tolerance\nChoice determines the impact of a network failure\n\n\nNon-functional requirements:\n\nAvailability - How long is the system up and running per year?\n\nAvailability of a system is the product of its components’ availability\nReliability, redundancy, fault tolerance\n\nConsistency - Data appears the same on all nodes regardless of the user and location.\n\nLinearizability\n\nScalability - Ability of a system to handle fast-growing user base and temporary load spikes\n\nVertical scaling - single server that we add more CPU/RAM to\n\nPros: fast inter-process communication, data consistency\nCons: single point of failure, hardware limits on maximum scale\n\nHorizontal scaling - cluster of servers in parallel\n\nPros: redundancy improves availability, linear cost of scaling\nCons: complex architecture, data inconsistency\n\n\nLatency - Amount of time taken for a single message to be delivered\n\nExperienced latency = network latency + system latency\nDatabase and data model choices affect latency\nCaching can be effective\n\nCompatibility - Ability of a system to operate seamlessly with other software, hardware and systems\nSecurity\n\nBasic security measures: TLS encrypted network traffic, API keys for rate limiting\n\n\nNon-functional requirements depend heavily on expected scale. The following help quantify expected scale, and relate to capacity estimation in the next step.\n\nDaily active users (DAU)\nPeak active users\nInteractions per user - including read/write ratio\nRequest size\nReplication factor - determines the storage requirement (typically 3x)\n\n\n\n\n\n\n\n\n\n\n\nInterview Tips\n\n\n\n\nSimplify wherever you can - simple assumptions and round heavily\nConvert all numbers to scientific notation\nKnow powers of 2\n\n\n\n\n\nRequests per second (RPS) is the key measure.\nRequests per day = Daily active users * Activities per user\nRequests per second = RPD / 10^5\nPeak load = RPS * Peak load factor\nRead/write ratio is important to get the total requests. Usually, information on either reads or writes is missing and must be inferred using the other.\nTotal RPS = Read RPS + Write RPS\n\n\n\nThe amount of data that can be transmitted between two nodes in a fixed period of time. Bits per second.\nThroughput, bandwidth and latency are all related and can be thought of with the motorway analogy.\n\nThroughput is the total number of vehicles that must pass through that road per day.\nBandwidth is the number of lanes.\nLatency is the time taken to get from one end of the road to another.\n\nThe bandwidth must be large enough to support the required throughput.\nBandwidth = Total RPS * Request size\nEven high bandwidth can result in low latency if there is an unexpectedly high peak throughput. Rush hour traffic jam.\nRequest size can be varied according to bandwidth, e.g. YouTube lowers the resolution if the connection is slow.\n\n\n\nMeasured in bits per second using the Write RPS. Long term storage (assuming the same user base) is also helpful.\nStorage capacity per second = Write RPS * Request size * Replication factor\nStorage capacity in 5 years = Storage capacity per second * 2*10^8 \n\n\n\n\nKey concepts are: entities, attributes, relations.\nOptimisation discussions are anchored by the data model.\nRelational databases: denormalization, SQL tuning, sharding, federation\nNon-relational databases: indexing, caching\nThe data model is not a full-blown data schema (which only applies to relational databases) but a more informal, high-level version.\nMust haves:\n\nDerive entities and attributes.\nDraw connections between entities. Then:\nSelect a database based on the requirements\nExtend data model to a proper data schema\nIdentify bottlenecks and apply quick fixes Later:\nDefer any detailed discussions to the design discussion section.\n\n\n\nEntities are “things” with a distinct and independent existence, e.g. users, files, tweets, posts, channels. Create a unique table for each entity.\nAttributes describe properties of an entity, e.g. a user’s name, date of birth. These are the fields in the table of that entity.\n\n\n\n\n\n\nInterview Tips\n\n\n\n\nDo not focus on irrelevant details\nDo not miss any critical functionality\nDouble check all tables after you extract all info from requirements\n\n\n\n\n\n\nExtract connections between entities from the requirements. Relationships can be one-to-one, one-to-many or many-to-many.\n\n\n\n\nSpecify the API endpoints with:\n\nFunction signature\nParameters\nResponse and status codes\n\nThis specifies a binding contract between the client and the application server. There can be APIs between every system component, but just focus on the client/server interface.\nProcess:\n\nRevisit the functional requirements\nDerive the goal of each endpoint\nDerive the signature from the goal and the inputs from the data model\nDefine the response outputs\n\nNaming conventions. Use HTTP request types in the call signature where appropriate: GET, POST, PUT, PATCH, DELETE\n\n\n\n\n\n\nInterview Tips\n\n\n\n\nAvoid vague inputs\nDon’t add extra parameters that are out of scope\nAvoid redundant parameters\nDon’t miss any parameters\nConfirm the output repsonse satisfies the requirements\nIdentify inefficiencies of the data structure\n\n\n\n\n\n\nSystem components:\n\nRepresentation layer\n\nWeb app\nMobile app\nConsole\n\nService (and load balancer)\nData store\n\nRelational database\n\nPros: linked tables reduce duplicate data; flexible queries\n\nKey-value store\n\nUseful for lots of small continuous reads and writes\nPros: Fast access, lightweight, highly scalable\nCons: Cannot query by value, no standard query language so data access in written in the application layer no data normalisation\n\n\n\nScaling:\n\nScale services by having multiple instances of the service with a load balancer in front\nScale database with federation (functional partitioning) - split up data by function\n\nSystem diagrams: arrows point in direction of user flow (not data flow)\n\n\n\nTypes of questions:\n\nNFR questions\n\nHow to achieve scalability\n\nIdentify each bottleneck and remedy it\n\nHow to improve resiliency\n\nJustification question\n\nChoice of components, e.g. what would change if you changed block storage for object storage or relational database\nAdditional components, e.g. is there a use case for a message queue and where\n\nExtension questions\n\nAdditional functional requirements not in the original scope\n\n\n\n\n\n\n\nThere are two parameters we can vary:\n\nLength of the key - should be limited so the URL is short enough\nRange of allowed characters - should be URL-safe\n\nUsing Base-64 encoding as the character range, then a 6-digit key length gives enough unique URLs; 64^6 = 68 billion\nEncoding options: 1. MD5 hash: not collision resistant and too long 2. Encoded counter, each URL is just an index int, and its value is the base-64 encoding of that int: length is not fixed and increases over time 3. Key range, generate all unique keys beforehand: no collisions, but storage intensive 4. Key range modified, generate 5-digit keys beforehand, then add the 6th digit on the fly.\n\n\n\n\n\n4 main operations (CRUD): Create, Read, Update, Delete.\nTransactions - group several operations together\n\nEither the entire transaction succeeds or it fails and rolls back to the initial state. Commit or abort on error.\nWithout transactions, if an individual operation fails, it can be complex to unwind the related transactions to return to the initial state.\nError handling is simpler as manual rollback of operations is not required.\nTransactions provide guarantees so we can reason about the database state before and after the transaction.\n\nTransactions enforce “ACID” guarantees:\n\nAtomicity - Transactions cannot be broken down into smaller parts.\nConsistency - All data points within the database must align to be properly read and accepted. Raise a consistency error if this is not the case. Database consistency is different from system consistency which states the data should be the same across all nodes.\nIsolation - Concurrently executing transactions are isolated from each other. Avoids race conditions if multiple clients are accessing the same database record.\nDurability - Once a transaction is committed to the database, it is permanently preserved. Backups and transaction logs can restore committed transactions in the case of a failure.\n\nRelational databases are a good fit when:\n\nData is well-structured\nUse case requires complex querying\nData consistency is important\n\nLimitations of relational databases:\n\nHorizontal scaling is complex\nDistributed databases are more complex to keep transaction guarantees\n\nTwo phase commit protocol (2PC):\n\nPrepare - Ask each node if it;s able to promise to carry out the transaction\nCommit - Block the nodes and do the commit\n\nBlocking causes problems on distributed systems where it may cause unexpected consequences if the database is unavailable for this short time.\n\n\n\n\n\nExamples of non-relational databases:\n\nKey-value store\nDocument store\nWide-column store\nGraph store\n\nThese vary a lot, and in general NoSQL simply means “no ACID guarantees”.\nTransactions were seen as the enemy of scalability, so needed to be abandoned completely for performance and scalability. ACID enforces consistency for relational databases; for non-relational databases there is eventual consistency.\nBASE:\n\nBasically Available - Always possible to read and write data even though it may not be consistent. E.g. reads may not reflect latest changes, writes may not be persisted.\nSoft state - Lack of consistency guarantees mean data state may change without any interactions with the application as the database reaches eventual consistency.\nEventual consistency - Data will eventually become consistent once inputs stops.\n\nBenefits:\n\nWithout atomicity constraint, overheads like 2PC are not required\nWithout consistency constraint, horizontal scaling is trivial\nWithout isolation constraint, no blocking is required which improves availability.\n\nNon-relational databases are a good fit when:\n\nLarge data volume that isn’t tabular\nHigh availability requirement\nLack of consistency across nodes is acceptable\n\nLimitations:\n\nConsistency is necessary for some use cases\nLack of standardisation\n\n\n\n\n\nProblem: if we have large files with small changes, we don’t want to have to re-upload and re-download the whole file every time something changes.\nSolution: identify the changes and only push/pull these. Similar to git.\nThe rsync algorithm makes it easier to compare changes by:\n\nSplit the file into blocks\nCalculate a checksum for each block (using MD5 hashing algorithm)\nTo compare files between the client and the server, we only need to send the hashes back and forth\nFor the mismatching hashes, transfer the corresponding blocks\n\n\n\n\nOne-way communication that broadcasts file updates one-to-many.\n\n\nThese are synchronous.\nPolling is an example of a pull API. We periodically query the server to see if there are any updates. If not, the server responds immediately saying there is no new data. This may result in delayed updates and can overwhelm the server with too many requests.\n\n\n\n\n\n\nFigure 1: Polling\n\n\n\n\n\n\nThese are asynchronous.\nLong-polling. The client connects to the server and makes a request. Instead of replying immediately, the server waits until there is an update and then sends the response. If there is no update for a long time, the connection times out and the client must reconnect. Server resources are tied up until the connection is closed, even if there is no update.\n\n\n\n\n\n\nFigure 2: LongPolling\n\n\n\nWebsockets. The client establishes a connection using an HTTP request and response. This establishes a TCP/IP connection for 2-way communication. This allows for real-time applications without long-polling.\n\n\n\n\n\n\nFigure 3: Web Sockets\n\n\n\nServer sent events. Event-based approach. EventSource API supported by all browsers. The connection is 1-directional; the client can only pass data to the server at the point of connection. After that, only the server can send data to the client. The server doesn’t know if the client loses connection.\n\n\n\n\n\n\nFigure 4: SSE\n\n\n\n\n\n\n\nA system component that implements asynchronous communication between services, decoupling them. An independent server which producers and consumers connect to. AKA message queues.\nMessage brokers persist the messages until there are consumers ready to receive them.\nMessage brokers are similar in principle to databases since they persist data. The differences are they: automatically delete messages after delivery, do not support queries, typically have a small working set, and notify clients about changes.\nRabbitMQ, Kafka and Redis are open source message brokers. Amazon, GCP and Azure have managed service implementations.\n\n\nA one-to-one relationship between sender and receiver. Each message is consumed by exactly one receiver.\nThe message broker serves as an abstraction layer. The producer only sends to the broker. The receiver only receives from the broker. The services do not need to know about one another. The broker also guarantees delivery, so the message does not get lost if the consumer is unavailable, it will retry.\n\n\n\n\n\n\nFigure 5: Point-to-Point\n\n\n\n\n\n\nA one-to-many relationship between sender and receiver. The publisher publishes to a certain topic. Any consumer who is interested can then subscribe to receive that message.\n\n\n\n\n\n\nFigure 6: Pub-Sub\n\n\n\n\n\n\n\n\n\nFiles are stored in folders, with metadata about creation time and modification time.\nBenefits:\n\nSimplicity - simple and well-known pattern\nCompatibility - works with most applications and OSes\nLimitations:\nPerformance degradation - as size increases, the resource demands increase.\nExpensive - although cheap in principle, they can get expensive when trying to work around the performance issues.\n\nUse cases: data protection, local archiving, data retrieval done by users.\n\n\n\nA system component that manages data as objects in a flat structure. An object contains the file and its metadata.\nBenefits:\n\nHorizontal scalability\n\nLimitations:\n\nObjects must be edited as a unit - degrades performance if only a small part of the file needs to be updated.\n\nUse cases: large amounts of unstructured data.\n\n\n\nA system component that breaks up and then stores data as fixed-size blocks, each with a unique identifier.\nBenefits:\n\nHighly structured - easy to index, search and retrieve blocks\nLow data transfer overheads\nSupports frequent data writes without performance degradation\nLow latency\n\nLimitations:\n\nNo metadata, so metadata must be handled manually\nExpensive\n\n\n\n\nA subclass of key-value stores, which hold computer-readable documents, like XML or JSON objects.\n\n\n\n\nIssues:\n\nMassive files: 4TB for 4 hours of 4K video\n\nHow can we reliably upload very large files?\n\nLots of different end-user clients/devices/connection speeds\n\nHow can we process and store a range of versions/resolutions?\n\n\nProcessing pipeline:\n\n\n\n\n\n\nFigure 7: Video Processing Pipeline\n\n\n\n\nFile chunker\n\nSame principle as the file-sharing problem.\nSplit the file into chunks, then use checksums to determine that files are present and correct.\nIf there is a network outage, we only need to send the remaining chunks.\n\nContent filter\n\nCheck if the video complies with the platform’s content policy wrt copyright, piracy, NSFW\n\nTranscoder\n\nData is decoded to an uncompressed format\nThis will fan out in the next step to create multiple optimised versions of the file\n\nQuality conversion\n\nConvert the uncompressed file into multiple different resolution options.\n\n\nArchitecture considerations:\n\nWait for the full file to upload before processing? Or process each chunk as it is uploaded?\nAn upload service persists chunks to a database. Object store is a good choice as in the file-sharing example.\nOnce a chunk is ready to be processed, it can be read and placed in the message queue for the processing pipeline.\nThe processing pipeline handles fixed-size chunks, so the hardware requirements are known ahead of time. A chunk can be processed as soon as a hardware unit becomes available.\n\n\n\n\n\n\n\nFigure 8: Video Upload Architecture\n\n\n\n\n\n\nThe challenges are similar to that of file-sharing, as this is essentially large files being transferred. Latency and processing power of the end-user device.\nTwo main transfer protocols:\n\nUDP\nTCP\n\n\n\nA connection-less protocol. Nodes do NOT establish a stable end-to-end connection before sending data. Instead, each packet has its destination address attached so the network can route it to the correct place.\nAdvantages:\n\nFast\nPackets are routed independently so if some are lost the others can still reach their destination\n\nDisadvantages:\n\nNo guarantee that data will arrive intact.\n\nUse cases:\n\nLow-latency applications like video conferencing or gaming.\nNot suitable for movie or audio streaming, as missing bits cannot be tolerated.\n\n\n\n\nA connection is established between 2 nodes. The nodes agree on certain parameters before any data is transferred. Parameters: IP addresses of source and destination, port numbers of source and destination.\nThree-way handshake establishes the connection:\n\nSender transfers their sequence number to the Receiver.\nReceiver acknowledges and sends its own sequence number.\nSender acknowledges.\n\nAdvantages:\n\nReliability; guarantees delivery and receives acknowledgements before further packets are sent.\nReceiver acknowledgements ensure the receiver is able to process/buffer the data in time before more packets are sent.\n\nDisadvantages:\n\nSlower due to error checking and resending lost packets.\nRequires three-way handshake to establish connection which is slower.\n\nUse cases:\n\nMedia streaming (HTTP live-streaming or MPEG-DASH)\n\nAdaptive bitrate streaming\n\nTCP adjusts transmission speed in response to network conditions\nWhen packet loss is detected, smaller chunks are sent at a lower transmission speed. The lower resolution file can be sent in this case.\n\n\n\n\n\nPerceived latency = Network latency + System latency\n\n\nCaching is the process of storing copies of frequently accessed data in a temporary storage location.\nCaches utilise the difference in read performance between memory and storage. Times to fetch 1MB from:\n\nHDD: 3 ms\nSSD: 0.2 ms\nRAM: 0.01 ms\n\nWhen a user requests data it first requests from the cache\n\nCache HIT: If the data is in the cache, return it\nCache MISS: If the data is not in the cache, pass the request to the database, update the cache and return the data\n\nA cache can grow stale over time. There are 2 common approaches to mitigate this:\n\nSet up a time-to-live (TTL) policy within the cache\n\n\nTrade off between short TTL (fresh data but worse performance) vs long TTL (data can be stale)\n\n\nImplement active cache invalidation mechanism\n\n\nComplicated to implement. Requires an “invalidation service” component to monitor and read from the database, then update the cache when necessary.\n\nApplication-level caching. Insert caching logic into the application’s source code to temporarily store data in memory.\nTwo approaches to application-level caching:\n\nQuery-based implementation\n\nHash the query as a key and store the value against the key in a key-value store.\nLimitations: hard to delete cached result with a complex query; if a cell changes then all cached query which might include it need updating.\n\nObject-based implementation\n\nThe result of a query is stored as an object\nBenefits: only one serialisation/deserialisation overhead when reading/writing; complexity of object doesn’t matter, serialized objects in cache can be used by multiple applications.\n\n\nPopular implementations: Redis, Memcache, Hazlecast\n\n\n\nA network of caching layer in different locations (Points of Presence, PoPs). Full data is stored on SSD and most frequently accessed data is stored in RAM.\nDon’t cache dynamic or time-sensitive data.\nInvalidating the cache. You can manually update the CDN, but there may be additional caches at the ISP- and browser-level which would still serve stale data.\n\n\n\n\n\n\nFigure 9: CDN Invalidation\n\n\n\nApproaches to invalidating cache:\n\nCaching headers - set a time when an object should be cleared, e.g. max-age=86400 caches for a day.\nCache busting - change the link to all files and assets, so the CDN treats them like new files.\n\n\n\n\n\nThese are specialised NoSQL databases with the following benefits:\n\nCan match even with typos or non-exact matches\nFull-text search means you can suggest autocomplete results and related queries as the user types\nIndexing allows faster search performance on big data.\n\nThe database is structures as a hashmap where the inverted index points at documents.\n\nDocument - Represents a specific entity, like a row in a relational database\nInverted index - Maps from content to the documents that include it. The inverted index splits each document into individual search terms and makes each point to the document itself.\nRanking algorithm - Produces a list of possibly relevant documents. Additional factors may impact this, like a user’s previous search history.\n\nThe database is a dictionary of {search_term_id: [document_ids, …]}\n\n\n\n\n\n\nFigure 10: Inverted Index\n\n\n\nPopular implementations: Lucene, Elastic search, Apache solr, Atas search (MongoDB), Redis search.\nBenefits of search engine databases:\n\nScalable - NoSQL so scale horizontally\nSchemaless\nWorks out of the box - just need ot decide upfront what attributes should be searchable.\n\nLimitations:\n\nNo ACID guarantees.\nNot efficient at reading/writing data, so requires another database to manage state.\n\nMaintaining consistency between the main database and the search engine database can be tricky.\n\nSome SQL databases have full-text search so may not require a dedicated search engine database.\n\n\n\n\n\n\n\nAn app that lets a user add and delete items from a todo list.\n\nRepresentation layer is a web app\nMicroservices handle the todo CRUD operations and the user details separately\n\nEach service is scaled horizontally, so a load balancer is placed in front of it\n\nRelational database stores data\n\nThere are two databases, one per service, which is an example of functional partitioning. This means todo service I/O does not interfere with user service I/O and vice versa.\n\n\n\n\n\n\n\n\nFigure 11: To-do App System Design\n\n\n\n\n\n\nTake an input URL and return a unique, shortened URL that redirects to the original.\nPlanned system architecture:\n\nPre-generate all 5 digit keys (1 billion entries rather than 64 billion for 6 digits)\nSystem retrieves 5-difit keys from database\nSystem appends 1 out of 64 characters\nThe system is extendable because if we run out of keys we can append 2 more characters rather than 1\n\n\n\nSystem analysis:\n\nSystem is read heavy - Once a short URL is created it will be read multiple times\nDistributed system as this has to scale\nAvailability &gt; Consistency - not so much of a concern if a link isn’t available to all users at the same time, but they must be unique and lead to their destination.\n\nRequirements engineering:-\nCore feature:\n\nA user can input a URL of arbitrary length and receive a unique short URL of fixed size.\nA user can navigate to a short link and be redirected to the original URL.\n\nSupport features:\n\nA user can see their link history of all created short URLs.\nLifecycle policy - links should expire after a default time span.\n\nNon-functional requirements:\n\nAvailability - the system should be available 99% of the time.\nScalability - the system should support billions of short URLs, and thousands of concurrent users.\nLatency - the system should return redirect from a short URL to the original in under 1 second.\n\nQuestions to capture scale:\n\nDaily active users, and how often do users interact per day\n\n100 million, 1 interaction per day\n\nPeak active users - are there events that lead to traffic spikes?\n\nNo spikes\n\nRead/write ratio\n\n10 to 1\n\nRequest size - how long are the originals URLs typically\n\n200 characters =&gt; 200 Bytes\n\nReplication factor\n\n1x - ignore replication\n\n\nCapacity estimation:-\n\nRequests per second\n\nReads per second = DAU * interactions per day / seconds in day = 10^8 * 1 / 10^5 = 1000 reads/s\nWrites per second = Reads per second / read write ratio = 1000 / 10 = 100 writes/s\nRequests per second = Reads per second + Writes per second = 1000 + 100 = 1100 requests/s\nNo peak loads to consider\n\nBandwidth\n\nBandwidth = Requests per second * Message size\nRead bandwidth = 1000 * 200 Bytes = 200kB/s\nWrite bandwidth = 100 * 200 Bytes = 20 kB/s\n\nStorage\n\nStorage per year = Write bandwidth * seconds per year * Replication factor = 20000 Bytes * (360024365) * 1 = 630 GB\n\n\nData model:-\nIdentify entities, attributes and relationships.\nEntities and attributes:\n\nLinks: Key (used to create short URL), Original URL, Expiry date\nUsers: UserID, Links\nKey ranges: Key range, In use (bool)\n\nRelationships:\n\nUsers own Links\nLinks belong to Key ranges\n\nData stores:\n\nUsers\n\nUser data is typically relational and we rarely want it all returned at once.\nConsistency is important as we want the user to have the same experience regardless of which server handles their log in, and don’t want userIds to clash.\nRelational database.\n\nLinks\n\nNon-functional requirements of low latency and high availability.\nData and relationships are not complex.\nKey-value store.\n\nKey ranges\n\nFavour data consistency as we don’t want to accidentally reuse the same key range.\nFiltering keys by status would help to find available key ranges.\nRelational database.\n\n\nAPI design:-\nEndpoints:\n\ncreateUrl(originalUrl: str) -&gt; shortUrl\n\nresponse code 200, {shortURL: str}\n\ngetLinkHistory(userId: str, sorting: {‘asc’, ‘desc’})\n\nresponse code 200, {links: [str], order: ‘asc’}\n\nredirectURL(shortUrl: str) -&gt; originalUrl\n\nresponse code 200, {originalUrl: str}\n\n\nSystem design:-\nUser flows for each of the required functional requirements.\n\n\n\n\n\n\nFigure 12: URL Shortener System Design\n\n\n\n\n\n\n\nRequirements engineering\nRequirements gathering:\n\nRead-heavy\nDistributed\nData consistency is more important than availability - files should be consistent for all users\n\nCore functional requirements:\n\nA user can upload a file to the server\nA user can download their files from the server\n\nSecondary functional requirements:\n\nA user can see a history of files uploaded and downloaded\nA user can see who has downloaded a specific file and when\n\nNon-functional requirements:\n\nConsistency - data should be consistent across users and devices\nResilience - customer data should never be lost\nMinimal latency\nCompatibility across different devices\nSecurity - multi-tenancy; customer data should be separate of one another\n\nQuestions to determine scale:\n\nDaily active users\nPeak active users - what events lead to a spike?\nInteractions per user - how many file syncs, how many files does a user store\nRequest size - how large are files?\nRead/write ratio\nReplication factor\n\nCapacity estimation\nThroughput\n\nPeak write RPS = 2 peak load ratio * 10^8 users * 2 files/day / 10^5 seconds = 4000 RPS\nPeak read RPS = 10 read/write ration * 4000 write RPS = 40000 RPS\nPeak total RPS = 44000 RPS\n\nBandwidth\n\nWrite bandwidth = 4000 Write RPS * 100 kB Request size = 400 MB/s\nRead bandwidth = 40000 Read RPS * 100 kB request size = 4 GB/s\nTotal bandwidth = Write bandwidth + Read bandwidth = 4.4 GB/s\n\nStorage\n\nStorage capacity = 5 * 10^8 Total users * 100 files per user * 1MB File Size * 3 Replication factor = 15*10^10 MB = 15000 TB\n\nData model\nUsers: Store Ids for the files that belong to them\nFiles: Metadata on the owner, file edit history, filename, size, chunks that comprise the file, version history\nAPI Design\nEndpoints:\n\ncompareHashes(fileId: str, chunkHashes: list)\n\nTakes a file ID and a list of chunk hashes and returns a list of the hashes that need resyncing .\nresponse code 200, {syncChunks: [Hash,…]}\n\nuploadChange(fileId: str, chunks: list)\n\nUpload the chunks to resync the file\nresponse code 200, {message: “Chunks uploaded successfully”}\n\nrequestUpdate(fileId: str, chunkHashes: list)\n\nUpdate the local version of the file to match that on the server.\nresponse code 200, {chunks: [Chunk,…]}\n\n\nSystem Design\n\n\n\n\n\n\nFigure 13: Dropbox System Design\n\n\n\nDesign Discussion\n\nWhat can we do to achieve a scalable design?\n\nIdentify bottlenecks and remedy each.\n\nWhat can we do to achieve resiliency?\n\n\n\n\n\n\nUdemy course\nExcalidraw session\nCapacity estimation cheat sheet\n“Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems” by Martin Kleppmann\nRsync algorithm\nSearch Engines Information Retrieval in Practice book\n\n\n\n\nFigure 1: Polling\nFigure 2: LongPolling\nFigure 3: Web Sockets\nFigure 4: SSE\nFigure 5: Point-to-Point\nFigure 6: Pub-Sub\nFigure 7: Video Processing Pipeline\nFigure 8: Video Upload Architecture\nFigure 9: CDN Invalidation\nFigure 10: Inverted Index\nFigure 11: To-do App System Design\nFigure 12: URL Shortener System Design\nFigure 13: Dropbox System Design"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#requirements-engineering",
    "href": "posts/software/system_design/system_design_notes.html#requirements-engineering",
    "title": "System Design Notes",
    "section": "",
    "text": "Functional and non-functional requirements. Scale of the system.\n\n\nWhat the system should do.\n\nWhich problem does the system solve\nWhich features are essential to solve these problems\n\nCore features: bare minimum required to solve the user’s problem. Support features: features that help the user solve their problem more conveniently.\n\n\n\nWhat the system should handle.\n\n\n\n\n\n\nInterview Tips\n\n\n\n\nIdentify the non-functional requirements the interviewer cares most about\nShow awareness of system attributes, trade-offs and user experience\n\n\n\nSystem analysis:\n\nRead or write heavy?\n\nImpacts requirements for database scaling, redundancy of services, effectiveness of caching.\nSystem priorities - is it worse if the system fails to read or to write?\n\nMonolithic or distributed architecture?\n\nScale is the key consideration. Large scale applications must be distributed, smaller scale can be single server.\n\nData availability vs consistency\n\nCAP trilemma - A system can have only two of: Consistency, Availability, Partition tolerance\nChoice determines the impact of a network failure\n\n\nNon-functional requirements:\n\nAvailability - How long is the system up and running per year?\n\nAvailability of a system is the product of its components’ availability\nReliability, redundancy, fault tolerance\n\nConsistency - Data appears the same on all nodes regardless of the user and location.\n\nLinearizability\n\nScalability - Ability of a system to handle fast-growing user base and temporary load spikes\n\nVertical scaling - single server that we add more CPU/RAM to\n\nPros: fast inter-process communication, data consistency\nCons: single point of failure, hardware limits on maximum scale\n\nHorizontal scaling - cluster of servers in parallel\n\nPros: redundancy improves availability, linear cost of scaling\nCons: complex architecture, data inconsistency\n\n\nLatency - Amount of time taken for a single message to be delivered\n\nExperienced latency = network latency + system latency\nDatabase and data model choices affect latency\nCaching can be effective\n\nCompatibility - Ability of a system to operate seamlessly with other software, hardware and systems\nSecurity\n\nBasic security measures: TLS encrypted network traffic, API keys for rate limiting\n\n\nNon-functional requirements depend heavily on expected scale. The following help quantify expected scale, and relate to capacity estimation in the next step.\n\nDaily active users (DAU)\nPeak active users\nInteractions per user - including read/write ratio\nRequest size\nReplication factor - determines the storage requirement (typically 3x)"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#capacity-estimation",
    "href": "posts/software/system_design/system_design_notes.html#capacity-estimation",
    "title": "System Design Notes",
    "section": "",
    "text": "Interview Tips\n\n\n\n\nSimplify wherever you can - simple assumptions and round heavily\nConvert all numbers to scientific notation\nKnow powers of 2\n\n\n\n\n\nRequests per second (RPS) is the key measure.\nRequests per day = Daily active users * Activities per user\nRequests per second = RPD / 10^5\nPeak load = RPS * Peak load factor\nRead/write ratio is important to get the total requests. Usually, information on either reads or writes is missing and must be inferred using the other.\nTotal RPS = Read RPS + Write RPS\n\n\n\nThe amount of data that can be transmitted between two nodes in a fixed period of time. Bits per second.\nThroughput, bandwidth and latency are all related and can be thought of with the motorway analogy.\n\nThroughput is the total number of vehicles that must pass through that road per day.\nBandwidth is the number of lanes.\nLatency is the time taken to get from one end of the road to another.\n\nThe bandwidth must be large enough to support the required throughput.\nBandwidth = Total RPS * Request size\nEven high bandwidth can result in low latency if there is an unexpectedly high peak throughput. Rush hour traffic jam.\nRequest size can be varied according to bandwidth, e.g. YouTube lowers the resolution if the connection is slow.\n\n\n\nMeasured in bits per second using the Write RPS. Long term storage (assuming the same user base) is also helpful.\nStorage capacity per second = Write RPS * Request size * Replication factor\nStorage capacity in 5 years = Storage capacity per second * 2*10^8"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#data-modeling",
    "href": "posts/software/system_design/system_design_notes.html#data-modeling",
    "title": "System Design Notes",
    "section": "",
    "text": "Key concepts are: entities, attributes, relations.\nOptimisation discussions are anchored by the data model.\nRelational databases: denormalization, SQL tuning, sharding, federation\nNon-relational databases: indexing, caching\nThe data model is not a full-blown data schema (which only applies to relational databases) but a more informal, high-level version.\nMust haves:\n\nDerive entities and attributes.\nDraw connections between entities. Then:\nSelect a database based on the requirements\nExtend data model to a proper data schema\nIdentify bottlenecks and apply quick fixes Later:\nDefer any detailed discussions to the design discussion section.\n\n\n\nEntities are “things” with a distinct and independent existence, e.g. users, files, tweets, posts, channels. Create a unique table for each entity.\nAttributes describe properties of an entity, e.g. a user’s name, date of birth. These are the fields in the table of that entity.\n\n\n\n\n\n\nInterview Tips\n\n\n\n\nDo not focus on irrelevant details\nDo not miss any critical functionality\nDouble check all tables after you extract all info from requirements\n\n\n\n\n\n\nExtract connections between entities from the requirements. Relationships can be one-to-one, one-to-many or many-to-many."
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#api-design",
    "href": "posts/software/system_design/system_design_notes.html#api-design",
    "title": "System Design Notes",
    "section": "",
    "text": "Specify the API endpoints with:\n\nFunction signature\nParameters\nResponse and status codes\n\nThis specifies a binding contract between the client and the application server. There can be APIs between every system component, but just focus on the client/server interface.\nProcess:\n\nRevisit the functional requirements\nDerive the goal of each endpoint\nDerive the signature from the goal and the inputs from the data model\nDefine the response outputs\n\nNaming conventions. Use HTTP request types in the call signature where appropriate: GET, POST, PUT, PATCH, DELETE\n\n\n\n\n\n\nInterview Tips\n\n\n\n\nAvoid vague inputs\nDon’t add extra parameters that are out of scope\nAvoid redundant parameters\nDon’t miss any parameters\nConfirm the output repsonse satisfies the requirements\nIdentify inefficiencies of the data structure"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#system-design-1",
    "href": "posts/software/system_design/system_design_notes.html#system-design-1",
    "title": "System Design Notes",
    "section": "",
    "text": "System components:\n\nRepresentation layer\n\nWeb app\nMobile app\nConsole\n\nService (and load balancer)\nData store\n\nRelational database\n\nPros: linked tables reduce duplicate data; flexible queries\n\nKey-value store\n\nUseful for lots of small continuous reads and writes\nPros: Fast access, lightweight, highly scalable\nCons: Cannot query by value, no standard query language so data access in written in the application layer no data normalisation\n\n\n\nScaling:\n\nScale services by having multiple instances of the service with a load balancer in front\nScale database with federation (functional partitioning) - split up data by function\n\nSystem diagrams: arrows point in direction of user flow (not data flow)"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#design-discussion",
    "href": "posts/software/system_design/system_design_notes.html#design-discussion",
    "title": "System Design Notes",
    "section": "",
    "text": "Types of questions:\n\nNFR questions\n\nHow to achieve scalability\n\nIdentify each bottleneck and remedy it\n\nHow to improve resiliency\n\nJustification question\n\nChoice of components, e.g. what would change if you changed block storage for object storage or relational database\nAdditional components, e.g. is there a use case for a message queue and where\n\nExtension questions\n\nAdditional functional requirements not in the original scope"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#system-components-deep-dive",
    "href": "posts/software/system_design/system_design_notes.html#system-components-deep-dive",
    "title": "System Design Notes",
    "section": "",
    "text": "There are two parameters we can vary:\n\nLength of the key - should be limited so the URL is short enough\nRange of allowed characters - should be URL-safe\n\nUsing Base-64 encoding as the character range, then a 6-digit key length gives enough unique URLs; 64^6 = 68 billion\nEncoding options: 1. MD5 hash: not collision resistant and too long 2. Encoded counter, each URL is just an index int, and its value is the base-64 encoding of that int: length is not fixed and increases over time 3. Key range, generate all unique keys beforehand: no collisions, but storage intensive 4. Key range modified, generate 5-digit keys beforehand, then add the 6th digit on the fly.\n\n\n\n\n\n4 main operations (CRUD): Create, Read, Update, Delete.\nTransactions - group several operations together\n\nEither the entire transaction succeeds or it fails and rolls back to the initial state. Commit or abort on error.\nWithout transactions, if an individual operation fails, it can be complex to unwind the related transactions to return to the initial state.\nError handling is simpler as manual rollback of operations is not required.\nTransactions provide guarantees so we can reason about the database state before and after the transaction.\n\nTransactions enforce “ACID” guarantees:\n\nAtomicity - Transactions cannot be broken down into smaller parts.\nConsistency - All data points within the database must align to be properly read and accepted. Raise a consistency error if this is not the case. Database consistency is different from system consistency which states the data should be the same across all nodes.\nIsolation - Concurrently executing transactions are isolated from each other. Avoids race conditions if multiple clients are accessing the same database record.\nDurability - Once a transaction is committed to the database, it is permanently preserved. Backups and transaction logs can restore committed transactions in the case of a failure.\n\nRelational databases are a good fit when:\n\nData is well-structured\nUse case requires complex querying\nData consistency is important\n\nLimitations of relational databases:\n\nHorizontal scaling is complex\nDistributed databases are more complex to keep transaction guarantees\n\nTwo phase commit protocol (2PC):\n\nPrepare - Ask each node if it;s able to promise to carry out the transaction\nCommit - Block the nodes and do the commit\n\nBlocking causes problems on distributed systems where it may cause unexpected consequences if the database is unavailable for this short time.\n\n\n\n\n\nExamples of non-relational databases:\n\nKey-value store\nDocument store\nWide-column store\nGraph store\n\nThese vary a lot, and in general NoSQL simply means “no ACID guarantees”.\nTransactions were seen as the enemy of scalability, so needed to be abandoned completely for performance and scalability. ACID enforces consistency for relational databases; for non-relational databases there is eventual consistency.\nBASE:\n\nBasically Available - Always possible to read and write data even though it may not be consistent. E.g. reads may not reflect latest changes, writes may not be persisted.\nSoft state - Lack of consistency guarantees mean data state may change without any interactions with the application as the database reaches eventual consistency.\nEventual consistency - Data will eventually become consistent once inputs stops.\n\nBenefits:\n\nWithout atomicity constraint, overheads like 2PC are not required\nWithout consistency constraint, horizontal scaling is trivial\nWithout isolation constraint, no blocking is required which improves availability.\n\nNon-relational databases are a good fit when:\n\nLarge data volume that isn’t tabular\nHigh availability requirement\nLack of consistency across nodes is acceptable\n\nLimitations:\n\nConsistency is necessary for some use cases\nLack of standardisation\n\n\n\n\n\nProblem: if we have large files with small changes, we don’t want to have to re-upload and re-download the whole file every time something changes.\nSolution: identify the changes and only push/pull these. Similar to git.\nThe rsync algorithm makes it easier to compare changes by:\n\nSplit the file into blocks\nCalculate a checksum for each block (using MD5 hashing algorithm)\nTo compare files between the client and the server, we only need to send the hashes back and forth\nFor the mismatching hashes, transfer the corresponding blocks\n\n\n\n\nOne-way communication that broadcasts file updates one-to-many.\n\n\nThese are synchronous.\nPolling is an example of a pull API. We periodically query the server to see if there are any updates. If not, the server responds immediately saying there is no new data. This may result in delayed updates and can overwhelm the server with too many requests.\n\n\n\n\n\n\nFigure 1: Polling\n\n\n\n\n\n\nThese are asynchronous.\nLong-polling. The client connects to the server and makes a request. Instead of replying immediately, the server waits until there is an update and then sends the response. If there is no update for a long time, the connection times out and the client must reconnect. Server resources are tied up until the connection is closed, even if there is no update.\n\n\n\n\n\n\nFigure 2: LongPolling\n\n\n\nWebsockets. The client establishes a connection using an HTTP request and response. This establishes a TCP/IP connection for 2-way communication. This allows for real-time applications without long-polling.\n\n\n\n\n\n\nFigure 3: Web Sockets\n\n\n\nServer sent events. Event-based approach. EventSource API supported by all browsers. The connection is 1-directional; the client can only pass data to the server at the point of connection. After that, only the server can send data to the client. The server doesn’t know if the client loses connection.\n\n\n\n\n\n\nFigure 4: SSE\n\n\n\n\n\n\n\nA system component that implements asynchronous communication between services, decoupling them. An independent server which producers and consumers connect to. AKA message queues.\nMessage brokers persist the messages until there are consumers ready to receive them.\nMessage brokers are similar in principle to databases since they persist data. The differences are they: automatically delete messages after delivery, do not support queries, typically have a small working set, and notify clients about changes.\nRabbitMQ, Kafka and Redis are open source message brokers. Amazon, GCP and Azure have managed service implementations.\n\n\nA one-to-one relationship between sender and receiver. Each message is consumed by exactly one receiver.\nThe message broker serves as an abstraction layer. The producer only sends to the broker. The receiver only receives from the broker. The services do not need to know about one another. The broker also guarantees delivery, so the message does not get lost if the consumer is unavailable, it will retry.\n\n\n\n\n\n\nFigure 5: Point-to-Point\n\n\n\n\n\n\nA one-to-many relationship between sender and receiver. The publisher publishes to a certain topic. Any consumer who is interested can then subscribe to receive that message.\n\n\n\n\n\n\nFigure 6: Pub-Sub\n\n\n\n\n\n\n\n\n\nFiles are stored in folders, with metadata about creation time and modification time.\nBenefits:\n\nSimplicity - simple and well-known pattern\nCompatibility - works with most applications and OSes\nLimitations:\nPerformance degradation - as size increases, the resource demands increase.\nExpensive - although cheap in principle, they can get expensive when trying to work around the performance issues.\n\nUse cases: data protection, local archiving, data retrieval done by users.\n\n\n\nA system component that manages data as objects in a flat structure. An object contains the file and its metadata.\nBenefits:\n\nHorizontal scalability\n\nLimitations:\n\nObjects must be edited as a unit - degrades performance if only a small part of the file needs to be updated.\n\nUse cases: large amounts of unstructured data.\n\n\n\nA system component that breaks up and then stores data as fixed-size blocks, each with a unique identifier.\nBenefits:\n\nHighly structured - easy to index, search and retrieve blocks\nLow data transfer overheads\nSupports frequent data writes without performance degradation\nLow latency\n\nLimitations:\n\nNo metadata, so metadata must be handled manually\nExpensive\n\n\n\n\nA subclass of key-value stores, which hold computer-readable documents, like XML or JSON objects.\n\n\n\n\nIssues:\n\nMassive files: 4TB for 4 hours of 4K video\n\nHow can we reliably upload very large files?\n\nLots of different end-user clients/devices/connection speeds\n\nHow can we process and store a range of versions/resolutions?\n\n\nProcessing pipeline:\n\n\n\n\n\n\nFigure 7: Video Processing Pipeline\n\n\n\n\nFile chunker\n\nSame principle as the file-sharing problem.\nSplit the file into chunks, then use checksums to determine that files are present and correct.\nIf there is a network outage, we only need to send the remaining chunks.\n\nContent filter\n\nCheck if the video complies with the platform’s content policy wrt copyright, piracy, NSFW\n\nTranscoder\n\nData is decoded to an uncompressed format\nThis will fan out in the next step to create multiple optimised versions of the file\n\nQuality conversion\n\nConvert the uncompressed file into multiple different resolution options.\n\n\nArchitecture considerations:\n\nWait for the full file to upload before processing? Or process each chunk as it is uploaded?\nAn upload service persists chunks to a database. Object store is a good choice as in the file-sharing example.\nOnce a chunk is ready to be processed, it can be read and placed in the message queue for the processing pipeline.\nThe processing pipeline handles fixed-size chunks, so the hardware requirements are known ahead of time. A chunk can be processed as soon as a hardware unit becomes available.\n\n\n\n\n\n\n\nFigure 8: Video Upload Architecture\n\n\n\n\n\n\nThe challenges are similar to that of file-sharing, as this is essentially large files being transferred. Latency and processing power of the end-user device.\nTwo main transfer protocols:\n\nUDP\nTCP\n\n\n\nA connection-less protocol. Nodes do NOT establish a stable end-to-end connection before sending data. Instead, each packet has its destination address attached so the network can route it to the correct place.\nAdvantages:\n\nFast\nPackets are routed independently so if some are lost the others can still reach their destination\n\nDisadvantages:\n\nNo guarantee that data will arrive intact.\n\nUse cases:\n\nLow-latency applications like video conferencing or gaming.\nNot suitable for movie or audio streaming, as missing bits cannot be tolerated.\n\n\n\n\nA connection is established between 2 nodes. The nodes agree on certain parameters before any data is transferred. Parameters: IP addresses of source and destination, port numbers of source and destination.\nThree-way handshake establishes the connection:\n\nSender transfers their sequence number to the Receiver.\nReceiver acknowledges and sends its own sequence number.\nSender acknowledges.\n\nAdvantages:\n\nReliability; guarantees delivery and receives acknowledgements before further packets are sent.\nReceiver acknowledgements ensure the receiver is able to process/buffer the data in time before more packets are sent.\n\nDisadvantages:\n\nSlower due to error checking and resending lost packets.\nRequires three-way handshake to establish connection which is slower.\n\nUse cases:\n\nMedia streaming (HTTP live-streaming or MPEG-DASH)\n\nAdaptive bitrate streaming\n\nTCP adjusts transmission speed in response to network conditions\nWhen packet loss is detected, smaller chunks are sent at a lower transmission speed. The lower resolution file can be sent in this case.\n\n\n\n\n\nPerceived latency = Network latency + System latency\n\n\nCaching is the process of storing copies of frequently accessed data in a temporary storage location.\nCaches utilise the difference in read performance between memory and storage. Times to fetch 1MB from:\n\nHDD: 3 ms\nSSD: 0.2 ms\nRAM: 0.01 ms\n\nWhen a user requests data it first requests from the cache\n\nCache HIT: If the data is in the cache, return it\nCache MISS: If the data is not in the cache, pass the request to the database, update the cache and return the data\n\nA cache can grow stale over time. There are 2 common approaches to mitigate this:\n\nSet up a time-to-live (TTL) policy within the cache\n\n\nTrade off between short TTL (fresh data but worse performance) vs long TTL (data can be stale)\n\n\nImplement active cache invalidation mechanism\n\n\nComplicated to implement. Requires an “invalidation service” component to monitor and read from the database, then update the cache when necessary.\n\nApplication-level caching. Insert caching logic into the application’s source code to temporarily store data in memory.\nTwo approaches to application-level caching:\n\nQuery-based implementation\n\nHash the query as a key and store the value against the key in a key-value store.\nLimitations: hard to delete cached result with a complex query; if a cell changes then all cached query which might include it need updating.\n\nObject-based implementation\n\nThe result of a query is stored as an object\nBenefits: only one serialisation/deserialisation overhead when reading/writing; complexity of object doesn’t matter, serialized objects in cache can be used by multiple applications.\n\n\nPopular implementations: Redis, Memcache, Hazlecast\n\n\n\nA network of caching layer in different locations (Points of Presence, PoPs). Full data is stored on SSD and most frequently accessed data is stored in RAM.\nDon’t cache dynamic or time-sensitive data.\nInvalidating the cache. You can manually update the CDN, but there may be additional caches at the ISP- and browser-level which would still serve stale data.\n\n\n\n\n\n\nFigure 9: CDN Invalidation\n\n\n\nApproaches to invalidating cache:\n\nCaching headers - set a time when an object should be cleared, e.g. max-age=86400 caches for a day.\nCache busting - change the link to all files and assets, so the CDN treats them like new files.\n\n\n\n\n\nThese are specialised NoSQL databases with the following benefits:\n\nCan match even with typos or non-exact matches\nFull-text search means you can suggest autocomplete results and related queries as the user types\nIndexing allows faster search performance on big data.\n\nThe database is structures as a hashmap where the inverted index points at documents.\n\nDocument - Represents a specific entity, like a row in a relational database\nInverted index - Maps from content to the documents that include it. The inverted index splits each document into individual search terms and makes each point to the document itself.\nRanking algorithm - Produces a list of possibly relevant documents. Additional factors may impact this, like a user’s previous search history.\n\nThe database is a dictionary of {search_term_id: [document_ids, …]}\n\n\n\n\n\n\nFigure 10: Inverted Index\n\n\n\nPopular implementations: Lucene, Elastic search, Apache solr, Atas search (MongoDB), Redis search.\nBenefits of search engine databases:\n\nScalable - NoSQL so scale horizontally\nSchemaless\nWorks out of the box - just need ot decide upfront what attributes should be searchable.\n\nLimitations:\n\nNo ACID guarantees.\nNot efficient at reading/writing data, so requires another database to manage state.\n\nMaintaining consistency between the main database and the search engine database can be tricky.\n\nSome SQL databases have full-text search so may not require a dedicated search engine database."
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#system-design-examples-and-discussion",
    "href": "posts/software/system_design/system_design_notes.html#system-design-examples-and-discussion",
    "title": "System Design Notes",
    "section": "",
    "text": "An app that lets a user add and delete items from a todo list.\n\nRepresentation layer is a web app\nMicroservices handle the todo CRUD operations and the user details separately\n\nEach service is scaled horizontally, so a load balancer is placed in front of it\n\nRelational database stores data\n\nThere are two databases, one per service, which is an example of functional partitioning. This means todo service I/O does not interfere with user service I/O and vice versa.\n\n\n\n\n\n\n\n\nFigure 11: To-do App System Design\n\n\n\n\n\n\nTake an input URL and return a unique, shortened URL that redirects to the original.\nPlanned system architecture:\n\nPre-generate all 5 digit keys (1 billion entries rather than 64 billion for 6 digits)\nSystem retrieves 5-difit keys from database\nSystem appends 1 out of 64 characters\nThe system is extendable because if we run out of keys we can append 2 more characters rather than 1\n\n\n\nSystem analysis:\n\nSystem is read heavy - Once a short URL is created it will be read multiple times\nDistributed system as this has to scale\nAvailability &gt; Consistency - not so much of a concern if a link isn’t available to all users at the same time, but they must be unique and lead to their destination.\n\nRequirements engineering:-\nCore feature:\n\nA user can input a URL of arbitrary length and receive a unique short URL of fixed size.\nA user can navigate to a short link and be redirected to the original URL.\n\nSupport features:\n\nA user can see their link history of all created short URLs.\nLifecycle policy - links should expire after a default time span.\n\nNon-functional requirements:\n\nAvailability - the system should be available 99% of the time.\nScalability - the system should support billions of short URLs, and thousands of concurrent users.\nLatency - the system should return redirect from a short URL to the original in under 1 second.\n\nQuestions to capture scale:\n\nDaily active users, and how often do users interact per day\n\n100 million, 1 interaction per day\n\nPeak active users - are there events that lead to traffic spikes?\n\nNo spikes\n\nRead/write ratio\n\n10 to 1\n\nRequest size - how long are the originals URLs typically\n\n200 characters =&gt; 200 Bytes\n\nReplication factor\n\n1x - ignore replication\n\n\nCapacity estimation:-\n\nRequests per second\n\nReads per second = DAU * interactions per day / seconds in day = 10^8 * 1 / 10^5 = 1000 reads/s\nWrites per second = Reads per second / read write ratio = 1000 / 10 = 100 writes/s\nRequests per second = Reads per second + Writes per second = 1000 + 100 = 1100 requests/s\nNo peak loads to consider\n\nBandwidth\n\nBandwidth = Requests per second * Message size\nRead bandwidth = 1000 * 200 Bytes = 200kB/s\nWrite bandwidth = 100 * 200 Bytes = 20 kB/s\n\nStorage\n\nStorage per year = Write bandwidth * seconds per year * Replication factor = 20000 Bytes * (360024365) * 1 = 630 GB\n\n\nData model:-\nIdentify entities, attributes and relationships.\nEntities and attributes:\n\nLinks: Key (used to create short URL), Original URL, Expiry date\nUsers: UserID, Links\nKey ranges: Key range, In use (bool)\n\nRelationships:\n\nUsers own Links\nLinks belong to Key ranges\n\nData stores:\n\nUsers\n\nUser data is typically relational and we rarely want it all returned at once.\nConsistency is important as we want the user to have the same experience regardless of which server handles their log in, and don’t want userIds to clash.\nRelational database.\n\nLinks\n\nNon-functional requirements of low latency and high availability.\nData and relationships are not complex.\nKey-value store.\n\nKey ranges\n\nFavour data consistency as we don’t want to accidentally reuse the same key range.\nFiltering keys by status would help to find available key ranges.\nRelational database.\n\n\nAPI design:-\nEndpoints:\n\ncreateUrl(originalUrl: str) -&gt; shortUrl\n\nresponse code 200, {shortURL: str}\n\ngetLinkHistory(userId: str, sorting: {‘asc’, ‘desc’})\n\nresponse code 200, {links: [str], order: ‘asc’}\n\nredirectURL(shortUrl: str) -&gt; originalUrl\n\nresponse code 200, {originalUrl: str}\n\n\nSystem design:-\nUser flows for each of the required functional requirements.\n\n\n\n\n\n\nFigure 12: URL Shortener System Design\n\n\n\n\n\n\n\nRequirements engineering\nRequirements gathering:\n\nRead-heavy\nDistributed\nData consistency is more important than availability - files should be consistent for all users\n\nCore functional requirements:\n\nA user can upload a file to the server\nA user can download their files from the server\n\nSecondary functional requirements:\n\nA user can see a history of files uploaded and downloaded\nA user can see who has downloaded a specific file and when\n\nNon-functional requirements:\n\nConsistency - data should be consistent across users and devices\nResilience - customer data should never be lost\nMinimal latency\nCompatibility across different devices\nSecurity - multi-tenancy; customer data should be separate of one another\n\nQuestions to determine scale:\n\nDaily active users\nPeak active users - what events lead to a spike?\nInteractions per user - how many file syncs, how many files does a user store\nRequest size - how large are files?\nRead/write ratio\nReplication factor\n\nCapacity estimation\nThroughput\n\nPeak write RPS = 2 peak load ratio * 10^8 users * 2 files/day / 10^5 seconds = 4000 RPS\nPeak read RPS = 10 read/write ration * 4000 write RPS = 40000 RPS\nPeak total RPS = 44000 RPS\n\nBandwidth\n\nWrite bandwidth = 4000 Write RPS * 100 kB Request size = 400 MB/s\nRead bandwidth = 40000 Read RPS * 100 kB request size = 4 GB/s\nTotal bandwidth = Write bandwidth + Read bandwidth = 4.4 GB/s\n\nStorage\n\nStorage capacity = 5 * 10^8 Total users * 100 files per user * 1MB File Size * 3 Replication factor = 15*10^10 MB = 15000 TB\n\nData model\nUsers: Store Ids for the files that belong to them\nFiles: Metadata on the owner, file edit history, filename, size, chunks that comprise the file, version history\nAPI Design\nEndpoints:\n\ncompareHashes(fileId: str, chunkHashes: list)\n\nTakes a file ID and a list of chunk hashes and returns a list of the hashes that need resyncing .\nresponse code 200, {syncChunks: [Hash,…]}\n\nuploadChange(fileId: str, chunks: list)\n\nUpload the chunks to resync the file\nresponse code 200, {message: “Chunks uploaded successfully”}\n\nrequestUpdate(fileId: str, chunkHashes: list)\n\nUpdate the local version of the file to match that on the server.\nresponse code 200, {chunks: [Chunk,…]}\n\n\nSystem Design\n\n\n\n\n\n\nFigure 13: Dropbox System Design\n\n\n\nDesign Discussion\n\nWhat can we do to achieve a scalable design?\n\nIdentify bottlenecks and remedy each.\n\nWhat can we do to achieve resiliency?"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#references",
    "href": "posts/software/system_design/system_design_notes.html#references",
    "title": "System Design Notes",
    "section": "",
    "text": "Udemy course\nExcalidraw session\nCapacity estimation cheat sheet\n“Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems” by Martin Kleppmann\nRsync algorithm\nSearch Engines Information Retrieval in Practice book\n\n\n\n\nFigure 1: Polling\nFigure 2: LongPolling\nFigure 3: Web Sockets\nFigure 4: SSE\nFigure 5: Point-to-Point\nFigure 6: Pub-Sub\nFigure 7: Video Processing Pipeline\nFigure 8: Video Upload Architecture\nFigure 9: CDN Invalidation\nFigure 10: Inverted Index\nFigure 11: To-do App System Design\nFigure 12: URL Shortener System Design\nFigure 13: Dropbox System Design"
  },
  {
    "objectID": "posts/software/docker/docker_notes.html",
    "href": "posts/software/docker/docker_notes.html",
    "title": "Docker",
    "section": "",
    "text": "Docker is a tool for creating and managing containers.\nA container is a standardised unit of software - a package of code and the dependencies required to run the code. The same container always yields the exact same application and execution behaviour.\nSupport for containers is built into all modern operating systems. Docker simplifies the creation and management of containers. Containers are a general idea, Docker is just the de facto standard tool for dealing with containers.\n\n\n\nWhy would we want independent, standardised, standalone applications packages?\n\nWe may have different dev vs prod environments. We want to build and test in the same way.\nMultiple developers keeping their development environments in sync with each other.\nClashing tools and versions between projects.\n\n\n\n\nWith a virtual machine (VM), we have:\n\nOur operating system\nA VM with virtual OS\nLibraries and dependencies\nOur own app\n\nThis does allow separated environments which environment-specific configurations which can be shared and reproduced. However, it involves a lot of duplication, can be slow with long boot times and reproducing on another server is possible but can be tricky.\nThere is a big overhead with VMs, because we recreate everything including the (virtual) OS every time we want a new VM. This wastes a lot of storage and tends to be slow.\nA container is a lightweight solution to the same problem. The Docker Engine interfaces with the OS, and the container sits on top of the Docker engine. This means each container only needs to contain its own code + data, it doesn’t need to reproduce the OS or any additional utlities.\n\nOS\nOS built-in container support\nDocker engine\nContainer - App, libraries\n\nThe benefits of a container compared to a VM:\n\nLow impact on OS, minimal storage usage\nSharing and rebuilding is easy\nEncapsulate apps and environments rather than whole machines\n\n\n\n\nThe specific steps depend on the OS, so check the docs.\nFor Linux, install Docker Engine. For Mac and Windows, install Docker Desktop (or Docker Toolbox if requirements not met).\nDocker playground is helpful to try things out in a browser with no installation required.\nDocker Hub (my second favourite *hub.com on the internet*) is a centralised repository that lets us share containers.\nDocker compose and Kubernetes help manage multi-container apps.\n* Number one is github, you dirty dog.\n\n\n\nThe Dockerfile file (no extension) contains the list of commands to create our image.\nTo build the container run this in a terminal in the same directory as the Dockerfile:\ndocker build .\nTo run the container:\ndocker run &lt;image_id&gt;\nOptionally, if we want to expose port 3000 inside the container to the outside world, run:\ndocker run -p 3000:3000 &lt;image_id&gt;\nSome other useful basic commands are listing running containers:\ndocker ps -a\nAnd stopping a container:\ndocker stop &lt;container_name&gt;"
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#what-is-docker",
    "href": "posts/software/docker/docker_notes.html#what-is-docker",
    "title": "Docker",
    "section": "",
    "text": "Docker is a tool for creating and managing containers.\nA container is a standardised unit of software - a package of code and the dependencies required to run the code. The same container always yields the exact same application and execution behaviour.\nSupport for containers is built into all modern operating systems. Docker simplifies the creation and management of containers. Containers are a general idea, Docker is just the de facto standard tool for dealing with containers."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#why-use-containers",
    "href": "posts/software/docker/docker_notes.html#why-use-containers",
    "title": "Docker",
    "section": "",
    "text": "Why would we want independent, standardised, standalone applications packages?\n\nWe may have different dev vs prod environments. We want to build and test in the same way.\nMultiple developers keeping their development environments in sync with each other.\nClashing tools and versions between projects."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#virtual-machines-vs-docker-containers",
    "href": "posts/software/docker/docker_notes.html#virtual-machines-vs-docker-containers",
    "title": "Docker",
    "section": "",
    "text": "With a virtual machine (VM), we have:\n\nOur operating system\nA VM with virtual OS\nLibraries and dependencies\nOur own app\n\nThis does allow separated environments which environment-specific configurations which can be shared and reproduced. However, it involves a lot of duplication, can be slow with long boot times and reproducing on another server is possible but can be tricky.\nThere is a big overhead with VMs, because we recreate everything including the (virtual) OS every time we want a new VM. This wastes a lot of storage and tends to be slow.\nA container is a lightweight solution to the same problem. The Docker Engine interfaces with the OS, and the container sits on top of the Docker engine. This means each container only needs to contain its own code + data, it doesn’t need to reproduce the OS or any additional utlities.\n\nOS\nOS built-in container support\nDocker engine\nContainer - App, libraries\n\nThe benefits of a container compared to a VM:\n\nLow impact on OS, minimal storage usage\nSharing and rebuilding is easy\nEncapsulate apps and environments rather than whole machines"
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#docker-setup",
    "href": "posts/software/docker/docker_notes.html#docker-setup",
    "title": "Docker",
    "section": "",
    "text": "The specific steps depend on the OS, so check the docs.\nFor Linux, install Docker Engine. For Mac and Windows, install Docker Desktop (or Docker Toolbox if requirements not met).\nDocker playground is helpful to try things out in a browser with no installation required.\nDocker Hub (my second favourite *hub.com on the internet*) is a centralised repository that lets us share containers.\nDocker compose and Kubernetes help manage multi-container apps.\n* Number one is github, you dirty dog."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#overview-of-a-container",
    "href": "posts/software/docker/docker_notes.html#overview-of-a-container",
    "title": "Docker",
    "section": "",
    "text": "The Dockerfile file (no extension) contains the list of commands to create our image.\nTo build the container run this in a terminal in the same directory as the Dockerfile:\ndocker build .\nTo run the container:\ndocker run &lt;image_id&gt;\nOptionally, if we want to expose port 3000 inside the container to the outside world, run:\ndocker run -p 3000:3000 &lt;image_id&gt;\nSome other useful basic commands are listing running containers:\ndocker ps -a\nAnd stopping a container:\ndocker stop &lt;container_name&gt;"
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#image-vs-container",
    "href": "posts/software/docker/docker_notes.html#image-vs-container",
    "title": "Docker",
    "section": "2.1. Image vs Container",
    "text": "2.1. Image vs Container\nA container is a running “unit of software”. An image is the blueprint for the container.\nSo we can define an image then create containers running in multiple places. The image contains the code and environment, the container is the running app."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#using-prebuilt-images",
    "href": "posts/software/docker/docker_notes.html#using-prebuilt-images",
    "title": "Docker",
    "section": "2.2. Using Prebuilt Images",
    "text": "2.2. Using Prebuilt Images\nWe can either use a prebuilt image or one that we’ve created ourselves.\nDocker Hub is useful for finding and sharing prebuilt images.\nTo run a prebuilt image from docker hub, node in this case, run this in a terminal:\ndocker run node\nThis will automatically pull the prebuilt node image if it hasn’t already been pulled, then run it.\nBy default, the container is isolated, but we can run it in interactive mode if we want to use the REPL.\ndocker run -it node\nWe can use exec to interact with the shell terminal of an already running container:\ndocker exec -it &lt;container_name&gt; sh"
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#building-our-own-image",
    "href": "posts/software/docker/docker_notes.html#building-our-own-image",
    "title": "Docker",
    "section": "2.3. Building Our Own Image",
    "text": "2.3. Building Our Own Image\nWe can build our own images; typically these will layer on top of other prebuilt images. The Dockerfile file configures the image.\nWe typically start with FROM to build on some base image, either on your local system or on DockerHub.\nFROM node\nWe then want to tell Docker which files on the local machine should go in the image. Often we want to COPY the contents of the current directory to the container file system, for example in a directory called app.\nCOPY . /app\nThe WORKDIR command tells docker that any subsequent commands we RUN should run from this directory. So instead of the above copy with absolute filepath, we could do:\nWORKDIR /app\nCOPY . .\nWe then want to RUN a command inside the image, for example, to install dependencies.\nRUN npm install\nThe command instruction, CMD, specifies code to execute when the container is started. Contrast this with image** is created. An example of the distinction is starting a server, which we do in the container. The syntax is a bit odd, instead of CMD node server.js we need to pass the command as an array of strings.\nCMD [“node”, “server.js”]\nIf you don’t specify a CMD, the CMD of the base image will be executed. It should be the last instruction in a Dockerfile.\nThe web server listens on a particular port, say port 80. The container is isolated, so we need to EXPOSE the port.\nEXPOSE 80\nThis is best practice to explicitly declare any used ports, but it is optional and does not actually make the port available. For this, we need to specify the port with the -p flag when creating the container with docker run. See the next section."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#running-a-container-based-on-our-image",
    "href": "posts/software/docker/docker_notes.html#running-a-container-based-on-our-image",
    "title": "Docker",
    "section": "2.4. Running a Container Based On Our Image",
    "text": "2.4. Running a Container Based On Our Image\nWe have a complete Dockerfile, next we need to build the image. In a terminal in the same directory as the Dockerfile, run:\ndocker build .\nThis outputs an image ID. We can then run this image. This blocks the terminal.\ndocker run &lt;image_id&gt;\nWe need the “publish” -p flag to make the port in the container accessible. Say we want to access the application through a local port 3000, and we are accessing the internal port 80 inside the container.\ndocker run -p 3000:80 &lt;image_id&gt;\nWe can then see our app if we visit localhost:3000 in a browser."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#images-are-read-only",
    "href": "posts/software/docker/docker_notes.html#images-are-read-only",
    "title": "Docker",
    "section": "2.5. Images Are Read-Only",
    "text": "2.5. Images Are Read-Only\nIf we are changing the code inside the image, we need to build the image again and then run a new container from the new image.\nThe contents of an image are set at the build step and cannot be altered afterwards."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#understanding-image-layers",
    "href": "posts/software/docker/docker_notes.html#understanding-image-layers",
    "title": "Docker",
    "section": "2.6. Understanding Image Layers",
    "text": "2.6. Understanding Image Layers\nImages are layer-based. This means that if there is a change to the image and the image is rebuilt, it will cache previous layers and if the layer is unchanged, it will use the cached layer. It will only recreate layers where there was a change (and all subsequent layers).\nThis means we can optimise our Dockerfile by copying code which changes frequently near the end, and code which is relatively static, like dependencies, should go earlier. So when we have a code change, we don’t need to reinstall everything."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#managing-images-and-containers",
    "href": "posts/software/docker/docker_notes.html#managing-images-and-containers",
    "title": "Docker",
    "section": "2.7. Managing Images and Containers",
    "text": "2.7. Managing Images and Containers\nThe following commands are useful when managing images and containers.\nImages:\n\nTag: -t\nList: docker images\nAnalyse: docker image inspect &lt;image_id&gt;\nRemove: docker rmi  &lt;image_id&gt;; docker prune\n\nContainers:\n\nName: -—name\nList: docker ps\nRemove: docker rm"
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#stopping-and-restarting-containers",
    "href": "posts/software/docker/docker_notes.html#stopping-and-restarting-containers",
    "title": "Docker",
    "section": "2.8. Stopping and Restarting Containers",
    "text": "2.8. Stopping and Restarting Containers\nStart a new container with docker run.\nIf nothing in our image has changed, we may just want to restart a previously run container:\ndocker start &lt;container_name&gt;"
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#attached-and-detached-containers",
    "href": "posts/software/docker/docker_notes.html#attached-and-detached-containers",
    "title": "Docker",
    "section": "2.9. Attached and Detached Containers",
    "text": "2.9. Attached and Detached Containers\nAn attached container is running and blocks the terminal. For example, when we use docker run. Attached simply means we are “listening” to the output of the container, which can be helpful if we want to look at the logs.\nA detached container is running but runs in the background without blocking the terminal. For example, when we run docker start.\nWe can configure whether to run in attached or detached mode. The -d flag runs in detached mode:\ndocker run -d &lt;image_id&gt;\nThe -a flag runs in attached mode:\ndocker start -a &lt;container_name&gt;\nWe can attach ourselves to a detached container using:\ndocker attach &lt;container_name&gt;\nTo see the log history, we can use\ndocker logs &lt;container_name&gt;\nWe can run this in “follow” mode with the -f flag to print the logs and keep listening (i.e. attach to the container)\ndocker logs -f &lt;container_name&gt;"
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#interactive-mode",
    "href": "posts/software/docker/docker_notes.html#interactive-mode",
    "title": "Docker",
    "section": "2.10 Interactive Mode",
    "text": "2.10 Interactive Mode\nIf we have an application that requires user input, or we just want to see what’s going on inside the container, we can run in interactive mode.\nWe do this with the interactive flag -i. This keeps STDIN open even if not attached. This is analogous to how the attach flag -a keeps STDOUT open.\nWe usually combine this with the TTY flag -t. This allocates a pseudo-TTY, which exposes a terminal we can use.\ndocker run -it &lt;image_id&gt;"
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#deleting-containers",
    "href": "posts/software/docker/docker_notes.html#deleting-containers",
    "title": "Docker",
    "section": "2.11. Deleting Containers",
    "text": "2.11. Deleting Containers\nWe can list all containers with:\ndocker ps\nWe can remove containers with:\ndocker rm &lt;container_name&gt;\nWe can pass multiple containers to remove separated by a space.\nWe can’t remove a running container, so we need to stop it first:\ndocker stop &lt;container_name&gt;\nWe can remove ALL stopped containers with\ndocker container prune"
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#deleting-images",
    "href": "posts/software/docker/docker_notes.html#deleting-images",
    "title": "Docker",
    "section": "2.12 Deleting Images",
    "text": "2.12 Deleting Images\nAnalogously to containers, we can list images with:\ndocker images \nWe remove images with:\ndocker rmi &lt;image_id&gt;\nWe can pass multiple images to remove separated by a space.\nWe can’t remove images which are being used by containers, even if that container is stopped. The container needs to be removed first before the image can be removed.\nWe can remove all unused images with:\ndocker image prune\nThis will remove all untagged images. To also remove tagged images we need the -a flag\ndocker image prune -a\nWe can pass the --rm flag to the docker run command to automatically remove the container once stopped."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#inspecting-images",
    "href": "posts/software/docker/docker_notes.html#inspecting-images",
    "title": "Docker",
    "section": "2.12 Inspecting Images",
    "text": "2.12 Inspecting Images\nWe can understand more about an image by inspecting it:\ndocker image inspect &lt;image_id&gt;\nThis tells us:\n\nID\nCreation date\nConfig such as ports, entry point, interactive, etc\nDocker version\nOS\nLayers"
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#copying-files-into-and-from-a-running-container",
    "href": "posts/software/docker/docker_notes.html#copying-files-into-and-from-a-running-container",
    "title": "Docker",
    "section": "2.13 Copying Files Into and From a Running Container",
    "text": "2.13 Copying Files Into and From a Running Container\nWe can copy files in and out of a running container with the cp command.\nTo copy a file into a container:\ndocker cp &lt;source file or folder&gt; &lt;container_name&gt;:/&lt;path in container&gt;\nTo copy out of a container, just swap the order of the arguments:\ndocker cp &lt;container_name&gt;:/&lt;path in container&gt; &lt;source file or folder&gt;\nThis allows us to add config or data to a running container, or copy results or logs out of the container."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#names-and-tags",
    "href": "posts/software/docker/docker_notes.html#names-and-tags",
    "title": "Docker",
    "section": "2.14. Names and Tags",
    "text": "2.14. Names and Tags\nWe can add --name and --tag in the docker run command to specify our own name and tag for a container.\nFor images, when we execute docker build we can specify a tag with --tag or -t. Image tags consist of name:tag e.g. python:3.11 or node:latest."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#sharing-images",
    "href": "posts/software/docker/docker_notes.html#sharing-images",
    "title": "Docker",
    "section": "2.15. Sharing Images",
    "text": "2.15. Sharing Images\n\n2.15.1. Registries\nEveryone who has an image can create a container from it; we are sharing images not containers.\nWe could share the source code and Dockerfile to allow somebody to build the image themselves.\nAlternatively, we can just share the built image. No build is required and we don’t need to share the full source code.\nWe can share in:\n\nDocker Hub\nAny private registry\n\n\n\n2.15.2. Push\nWe can push to and pull from the registry with the following commands. By default these refer to Docker Hub.\ndocker push &lt;image_name&gt;\ndocker pull &lt;image_name&gt;\nWe pass host:name rather than just an image name if we want to push or pull from a private registry.\nSteps:\n\nCreate the image in Docker Hub.\nCreate an image on our local machine with the same image name.\nPush with the docker push command above.\n\nWe can re-tag an existing image with:\ndocker tag &lt;old_image_name&gt; &lt;new_image_name&gt;\nThis clones the image; the old name still exists.\nWe need to login to be able to push to a docker hub repo:\ndocker login \nWe can also logout with:\ndocker logout\n\n\n2.15.3. Pull\ndocker pull &lt;image_name&gt;\nThis will pull the latest version by default. You can specify a tag to override this.\nWe can execute run and it will automatically download the image if you don’t already have it locally. If you do have it locally, you might have an out of date version and docker run won’t tell you this.\ndocker run &lt;image_name&gt;"
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#summary",
    "href": "posts/software/docker/docker_notes.html#summary",
    "title": "Docker",
    "section": "2.16. Summary",
    "text": "2.16. Summary\n\nImages (blueprints) vs Containers (the running instance of the app).\nImages can be pulled or built.\nThe Dockerfile configures the image when building our own images.\nImages are built in layers.\nWe can run containers from an image, and configure some helpful flags when running.\nWe can manage containers: list, remove, stop, start\nWe can manage images: list, remove, push, pull"
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#different-categories-of-data",
    "href": "posts/software/docker/docker_notes.html#different-categories-of-data",
    "title": "Docker",
    "section": "3.1. Different Categories of Data",
    "text": "3.1. Different Categories of Data\n\nApplication. Code and environment. Added to the image in the build phase. This is read-only and stored in images.\nTemporary application data. Fetched or produced by the container, e.g. data input by a user of a web server. Stored in memory or temporary files. Read and write, but only temporary, so stored in containers.\nPermanent application data. Fetched or produced in the running container, e.g. user account data. These are stored in files or a database which we want to persist even if the container stops or is removed. Read and write permanent data, so store in volumes.\n\nThe container has its own file system, which is lost when the container is removed (not when the container is stopped). This means the temporary data is lost when the container is removed.\nThis is a problem as we need to rebuild the image and start a new container whenever we make a code change.\nVolumes solve this problem."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#volumes",
    "href": "posts/software/docker/docker_notes.html#volumes",
    "title": "Docker",
    "section": "3.2. Volumes",
    "text": "3.2. Volumes\nVolumes are folders on the host machines which are mounted (made available) into containers. A container can read and write to a volume.\nThe COPY command in a Dockerfile is just a one-time snapshot. A volume syncs the data between the two; if either the container or the host machine updates the data then it is reflected in the other.\nThis allows us to persist data if a container shuts down or is removed."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#how-to-add-a-volume",
    "href": "posts/software/docker/docker_notes.html#how-to-add-a-volume",
    "title": "Docker",
    "section": "3.3. How to Add a Volume",
    "text": "3.3. How to Add a Volume\nThe VOLUME instruction can be used in a Dockerfile. It takes an array of the path(s) inside the container to map to volumes.\nVOLUME [ “/app/feedback” ]\nNote that we do not specify a path on the host machine to map to. Docker abstracts this away from us; we never know the location on our host machine, we only interact with it via docker volumes. More on this later."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#types-of-external-data-storage",
    "href": "posts/software/docker/docker_notes.html#types-of-external-data-storage",
    "title": "Docker",
    "section": "3.4. Types of External Data Storage",
    "text": "3.4. Types of External Data Storage\nThere are two types: volumes (managed by Docker) and bind mounts (managed by you).\n\n3.4.1. Volumes\nVolumes are managed by Docker. They can be anonymous or named.\nIn either case, Docker sets up a folder on your host machine in an unknown location which is accessed via the ‘docker volume’ command. This maps to the internal directory in the container.\nAnonymous volumes are removed when the container is removed. This might not be very helpful for our use cases if we want to persist data.\nThis is where named volumes should be used. Named volumes do survive the container being removed. They are useful for data which should be persisted but which you don’t need to edit directly, as it can only be accessed via docker volumes.\nWe can’t create named volumes inside the docker image, it needs to be create when we run the container using the -v flag.\ndocker run -v &lt;volume_name&gt;:&lt;path/in/container&gt;\nWe can delete volumes with:\ndocker volume rm &lt;volume_name&gt;\nOr\ndocker volume prune\n\n\n3.4.2 Bind Mounts\nBind mounts are managed by you, by defining a path on your host machine. They are good for persistent editable data.\nWe set the bind mount up when we run a container, not when we build an image. Wrap the path in quotes in case it path has special characters or spaces.\ndocker run -v “&lt;absolute/path/on/host/machine&gt;:&lt;path/in/container&gt;” \nYou can also use shortcuts to current working directory:\n-v $(pwd):/app\nYou need to ensure docker has access to the folder you’re sharing. In the docker preferences UI, check Resources-&gt;File Sharing contains the folder (or parent folder) that you want to mount."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#combining-and-merging-different-volumes",
    "href": "posts/software/docker/docker_notes.html#combining-and-merging-different-volumes",
    "title": "Docker",
    "section": "3.5. Combining and Merging Different Volumes",
    "text": "3.5. Combining and Merging Different Volumes\nWe can have: - Data copied in the image build step (with the COPY command) - Volumes - named and anonymous - Bind mounts\nIf two or more of these refer to the same path in the container, the one with the longer file path (more specific) wins.\nThis can be helpful when, for example, we want a node project with “hot reload”.\n\nIn the image build step, copy the repo and npm install the dependencies\nWhen running the container, bind mount the repo with the host machine. BUT this would overwrite the node modules folder in the repo, undoing the npm install step and meaning we have no third party libraries.\nAlso mount an (anonymous) volume specifically for the node modules folder. Since this is a more specific file path, it takes priority and will not be overwritten by the bind mount.\n\nThis is a fairly common pattern in a variety of use cases. Use anonymous volumes with a specific path to prevent that folder being overwritten."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#restarting-web-servers",
    "href": "posts/software/docker/docker_notes.html#restarting-web-servers",
    "title": "Docker",
    "section": "3.6. Restarting Web Servers",
    "text": "3.6. Restarting Web Servers\nIf you make a change to server code and want to restart the web server, it might not be trivial to do so.\nYou can stop and restart the container. This at least saves the trouble of rebuilding.\nOr in node, you can add the nodemon package as a dependency, which watches for changes to the server code and restarts the server when required.\nThere may be other similar approaches for other languages and use cases besides web servers, but this type of thing is something to be aware of."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#read-only-volumes",
    "href": "posts/software/docker/docker_notes.html#read-only-volumes",
    "title": "Docker",
    "section": "3.7. Read-Only Volumes",
    "text": "3.7. Read-Only Volumes\nBy default, volumes are read-write. We may want to make a volume read-only, which we can do with an additional colon after the volume name. This applies to the folder and all sub-folders.\n-v “bind/mount/path:path/in/container:ro”\nIf we want a specific subfolder to be writable, we can do a similar trick like we did with the anonymous volume, where we specify a second more specific volume that allows read/write access."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#managing-docker-volumes",
    "href": "posts/software/docker/docker_notes.html#managing-docker-volumes",
    "title": "Docker",
    "section": "3.8. Managing Docker Volumes",
    "text": "3.8. Managing Docker Volumes\nVolumes can be listed with:\ndocker volume ls\nBind mounts do not appear here, as they are self-managed.\nYou can also create a volume directly with\ndocker volume create &lt;volume_name&gt;\nSee information on a volume, including the mount point where the data actually is stored on the host machine, with the inspect command:\ndocker volume inspect \nRemove volumes with:\ndocker volume rm &lt;volume_name&gt;\ndocker volume prune"
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#dockerignore-file",
    "href": "posts/software/docker/docker_notes.html#dockerignore-file",
    "title": "Docker",
    "section": "3.9. dockerignore file",
    "text": "3.9. dockerignore file\nWe can use a .dockerignore file to specify folders and files to ignore.\nThis is similar in principle to a .gitignore file."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#environment-variables",
    "href": "posts/software/docker/docker_notes.html#environment-variables",
    "title": "Docker",
    "section": "4.1. Environment Variables",
    "text": "4.1. Environment Variables\nEnvironment variables are variables that can be set in the Dockerfile or on docker run. They are available inside the Dockerfile and in application code.\nInsider the Dockerfile:\nENV &lt;VARIABLE_NAME&gt; &lt;default value&gt;\nENV PORT 80\nEXPOSE $PORT\nWhen we use environment variables in the Dockerfile, preface them with a $ to indicate that they are variable names not literal names.\nWe can use them in code too. The syntax depends on the language. In node, for example, it is process.env.PORT.\nThen when we run the container with an environment variable passed\ndocker run —env PORT=8000\nIf we have multiple environment variables, pass each with a --env or -e flag followed by the key=value pair.\nWe can create an environment file with all environment variables and values in. This is helpful if we have lots of variables to keep track of. Say we have a file called .env, we can pass this to docker run with:\ndocker run --env-file ./.env"
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#arguments",
    "href": "posts/software/docker/docker_notes.html#arguments",
    "title": "Docker",
    "section": "4.2. Arguments",
    "text": "4.2. Arguments\nArguments are variables that can be passed to the docker build command to be used inside the Dockerfile. It is not accessible to CMD or any application code.\nIn the Dockerfile, declare arguments with ARG, then use them prefixed with a $:\nARG DEFAULT_PORT=80\n\nENV PORT $DEFAULT_PORT\nTo overwrite the argument value in the build command:\ndocker build —build-arg DEFAULT_PORT=69"
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#types-of-communication",
    "href": "posts/software/docker/docker_notes.html#types-of-communication",
    "title": "Docker",
    "section": "5.1. Types of Communication",
    "text": "5.1. Types of Communication\nWe may want to communicate from our container to:\n\nAnother container\nAn external service (such as a website)\nOur host machine\n\nIt is best practice to have a single responsibility per container. So if we have a web server and a database, those should be two separate containers that can communicate with each other, rather than one monolith container.\nIf we were to just lump everything in one container, that’s essentially the same as lumping everything into a VM, so we’ve lost the modularity benefit of Docker. We’d need to rebuild the image and run a new container whenever anything changes anywhere.\n\n5.1.1. Container to Web Communication\nOut of the box, this works from containers. You don’t need any special set up to be able to send requests to a web page or web API from the container.\n\n\n5.1.2. Container to Host Machine Communication\nReplace localhost with host.docker.internal.\nThis tells Docker that we are actually referring to the host machine, and Docker will replace this with the IP address of the host machine as seen from inside the container.\nSo we just need to change our code so that any endpoints that are pointing at localhost get “translated” by Docker.\n\n\n5.1.3. Container to Container Communication\nIf we inspect the container we want to connect to, we can see its IP address under NetworkSettings.\nWe can replace localhost with the IP address of the container in our code with connects to it.\nBut this is cumbersome and impractical. We need to inspect the IP address of container B, change our code for application A so that it references this IP address, then rebuild image A before we can finally run container A.\nContainer networks simplify this."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#docker-container-networks",
    "href": "posts/software/docker/docker_notes.html#docker-container-networks",
    "title": "Docker",
    "section": "5.2. Docker Container Networks",
    "text": "5.2. Docker Container Networks\nWe can run multiple containers in the same network with:\ndocker run —network &lt;network_name&gt; \nAll containers within a network can communicate with each other and all IP addresses are automatically resolved.\nWe need to explicitly create the network before we can use it. Docker does not automatically do this. To create a network called my-network:\ndocker network create my-network\nWe can list networks with:\ndocker network ls\nTo communicate with another container in the network, just replace localhost in the URL with the target container name. Docker will automatically translate the IP address.\nWe don’t need to publish a port (with the -p flag) if the only connections are between containers in the network. We only need to publish a port when we want to communicate with a container from outside the network, such as from our host machine."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#network-drivers",
    "href": "posts/software/docker/docker_notes.html#network-drivers",
    "title": "Docker",
    "section": "5.3. Network Drivers",
    "text": "5.3. Network Drivers\nDocker supports different network drivers, which we can set with the --driver argument when creating the network.\nThe driver options are:\n\nbridge - This lets containers find each other by name when in the same network. This is the default driver and most common use case.\nhost - For standalone containers, isolation between container and host system is removed (i.e. they share localhost as a network).\noverlay - Multiple Docker daemons (i.e. Docker running on different machines) are able to connect with each other. Only works in “Swarm” mode which is a dated / almost deprecated way of connecting multiple containers.\nmacvlan: You can set a custom MAC address to a container - this address can then be used for communication with that container.\nnone - All networking is disabled.\n\nYou can also install third party plugins to add other behaviours and functionality."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#how-docker-resolves-ip-addresses",
    "href": "posts/software/docker/docker_notes.html#how-docker-resolves-ip-addresses",
    "title": "Docker",
    "section": "5.4. How Docker Resolves IP Addresses",
    "text": "5.4. How Docker Resolves IP Addresses\nWe’ve seen that, depending on the type of networking, we can refer to any of the following in our source code, and Docker will translate this to the correct IP address:\nlocalhost\nhost.docker.internal\ncontainer-name\nDocker owns the entire environment in which it runs. When it sees an outgoing request with one of the “translatable” names, it will replace that placeholder in the URL with the relevant IP address.\nThe source code isn’t changed. The translation only happens for outbound requests from inside the container / network."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#what-problem-does-docker-compose-solve",
    "href": "posts/software/docker/docker_notes.html#what-problem-does-docker-compose-solve",
    "title": "Docker",
    "section": "6.1. What Problem Does Docker Compose Solve?",
    "text": "6.1. What Problem Does Docker Compose Solve?\nWe have seen that we can create multiple containers and add them to the same network to allow them to communicate.\nThis is a bit inconvenient though, because we need to build/pull multiple images and run all of the containers with very specific commands in order to publish the correct ports, add the required volumes, put the containers in the correct network, etc.\nDocker Compose is a convenience tool to automate the setup of multiple containers. This way, we can start and stop a multi-container application with a single command.\nDocker Compose allows us to replace multiple docker build and docker run commands with one configuration file containing the orchestration steps. This lets us automate the set up of multiple images and containers.\nWe still need Dockerfile files for each of the custom images.\nDocker Compose is not suited for managing multiple containers on different host machines."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#writing-a-docker-compose-file",
    "href": "posts/software/docker/docker_notes.html#writing-a-docker-compose-file",
    "title": "Docker",
    "section": "6.2. Writing a Docker Compose File",
    "text": "6.2. Writing a Docker Compose File\nWe specify all of the details relevant to our containers such as:\n\nPublished ports\nEnvironment variables\nVolumes\nNetworks\n\nThe configuration file is named docker-compose.yaml.\nBy default in Docker Compose:\n\nWhen a container stops it is removed, so we don’t need to add the --rm flag since it is implicitly added.\nAll containers are added to the same network automatically.\nIf we reference the same named volumes in different services, that volume will be shared so different services can read/write the same volume.\n\nWe specify each of our services (essentially containers). If we want to build a custom image and then use it, we use the build key. Any args we pass to the build command can be specified under it. If one container requires another to be running first, we can specify the depends_on key.\nThere are Docker Compose equivalent of flags like the interactive flag -i -&gt; stdin_open: true, and TTY -t- -&gt;tty: true`.\nIf we have a bind mount, we can specify it under volumes with the relative path from the docker-compose.yml file. This is easier than outside of docker compose where we needed the absolute path.\nversion: “3.8”\nservices: \n\n  backend:\n    # Build a custom image then use it\n    build: \n      context: ./backend\n      # Specify any args passed to the Dockerfile\n      args:\n        arg-name-1: 69\n    ports: \n      - “3000:3000”\n    volumes:\n      # Relative path to bind mount\n      - ./backend:/app\n    depends_on:\n      # The backend service required `db` to be running first\n      - db\n\n  frontend:\n  # The equivalent of interactive mode (-i and -t flags)\n    stdin_open: true\n    tty: true\n\n  db:\n    image: “mongo”  # can be an image from DockerHub, private repo or local machine\n    volumes:\n      - data:data/db  # This is the same URL as it is outside of docker compose \n    environment:\n      ENV_VAR_KEY: “env-var-value”\n    env_file:  # Instead of environment \n    - ./relative/path/from/compose/file\n    networks:\n    # By default, every container in the compose file is added to the same network automatically, so we often don’t need to specify a network here unless we want something non-standard \n    - any-custom-networks\n  &lt;any other user-selected names for containers&gt;\n\n# Any named volumes used above need to be specified here as a top-level key \nvolumes:\n  # Just the name of the volume as a key, with no value. \n  # This syntax lets docker know this is a named volume.\n  data:"
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#docker-compose-up-and-down",
    "href": "posts/software/docker/docker_notes.html#docker-compose-up-and-down",
    "title": "Docker",
    "section": "6.3. Docker Compose Up and Down",
    "text": "6.3. Docker Compose Up and Down\nFrom the same directory as the docker-compose.yml file, run this in a terminal to start the containers:\ndocker-compose up\nAs before, we can run in detached mode with the -d flag.\nBy default, Docker will use the images locally if it finds them there. We can force Docker to rebuild all images instead with the --build flag. We can also just build images without starting containers with\ndocker-compose build\nWe can stop all services with:\ndocker-compose down\nThis does not delete volumes though, unless you add the -v flag."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#container-names",
    "href": "posts/software/docker/docker_notes.html#container-names",
    "title": "Docker",
    "section": "6.4. Container Names",
    "text": "6.4. Container Names\nWe specify the service names in our docker-compose.yml file.\nThe container names will be:\n&lt;project-directory-name&gt;_&lt;service-name&gt;_1\nYou can explicitly set the container name with the container_name key under the corresponding service in the docker-compose.yml file."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#single-container-applications",
    "href": "posts/software/docker/docker_notes.html#single-container-applications",
    "title": "Docker",
    "section": "6.5. Single Container Applications",
    "text": "6.5. Single Container Applications\nThe main benefit of Docker Compose is for multi-container applications.\nBut you can, and might prefer to, use it for a single container application. It makes it easier to keep track of long commands and different steps required to build an image and run a container with specific flags."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#why-would-we-need-utility-containers",
    "href": "posts/software/docker/docker_notes.html#why-would-we-need-utility-containers",
    "title": "Docker",
    "section": "7.1. Why Would We Need Utility Containers?",
    "text": "7.1. Why Would We Need Utility Containers?\nIn some cases, we need some software installed to create a project before we can then dockerize it.\nAs an example, if we have a node project we typically run ‘npm init’ to populate the package.json file with the package details and dependencies which allows us to run the application. And then we can dockerize that application.\nBut that would require us to have node and npm installed on our host machine, defeating the purpose of running it in a container."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#running-commands-in-containers",
    "href": "posts/software/docker/docker_notes.html#running-commands-in-containers",
    "title": "Docker",
    "section": "7.2. Running Commands in Containers",
    "text": "7.2. Running Commands in Containers\nWe can run containers in interactive mode with:\ndocker run -it node\nWe can execute any command, not just limited to the CMD command defined in the image, using ‘exec’. Say, for example, we want to run npm init inside a running container:\ndocker exec -it &lt;container_name&gt; npm init\nThis allows us to run additional commands without interrupting the main process. A use case of this is if we have a web server running and want to check its logs.\nWe can also pass a command to docker run to override the default CMD command:\ndocker run -it node npm init"
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#building-a-utility-container",
    "href": "posts/software/docker/docker_notes.html#building-a-utility-container",
    "title": "Docker",
    "section": "7.3. Building a Utility Container",
    "text": "7.3. Building a Utility Container\nWe can just have a minimal Dockerfile without any CMD:\nFROM node:14-alpine\nWORKDIR /app\nWe can build our utility image, let’s call it node-util, from the Dockerfile as usual:\ndocker build -t node-util .\nThen we can run it in interactive mode with a bind mount to our project directory on the host machine, and execute npm init to create the package.json on our host machine\ndocker run -it -v /absolute/path/to/project:/app node-util npm init\nSo it’s as if we’ve got node on our machine, but it’s actually running in Docker."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#using-entrypoint",
    "href": "posts/software/docker/docker_notes.html#using-entrypoint",
    "title": "Docker",
    "section": "7.4. Using ENTRYPOINT",
    "text": "7.4. Using ENTRYPOINT\nWe can use entry point if we want to restrict the set of commands that can be run in interactive mode.\nIt looks similar to CMD, but the difference is any commands that we specify in docker run or docker exec will be appended to the ENTRYPOINT, rather than replacing the CMD.\nFor example, if we only want to allow npm commands, in our Dockerfile we can add:\nENTRYPOINT [ “npm” ]\nThen when we rebuild the image and run:\ndocker run -it -v /absolute/path/to/project:/app node-util init\nThis will execute npm init - the init command we passed is appended to the npm entry point."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#docker-compose-with-utility-containers",
    "href": "posts/software/docker/docker_notes.html#docker-compose-with-utility-containers",
    "title": "Docker",
    "section": "7.5. Docker Compose with Utility Containers",
    "text": "7.5. Docker Compose with Utility Containers\nWe’ll often want to use docker-compose for utility containers because it encapsulates all of the logic around bind mounts, volumes and other optional flags to the build and run commands that we would typically use for utility containers.\nThe same logic to run arbitrary commands applies to containers created using docker-compose, just with some slightly different syntax.\nIf we are running a service called npm-service with an npm entry point:\ndocker-compose run npm-service init\nNote that a container started in this way will not automatically remove itself when executing docker-compose down unlike the usual behaviour. We can pass the usual --rm flag if we want that behaviour.\nTo execute commands in already running containers:\ndocker-compose exec npm-service init"
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#a-note-on-file-permissions",
    "href": "posts/software/docker/docker_notes.html#a-note-on-file-permissions",
    "title": "Docker",
    "section": "7.6. A Note on File Permissions",
    "text": "7.6. A Note on File Permissions\nOn Linux, Docker runs as the root user, so any files created by the utility container will have the root user as its owner. This might cause some issues in certain cases, so be aware of this if seeing any funky behaviour on Linux.\nOn Mac and Windows, Docker runs inside a virtual machine, so this issue doesn’t occur."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#production-considerations",
    "href": "posts/software/docker/docker_notes.html#production-considerations",
    "title": "Docker",
    "section": "8.1. Production Considerations",
    "text": "8.1. Production Considerations\nSome potential differences between production and dev environments:\n\nDon’t use bind mounts in production.\n\nThey are convenient for dev environments where we want instant updates without restarting the container. In production, we want the source code in the image, not on the remote machine. So we use COPY rather than bind mounts. We can do this easily by always COPYing the source code in the Dockerfile so it is built into the image, then in dev we add the bind mount for the source code when we run the container.\n\nContainerised apps might need different steps in dev vs prod.\n\nE.g. React apps that need a build step rather than a preview in dev.\n\nMulti-container projects might need to be split across multiple remote machines.\nTrade-off between control and responsibility\n\nManaged solutions vs self-managed."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#a-minimal-self-managed-deployment",
    "href": "posts/software/docker/docker_notes.html#a-minimal-self-managed-deployment",
    "title": "Docker",
    "section": "8.2. A (Minimal) Self-Managed Deployment",
    "text": "8.2. A (Minimal) Self-Managed Deployment\nUsing AWS as our cloud provider, these are the main steps to deploy:\n\nCreate an EC2 instance, VPC and security group.\nConfigure the security group to expose any required ports.\nConnect to the instance via ssh.\nInstall Docker.\npull the image and run the container.\n\n\n8.2.1 Creating the EC2 Instance\nMost of the default settings in the EC2 wizard can be left as is. It’s easiest to use the Amazon Linux image.\nEnsure a VPC is selected, and that you download the key-pair to allow you to ssh into the instance later.\n\n\n8.2.2. Connecting to the Instance\nSelect the instance from the EC2 dashboard and select the connection method (SSH client). This wizard will show the steps required to connect.\nFrom your bash terminal on your local machine in the same directory as your key file, run:\nchmod 400 key-file-name.pem\nssh -i “key-file-name.pem” ec2.url.goes.here\n\n\n8.2.3. Configure the Security Group\nBy default in EC2, the only way to connect to the instance is via ssh. We can change this to allow accessing it via the IP address.\nIn the EC2 dashboard, find the instance and go to Security Groups. Change the Inbound Rules to allow http connections from source=anywhere.\n\n\n8.2.4. Install Docker\nWe can use amazon-linux-extras to install packages if we are using the Amazon Linux image for our EC2 instance. Otherwise, just use the standard package manager for your OS.\nIn the ssh terminal:\nsudo yum update -y\nsudo amazon-linux-extras install docker \nsudo service docker start\n\n\n8.2.5. Pull the Image and Run the Container\nNow we can run Docker commands, so we can pull the image we want and run the container. Assuming our image exists on Docker Hub or some private repository, we can pull the image and run a container as usual:\ndocker pull &lt;image_name&gt;\ndocker run &lt;image_name&gt;\nWe can find the IP address of our instance in the EC2 dashboard, then visit this in a browser to use our application.\n\n\n8.2.6. The (Dis)advantages of Self-Managed\nThe approach in this minimal example was to set up everything ourselves, i.e. self-managed.\nThis means we own, and are responsible for, every element: the security, managing networks and firewalls, ensuring packages are up to date, selecting the right size of instance, etc.\nThis can be an advantage if we have the skills in the team to cover all of these disparate areas. If not, we may want to choose a managed approach."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#a-minimal-managed-deployment",
    "href": "posts/software/docker/docker_notes.html#a-minimal-managed-deployment",
    "title": "Docker",
    "section": "8.3. A (Minimal) Managed Deployment",
    "text": "8.3. A (Minimal) Managed Deployment\nThere are many different managed Docker solutions. Elastic Container Service (ECS) is Amazon’s offering.\nNote that with any managed provider, you won’t be using Docker commands directly and will have to work within that provider’s specific “rules”.\nECS has 4 main concepts: clusters, services, tasks, containers.\n\n8.3.1. Container\nIn the ECS dashboard, we use the wizard to define details like container name, image name, port mappings, etc. This essentially determines how ECS will run the docker run command later.\nThere are also options for logging (via CloudWatch).\n\n\n8.3.2. Task\nThe blueprint for your application - how AWS should launch your container. Not how it should execute docker run, but how the server that Docker runs on should be configured.\nYou can run the container on EC2 or Fargate. Fargate is serverless, and will start and stop an instance when a request is received. Contrast this with an EC2 instance which is always running whether it is handling requests or not.\n\n\n8.3.3. Service\nHow the task should be executed. We can define security groups and load balancers here.\n\n\n8.3.4. Cluster\nOverall network in which our services run. If we have multiple containers in one application, we can run them on the same cluster.\n\n\n8.3.5. Updating Managed Images\nWe can build a new image and push it to our remote repository. If we have an ECS instance running, we can force it to pick up the new version of the image in the ECS dashboard.\nECS -&gt; Clusters -&gt; Default \n-&gt; Tasks -&gt; Task Definition (of the task you want to update) \n-&gt; Create New Revision -&gt; Actions: Update Service\nThis creates a new task but will pull the latest image."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#multi-container-deployments",
    "href": "posts/software/docker/docker_notes.html#multi-container-deployments",
    "title": "Docker",
    "section": "8.4. Multi-Container Deployments",
    "text": "8.4. Multi-Container Deployments\nDocker Compose was helpful for multi-container applications running on our local machine, but it is less useful for deployments.\n\nWe may want to specify how much CPU each service has.\nWe may want each service running on different machines\nIf using a managed service, that provider may require details beyond what is in the docker-compose file\n\nIf you run all containers in the same task in ECS, you can communicate between containers using localhost because they will all be run on the same machine.\n\n8.4.1. Load Balancer\nWe can add a load balancer when launching the service. This should be an ‘internet-facing’ Application Load Balancer (ALB). You may need to create one manually if you did not specify the load balancer in the initial set up wizard. It should use the same VPC as your service. Configure the security groups and configure routing with Target type=IP. Change the health check path to a valid URL which should return a 200 status code when pinged.\n\n\n8.4.2. Stable IP Address\nWe can use the DNS name to send requests to, rather than the public IP address (which can change as services are stopped and started)."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#efs-volumes",
    "href": "posts/software/docker/docker_notes.html#efs-volumes",
    "title": "Docker",
    "section": "8.5. EFS Volumes",
    "text": "8.5. EFS Volumes\nIf we restart a task, for example when updating a service, the volumes attached to it will be lost.\nElastic File System (EFS) volumes are persistent storage.\nWe can add these in the Volumes section of the ECS wizard. It should be in the same VPC as used for ECS. Under Network Access, we need to add a new security group with inbound rule Type=NFS and source as the security group used for the service.\nThis will allow the services to communicate with EFS."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#database-containers",
    "href": "posts/software/docker/docker_notes.html#database-containers",
    "title": "Docker",
    "section": "8.6. Database Containers",
    "text": "8.6. Database Containers\nYou can manage your own database containers, but:\n\nScaling and managing availability can be challenging. Ensuring consistency between all running containers\nPerformance can be bad during traffic spikes\nBackups and security can be challenging\n\nFor these reasons, you may prefer a managed database solution like AWS RDS, MongoDB Atlas, etc.\n\n8.6.1. Managed Database\nWhen we use a cloud-based managed database, we may want to have different dev and prod environments.\nOne option would be to have two different databases, one for dev and one for prod. We can set an environment variable like DB_NAME so that we can overwrite the connection URL with the dev/prod version when we run the containers that connect to it.\nAn alternative would be to use a container in dev and the cloud-version in prod. We just need to be careful that we use the image version of, say, MongoDB, that corresponds to the cloud version. Otherwise the dev and prod versions might not be compatible."
  },
  {
    "objectID": "posts/software/docker/docker_notes.html#build-only-containers",
    "href": "posts/software/docker/docker_notes.html#build-only-containers",
    "title": "Docker",
    "section": "8.6.2. Build-Only Containers",
    "text": "8.6.2. Build-Only Containers\nIn some applications, for example React apps, we use something like npm run preview in a dev environment which allows us to hit reload.\nBut in production we build first, which compiles an optimised file.\nnpm run build\nnpm run start\nSo we may want different Dockerfiles for dev and prod. We can name the files to distinguish, e.g. Dockerfile.prod, then when we run docker build we pass the -f flag to specify the file name (by default it looks for Dockerfile with no extension).\nMulti-stage builds allow us to use one Dockerfile to run multiple build steps or “stages”. Stages can copy the contents from each other. You can either build the complete image or select individual stages.\nWith multi-stage builds, we need to use RUN instead of CMD.\nWe may want a different base image for different stages of the build. We can use another FROM statement to create a new stage. Each FROM statement in a file effectively delineates each stage.\nWe can COPY between stages by passing the --from flag with the stage name. E.g.\nFROM node as build_stage\n\n…\n\nFROM nginx as server_stage\n\nCOPY --from=build /path/in/build/stage /path/in/server/stage\nWe can set the build target to only build specific stages by using the --target flag, referring to the alias names we gave the stages in the Dockerfile.\ndocker build --target build_stage ."
  },
  {
    "objectID": "posts/business/marketing/marketing.html",
    "href": "posts/business/marketing/marketing.html",
    "title": "Marketing Notes",
    "section": "",
    "text": "Notes from “The 1-Page Marketing Plan” by Allan Dib.\n\n\n\n\nThe overall phases of the direct response marketing strategy are:\n\n\n\nPhase\nStatus\nGoal\n\n\n\n\nBefore\nProspect\nGet them to know you\n\n\nDuring\nLead\nGet them to like you\n\n\nAfter\nCustomer\nGet them to trust you\n\n\n\nDirect response marketing should be:\n\nSpecific\nTrackable\nInclude an offer\nFollow up\n\nA marketing plan encompasses:\n\n\n\n\n\n\n\nStage\nExample\n\n\n\n\nAdvertising\nSign for the circus\n\n\nPromotion\nPut the sign on an elephant and walk it into town\n\n\nPublicity\nElephant tramples the mayor’s flowers and it makes the news\n\n\nPR\nYou get the mayor to laugh it off\n\n\nSales\nPeople come to the circus and spend money\n\n\n\nStrategy changes with scale\n\nDon’t blindly copy large companies. “Branding” to “get your name out there” is expensive and ineffective.\nThe one and only objective of marketing for a small company is “make a profit” For large companies it may be: appease shareholders, board, managers, existing clients, win advertising awards. Making a profit is a low priority for them.\nStrategy without tactics = paralysis by analysis\nTactics without strategy = bright shiny object syndrome\n\nProduct quality is a customer retention tool, not a customer acquisition tool.\n\nPeople don’t know how good the product is until they’ve already bought it\n“If you build it, they will come” doesn’t work\n\nThe 80/20 rule (Pareto principle) states that 80% of the outputs come from 20% of the inputs. Applying the 80/20 rule to the 80/20 rule gives the 64/4 rule: 64% of the outputs come from 4% of the inputs. Identify this 4% and focus on it. Often this is marketing.\nSpend money to make time, not vice versa. Money is a renewable resource, time is finite.\n\n\n\nThe niche should be as specific as possible - an inch wide and a mile deep.\n\nToo broad a target market is useless as the message gets diluted\nMass marketing/branding can work for larger companies but not small. Large companies have enough firepower to fire arrows in all directions. Small companies need to fire in a specific direction.\n\nBecome a specialist in your field to make price irrelevant\n\nBecome an educator and build trust\nYou don’t want to be a commodity shopped on price. A customer won on price will be lost on price.\n\nUse the PVP index to identify the ideal niche. Assign values to each possible segment for:\n\nPersonal fulfillment\nValue to the marketplace\nProfitability\n\nQuestions for the target prospect:\n\nWhat keeps them up at night?\nWhat are they afraid of?\nWhat are they angry about? Who are they angry at?\nWhat trends affect them?\nWhat do they secretly desire most? What one thing do they crave above all else?\nHow do they make decisions? Analytical, emotional, social proof\nWhat language/jargon do they use?\nWhat magazines do they read? What websites do they visit?\nWhat is a typical day like?\nWhat is the single most dominant emotion this market feels?\n\nCreate an avatar for each prospect. Put a name and photo against each market segment and describe a specific person in detail. Describe their life, job, activities, relationships. Answer each of the questions above from their POV.\n\n\n\nThere are two key elements of the message:\n\nPurpose of the ad\nWhat it focuses on\n\nKeep the message focused: 1 ad = 1 objective. “Marketing on purpose” rather than “marketing by accident”. There should be a clear call to action. You should know the exact action you want the prospect to take next.\nThe following two questions help refine the USP:\n\nWhy should they buy?\nWhy should they buy from me?\n\nKey takeaways for USP:\n\nWhy should they buy from you rather than competitor\nIf you remove the company name and logo, would people still know it’s you?\nQuality and service are not USPs. The customer only finds these out after they buy from you. USPs should attract customers before they buy.\nLowest price is not a USP. There’s always someone willing to go out of business quicker than you.\n\nPeople rarely want what you’re selling, they want the result of what you’re selling. Sell the problem. People don’t know what they want until they’re presented with it. Purchasing is done with emotions and justified with logic after the fact. “If I had asked people what they wanted, they’d have said faster horses” - Henry Ford.\nWhen you confuse them, you lose them.\n\nExplain the product and its benefit in one sentence.\nThe biggest threat isn’t that someone buys from a competitor, it’s that they do nothing.\nElevator pitch of the form: “You know ? What we do is ? In fact .”\n\nBe remarkable\n\nTransform something ordinary into something that makes the customer smile.\nUnique is dangerous, you just need to be “different enough”.\n\nCraft the offer\n\nIf you don’t give a compelling reason to buy, customers will default to price, and you become a commodity.\nDon’t just offer a discount, think about which product will definitely solve their biggest problem.\nAdd value rather than discounting, through bundles and customisation.\nCreate “apples to oranges” comparisons to your competitors to get out of the commmodity game.\nTarget pain points. Sell the problem, not a feature list. Customers in pain are price insensitive.\n\nCreate the offer:\n\nValue to customer\nLanguage and jargon they use\nReason why this offer is here. People are skeptical if it is too good to be true.\nValue stacking. Add in high-margin bonuses.\nUpsells\nOutrageous guarantee\nCreate scarcity. People react more to fear of loss than prospect of gain.\n\nWriting copy:\n\nBe yourself and be authentic. “Professional” = boring.\n“The enemy in common”. People buy with emotions: fear, love, greed, guilt, pride.\nCopy should address the bad parts of the product, not just the good. Who is this not for?\n\nNaming:\n\nTitle=content\nChoose clarity over cleverness\n\n\n\n\n\n“Half the money I spend on advertising is wasted; the trouble is I don’t know which half” - John Wanamaker\n\nMeasure ROI of ad campaign\n\nIf it made more money that it lost, it was a success. The only metric that matters.\nCustomer acquisition cost vs lifetime value of customer\nOther metrics like response rate are only useful to the extent that they help you calculate the above.\nSpecific target market is key. The goal of the ad is to make they prospect say “Hey, that’s for me”.\nWhat gets measured gets managed. Conversely, what you don’t know will hurt you.\n\nLifetime value of customer = Frontend (initial purchase) + Backend (subsequent purchases)\n\nIdeally the frontend alone exceeds the customer acquisition cost to be sustainable. Then the cost of the ad is immediately recouped and the ad campaign can be reupped for another cycle.\n\nSuccessful marketing has three components:\n\nMarket\nMessage\nMedia\n\nSocial media is a media not a strategy and it’s not free. It’s only free if your time is worthless.\nEmail is preferable to social media because you own the mailing list. The customer database which is a valuable asset in its own right.\n\nEmail regularly - weekly at least\nGive value - 3 value-building emails for each offer email\nSnail mail complements email - less cluttered and less competitive because fewer people still use it.\n\nMarketing budget\n\nThe only time for a budget is during the testing phase when your finding what works (what delivers a positive ROI)\nOnce you have a campaign that works, go all in. It makes money, it’s not an expense so don’t limit it.\nIf you could buy £10 notes for £5 you’d buy as many as possible, you wouldn’t limit it with a budget. The same applies for a marketing campaign with a £5 cost of acquisition and £10 lifetime value of customer.\n\nAvoid single points of failure\n\nOne customer, supplier, ad medium, lead generation method.\nPaid media is reliable and allows ROI to be tracked and measured.\n\n\n\n\n\n\n\nTreat marketing like farming not hunting. Avoid “random acts of marketing” that hunters do. Build a CRM system and infrastructure for new leads, follow-ups, conversions, etc.\nThe first goal is to generate leads - find people who are interested.\n\nAvoid trying to sell from the ad immediately\nCapture all who could be interested, not just those ready to buy immediately\nBuild a customer database, then sell to them later\n\nThe market for your product might break down something like:\n\n3% - ready to buy now\n7% - open to buying\n30% - interested but not right now\n60% - not interested\n\nBy selling directly from the ad you limit yourself just to the 3%. Capture leads first and nurture them to appeal to the 40% who are interested.\nEducate customers to offer value, then you become a trusted advisor rather than a pest salesman.\n\n\n\nThe process of taking people from being vaguely interested to desiring it. The nurturing process means they should be predisposed to buying from you before you ever try to sell to them.\nStay in contact.\n\nMost stop after 1 or 2 follow-ups. The most effective salespeople do 10+ follow-ups.\nThe money is in the follow-up\n\nBuild trust and add value. Don’t pester to close.\n\nWhen the customer is ready to buy, you will be top of mind and they are predisposed to buy. It should be when they are ready to buy, not when you are ready to sell.\nDo not pressure prospects to close. Don’t be a pest, be a welcome guest.\nContact regularly - “market until they buy or die”.\nMost contact should be value add, with occasional pitch/offer emails. 3:1 ratio of value add:pitch.\n\nShock and awe package.\n\nPhysical parcel mailed to high probability prospects.\n“Lumpy” mail grabs attention - snail mail with something physical inside.\nNeed numbers on conversion probabilities and lifetime value of customer for this approach to be viable. Good marketing can’t beat bad math.\nHigh value contents should:\n\nProvide value\nPosition you as a trusted expert\nMove prospect further down the buying cycle.\n\n\nMake lots of offers. A/B test with small segments. Take risks, write compelling copy and make outrageous guarantees.\nThree major types of people:\n\n\n\nType\nRole\nResponsibility\n\n\n\n\nEntrepreneur\n“Make it up”\nVision\n\n\nSpecialist\n“Make it real”\nImplement\n\n\nManager\n“Make it recur”\nDaily BAU\n\n\n\nMarketing calendar.\n\nWhat needs to be done and when\nDaily, weekly, monthly, quarterly, annually\nEvent-triggers tasks, e.g. on signup, on complaint\n\nDelegate.\n\nIf someone else can do the job 80% as good as you, delegate it.\nSeparate majors and minors. Focus on the majors, dump or delegate the minors.\nDon’t mistake movement for achievement.\nDays are expensive. You can get more money but not more time.\n\n\n\n\nOnce you reach a certain level of expertise, the real profit is in how you market yourself. A concert violinist makes more money performing concerts than busking.\nTrust is key in sales.\n\nDifficult for unknown businesses.\nPeople are wary because of dodgy sellers. Assume every dog bites.\nEducate -&gt; Position as expert -&gt; Trust\nDelay the sale to show:\n\nYou are willing to give before you take\nYou are an educator, so can be trusted\n\nStop selling and start educating, consulting, advising.\nEntrepreneur is someone who solves people’s problems for profit.\n\nOutrageous guarantees.\n\nReverse the risk from the customer to you.\nGuarantee needs to be specific and address the prospect’s reservations directly. Not just a generic money-back guarantee.\nIf you’re an ethical operator, you’re already implicitly providing a guarantee, just not marketing it. Use it!\n\nPricing.\n\nThe marketing potential of price is often underutilised.\nDon’t just naively undercut the market leader or use cost-plus pricing.\nSmall number of options/tiers\n\nMore choice means you are more likely to pique interest but less likely to keep interest.\nFear of a suboptimal choice; the paradox of choice.\nStandard and premium tiers and a good rule of thumb.\n\n“Unlimited” option can often be profitable because customers overestimate how much they will use, so overpay for unlimited use.\nUltra high ticket item\n\nMakes other tiers seem more reasonably priced\n10% of customers would pay 10x more. 1% of customers would pay 100x more.\n\n\nOther price considerations:\n\nDon’t discount, unless specifically using a loss-leader strategy.\nTry before you buy (the puppy dog close)\n\nThe sale feels less permanent so more palatable for the customer.\nThe onus is on the customer to return - shifts the power balance.\n\nDon’t segregate departments - everyone can warm up a prospect.\nPayment plans - people are less attached to future money than present money.\n\n\n\n\n\n\n\nThe goal is to turn customers into a tribe of raving fans. Most companies stop marketing at the sell, but this is where the process begins. Most of the money is in the backend not the frontend.\nSell them what they want, give them what they need.\n\nThese don’t always overlap\nIf they buy a treadmill and don’t use it, they’ll blame you when they don’t lose weight. You need to make sure they use your product correctly after they buy it.\nSplit the process up for them to make it less daunting.\n\nCreate a sense of theatre\n\nPeople don’t just want ot be serviced, they want to be entertained.\nInnovate on: pricing, financing, packaging, support, delivery etc, not just product. For example, “will it blend” YouTube videos to sell ordinary blenders.\n\nUse technology to reduce friction. Treat tech like an employee. Why am I hiring it? What are its KPIs?\nBecome a voice of value to your tribe.\n\nEducation-based marketing leads to trust and becoming an expert in the field.\nTell your audience the effort that goes into the product. For example, shampoo bottle that says how many mint leaves went into it.\n\nCreate replicable systems\n\nSystems ensure you have a business, rather than you are the business\nOften overlooked because it is “back office” or “not urgent”\nBenefits: build valuable asset; leverage and scalability; consistency; lower labour costs\nAreas:\n\nMarketing (leads)\nSales (follow-up)\nFulfillment\nAdmin\n\nCreate an operations manual:\n\nImagine the business is 10x the size\nIdentify all roles\nIdentify all tasks - what does each role do\nChecklist for each task\n\n\nExit strategy. Start with the end in mind. If the goal is to sell for $50 million, any decision can be framed as “will this help me get to $50 million?”. Who will buy the company? Why? Customer base, revenue or IP?\n\n\n\nPeople are 21x more likely to buy from a company they’ve previously bought from. Increase revenue by selling more to existing customers.\n\nRaise prices\n\n\nPeople are less price sensitive than you’d expect.\nSome may leave but these were the lowest-value customers - this may be polluted revenue you want to fire anyway. A customer won on price can be lost on price.\nGive a reason why prices increased. Explain the benefits they’ve received and future innovations.\nGrandfather existing customers on old price as a loyalty bonus.\n\n\nUpselling\n\n\n“Contrast principle” - people are less price sensitive to add-ons. If they’ve already bought the suit then the shirt seems cheap.\n“Customers who bought X also bought Y” - people want to fit social norms.\nSell more when the client is “hot and heavy” in the buying mood. Some people naively think you should overwhelm them with more selling.\n\n\nAscension\n\n\nMove customers up to a higher-priced tier.\nProduct mix - standard, premium, ultra-high.\n\n\nFrequency of purchases\n\n\nReminders, subscriptions\nReasons to come back; voucher for next purchase.\n\n\nReactivation\n\n\nLure back lapsed customers with strong offer and call to action.\nAsk why they haven’t returned.\nApologise and make corrections if necessary. Make the feel special if they do return.\n\nKey numbers - what gets measured gets managed\n\nLeads\nConversion rate\nAverage transaction value\nBreak even point\n\nFor subscription models measure:\n\nMonthly recurring revenue\nChurn rate\nCustomer lifetime value\n\nPolluted revenue - dollars from toxic customers != dollars from raving fans. Types of customers:\n\nTribe\nChurners - can’t afford you so drop you on price or intro offer ending\nVampires - suck up all your time, you can’t afford them\nSnow leopard - big customer but not replicable\n\nNet promoter score\n\n“How likely are you to recommend us to a friend?”\nDetractors 0-6; Passive 7-8; Promoters 9-10\nNPS=+100 if everyone is a promoter, -100 if everyone is a detractor\n\nFire problem customers\n\n“The customer is always right” is naive. It should be “the right customer is always right”.\nGenuine customer complaints are valuable and may help retain other customers who had the same issue but didn’t speak up.\nLow-value, price-sensitive customers complain the most. They take up your time and energy at the expense of your tribe.\nFiring detractors creates scarcity and frees your time to focus on the tribe. With limited supply, customers have to play and pay by your rules.\n\n\n\n\nOrchestrating referrals is an active task, not needy or desperate.\n\nDon’t just deliver a good product and hope for the best\nPeople refer because it makes them feel good, helpful or knowledgeable, not to do you a favour.\nAppeal to their ego, offer them something valuable, give them a reason to refer.\n\nAsk (for a referral) and ye shall receive.\n\n“Law of 250” - most people know 250 people well enough to invite to a wedding or funeral. This is 250 potential referrals per customer.\nSet expectation of referrals beforehand - “We’re going to do a great job and we need your help… Most people refer 3 others.”\n\nBe specific about who you ask and what kind of referral you want. Conquer the bystander effect.\nMake referral profiles for each customer group in your database.\n\nWho do they know?\nHow can you make them remember you?\nWhat will make them look good?\nHow will they provide value to their referral target\n\nCustomer buying behaviour\n\nWho has your customers before you?\nJoint venture for referrals\nSell/exchange leads, resell complementary products, affiliate referral partner\n\nBrand = personality of the business. Think of the business as a person:\n\nName\nDesign (what they wear)\nPositioning (how they communicate)\nBrand promise (core values)\nTarget market (who they associate with)\nBrand awareness (how well known / popular)\n\nBest form of brand building is selling\n\nBranding is what you do after someone has bought from you.\nBrand equity - customers crossing the road to buy from you and walk past competitors.\n\n\n\n\n\nImplementation is crucial\n\nKnowing and not doing is the same as not knowing.\nCommon pitfalls:\n\nParalysis by analysis - 80% out the door is better than 100% in the drawer; money loves speed.\nFail to delegate\n“My business is different” - Spend your effort figuring out how to make it work rather than why it won’t work.\n\n\nTime is not money\n\nEntrepreneurs get paid for the value they create, not the time or effort they put in.\nMaking money is a side effect of creating value.\n\nThe value of a business is the eyeballs it has access to.\nQuestions to ponder:\n\nWhat business should I be in?\nWhat technologies will disrupt it?\nHow can I take advantage of tech rather than fight it?"
  },
  {
    "objectID": "posts/business/marketing/marketing.html#the-before-phase",
    "href": "posts/business/marketing/marketing.html#the-before-phase",
    "title": "Marketing Notes",
    "section": "",
    "text": "The overall phases of the direct response marketing strategy are:\n\n\n\nPhase\nStatus\nGoal\n\n\n\n\nBefore\nProspect\nGet them to know you\n\n\nDuring\nLead\nGet them to like you\n\n\nAfter\nCustomer\nGet them to trust you\n\n\n\nDirect response marketing should be:\n\nSpecific\nTrackable\nInclude an offer\nFollow up\n\nA marketing plan encompasses:\n\n\n\n\n\n\n\nStage\nExample\n\n\n\n\nAdvertising\nSign for the circus\n\n\nPromotion\nPut the sign on an elephant and walk it into town\n\n\nPublicity\nElephant tramples the mayor’s flowers and it makes the news\n\n\nPR\nYou get the mayor to laugh it off\n\n\nSales\nPeople come to the circus and spend money\n\n\n\nStrategy changes with scale\n\nDon’t blindly copy large companies. “Branding” to “get your name out there” is expensive and ineffective.\nThe one and only objective of marketing for a small company is “make a profit” For large companies it may be: appease shareholders, board, managers, existing clients, win advertising awards. Making a profit is a low priority for them.\nStrategy without tactics = paralysis by analysis\nTactics without strategy = bright shiny object syndrome\n\nProduct quality is a customer retention tool, not a customer acquisition tool.\n\nPeople don’t know how good the product is until they’ve already bought it\n“If you build it, they will come” doesn’t work\n\nThe 80/20 rule (Pareto principle) states that 80% of the outputs come from 20% of the inputs. Applying the 80/20 rule to the 80/20 rule gives the 64/4 rule: 64% of the outputs come from 4% of the inputs. Identify this 4% and focus on it. Often this is marketing.\nSpend money to make time, not vice versa. Money is a renewable resource, time is finite.\n\n\n\nThe niche should be as specific as possible - an inch wide and a mile deep.\n\nToo broad a target market is useless as the message gets diluted\nMass marketing/branding can work for larger companies but not small. Large companies have enough firepower to fire arrows in all directions. Small companies need to fire in a specific direction.\n\nBecome a specialist in your field to make price irrelevant\n\nBecome an educator and build trust\nYou don’t want to be a commodity shopped on price. A customer won on price will be lost on price.\n\nUse the PVP index to identify the ideal niche. Assign values to each possible segment for:\n\nPersonal fulfillment\nValue to the marketplace\nProfitability\n\nQuestions for the target prospect:\n\nWhat keeps them up at night?\nWhat are they afraid of?\nWhat are they angry about? Who are they angry at?\nWhat trends affect them?\nWhat do they secretly desire most? What one thing do they crave above all else?\nHow do they make decisions? Analytical, emotional, social proof\nWhat language/jargon do they use?\nWhat magazines do they read? What websites do they visit?\nWhat is a typical day like?\nWhat is the single most dominant emotion this market feels?\n\nCreate an avatar for each prospect. Put a name and photo against each market segment and describe a specific person in detail. Describe their life, job, activities, relationships. Answer each of the questions above from their POV.\n\n\n\nThere are two key elements of the message:\n\nPurpose of the ad\nWhat it focuses on\n\nKeep the message focused: 1 ad = 1 objective. “Marketing on purpose” rather than “marketing by accident”. There should be a clear call to action. You should know the exact action you want the prospect to take next.\nThe following two questions help refine the USP:\n\nWhy should they buy?\nWhy should they buy from me?\n\nKey takeaways for USP:\n\nWhy should they buy from you rather than competitor\nIf you remove the company name and logo, would people still know it’s you?\nQuality and service are not USPs. The customer only finds these out after they buy from you. USPs should attract customers before they buy.\nLowest price is not a USP. There’s always someone willing to go out of business quicker than you.\n\nPeople rarely want what you’re selling, they want the result of what you’re selling. Sell the problem. People don’t know what they want until they’re presented with it. Purchasing is done with emotions and justified with logic after the fact. “If I had asked people what they wanted, they’d have said faster horses” - Henry Ford.\nWhen you confuse them, you lose them.\n\nExplain the product and its benefit in one sentence.\nThe biggest threat isn’t that someone buys from a competitor, it’s that they do nothing.\nElevator pitch of the form: “You know ? What we do is ? In fact .”\n\nBe remarkable\n\nTransform something ordinary into something that makes the customer smile.\nUnique is dangerous, you just need to be “different enough”.\n\nCraft the offer\n\nIf you don’t give a compelling reason to buy, customers will default to price, and you become a commodity.\nDon’t just offer a discount, think about which product will definitely solve their biggest problem.\nAdd value rather than discounting, through bundles and customisation.\nCreate “apples to oranges” comparisons to your competitors to get out of the commmodity game.\nTarget pain points. Sell the problem, not a feature list. Customers in pain are price insensitive.\n\nCreate the offer:\n\nValue to customer\nLanguage and jargon they use\nReason why this offer is here. People are skeptical if it is too good to be true.\nValue stacking. Add in high-margin bonuses.\nUpsells\nOutrageous guarantee\nCreate scarcity. People react more to fear of loss than prospect of gain.\n\nWriting copy:\n\nBe yourself and be authentic. “Professional” = boring.\n“The enemy in common”. People buy with emotions: fear, love, greed, guilt, pride.\nCopy should address the bad parts of the product, not just the good. Who is this not for?\n\nNaming:\n\nTitle=content\nChoose clarity over cleverness\n\n\n\n\n\n“Half the money I spend on advertising is wasted; the trouble is I don’t know which half” - John Wanamaker\n\nMeasure ROI of ad campaign\n\nIf it made more money that it lost, it was a success. The only metric that matters.\nCustomer acquisition cost vs lifetime value of customer\nOther metrics like response rate are only useful to the extent that they help you calculate the above.\nSpecific target market is key. The goal of the ad is to make they prospect say “Hey, that’s for me”.\nWhat gets measured gets managed. Conversely, what you don’t know will hurt you.\n\nLifetime value of customer = Frontend (initial purchase) + Backend (subsequent purchases)\n\nIdeally the frontend alone exceeds the customer acquisition cost to be sustainable. Then the cost of the ad is immediately recouped and the ad campaign can be reupped for another cycle.\n\nSuccessful marketing has three components:\n\nMarket\nMessage\nMedia\n\nSocial media is a media not a strategy and it’s not free. It’s only free if your time is worthless.\nEmail is preferable to social media because you own the mailing list. The customer database which is a valuable asset in its own right.\n\nEmail regularly - weekly at least\nGive value - 3 value-building emails for each offer email\nSnail mail complements email - less cluttered and less competitive because fewer people still use it.\n\nMarketing budget\n\nThe only time for a budget is during the testing phase when your finding what works (what delivers a positive ROI)\nOnce you have a campaign that works, go all in. It makes money, it’s not an expense so don’t limit it.\nIf you could buy £10 notes for £5 you’d buy as many as possible, you wouldn’t limit it with a budget. The same applies for a marketing campaign with a £5 cost of acquisition and £10 lifetime value of customer.\n\nAvoid single points of failure\n\nOne customer, supplier, ad medium, lead generation method.\nPaid media is reliable and allows ROI to be tracked and measured."
  },
  {
    "objectID": "posts/business/marketing/marketing.html#the-during-phase",
    "href": "posts/business/marketing/marketing.html#the-during-phase",
    "title": "Marketing Notes",
    "section": "",
    "text": "Treat marketing like farming not hunting. Avoid “random acts of marketing” that hunters do. Build a CRM system and infrastructure for new leads, follow-ups, conversions, etc.\nThe first goal is to generate leads - find people who are interested.\n\nAvoid trying to sell from the ad immediately\nCapture all who could be interested, not just those ready to buy immediately\nBuild a customer database, then sell to them later\n\nThe market for your product might break down something like:\n\n3% - ready to buy now\n7% - open to buying\n30% - interested but not right now\n60% - not interested\n\nBy selling directly from the ad you limit yourself just to the 3%. Capture leads first and nurture them to appeal to the 40% who are interested.\nEducate customers to offer value, then you become a trusted advisor rather than a pest salesman.\n\n\n\nThe process of taking people from being vaguely interested to desiring it. The nurturing process means they should be predisposed to buying from you before you ever try to sell to them.\nStay in contact.\n\nMost stop after 1 or 2 follow-ups. The most effective salespeople do 10+ follow-ups.\nThe money is in the follow-up\n\nBuild trust and add value. Don’t pester to close.\n\nWhen the customer is ready to buy, you will be top of mind and they are predisposed to buy. It should be when they are ready to buy, not when you are ready to sell.\nDo not pressure prospects to close. Don’t be a pest, be a welcome guest.\nContact regularly - “market until they buy or die”.\nMost contact should be value add, with occasional pitch/offer emails. 3:1 ratio of value add:pitch.\n\nShock and awe package.\n\nPhysical parcel mailed to high probability prospects.\n“Lumpy” mail grabs attention - snail mail with something physical inside.\nNeed numbers on conversion probabilities and lifetime value of customer for this approach to be viable. Good marketing can’t beat bad math.\nHigh value contents should:\n\nProvide value\nPosition you as a trusted expert\nMove prospect further down the buying cycle.\n\n\nMake lots of offers. A/B test with small segments. Take risks, write compelling copy and make outrageous guarantees.\nThree major types of people:\n\n\n\nType\nRole\nResponsibility\n\n\n\n\nEntrepreneur\n“Make it up”\nVision\n\n\nSpecialist\n“Make it real”\nImplement\n\n\nManager\n“Make it recur”\nDaily BAU\n\n\n\nMarketing calendar.\n\nWhat needs to be done and when\nDaily, weekly, monthly, quarterly, annually\nEvent-triggers tasks, e.g. on signup, on complaint\n\nDelegate.\n\nIf someone else can do the job 80% as good as you, delegate it.\nSeparate majors and minors. Focus on the majors, dump or delegate the minors.\nDon’t mistake movement for achievement.\nDays are expensive. You can get more money but not more time.\n\n\n\n\nOnce you reach a certain level of expertise, the real profit is in how you market yourself. A concert violinist makes more money performing concerts than busking.\nTrust is key in sales.\n\nDifficult for unknown businesses.\nPeople are wary because of dodgy sellers. Assume every dog bites.\nEducate -&gt; Position as expert -&gt; Trust\nDelay the sale to show:\n\nYou are willing to give before you take\nYou are an educator, so can be trusted\n\nStop selling and start educating, consulting, advising.\nEntrepreneur is someone who solves people’s problems for profit.\n\nOutrageous guarantees.\n\nReverse the risk from the customer to you.\nGuarantee needs to be specific and address the prospect’s reservations directly. Not just a generic money-back guarantee.\nIf you’re an ethical operator, you’re already implicitly providing a guarantee, just not marketing it. Use it!\n\nPricing.\n\nThe marketing potential of price is often underutilised.\nDon’t just naively undercut the market leader or use cost-plus pricing.\nSmall number of options/tiers\n\nMore choice means you are more likely to pique interest but less likely to keep interest.\nFear of a suboptimal choice; the paradox of choice.\nStandard and premium tiers and a good rule of thumb.\n\n“Unlimited” option can often be profitable because customers overestimate how much they will use, so overpay for unlimited use.\nUltra high ticket item\n\nMakes other tiers seem more reasonably priced\n10% of customers would pay 10x more. 1% of customers would pay 100x more.\n\n\nOther price considerations:\n\nDon’t discount, unless specifically using a loss-leader strategy.\nTry before you buy (the puppy dog close)\n\nThe sale feels less permanent so more palatable for the customer.\nThe onus is on the customer to return - shifts the power balance.\n\nDon’t segregate departments - everyone can warm up a prospect.\nPayment plans - people are less attached to future money than present money."
  },
  {
    "objectID": "posts/business/marketing/marketing.html#the-after-phase",
    "href": "posts/business/marketing/marketing.html#the-after-phase",
    "title": "Marketing Notes",
    "section": "",
    "text": "The goal is to turn customers into a tribe of raving fans. Most companies stop marketing at the sell, but this is where the process begins. Most of the money is in the backend not the frontend.\nSell them what they want, give them what they need.\n\nThese don’t always overlap\nIf they buy a treadmill and don’t use it, they’ll blame you when they don’t lose weight. You need to make sure they use your product correctly after they buy it.\nSplit the process up for them to make it less daunting.\n\nCreate a sense of theatre\n\nPeople don’t just want ot be serviced, they want to be entertained.\nInnovate on: pricing, financing, packaging, support, delivery etc, not just product. For example, “will it blend” YouTube videos to sell ordinary blenders.\n\nUse technology to reduce friction. Treat tech like an employee. Why am I hiring it? What are its KPIs?\nBecome a voice of value to your tribe.\n\nEducation-based marketing leads to trust and becoming an expert in the field.\nTell your audience the effort that goes into the product. For example, shampoo bottle that says how many mint leaves went into it.\n\nCreate replicable systems\n\nSystems ensure you have a business, rather than you are the business\nOften overlooked because it is “back office” or “not urgent”\nBenefits: build valuable asset; leverage and scalability; consistency; lower labour costs\nAreas:\n\nMarketing (leads)\nSales (follow-up)\nFulfillment\nAdmin\n\nCreate an operations manual:\n\nImagine the business is 10x the size\nIdentify all roles\nIdentify all tasks - what does each role do\nChecklist for each task\n\n\nExit strategy. Start with the end in mind. If the goal is to sell for $50 million, any decision can be framed as “will this help me get to $50 million?”. Who will buy the company? Why? Customer base, revenue or IP?\n\n\n\nPeople are 21x more likely to buy from a company they’ve previously bought from. Increase revenue by selling more to existing customers.\n\nRaise prices\n\n\nPeople are less price sensitive than you’d expect.\nSome may leave but these were the lowest-value customers - this may be polluted revenue you want to fire anyway. A customer won on price can be lost on price.\nGive a reason why prices increased. Explain the benefits they’ve received and future innovations.\nGrandfather existing customers on old price as a loyalty bonus.\n\n\nUpselling\n\n\n“Contrast principle” - people are less price sensitive to add-ons. If they’ve already bought the suit then the shirt seems cheap.\n“Customers who bought X also bought Y” - people want to fit social norms.\nSell more when the client is “hot and heavy” in the buying mood. Some people naively think you should overwhelm them with more selling.\n\n\nAscension\n\n\nMove customers up to a higher-priced tier.\nProduct mix - standard, premium, ultra-high.\n\n\nFrequency of purchases\n\n\nReminders, subscriptions\nReasons to come back; voucher for next purchase.\n\n\nReactivation\n\n\nLure back lapsed customers with strong offer and call to action.\nAsk why they haven’t returned.\nApologise and make corrections if necessary. Make the feel special if they do return.\n\nKey numbers - what gets measured gets managed\n\nLeads\nConversion rate\nAverage transaction value\nBreak even point\n\nFor subscription models measure:\n\nMonthly recurring revenue\nChurn rate\nCustomer lifetime value\n\nPolluted revenue - dollars from toxic customers != dollars from raving fans. Types of customers:\n\nTribe\nChurners - can’t afford you so drop you on price or intro offer ending\nVampires - suck up all your time, you can’t afford them\nSnow leopard - big customer but not replicable\n\nNet promoter score\n\n“How likely are you to recommend us to a friend?”\nDetractors 0-6; Passive 7-8; Promoters 9-10\nNPS=+100 if everyone is a promoter, -100 if everyone is a detractor\n\nFire problem customers\n\n“The customer is always right” is naive. It should be “the right customer is always right”.\nGenuine customer complaints are valuable and may help retain other customers who had the same issue but didn’t speak up.\nLow-value, price-sensitive customers complain the most. They take up your time and energy at the expense of your tribe.\nFiring detractors creates scarcity and frees your time to focus on the tribe. With limited supply, customers have to play and pay by your rules.\n\n\n\n\nOrchestrating referrals is an active task, not needy or desperate.\n\nDon’t just deliver a good product and hope for the best\nPeople refer because it makes them feel good, helpful or knowledgeable, not to do you a favour.\nAppeal to their ego, offer them something valuable, give them a reason to refer.\n\nAsk (for a referral) and ye shall receive.\n\n“Law of 250” - most people know 250 people well enough to invite to a wedding or funeral. This is 250 potential referrals per customer.\nSet expectation of referrals beforehand - “We’re going to do a great job and we need your help… Most people refer 3 others.”\n\nBe specific about who you ask and what kind of referral you want. Conquer the bystander effect.\nMake referral profiles for each customer group in your database.\n\nWho do they know?\nHow can you make them remember you?\nWhat will make them look good?\nHow will they provide value to their referral target\n\nCustomer buying behaviour\n\nWho has your customers before you?\nJoint venture for referrals\nSell/exchange leads, resell complementary products, affiliate referral partner\n\nBrand = personality of the business. Think of the business as a person:\n\nName\nDesign (what they wear)\nPositioning (how they communicate)\nBrand promise (core values)\nTarget market (who they associate with)\nBrand awareness (how well known / popular)\n\nBest form of brand building is selling\n\nBranding is what you do after someone has bought from you.\nBrand equity - customers crossing the road to buy from you and walk past competitors."
  },
  {
    "objectID": "posts/business/marketing/marketing.html#conclusion",
    "href": "posts/business/marketing/marketing.html#conclusion",
    "title": "Marketing Notes",
    "section": "",
    "text": "Implementation is crucial\n\nKnowing and not doing is the same as not knowing.\nCommon pitfalls:\n\nParalysis by analysis - 80% out the door is better than 100% in the drawer; money loves speed.\nFail to delegate\n“My business is different” - Spend your effort figuring out how to make it work rather than why it won’t work.\n\n\nTime is not money\n\nEntrepreneurs get paid for the value they create, not the time or effort they put in.\nMaking money is a side effect of creating value.\n\nThe value of a business is the eyeballs it has access to.\nQuestions to ponder:\n\nWhat business should I be in?\nWhat technologies will disrupt it?\nHow can I take advantage of tech rather than fight it?"
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html",
    "href": "posts/business/science_of_leadership/leadership.html",
    "title": "Science of Leadership",
    "section": "",
    "text": "There are four “parts” of the brain:\n\nReptilian brain\nPaleolimbic brain\nNeolimbic brain\nPrefrontal brain\n\n\n\nAlso called the “primitive brain”. Responsible for survival instincts; only concerned with individual self, not others. When the reptilian brain is active, we feel stressed.\nName comes from the (erroneous) belief that this comes from reptilian ancestors, whereas it actually goes back even further likely to a common ancestor of all vertebrates.\nControls the three possible responses to a threat:\n\nFlight\nFight\nFreeze\n\nIn the modern world, this survival instinct kicks in when we experience stress or anxiety, even if it is not life threatening. As a response to stress, these manifest as:\n\nFlight - Anxiety and fidgeting.\nFight - Aggression and anger.\nFreeze - Helplessness.\n\nWe can recognise the reptilian brain being active when people behave in one of the three stress responses.\n\n\n\nResponsible for the survival of the group. Stems from the herd behaviour of early mammals; living together maximises survival but this social set up requires regulating.\nThe paleolimbic brain defines the position of the individual within the wider group. This manifests as self-confidence and trust.\n\n\n\nTrust vs Self-Confidence\n\n\n\nDominance isn’t just being brash; dominant people can be charming and nice, but this is a manipulation tactic and not genuine. Others are evaluated in terms of their usefulness to the dominant person. “Too much” self-confidence becomes a belief that one is entitled to more than others. In the extreme case, this leads to Narcissistic Personality Disorder.\nSubmissive people believe all of their successes are due to luck and anything that goes wrong is their fault. This is in contrast to dominant people, who will take credit for all success and blame others for all failures. In its most extreme form, submissiveness leads to melancholic depression.\nMarginal people have no trust in anything or anyone. Conspiracy nuts.\nAxial people have too much trust in others to the point of gullibility. Becomes mystical delirium in its most extreme form.\n\nDominant behaviour is the most problematic. But the paleolimbic brain is cowardly. It is rare in nature that power struggles are actually fights to the death. After encountering some resistance, the dominant type will back off.\nThe paleolimbic brain can change but only very slowly. The exception is traumatic events, which can quickly shift the paleolimbic brain “downwards”, but there are never quick shifts “upwards”.\nWe can recognise the paleolimbic brain being active when people behave territorially.\n\n\n\nResponsible for our intrinsic and extrinsic motivations; our likes and dislikes.\nWhere our memory resides. Usually “in charge” for routine tasks and uses the minimum attention span required to achieve them; our “standby mode”.\nThree layers of motivation:\n\nIntrinsic motivation. This is fixed, and is a result of genes and very early experiences. This remains fixed from 3 months old for the rest of our lives. Intrinsic motivations give us energy and joy.\nExtrinsic motivations. These continuously evolve for our whole lives. Likes and dislikes. They cost us energy and will fade if we don’t succeed. Extrinsic motivations push us to do what others expect of us, adhere to social pressures. Irritations and prejudices stem from here. Intrinsic motivations will last, but extrinsic motivations will fade away.\nObsessions. A passion that has gone beyond a tipping point. We are often blind to our obsessions.\n\n\n\n\nThis is unique to humans; ours is far bigger than any other animal. It is suited for adaptation and creativity in unknown and complex situations.\nNeolimbic brain handles the well-known and simple tasks. It is not able to see further than our previous experiences. Prefrontal brain handles the new, unknown and complex tasks.\nOr at least it should kick in for these new situations. Often creativity is limited because the neolimbic brain kicks in, meaning we can’t see any possibilities which we haven’t already experienced."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#reptilian-brain",
    "href": "posts/business/science_of_leadership/leadership.html#reptilian-brain",
    "title": "Science of Leadership",
    "section": "",
    "text": "Also called the “primitive brain”. Responsible for survival instincts; only concerned with individual self, not others. When the reptilian brain is active, we feel stressed.\nName comes from the (erroneous) belief that this comes from reptilian ancestors, whereas it actually goes back even further likely to a common ancestor of all vertebrates.\nControls the three possible responses to a threat:\n\nFlight\nFight\nFreeze\n\nIn the modern world, this survival instinct kicks in when we experience stress or anxiety, even if it is not life threatening. As a response to stress, these manifest as:\n\nFlight - Anxiety and fidgeting.\nFight - Aggression and anger.\nFreeze - Helplessness.\n\nWe can recognise the reptilian brain being active when people behave in one of the three stress responses."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#paleolimbic-brain",
    "href": "posts/business/science_of_leadership/leadership.html#paleolimbic-brain",
    "title": "Science of Leadership",
    "section": "",
    "text": "Responsible for the survival of the group. Stems from the herd behaviour of early mammals; living together maximises survival but this social set up requires regulating.\nThe paleolimbic brain defines the position of the individual within the wider group. This manifests as self-confidence and trust.\n\n\n\nTrust vs Self-Confidence\n\n\n\nDominance isn’t just being brash; dominant people can be charming and nice, but this is a manipulation tactic and not genuine. Others are evaluated in terms of their usefulness to the dominant person. “Too much” self-confidence becomes a belief that one is entitled to more than others. In the extreme case, this leads to Narcissistic Personality Disorder.\nSubmissive people believe all of their successes are due to luck and anything that goes wrong is their fault. This is in contrast to dominant people, who will take credit for all success and blame others for all failures. In its most extreme form, submissiveness leads to melancholic depression.\nMarginal people have no trust in anything or anyone. Conspiracy nuts.\nAxial people have too much trust in others to the point of gullibility. Becomes mystical delirium in its most extreme form.\n\nDominant behaviour is the most problematic. But the paleolimbic brain is cowardly. It is rare in nature that power struggles are actually fights to the death. After encountering some resistance, the dominant type will back off.\nThe paleolimbic brain can change but only very slowly. The exception is traumatic events, which can quickly shift the paleolimbic brain “downwards”, but there are never quick shifts “upwards”.\nWe can recognise the paleolimbic brain being active when people behave territorially."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#neolimbic-brain",
    "href": "posts/business/science_of_leadership/leadership.html#neolimbic-brain",
    "title": "Science of Leadership",
    "section": "",
    "text": "Responsible for our intrinsic and extrinsic motivations; our likes and dislikes.\nWhere our memory resides. Usually “in charge” for routine tasks and uses the minimum attention span required to achieve them; our “standby mode”.\nThree layers of motivation:\n\nIntrinsic motivation. This is fixed, and is a result of genes and very early experiences. This remains fixed from 3 months old for the rest of our lives. Intrinsic motivations give us energy and joy.\nExtrinsic motivations. These continuously evolve for our whole lives. Likes and dislikes. They cost us energy and will fade if we don’t succeed. Extrinsic motivations push us to do what others expect of us, adhere to social pressures. Irritations and prejudices stem from here. Intrinsic motivations will last, but extrinsic motivations will fade away.\nObsessions. A passion that has gone beyond a tipping point. We are often blind to our obsessions."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#prefrontal-brain",
    "href": "posts/business/science_of_leadership/leadership.html#prefrontal-brain",
    "title": "Science of Leadership",
    "section": "",
    "text": "This is unique to humans; ours is far bigger than any other animal. It is suited for adaptation and creativity in unknown and complex situations.\nNeolimbic brain handles the well-known and simple tasks. It is not able to see further than our previous experiences. Prefrontal brain handles the new, unknown and complex tasks.\nOr at least it should kick in for these new situations. Often creativity is limited because the neolimbic brain kicks in, meaning we can’t see any possibilities which we haven’t already experienced."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#reptilian-leader",
    "href": "posts/business/science_of_leadership/leadership.html#reptilian-leader",
    "title": "Science of Leadership",
    "section": "2.1. Reptilian Leader",
    "text": "2.1. Reptilian Leader\nLed by stress. Reactive, constantly in survival mode.\nWe can think of 3 sub-categories of reptilian leader based on their typical threat response:\n\nFight - Aggressive\nFlight - Chaotic.\nFreeze - Terrified, never making decisions."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#paleolimbic-leader",
    "href": "posts/business/science_of_leadership/leadership.html#paleolimbic-leader",
    "title": "Science of Leadership",
    "section": "2.2. Paleolimbic Leader",
    "text": "2.2. Paleolimbic Leader\nThey are territorial. Manipulative, deceitful, power games.\nWe can think of 4 sub-categories of paleolimbic leader based on trust vs assertiveness:\n\nDominant. Rule through fear, initiates aggression, shouts, intimidates and publicly humiliates.\nMarginal. Paranoid. Don’t trust anyone so micromanage.\nAxial. No boundaries. Wants to be everybody’s friend.\nSubmissive. Exchanges favours to bargain for things. Takes responsibility for every failure. Often does the work of his team, not because of a lack of trust like a marginal leader, but because they lack the skills to rally the team or impose their views.\n\nNobody is always one single type. A common combination is for a leader to be dominant with their team but submissive to their boss. Scheme and manipulate."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#neolimbic-leader",
    "href": "posts/business/science_of_leadership/leadership.html#neolimbic-leader",
    "title": "Science of Leadership",
    "section": "2.3. Neolimbic Leader",
    "text": "2.3. Neolimbic Leader\nAuthentic.\nSub-categories:\n\nPhilosopher. Optimistic, curious, seek consent. Performs well in stressful environments where they have a calming influence on the team.\nInnovator. Innovation, abstract, theoretical, interdisciplinary. Respectful and empowering for their team. Perform well in research environments.\nAnimator. Change, strong sensations. High energy and playful. Perform well in consulting where the constant change of environment is a draw.\nAdministrator. Admin and safety, planning. Love procedures and process.\nStrategist. People management. Mediate and actively solve problems. Support their team. Excel in HR, middle management, logistics and planning.\nCompetitor. Fighting and conquest. Hands on. Status driven. Perform well in sales and short-term projects.\nParticipative. Sharing and caring. Leadership with kindness, where everyone is important. Perform well in HR, coaching.\nSupportive. Selfless. Demanding towards themselves and tolerant towards others. Perform well in back office or customer service.\n\nIn practice, we are all some combination of 2-4 of these archetypes."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#prefrontal-leader",
    "href": "posts/business/science_of_leadership/leadership.html#prefrontal-leader",
    "title": "Science of Leadership",
    "section": "2.4. Prefrontal Leader",
    "text": "2.4. Prefrontal Leader\nCalm, serene, curious.\nAble to adapt and handle complex situations.\nFavours reflection over rushing decisions. Listens to the team without caving to peer pressure."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#survival-instincts",
    "href": "posts/business/science_of_leadership/leadership.html#survival-instincts",
    "title": "Science of Leadership",
    "section": "3.1. Survival Instincts",
    "text": "3.1. Survival Instincts\nThere are only really two scenarios from a survival point of view: facing danger and safe.\n\n3.1.1. Facing Danger\nWhen facing danger, there are the three familiar stress responses: fight, flight or freeze.\nStress is jokingly referred to as a “visually transmitted disease”. Research shows that seeing someone stressed is enough to make us stressed. From an evolutionary standpoint, if another person in the tribe has identified a danger then we should also be more vigilant.\nWe should be extra conscious as leaders that we are reducing the stress around us, not just picking it up and amplifying it.\nSometimes we are unknowingly the threat, causing stress among the team, particularly as a dominant boss. An example of this is setting deadlines; naively this is used to give the team some momentum. Brain research shows it actually limits our thinking and leads to worse decision making.\nA tight deadline increases stress levels causing the reptilian brain to engage, when ideally we would be using the prefrontal cortex. The more stressful the deadline, the less open we are to alternative approaches. Putting extra pressure on a team is counterproductive; the three stress responses are fight (aggression), flight (confusion) or freeze (demoralisation).\n\n\n3.1.2. Safe\nWhen safe, there are two considerations: avoid danger and avoid starvation.\n\nAvoid danger\nWe are hard-wired to avoid danger. People with “reckless genes” tend to remove themselves from the gene pool so there is an evolutionary bias towards avoiding danger. “Successful people are simply those with successful habits”.\nThis manifests as:\n\nResistance to change. To get people to embrace change you have to deactivate their reptilian brain, take away the perceived danger. Activate their limbic system by making them think for themselves; give them info and let them reach the conclusion.\nLack of initiative. Address this by taking away the danger, assure them that no one will be in trouble for bad ideas. A “flop meeting” where everybody shares things that have gone wrong. Reward initiative taking with money, attention, etc.\nNeed for repetition. Repetition tells our brain that something is true, not just an outlier. “It takes 5 (repetitions) to stay alive”.\nHerding. A good survival strategy is to eat things that we’ve seen others eat so that we know it’s safe. This is still true in “non-survival” settings. Social proof is powerful. Start with willing early adopters, then point to their success when getting others on board.\n\nHow to handle change as a manager:\n\nBe on their side\nReward initiative taking\nRepeat the message\nCreate momentum through early adopters\n\n\n\nAvoid starvation\nWe have an in-built need to feel safe from the fluctuations in food supply from a bad hunt or bad harvest. Studies show the modern day equivalent is with money.\nMoney is a double edged sword. Not enough salary means people are forced into submissiveness, activating the paleolimbic brain and resulting in frustration and resentment. Too much money has the opposite effect; still activating the paleolimbic brain but forcing them into dominance, creating greedy, entitled people."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#personality",
    "href": "posts/business/science_of_leadership/leadership.html#personality",
    "title": "Science of Leadership",
    "section": "3.2. Personality",
    "text": "3.2. Personality\nPersonality profiles are an outdated practice based on Jung’s work.\nPersonality depends on many factors: genetics determine some part of it, but which genes get activated depend on our environment, how much attention we received as a child, etc. Nature and nurture.\nIntrinsic/primary motivations. Doing the activity gives us energy; we get joy from the act of doing it regardless of results. These are lifelong activities that we enjoy.\nRefer back to the 8 neolimbic leader archetypes."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#relational-stress-management",
    "href": "posts/business/science_of_leadership/leadership.html#relational-stress-management",
    "title": "Science of Leadership",
    "section": "3.3. Relational Stress Management",
    "text": "3.3. Relational Stress Management\nA person may move between these stress modes over time when in the “facing danger” frame of mind, but it is helpful to recognise which mode they are in and address it accordingly.\n\n3.3.1. Flight / Flee\nThe signs:\n\nHands and feet - Contained movements: Fidgeting, nail biting, leg shaking, tapping, fiddling.\nEyes - Erratic eye movement and avoiding eye contact. Looking for an escape route.\nFast breathing and sweating.\n\nHow not to handle it:\n\nTelling them to calm down doesn’t help. This will be interpreted as a further restriction of their options, causing them to panic more.\nDon’t ask closed questions (“yes” or “no”). Their answers will most likely be evasive (“I don’t know”).\nDo not shout or threaten. Reminding them of the negative consequences or deadlines makes things worse.\nJudging and moralising.\n\nHow to handle it:\n\nAsk open questions. This helps remind them that they are free to choose and do have options. If they are having trouble organising their thoughts you can suggest some options.\nDe-dramatise and use humour. Put the situation into perspective.\nInvite them to take a walk with you. Their instinct is to flee and this gives them the option of doing that.\n\n\n\n3.3.2. Fight\nThe signs:\n\nStance - Standing tall\nVoice - Raising their voice\nEyes - Squinting to focus on “the enemy”\nNeck and jaw clench\n\nHow not to handle it:\n\nDon’t overpower them by yelling harder. The fight will escalate.\nDon’t undermine their authority. Don’t interrupt or laugh at them.\nDon’t annoy them. They will be impatient so being slow will anger them.\n\nHow to handle it:\n\nListen without interruption or resistance. It takes two to fight so if you don’t engage the fight won’t happen.\nTake responsibility, don’t make excuses. If you weren’t to blame you can say “I’m sorry if what I did made you feel that way. That wasn’t my intention”. “If I were you I’d be angry too”.\nOffer solutions\n\n\n“You don’t have to attend every argument you’re invited to”.\n\nPick your battles. Some fights aren’t worth having.\n\n\n3.3.3. Freeze\nThe signs:\n\nMovement - They literally stop moving; they drop their jaw or put hands in front of their mouth. This stops the arrival of oxygen to the lungs in an attempt to slow the heart rate.\nShoulders and head hang\nEyes - Looking down\nCover face with hands\nVoice - Lower their voice\nSpeech - Talk slower and use smaller sentences and words.\nSighs and maybe even tears.\n\nHow not to handle it:\n\nDon’t shake them up, tell them to get a grip etc. The frozen person doesn’t want to be autonomous.\n\nHow to handle it:\n\nUnderstand their POV\nMirror their pose\nRespect their silence and give them space\nDefine small objectives together\n\nTo get them out of their reptilian brain mode we need to take away the perceived danger. In freeze mode, this means offering them shelter."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#socratic-questioning",
    "href": "posts/business/science_of_leadership/leadership.html#socratic-questioning",
    "title": "Science of Leadership",
    "section": "4.1. Socratic Questioning",
    "text": "4.1. Socratic Questioning\nIf we encounter unacceptable behaviour, we can address it head on and tell someone not to do that. But it doesn’t change their behaviour or perceptions. It just means that they will continue with their belief system, but be more careful about whom they share their comments with and when; they will learn to hide it better but they won’t learn to be a better person.\nThe Socratic questioning technique can help challenge people’s belief systems. This needs to be one-on-one; others can’t be listening as this needs to be open and honest. Invite the person somewhere quiet. It is important to stay neutral and not express judgement. Explain that you overheard XYZ comment and you want to address it. From then on, just act as a mirror to their responses, question what they say. Let them reach the conclusion themselves and eventually apologise.\nMaking crass remarks is usually the paleolimbic brain in charge. The Socratic method forces the prefrontal brain to kick in."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#extrinsic-motivations",
    "href": "posts/business/science_of_leadership/leadership.html#extrinsic-motivations",
    "title": "Science of Leadership",
    "section": "4.2. Extrinsic Motivations",
    "text": "4.2. Extrinsic Motivations\nEmotional feelings of hurt generate the same brain response as physical pain. We have evolved to be deeply social animals and this gives rise to our extrinsic or secondary motivations. This kicks in at the neolimbic level to help us come together as a group and adapt to each other.\nThe more alike we are, the more likely we were to be accepted into the group. This is why rejection hurts so much - it’s a warning mechanism that we are in physical danger, isolated from the safety of the group.\nThe opinions of others mattered and we adapted our behaviour to the norms of the group.\nCharacteristics of extrinsic motivations:\n\nVariable. They change over time or depending on the group.\nCost energy. They don’t come naturally and require intentional work.\nResults-driven. If the behaviour is successful, the motivation is reinforced, otherwise it fades away.\n\nWhen hiring people, it is helpful to determine whether the role fits a person’s intrinsic or extrinsic motivations. A successful way to do this is to match the job description to a personality type (or two) and ask questions to determine whether a person shows traits of that personality type.\nAnother case is when we change somebody’s role to include more tasks which are not aligned with their intrinsic motivations. Research shows that only 30% of the job needs to be aligned with our intrinsic motivations in order for their overall job to feel enjoyable. (We have a high tolerance for bitch work.) But obviously we should aim for higher than 30%."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#employee-engagement",
    "href": "posts/business/science_of_leadership/leadership.html#employee-engagement",
    "title": "Science of Leadership",
    "section": "4.3. Employee Engagement",
    "text": "4.3. Employee Engagement\nThere are two ingredients to achieve employee engagement:\n\nOxytocin makes sure the team bonds.\nDopamine makes sure each individual enjoys their work.\n\n\n4.3.1. Oxytocin\nOxytocin is the love hormone and regulates our relaxation, trust and psychological stability. It can neutralise the negative effects of stress by lowering cortisol levels. It causes our team to be more cohesive, less stressed, help each other more.\nCaring and sharing is the key to releasing oxytocin. Collaborative projects, create interdependencies, promote knowledge sharing. Give them shared responsibility and shared rewards.\n\n\n4.3.2. Dopamine\nHormone responsible for reward and pleasure.\nIt fades away and we become used to a stimulus - if an activity (or drug) gives us a dopamine hit now, then next time it will give slightly less. Or from another perspective, we will need more of the stimulus to get the same dopamine hit next time. This is the key behind addiction.\nAnticipation of a stimulus triggers the dopamine hit, not just the stimulus itself, so anticipation is a key part. Hire based on intrinsic motivations so that people anticipate a dopamine hit from doing their job.\nThe job needs to evolve to keep things interesting and keep releasing dopamine. Adapt objectives. Set goals over shorter time frames to give a steady stream of dopamine hits. Celebrate achievements."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#red-flags",
    "href": "posts/business/science_of_leadership/leadership.html#red-flags",
    "title": "Science of Leadership",
    "section": "4.4.Red Flags",
    "text": "4.4.Red Flags\n\n4.4.1. Instant Gratification\nThe ability to delay gratification is a strong predictor of success in life.\nDelaying gratification is about more than just sheer willpower. It depends on our upbringing and our level of trust. If you believe the world is stable and trustworthy, you will feel more comfortable sacrificing now and be confident that you will see a reward later. If you see the world as hostile, you will prioritise instant, certain rewards.\nWe need to answer the following questions about our team:\n\nHow many of your team members understand why they are working? Do they know the vision?\nHow important is that vision to them?\nDo they trust your capabilities to achieve that vision?\n\nRepetition is key. We can’t just say it once and expect them to believe it. What the brain hears repeated becomes true. Repetition is key. Repetition is key.\nThe manager is part of the team, not a separate entity.\n\n\n4.4.2. Self Evaluation\nThe Dunning-Kruger effect: ignorance begets confidence, not knowledge.\nIncompetent people overestimate their skills. Competent people underestimate their skills.\nHiring and promotions are susceptible to being gamed by overconfident incompetent people. Micromanagement is another effect if a clueless manager thinks he knows more than his team.\nThose more susceptible to overconfidence:\n\nAt a paleolimbic level this is dominant and axial behaviour\nAt a neolimbic level this is extroverted personalities: philosopher, animator, strategist, participative\n\nThe opposite is true for those susceptible to underestimating their abilities.\nPeer review and 360 feedback helps combat the Dunning-Kruger effect. Measurable standards help too. Check references when hiring.\n\n\n4.4.3. Workaholism\nObsessions vs passions.\nPassion is about trying to repeat a pleasant experience. Obsession is about trying to avoid an unpleasant experience.\nWe overcompensate by behaving in an obsessive way. This leads to workaholism. It is easy to think this is desirable in the workplace, but we should seek to avoid it. It is unsustainable, leads to resentment in that employee and others in the team, and leads to burnout. It is an indication of more deep-rooted problems in that person. They are fleeing something else.\nAn obsession is different from an intrinsic motivation because:\n\nThere is no sense of reward when achieving something, just the immediate need to do even more. Filling a bottomless void.\nGenerates strong emotional reactions\nEnergy draining\n\nHow to handle workaholism in an employee?\n\nTake away the object of obsession, i.e. tell them to take a holiday. If they object, they likely are addicted.\n“Blow hot and cold” - we value and appreciate you, but we also prioritise balance and want to place a clear limit."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#the-hawthorne-experiment",
    "href": "posts/business/science_of_leadership/leadership.html#the-hawthorne-experiment",
    "title": "Science of Leadership",
    "section": "5.1. The Hawthorne Experiment",
    "text": "5.1. The Hawthorne Experiment\nA team is not the sum of individuals.\nPrevious neuroscience research on productivity focused on the individual. The Hawthorne experiments studied productivity in a social setting. The conclusions were that the environment can have as strong an impact on individual performance as innate individual ability.\n\n\n\n\n\n\nThe Hawthorne Effect\n\n\n\nIndividuals alter their behavior or performance when they know they are being observed. This is independent of any actual intervention being applied.\n\n\nWorking in a group leads all members to work to that standard, which is oftentimes higher than any externally imposed target standard set by management. Let people work in teams and rely on each other.\nWe work differently in a group than we would as individuals. Sometimes this means 1+1=3 but other times it means 1+1=1."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#the-conformity-experiment",
    "href": "posts/business/science_of_leadership/leadership.html#the-conformity-experiment",
    "title": "Science of Leadership",
    "section": "5.2. The Conformity Experiment",
    "text": "5.2. The Conformity Experiment\nIn a group setting when asked to say which of 3 lines is the same length as the example line, 5 stooges and 1 real person answered in sequence. The stooges all gave the same wrong answer and 37% of the time the real person agreed despite it being clearly wrong.\n\n\n\n\n\n\nThe Conformity Effect\n\n\n\nIndividuals may trust the group over their own knowledge, even when the group is clearly wrong.\n\n\nImplications:\n\nDon’t assume “yes” just because no one says “no”.\nDon’t ask “any questions?“. Ask “who has the first question?”.\nDon’t use meetings to generate ideas, ask feedback or get buy-in.\nInteract on both individual-level and group-level.\nHave everyone prepare BEFORE meetings.\nFear reinforces the conformity effect."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#fear-of-authority",
    "href": "posts/business/science_of_leadership/leadership.html#fear-of-authority",
    "title": "Science of Leadership",
    "section": "5.3. Fear of Authority",
    "text": "5.3. Fear of Authority\nFear of authority is reinforced from an early age, and we are conditioned to blindly comply.\nThe Milgram electric shock experiment is a good example of conformity to authority. Participants delivered painful electric shocks to unseen participants just because somebody in a white coat told them to proceed.\nIn the Stanford prison experiment, participants were split into guards and prisoners. Without prompting, the guards adopted heavy-handed tactics and abused the prisoners. The 2 week experiment was cut short early after 6 days due to fears around the prisoners’ safety.\nPut normal people in an abnormal context and we can make them do almost anything.\n\n\n\n\n\n\nAuthority vs Leadership\n\n\n\nAuthority is less effective than leadership. It triggers the fear response which encourages conformity; safety in numbers.\nLeadership is about empowering people. Authority is the opposite.\n\n\nImplications:\n\nAuthority forces people to comply, but at the expense of any critical thinking.\nAuthority negatively effects autonomy, responsibility, creativity, proactivity\nAuthority generates resentment, passive aggression, high levels of stress\nPower corrupts"
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#the-pygmalion-effect",
    "href": "posts/business/science_of_leadership/leadership.html#the-pygmalion-effect",
    "title": "Science of Leadership",
    "section": "5.4. The Pygmalion Effect",
    "text": "5.4. The Pygmalion Effect\nThe Pygmalion effect is also known as the self-fulfilling prophecy. It is a psychological phenomenon where high expectations lead to improved performance, and low expectations lead to diminished performance.\n\n\n\n\n\n\nThe Pygmalion Effect\n\n\n\nHow we think about others influences how they perform.\n\n\nImplications:\n\nManager expectations determine performance and career progress\nGood managers create high performance expectations\nWe do what we believe we’re expected to do\n\nConditions:\n\nYou have to mean it. The high expectations need to be genuine.\nExpectations should be realistic (not just high). If someone repeatedly tried and fails to meet expectations then motivation drops. Motivation (y axis) vs probability of success (x-axis). Upside down parabola, motivation peaks when p(success)=0.5 - too easy or too hard are both demotivating.\nExpectations are linked to how managers see themselves.\nYoung people are most impacted.\n\nMotivation peaks when P(success) = 0.5 - too easy or too hard are both demotivating.\n\n\n\nStrength of Motivation vs Probability of Success\n\n\nConclusions:\n\nHire intrinsically motivated people.\nCommunicate our high expectations repeatedly.\nAdapt their goals along the way.\nDevelop our own training skills."
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html",
    "href": "posts/business/public_speaking/public_speaking.html",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "Notes from reading “Ted Talks: The offical TED guide to public speaking” by Chris Anderson.\n\n\n\n\nPresentation literacy is its own skill. Your goal is to be you, not a version of someone you like.\n\n\n\nIdeas are all that matter in the talk. This can be a novel idea, or a novel story or presentation of an existing idea.\nYour goal is to recreate the core idea inside your audience’s mind. The language you use must be shared by the speaker and listener for it to have this effect. Focus on what you can give to the audience.\nVisualise the talk as a journey that you guide the user through. Start where the audience starts. Don’t make any impossible leaps or unexpected changes of direction.\n\nWhat issues matter the most\nHow are they related?\nHow can they be easily explained?\nWhat are the key open questions? Controversies?\n\n\n\n\nFour talk styles to AVOID.\n\nSales Pitch\n\nYou should give not take.\nGenerosity evokes a response.\n\nRamble\n\nInsults the audience; suggests that their time doesn’t matter to you.\n\nOrganisation bore\n\nYour company’s structure is boring.\nFocus on the work and ideas, not the company, products or individuals.\n\nInspiration tryhard\n\nAll style and no substance.\nComes across as manipulative.\nInspiration has to be earned. “Like love you don’t get it by pursuing it”.\nInspiration is the response to authenticity, courage, wisdom, selflessness.\n\n\n\n\n\nThe throughline is a connecting theme that ties the narrative elements\n\nA focused, precise, unexpected idea\nNot the same as a theme\nYou should be able to encapsulate it in &lt;15 words\n\nTrace the path the journey will take. If the audience knows where you’re going, it will be easier to follow.\nCut down the talk to keep the throughline focused.\n\nDON’T just keep all the content but say it in less detail. Overstuffed = underexplained\nCut back the range of topics. Say less with more impact.\n“The power of the deleted word”.\nAn 18 minute time limit is short enough to keep an audience’s attention, long enough to cover enough ground and precise enough to be taken seriously.\n\nThe overall structure can be thought of in 3 parts: What? So what? What now? In more detail, this is:\n\nIntro - what will be covered\nContext - why this issue matters\nMain concepts\nPractical implications\n\nTalk about ideas not issues. Write for an audience of one.\nChecklist:\n\nIs this a topic I’m passionate about? Do I know enough to be worth the audience’s time? Do I have credibility?\nDoes it inspire curiosity?\nWill knowing this make a difference to the audience?\nIs the talk a gift or an ask?\nIs the information fresh?\nCan the topic be covered in enough detail with examples in the time slot?\nWhat are the 15 words that encapsulate the talk?\nWould those 15 words persuade someone they’re interested in hearing the talk?\n\n\n\n\n\n\n\nA human connection is required before you can inform, persuade or inspire. Knowledge has to be pulled in, not pushed in. Be AUTHENTIC.\nDos\n\nEye contact.\nVulnerability, but authentic.\nHumour signals to the group that you’ve all connected.\n\nDon’ts\n\nEgo breeds contempt.\nAvoid tribal topics/language, e.g. politics.\n\n\n\n\nStorytelling helps people to imagine and dream -&gt; empathise -&gt; care\nKey elements:\n\nCharacter\nTension\nRight level of detail\nSatisfying resolution\n\nIf you’re telling a story, know why you’re telling it. The goal is to give, not boost your ego.\nConnect the dots enough, but don’t force-feed the conclusion.\n\n\n\nSteps:\n\nStart where the audience is. No assumed knowledge or jargon.\nSpark curiosity. Create a knowledge gap that the listener wants to fill.\nIntroduce new concepts one-by-one. Name each concept.\nMetaphors and analogies. Reveal the “shape” of new concepts.\nExamples confirm understanding.\n\nKnowledge is built hierarchically. Explain the structure and where each concept fits. Convert a web of ideas into a string of words.\nCurse of knowledge. Revisit assumed knowledge.\nMake clear what the concept isn’t. An explanation is a small mental model in a large space of possibilities. Reduce the size of that space.\n\n\n\nDemolish the listener’s existing knowledge and rebuild it; replace it with something better. Genius is something that comes to you, not something you are.\nAn “intuition pump” is an example or metaphor that makes the conclusion more plausible. Not a rigorous argument, but nudges the listener in the right direction. Prime with emotion or story, then persuade with reason.\nTurn the audience into detectives. Tell a story. Start with a big mystery, traverse the possible solutions until only one plausible conclusion remains.\n\n\n\nThree approaches to revelatory talks:\n\nThe wonder walk\n\nReveal a succession of inspiration images\nObvious throughline with accessible language and silence\nThe message should be “how intriguing is this” not “look what I achieved”\n\nDemo\n\nInitial tease, background context, demo, implications\n\nDreamscape\n\nCreate a world that doesn’t exist but someday might\nPaint a bold picture of an alternative future\nMake others desire that future\nEmphasise human values not just clever tech, otherwise the audience might freak out at the possible negative implications of the technology\n\n\n\n\n\n\n\n\nSlides can distract. If you do use them, make sure they add something.\nVisuals should:\n\nReveal - set context, prime, BAM!\nExplain - one idea per slide with a clickbait headline not a summary. Highlight the point you’re making.\nDelight - show impressive things and let them speak for themselves. Don’t need to explain every image/slide.\n\nDon’t let the slides be spoilers for what you’re about to say. The message loses its impact, it’s not news anymore.\n\n\n\nThe goal is to have a set structure that you speak naturally and authentically about.\nBoth approaches have the same result:\n\nScript and memorise until natural, or\nFreestyle around bullet points and rehearse until structure is set\n\nBeing read to and being talked to are very different experiences\n\nDictating the speech rather than writing it can help make sure it comes across how you would speak rather than how you would write.\nDon’t end up in the uncanny valley between reading and speaking\nIn some cases, reading is powerful if it’s clear that this is a poetic, written piece. Abandon the script for an impactful finale.\n\nWhat you are saying matters more than the exact word choice.\nRehearse your impromptu remarks.\n\n\n\nPractice speaking by speaking!\nRehearse in phases:\n\nEditing and cutting\nPacing and timing\nDelivery\n\nAim to use &lt;90% of the time limit. results in tighter writing, and time to riff and enjoy the reaction.\n\n\n\nYou have the audience’s attention at the start, then it’s yours to lose. Opener has to capture attention and keep it. 2 stages:\n\n10 seconds to capture attention\n1 minute to hold it\n\nOpening techniques:\n\nDrama\nCuriosity - more specific questions are more intriguing\nCompelling slide/image - “the next image changed my life”\nTense but don’t give away - show where you’re going but save the reveal\n\nThe closer dictates how the whole talk will be remembered.\nClosing techniques:\n\nCamera pullback - big picture and implications\nCall to action\nPersonal commitment\nInspiring values/vision\nEncapsulation - neatly reframe the main idea\nNarrative symmetry - callback to opener\nLyrical inspiration - poetic conclusion\n\n\n\n\n\n\n\n\nChoose an outfit early\nDress like the audience but smarter\nDress for the people in the back row\n\n\n\n\n\nEmbrace the nerves and draw attention to it. Vulnerability humanises you.\nPick friendly faces in the audience and speak to them.\nBackup plan - have a story ready to fill in during unexpected technical issues\nFocus on the message - “THIS MATTERS”\n\n\n\n\nVisual barriers between the speaker and audience can create a sense of authority but at the expense of a human connection. If they can see you, you’re vulnerable. If you’re vulnerable, they can connect.\nReferring to notes is fine if done sparingly and unambiguously. It humanises you if done honestly. It appears sneaky and dishonest if you try to do it secretly.\n\n\n\nSpeaking style adds another stream of input parallel to the words themselves. It tells the listener how they should interpret the words.\nSix voice tools. Vary each of them depending on the meaning and emotion.\n\nVolume\nPitch\nPace\nTimbre\nTone\nProsody (singsong rise and fall)\n\nSpeed should be a conversational pace, approx 130-170 words per minute.\n\nPeople often worry about speaking too fast\nSpeaking too slow is a more common problem (overcorrection?)\nUnderstanding outpaces articulation, the listener is “dying of word starvation”\n\nSpeak, don’t orate.\nBody language\n\nStand and move intentionally.\nStop to make a point then walk to the next point.\n\n\n\n\nAdd too many ingredients and you risk losing attention. What you say can just be signposts for what you show - set it up, show it, shut up.\n\n\n\n\n\n\nKnowledge of specific facts is becoming more specialised and commoditised. Understanding how everything fits together is becoming broader, more unified.\nConcepts of specialised knowledge are outdated, stemming from industrial age thinknig. Modern thinking values:\n\nContextual knowledge\nCreating knowledge\nUnderstanding of humanity\n\nAs people become more interconnected, innovation becomes crowd-accelerated.\nHappiness is finding something bigger than you and dedicating your life to it. Nudge the world - give it questions to spark conversations. “The future is not yet written, we are all collectively in the process of writing it”"
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html#foundation",
    "href": "posts/business/public_speaking/public_speaking.html#foundation",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "Presentation literacy is its own skill. Your goal is to be you, not a version of someone you like.\n\n\n\nIdeas are all that matter in the talk. This can be a novel idea, or a novel story or presentation of an existing idea.\nYour goal is to recreate the core idea inside your audience’s mind. The language you use must be shared by the speaker and listener for it to have this effect. Focus on what you can give to the audience.\nVisualise the talk as a journey that you guide the user through. Start where the audience starts. Don’t make any impossible leaps or unexpected changes of direction.\n\nWhat issues matter the most\nHow are they related?\nHow can they be easily explained?\nWhat are the key open questions? Controversies?\n\n\n\n\nFour talk styles to AVOID.\n\nSales Pitch\n\nYou should give not take.\nGenerosity evokes a response.\n\nRamble\n\nInsults the audience; suggests that their time doesn’t matter to you.\n\nOrganisation bore\n\nYour company’s structure is boring.\nFocus on the work and ideas, not the company, products or individuals.\n\nInspiration tryhard\n\nAll style and no substance.\nComes across as manipulative.\nInspiration has to be earned. “Like love you don’t get it by pursuing it”.\nInspiration is the response to authenticity, courage, wisdom, selflessness.\n\n\n\n\n\nThe throughline is a connecting theme that ties the narrative elements\n\nA focused, precise, unexpected idea\nNot the same as a theme\nYou should be able to encapsulate it in &lt;15 words\n\nTrace the path the journey will take. If the audience knows where you’re going, it will be easier to follow.\nCut down the talk to keep the throughline focused.\n\nDON’T just keep all the content but say it in less detail. Overstuffed = underexplained\nCut back the range of topics. Say less with more impact.\n“The power of the deleted word”.\nAn 18 minute time limit is short enough to keep an audience’s attention, long enough to cover enough ground and precise enough to be taken seriously.\n\nThe overall structure can be thought of in 3 parts: What? So what? What now? In more detail, this is:\n\nIntro - what will be covered\nContext - why this issue matters\nMain concepts\nPractical implications\n\nTalk about ideas not issues. Write for an audience of one.\nChecklist:\n\nIs this a topic I’m passionate about? Do I know enough to be worth the audience’s time? Do I have credibility?\nDoes it inspire curiosity?\nWill knowing this make a difference to the audience?\nIs the talk a gift or an ask?\nIs the information fresh?\nCan the topic be covered in enough detail with examples in the time slot?\nWhat are the 15 words that encapsulate the talk?\nWould those 15 words persuade someone they’re interested in hearing the talk?"
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html#talk-tools",
    "href": "posts/business/public_speaking/public_speaking.html#talk-tools",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "A human connection is required before you can inform, persuade or inspire. Knowledge has to be pulled in, not pushed in. Be AUTHENTIC.\nDos\n\nEye contact.\nVulnerability, but authentic.\nHumour signals to the group that you’ve all connected.\n\nDon’ts\n\nEgo breeds contempt.\nAvoid tribal topics/language, e.g. politics.\n\n\n\n\nStorytelling helps people to imagine and dream -&gt; empathise -&gt; care\nKey elements:\n\nCharacter\nTension\nRight level of detail\nSatisfying resolution\n\nIf you’re telling a story, know why you’re telling it. The goal is to give, not boost your ego.\nConnect the dots enough, but don’t force-feed the conclusion.\n\n\n\nSteps:\n\nStart where the audience is. No assumed knowledge or jargon.\nSpark curiosity. Create a knowledge gap that the listener wants to fill.\nIntroduce new concepts one-by-one. Name each concept.\nMetaphors and analogies. Reveal the “shape” of new concepts.\nExamples confirm understanding.\n\nKnowledge is built hierarchically. Explain the structure and where each concept fits. Convert a web of ideas into a string of words.\nCurse of knowledge. Revisit assumed knowledge.\nMake clear what the concept isn’t. An explanation is a small mental model in a large space of possibilities. Reduce the size of that space.\n\n\n\nDemolish the listener’s existing knowledge and rebuild it; replace it with something better. Genius is something that comes to you, not something you are.\nAn “intuition pump” is an example or metaphor that makes the conclusion more plausible. Not a rigorous argument, but nudges the listener in the right direction. Prime with emotion or story, then persuade with reason.\nTurn the audience into detectives. Tell a story. Start with a big mystery, traverse the possible solutions until only one plausible conclusion remains.\n\n\n\nThree approaches to revelatory talks:\n\nThe wonder walk\n\nReveal a succession of inspiration images\nObvious throughline with accessible language and silence\nThe message should be “how intriguing is this” not “look what I achieved”\n\nDemo\n\nInitial tease, background context, demo, implications\n\nDreamscape\n\nCreate a world that doesn’t exist but someday might\nPaint a bold picture of an alternative future\nMake others desire that future\nEmphasise human values not just clever tech, otherwise the audience might freak out at the possible negative implications of the technology"
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html#preparation-process",
    "href": "posts/business/public_speaking/public_speaking.html#preparation-process",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "Slides can distract. If you do use them, make sure they add something.\nVisuals should:\n\nReveal - set context, prime, BAM!\nExplain - one idea per slide with a clickbait headline not a summary. Highlight the point you’re making.\nDelight - show impressive things and let them speak for themselves. Don’t need to explain every image/slide.\n\nDon’t let the slides be spoilers for what you’re about to say. The message loses its impact, it’s not news anymore.\n\n\n\nThe goal is to have a set structure that you speak naturally and authentically about.\nBoth approaches have the same result:\n\nScript and memorise until natural, or\nFreestyle around bullet points and rehearse until structure is set\n\nBeing read to and being talked to are very different experiences\n\nDictating the speech rather than writing it can help make sure it comes across how you would speak rather than how you would write.\nDon’t end up in the uncanny valley between reading and speaking\nIn some cases, reading is powerful if it’s clear that this is a poetic, written piece. Abandon the script for an impactful finale.\n\nWhat you are saying matters more than the exact word choice.\nRehearse your impromptu remarks.\n\n\n\nPractice speaking by speaking!\nRehearse in phases:\n\nEditing and cutting\nPacing and timing\nDelivery\n\nAim to use &lt;90% of the time limit. results in tighter writing, and time to riff and enjoy the reaction.\n\n\n\nYou have the audience’s attention at the start, then it’s yours to lose. Opener has to capture attention and keep it. 2 stages:\n\n10 seconds to capture attention\n1 minute to hold it\n\nOpening techniques:\n\nDrama\nCuriosity - more specific questions are more intriguing\nCompelling slide/image - “the next image changed my life”\nTense but don’t give away - show where you’re going but save the reveal\n\nThe closer dictates how the whole talk will be remembered.\nClosing techniques:\n\nCamera pullback - big picture and implications\nCall to action\nPersonal commitment\nInspiring values/vision\nEncapsulation - neatly reframe the main idea\nNarrative symmetry - callback to opener\nLyrical inspiration - poetic conclusion"
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html#on-stage",
    "href": "posts/business/public_speaking/public_speaking.html#on-stage",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "Choose an outfit early\nDress like the audience but smarter\nDress for the people in the back row\n\n\n\n\n\nEmbrace the nerves and draw attention to it. Vulnerability humanises you.\nPick friendly faces in the audience and speak to them.\nBackup plan - have a story ready to fill in during unexpected technical issues\nFocus on the message - “THIS MATTERS”\n\n\n\n\nVisual barriers between the speaker and audience can create a sense of authority but at the expense of a human connection. If they can see you, you’re vulnerable. If you’re vulnerable, they can connect.\nReferring to notes is fine if done sparingly and unambiguously. It humanises you if done honestly. It appears sneaky and dishonest if you try to do it secretly.\n\n\n\nSpeaking style adds another stream of input parallel to the words themselves. It tells the listener how they should interpret the words.\nSix voice tools. Vary each of them depending on the meaning and emotion.\n\nVolume\nPitch\nPace\nTimbre\nTone\nProsody (singsong rise and fall)\n\nSpeed should be a conversational pace, approx 130-170 words per minute.\n\nPeople often worry about speaking too fast\nSpeaking too slow is a more common problem (overcorrection?)\nUnderstanding outpaces articulation, the listener is “dying of word starvation”\n\nSpeak, don’t orate.\nBody language\n\nStand and move intentionally.\nStop to make a point then walk to the next point.\n\n\n\n\nAdd too many ingredients and you risk losing attention. What you say can just be signposts for what you show - set it up, show it, shut up."
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html#reflection",
    "href": "posts/business/public_speaking/public_speaking.html#reflection",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "Knowledge of specific facts is becoming more specialised and commoditised. Understanding how everything fits together is becoming broader, more unified.\nConcepts of specialised knowledge are outdated, stemming from industrial age thinknig. Modern thinking values:\n\nContextual knowledge\nCreating knowledge\nUnderstanding of humanity\n\nAs people become more interconnected, innovation becomes crowd-accelerated.\nHappiness is finding something bigger than you and dedicating your life to it. Nudge the world - give it questions to spark conversations. “The future is not yet written, we are all collectively in the process of writing it”"
  },
  {
    "objectID": "projects_section/xai_research/xai.html",
    "href": "projects_section/xai_research/xai.html",
    "title": "Explainable AI in Healthcare",
    "section": "",
    "text": "This is a research project I completed which aimed to quantify the confidence we should have in a trained model applied ot our data set.\nThe full paper is available here.\nSlides from a presentation I gave at the Nuffield Department of Orthopaedics, Rheumatology and Musculoskeletal Sciences (NDORMS) at the University of Oxford are available here.\n\n\n\n Back to top"
  },
  {
    "objectID": "projects_section/tradeintel/tradeintel.html",
    "href": "projects_section/tradeintel/tradeintel.html",
    "title": "TradeIntel",
    "section": "",
    "text": "This is a robo-advisor app to give tailored stock portfolio recommendations.\nThe user can input high-level preferences like their risk tolerance, industry preferences, and how closely they would like to follow the broader market. We then perform a portfolio optimisation process to recommend a robust portfolio based on those criteria.\nThis is available on the App Store\n\n\n\n\n Back to top"
  },
  {
    "objectID": "projects_section/bjj/bjj.html",
    "href": "projects_section/bjj/bjj.html",
    "title": "Brazilian Jiu Jitsu Taxonomy",
    "section": "",
    "text": "This is a (work-in-progress) interactive React app I made to:\n\nKeep track of my BJJ notes\nPractice some web development\n\nCheck it out here.\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html",
    "title": "Neuroscience for Parents",
    "section": "",
    "text": "We contribute in two ways to our children:\n\nTheir genes (nature)\nTheir environment (nurture) which determines which of their genes activate.\n\nSome assorted facts and thoughts on neuroscience applied to parenting:\n\nBeing a parent physically changes our brain and limits neurogenesis. The brain of pregnant women shrinks during pregnancy (and expands back to normal size afterwards).\nThe brain of a neglected child is physically different from a healthy child.\nStudies show that couples with kids are less happy than couples without kids. But once the kids move out, couples that had kids are happier. It’s worth it in the long run!\nKids are not mini-adults and we shouldn’t treat them as if they were. Their brains are physically different with more grey matter.\n\nThe course largely follows the Neurocognitive and Behavioural Approach (NBA).\nThere are four “parts” of the brain:\n\nReptilian brain\nPaleolimbic brain\nNeolimbic brain\nPrefrontal brain"
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#its-function",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#its-function",
    "title": "Neuroscience for Parents",
    "section": "1.1. Its Function",
    "text": "1.1. Its Function\nThis “primitive brain” is responsible for our survival instincts: the fight/flight/freeze response. Specifically our individual survival.\nThe name comes from the (erroneous) belief that this part of the brain has its root in a reptilian ancestor. It actually likely goes back even further to a common ancestor of all vertebrates including amphibians and fish, as they all have this brain structure.\nThe reptilian brain still kicks in in our modern world when there aren’t so many life and death situations. This is stress, and the survival instincts lead to our stress responses:\n\nFight - Aggression\nFlight - Anxiety\nFreeze - Helplessness"
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#the-reptilian-baby",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#the-reptilian-baby",
    "title": "Neuroscience for Parents",
    "section": "1.2. The Reptilian Baby",
    "text": "1.2. The Reptilian Baby\nWhen a baby is born, it is not a blank slate. It’s actually the opposite, a baby has far more neural connections than an adult, and over the first 3 months of its life it deletes connections based on its environment.\nBabies can only cry to communicate, so a baby’s cry could be any one of the three stress responses:\n\nFight: “I’m hungry, give me food”\nFlight: “I’m wet, get me out of here”\nFreeze: “Where’s daddy gone?”\n\nOur response needs to be appropriate for the stress type. Comforting a baby will work well for a “freeze” response, but if they’re in “fight” mode the best thing to do is address the root cause and feed them, otherwise they will just cry harder.\nAt this early stage when the baby is driven purely by the survival instincts of the reptilian brain, it is best to give the baby what they want.\nEvents can be especially traumatic for children because their prefrontal brain, which would help work through emotions and trauma, isn’t developed yet and doesn’t fully mature until 24 years old.\nHow can you recognise when the reptilian brain is active? They are in one of the three stress responses:\n\nFight - Tension in jaws, arms and legs, stamping feet\nFlee - Movement and avoiding confrontation, avoiding eye contact\nFreeze - Tears or apathy\n\nHow to deal with each stress response?\nIn all cases, show them that you are on their side. Ask them if they want a hug and if they say no assure them that a hug is still an option later. Hugs release oxytocin, good for bonding and stress relief.\n\nFight - Empathise with their outrage. Acknowledge their feelings if you are the cause of their anger.\nFlee - Give them options, show them you are on their side.\nFreeze - Give support and sympathy.\n\nWhen the reptilian brain is active, hug them and give them what they want. But don’t do this if the paleolimbic brain is active."
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#movement",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#movement",
    "title": "Neuroscience for Parents",
    "section": "1.3. Movement",
    "text": "1.3. Movement\n\nKids aren’t made to sit still!\n\nThe reptilian brain may be more accurately referred to as the “primal brain”. It handles many fundamental tasks including breathing, heart rate, sleep and movement.\nThe point of the brain is to coordinate movement. Organisms which are alive but don’t move don’t have a brain. Organisms which move do have a brain.\nThe cerebellum is 10% of the brain’s volume but has 50% of the neurons, and is responsible for balance, muscle tone and coordination.\nClimbing trees is ridiculously good for your brain. Some studies show it can boost working memory by 50%."
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#food",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#food",
    "title": "Neuroscience for Parents",
    "section": "1.4. Food",
    "text": "1.4. Food\nHunger and thirst are handled by the primal brain, specifically the hypothalamus.\nOur brain is 2% of our body’s mass but 20% of its energy intake.\nThe brain grows at a rate of 250k nerve cells per minute during pregnancy. In the first 90 days a newborn baby’s brain goes from 33% to 55% of an adult’s brain size.\nThe amount of Omega-3 in breast milk is the strongest predictor of test performance; more than income or schools.\nAlcohol dramatically slows down neurogenesis.\nSugar slows down neurogenesis and gives kids a dopamine hit, messing with their rewards structure. It also leads to memory deficiency and cognitive deficiency, and impaired impulsive control."
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#sleep",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#sleep",
    "title": "Neuroscience for Parents",
    "section": "1.5. Sleep",
    "text": "1.5. Sleep\nVery important.\nThe brain of adolescents goes through a maturation process from the back of the brain to the front called myelination which strengthens the existing connections.\nLack of sleep impacts academic performance in two ways:\n\nInability to focus in school\nMemory consolidation happens during sleep\n\nIt can also make us more aggressive; studies have shown a lack of sleep increases the size of our amygdala, the centre for fear and aggression.\nRituals are important for sleep:\n\nBath\nStory\nConsistent bedtime\n\nRegularity is as important as number of hours of sleep. You can’t just make up the sleep debt at the weekend.\nA possible confounding factor: the same self discipline required for academic success is required for good sleep hygiene.\n“Co-sleeping is bad” is a myth. We are the only mammals that don’t co-sleep regularly. The myth that they will grow up to have attachment issues is false. Actually the opposite is true: children who co-sleep grow up to have a secure attachment. The knowledge that they have a safe haven with their parents gives them the confidence to venture out and explore."
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#its-function-1",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#its-function-1",
    "title": "Neuroscience for Parents",
    "section": "2.1. Its Function",
    "text": "2.1. Its Function\nResponsible for survival of the group. Living together maximises chances of survival of the group, but this social life needs to be regulated.\nThe paleolimbic brain defines the position of the individual within the group.\nThere are two key dimensions to this:\n\nSelf-confidence\nTrust in others\n\n\n\n\nTrust vs Self-Confidence\n\n\n\nDominance isn’t just being brash; dominant people can be charming and nice, but this is a manipulation tactic and not genuine. Others are evaluated in terms of their usefulness to the dominant person. “Too much” self-confidence becomes a belief that one is entitled to more than others. In the extreme case, this leads to Narcissistic Personality Disorder.\nSubmissive people believe all of their successes are due to luck and anything that goes wrong is their fault. This is in contrast to dominant people, who will take credit for all success and blame others for all failures. In its most extreme form, submissiveness leads to melancholic depression.\nMarginal people have no trust in anything or anyone. Conspiracy nuts.\nAxial people have too much trust in others to the point of gullibility. Becomes mystical delirium in its most extreme form."
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#the-parental-instinct",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#the-parental-instinct",
    "title": "Neuroscience for Parents",
    "section": "2.2. The Parental Instinct",
    "text": "2.2. The Parental Instinct\n70% of our brain growth happens after birth. Babies are helpless and we have evolved to treat their survival like our own.\nThere is a “switch” in the limbic amygdala, which is responsible for territorial power games, that is activated when we have children. It causes us to be territorial about our kids.\nWhen it is active, there is no use trying to be rational, the person needs to calm down before being able to reason."
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#the-paleolimbic-toddler",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#the-paleolimbic-toddler",
    "title": "Neuroscience for Parents",
    "section": "2.3. The Paleolimbic Toddler",
    "text": "2.3. The Paleolimbic Toddler\nThe paleolimbic brain kicks in around 2-2.5 years old. They enter the socialisation phase; younger than this they play “next to each other” but alone, but at this stage they actually engage with each other.\nWhenever they turn into a “gremlin” that’s the paleolimbic brain in action. You can’t placate the paleolimbic brain like you do with the reptilian brain. Instead you need to assert yourself. Explanations are wasted as the paleolimbic brain won’t see reason, only territoriality.\nPick your battles and let your child win sometimes. This is important for their self-esteem."
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#self-confidence",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#self-confidence",
    "title": "Neuroscience for Parents",
    "section": "2.4. Self-Confidence",
    "text": "2.4. Self-Confidence\nFor the 10 years around 2-12 years old we start identifying our sense of self-worth and trust towards others. After this age, our level of self-confidence is relatively fixed, and can only change slowly.\nDiscipline is crucial in this stage, but extremes in either direction can be harmful:\n\nNo discipline means they learn that they can do whatever they want and use others as objects and tools to further their desires. This creates manipulative, dominant people.\nToo much discipline limits their exploration and teaches them they can’t do things alone and are not worthy of the trust of others. This creates submissive people.\n\nDefine a framework within which your child can explore, but with rules about what happens when they stray out of bounds. “Don’t point a gun if you’re not ready to shoot”. If you threaten a punishment but don’t actually follow through with it, the child’s brain sees the absence of a punishment as a reward and so this reinforces bad behaviour.\nThe child will fluctuate over these years. Just because they act dominant one day doesn’t mean you’ve raised a bully; they’re exploring their bounds and fluctuating their levels of self-confidence."
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#beat-the-bully",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#beat-the-bully",
    "title": "Neuroscience for Parents",
    "section": "2.5. Beat the Bully",
    "text": "2.5. Beat the Bully\nBullying is dominant paleolimbic behaviour. It relies on intimidation, so if the intimidation stops the cycle breaks.\nTeach the victim to stand up tall and say “No. No. No.” when their boundaries are being breached by the bully."
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#submissive-vs-introvert-vs-shy",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#submissive-vs-introvert-vs-shy",
    "title": "Neuroscience for Parents",
    "section": "2.6. Submissive vs Introvert vs Shy",
    "text": "2.6. Submissive vs Introvert vs Shy\nThese are very different behaviours but often interpreted as similar.\n\n\n\n\n\n\n\n\n\nBehaviour\nBrain region responsible\nRoot cause\nHow to handle it?\n\n\n\n\nSubmissiveness\nPaleolimbic\nThis is about obedience.\nWe should try to avoid submissiveness and let them connect.\n\n\nIntroversion\nNeolimbic\nThis is a character trait.\nFocus on what your child is good at.\n\n\nShyness\nReptilian\nThis is a fear response.\nShow them how to make friends. Invite potential friends over and make the introduction for them. Then ask the other child questions about themselves in front of your kid and let them observe. Read stories about overcoming shyness. Don’t tease or criticise the shyness."
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#teach-kids-to-trust-us",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#teach-kids-to-trust-us",
    "title": "Neuroscience for Parents",
    "section": "2.7. Teach Kids to Trust Us",
    "text": "2.7. Teach Kids to Trust Us\nThe Marshmallow Test: The ability to delay gratification is one of the strongest predictors of success.\nDelaying gratification is about more than just pure willpower. It is about how our world view was shaped in our early years.\nIf the world is seen as hostile, we will take short-term gains as we don’t trust in future rewards. If we see the world as stable and trustworthy, we are more willing to defer rewards for better future rewards.\nTo encourage this:\n\nWhen you promise something, make sure you deliver\nIf they want something, make them earn it (by cleaning their room, etc)\nWhen you warn them of something, follow through. There needs to be consequences.\n\nTeach them that action leads to reaction, both positive and negative."
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#social-pain",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#social-pain",
    "title": "Neuroscience for Parents",
    "section": "2.8. Social Pain",
    "text": "2.8. Social Pain\nOxytocin is the hormone responsible for bonding, but it also leads to the “us vs them” mentality. It reinforces in-groups and out-groups.\nFrom an evolutionary point of view, it was much safer to be part of the tribe, and rejection could even mean death. This is why rejection still hurts so much in modern society, it is deeply ingrained. Brain scans show that rejection can have a similar brain response as physical pain. There is an overlap in the neural pathways.\nAdolescents need to discover their own clan, this is a crucial part of their development. But a side effect of this is that we as parents are now in the “out-group” as they form a new in-group with friends."
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#its-function-2",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#its-function-2",
    "title": "Neuroscience for Parents",
    "section": "3.1. Its Function",
    "text": "3.1. Its Function\nResponsible for our memory and motivations. Usually runs the show when we are on autopilot, handling situations we’ve experienced before. It uses the minimum amount of energy for routine tasks to allow us to multitask.\nThree layers of motivation:\n\nIntrinsic motivations. Fixed by the time we are 3 months old. Things we enjoy that give us energy. This leads to us having 2-4 of the “personality types” detailed below.\nExtrinsic motivations. Our likes and dislikes. These change over time. They cost us energy and will fade if we don’t see success. Extrinsic motivations push us to do what others expect of us. The motivation boost from a substantial salary raise only lasts 3-6 months.\nObsessions. A passion that has gone beyond a tipping point. We will never be satisfied, this is the root of addiction. Not just the usual suspects of drugs, alcohol, etc but also workaholism, retail therapy."
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#expectations",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#expectations",
    "title": "Neuroscience for Parents",
    "section": "3.2. Expectations",
    "text": "3.2. Expectations\n\n3.2.1. The Pygmalion Effect\nHow we think about others will influence how they will perform. A self-fulfilling prophecy. People will conform to the expectations of them.\nThere have been many studies that illustrate this effect. One example is where teachers were given a list of the “high potential” students at the start of the school year, but explicitly told not to show any preferences. The students were assessed at the end of the year and those high potential students would score the highest. But that initial list of high potential students was random. The teacher’s higher expectations resulted in higher outcomes.\nThe flip side of the Pygmalion Effect is the Gollem Effect. Negative expectations lead to negative outcomes.\nEven when we try to hide our preferences from a child, they are very good at picking up on them.\n\n\n3.2.2. How Demanding Should We Be?\n\nToo high expectations causes them to be insecure, relying on external recognition/validation and have attachment issues.\nToo low expectations causes them to conform to these low expectations and underachieve.\n\nFocus on effort not results. Be demanding on effort and forgiving on results. Encourage them to work hard, not achieve a lot. One will naturally lead to the other.\nOtherwise, the risk is they will learn that if they don’t get results then they are not worthy of our love.\n\n\n3.2.3. How High Should We Put the Bar?\nDiscuss this with the child and set up the rules together. Challenge them to exceed their own expectations.\n\n\n\nMotivation vs Probability of Success\n\n\nPart of our job as the parent is to gauge their ability and set goals accordingly so that they have a probability of success in the sweet spot. Not so low that there is no change of succeeding, and not so high that the task is trivial."
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#punishments",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#punishments",
    "title": "Neuroscience for Parents",
    "section": "3.3. Punishments",
    "text": "3.3. Punishments\nOperant conditioning: Through our punishments and rewards we shape the personality of our kids.\nThis is a lifelong thing, it doesn’t stop in childhood.\nNever make a threat you won’t act upon. This damages your authority. If you say “I’m counting to three” then you have to actually do something when you reach three.\nTypes of punishment: timeouts, grounding, confiscate a toy.\nPhysical punishment is always detrimental. It reinforces the world view that “I am bigger and stronger than you, therefore I can force you to do what I want”. Not a great lesson to teach them. Can they hit smaller kids at school? Or grow up and hit smaller adults (especially women)? Or is the lesson that only adults can hit kids?\nHow to punish:\n\nConsequences have to be clear and accepted.\n\n\nYou can even let them decide - “You know you’re not supposed to hit and that was naughty. What do you think the consequence of that should be?”\nYou can always disagree with their proposed punishment, but it at least makes them think rationally about their behaviour which disengages the reptilian brain that might have caused their behaviour in the first place.\n\n\nTransgressions should always have consequences.\n\n\nYou can’t let them off the hook. From the brain’s perspective, a lack of punishment where it was anticipated is perceived the same as a reward; you get a dopamine hit. This encourages the behaviour.\nForcing consequences helps them establish a stable world view and value system.\n\n\nNo drama.\n\n\nYou don’t need to shout or get angry. Just calmly and directly enforce the consequences."
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#parenting-styles",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#parenting-styles",
    "title": "Neuroscience for Parents",
    "section": "3.4. Parenting Styles",
    "text": "3.4. Parenting Styles\n\n3.4.1. When to Intervene\nChildhood is a lab where kids are able to experiment without having to experience the full consequences that they would as an adult. This experimentation is crucial for their development. They need to experience things for themselves, both positive and negative.\nWhen parents intervene too much, they deny children the opportunity to experiment. By trying to make things easier in the short-term, we make things harder in the long-term. The subconscious message you’re sending is that they are not capable of doing things themselves, they need permission from someone else before they can do the things that interest them. This is what psychologists call “learned helplessness”.\nWe should help, just not right away.\n\n\n3.4.2. Attachment Theory\nWhat kids need most is safety and exploration. The more secure a person feels at home, the more likely they are to venture out and try new things.\nAttachment even at age 1 is a strong predictor of future attainment and relationships later in life.\nThere is no single “correct” parenting style. Lenient vs authoritarian doesn’t seem to affect the attachment level.\nIt’s ok to screw up from time to time. Children aren’t so fragile that one misstep will ruin them. The overall pattern of care needs to be reliable and predictable. We are also not the only relationship in their life. They can learn lessons from extended family, teachers, friends etc.\nTypes of attachment:\n\nSecurely attached.\n\nParents are attuned to the child’s desires and mirror their moods\nParents calm the child when they are anxious or sad, play with them when they are happy.\n\nAvoidantly attached.\n\nParents are emotionally withdrawn and psychologically unavailable.\nLack of communication and affection.\nThe child learns that they have to take care of themselves.\nThe child will become independent but struggle to make relationships. They suffer from anxiety and are unsure in social situations, they withdraw before others get too close.\n\nAmbivalent / disorganised attached.\n\nParents are inconsistent; they are sometimes present and loving, then disappear for a while.\nChildren grow up to be more fearful and struggle to control impulses."
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#your-child-has-personality",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#your-child-has-personality",
    "title": "Neuroscience for Parents",
    "section": "3.5. Your Child Has Personality",
    "text": "3.5. Your Child Has Personality\n\n“Let them become who they are”.\n\nThe genes you pass on are not the sole determinant of how your child ends up.\nDuring the first 3 months of the child’s life, their reptilian brain is in control, and their experiences in these 3 months influence their intrinsic motivations and personality.\nThe baby only know 4 experiences:\n\nFlee. This leads to a love of movement and change.\nFight. This leads to a love of challenges and winning.\nFreeze. This leads to a love of sharing and caring.\nStandby mode. No perceived danger. This leads to a love of contemplation and thinking.\n\nThe things that trigger these responses are things like noises in the environment, how long they cry before being soothed, etc.\nWhich category does your child fall under? This determines what their intrinsic motivations are, and we should encourage choices that support these primary motivations.\nIt can be easy to project our own motivations on to others and encourage kids to do the things we like. These become the child’s extrinsic motivations. Some people can eventually become successful in things they don’t really like, but they won’t achieve joy, fulfilment or happiness.\nBeing overbearing and cheering them on too much can turn an intrinsic motivation into an extrinsic motivation. They initially love something, but then just carry on doing it because they get praised for it.\n\n“Parents will do anything for their kids except let them be themselves” - Banksy\n\nLet the child experience life as much as possible. Don’t restrict too much, don’t push too much, just let them be."
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#mindsets",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#mindsets",
    "title": "Neuroscience for Parents",
    "section": "3.6. Mindsets",
    "text": "3.6. Mindsets\nThe idea of fixed mindset vs growth mindset comes from Carol Dweck.\n\nFixed mindset focuses on results.\nGrowth mindset focuses on effort, experimentation. Enjoying the process.\n\nThe outcome might still be the same, but the difference is how you feel about the outcome. If you fail an exam after studying hard, with a fixed mindset you’ll be miserable and beat yourself up. With a growth mindset, you’ll see the positives and try again.\nA growth mindset is crucial to put in the sustained effort required for real success."
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#compliments",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#compliments",
    "title": "Neuroscience for Parents",
    "section": "3.7. Compliments",
    "text": "3.7. Compliments\nWhatever you praise your child for reinforces that behaviour. If you praise your child for working hard, they’ll learn to persevere. If you praise them for being smart, they’ll believe success is something you’re born with and quit when things feel difficult, taking fewer risks to avoid looking stupid.\nPraise effort not inborn traits.\nDon’t overdo it. The brain gets used to it and needs more and more of it to get the same dopamine hit. They may also start to love the praise more than the activity, making them turn away from activities that interest them unless they’re being praised."
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#small-social-traumas",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#small-social-traumas",
    "title": "Neuroscience for Parents",
    "section": "3.8. Small Social Traumas",
    "text": "3.8. Small Social Traumas\nNever ridicule your child, and especially not in public.\nChildren are not mini-adults. They are emotionally vulnerable and lack the tools to process ridicule.\nBefore the age of 17, the prefrontal cortex is not mature enough to put things into perspective and deal with being ridiculed. Their brain will experience the negative effects and learn to do everything it can do avoid being put in that situation ever again. They learn to avoid not address. This becomes an avoidance and compensation mechanism.\nAfter age 17, we don’t develop new compensation mechanisms as we are then able to process these small social traumas."
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#its-function-3",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#its-function-3",
    "title": "Neuroscience for Parents",
    "section": "4.1. Its Function",
    "text": "4.1. Its Function\nThis is the part of the brain mostly unique to humans; other mammals have one but ours is huge in comparison.\nIt is responsible for creativity, dealing with unknown and complex situations.\nAt least it should be, but often we get spooked and one of the other parts of the brain takes over, leading to poor decisions and resistance to change. When we use our limbic brain, we cannot see beyond what we have already experienced.\nFeeling calm and in control is a sign our prefrontal brain is active."
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#techniques-to-stay-calm",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#techniques-to-stay-calm",
    "title": "Neuroscience for Parents",
    "section": "4.2. Techniques to Stay Calm",
    "text": "4.2. Techniques to Stay Calm\nThese techniques force the prefrontal brain to activate. This helps when your primal brain is taking charge and you subconsciously see your children as a “threat”.\n\nImagine yourself 10 years from now looking back on this moment. You probably won’t even remember it. If you do, you’ll probably look back fondly. This helps put things into perspective.\nLook at the situation and imagine how it could be worse."
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#learning",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#learning",
    "title": "Neuroscience for Parents",
    "section": "4.3. Learning",
    "text": "4.3. Learning\nParents and community have a greater effect on achievement than school.\nThe best approach to learning is to force them to get things wrong.\nRetrieving information strengthens the neural pathways in their brain. Let them fail with you.\nRepeating something ~6 times helps it stick. Better to do this as phased repetition spread over many days rather than cramming in a single night.\nSleep is crucial to consolidate memories and neural pathways."
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#adolescence",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#adolescence",
    "title": "Neuroscience for Parents",
    "section": "4.4. Adolescence",
    "text": "4.4. Adolescence\n\n4.4.1. The Adolescent Brain\nIt would be easy to explain away the changes in behaviour during adolescence as “their brains aren’t fully mature”. But this isn’t the whole story.\nOur brains are 90% of their full size by age 6. From 12 onwards, the brain goes through a “rewiring” process from rear to front called myelination.\nKids are not mini-adults, but teens are not children. They need to be treated differently during adolescent years than when they were a child.\nAsk yourself: are your children there to attend to your emotional needs, or are you there to attend to theirs? The answer should be clear.\nAdolescence is not cultural, it’s in our genes. The purpose is to produce a person primed to leave a safe home and move into unfamiliar territory.\nThe adolescent brain becomes very sensitive to:\n\nDopamine.\n\nReward hormone.\nIt explains why teenagers learn fast, but also why success and defeat are met with extreme emotions.\nIt is why they take risks; this is different from impulsivity which drops from age 6. An example of the distinction is planning a bungee jump - risky but not impulsive.\n\nOxytocin.\n\nResponsible for bonding.\nIt explains why friendship is so important during adolescence.\n\n\nThe result of the two hormones is someone socially active, sensitive to peer pressure, looking to meet new people and try new things.\nTeenagers understand risk as well as adults. The reason for higher risk taking isn’t a lack of understanding the consequences, it’s that they weigh the risk vs reward differently. The dopamine makes the reward hit harder. Their utility function is steeper.\n\n\n4.4.2. How to Parent a Teen\nThey still need a framework of rules and structure, but one that gradually expands as they mature.\nThe paleolimbic brain is often active, leading to territorial clashes. A light but steady hand when parenting, staying connected but allowing independence. Clearly define the rules and consequences for breaking the rules. Negotiate the rules, don’t impose them. Revisit the rules periodically, every 6 months.\nMake sure they sleep enough because sleep health is so important to brain development and overall health."
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#screen-time",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#screen-time",
    "title": "Neuroscience for Parents",
    "section": "4.5. Screen Time",
    "text": "4.5. Screen Time\nChildren, and everybody in general, spend a lot of time looking at screens. Screens are designed for dopamine hits.\nOne of the arguments in favour of screen time is essentially peer pressure - they’re going to need to know how to use technology so they better start now.\nThe effects of screens on the brain:\n\nGrey matter atrophy\nLoss of white matter integrity\nReduced cortical thickness\nImpaired cognitive function\n\nThere are also links with attention issues and depression.\nThere is also the opportunity cost of what they could otherwise be doing: playing, socialising, learning. These alternative activities would develop motor skills and social skills.\nThere is also the issue of sleep deprivation, both from the constant dopamine hits making it harder to fall asleep, and the blue lights in screens.\nRecommendations:\n\nLimit screen time to &lt;2 hours a day. None at all for children under 2 years old, some say 3 years old.\nIt’s easier to “quit” when you haven’t started in the first place. Prevention is the best cure. Don’t allow many screens in the first place then excessive screen time won’t be as much of an issue."
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#grit",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#grit",
    "title": "Neuroscience for Parents",
    "section": "4.6. Grit",
    "text": "4.6. Grit\nThe capacity to continue when things get tough.\nThis is a bigger predictor for “success” than IQ or education. The average IQ for Nobel peace prize winners is “only” 121.\nDelayed gratification, growth mindset, learned helplessness are all related to grit. When you encounter an obstacle, do you give up or try harder?\nKids need to fail to learn how to respond to failure. Love the process.\nAdversity in childhood can help develop grit, but it can also lead to learned helplessness. The difference is the perception of control. Where they feel they can influence the outcome, they are more likely to continue trying. Where they feel like it was beyond their control, they are likely to give up.\nWe need to be demanding and supportive. The demands should grow with their ability (and hence age). Recall the Pygmalion Effect. Remind them to try harder, try differently.\nOur brains evolved for instant gratification and energy efficiency. This manifests as short-term thinking and laziness. So we are fighting an uphill battle against their ingrained nature to encourage children to try doing hard things.\nIf they do a bad job but tried hard, praise the effort and encourage them to think how they could do better next time. If they do a bad job and didn’t try, be straight with them. Your job as parent is to be demanding yet supportive; don’t skimp on the demanding part of the situation calls for it. “Tough love”."
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#the-perfect-parent-myth",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#the-perfect-parent-myth",
    "title": "Neuroscience for Parents",
    "section": "5.1. The Perfect Parent Myth",
    "text": "5.1. The Perfect Parent Myth\nYou can’t (and shouldn’t try to!) protect your kids from everything. Learning how to deal with failure and pain is one of the most important lessons you can teach them.\nDon’t hold yourself to impossibly high standards. One of the groups most at risk of burnout is stay-at-home parents of small kids."
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#it-takes-a-village",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#it-takes-a-village",
    "title": "Neuroscience for Parents",
    "section": "5.2. It Takes a Village",
    "text": "5.2. It Takes a Village\nYou are an important person in your child’s life, but you are not the only person. They will be influenced by parents, extended family, friends, teachers, sports coaches, music teachers, etc.\nIt can help to shift the narrative from “I am raising my child” to “I am assembling the team of people who will influence them”. You are a coach, not a player trying to play every position.\nThe child may express parts of their personality in different ways with different people."
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#the-birth-order-effect",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#the-birth-order-effect",
    "title": "Neuroscience for Parents",
    "section": "5.3. The Birth Order Effect",
    "text": "5.3. The Birth Order Effect\nYou raise all your children the same and they turn out differently. WTF?\nThe simple fact of having siblings, older and younger, affects our personality. This is called the “birth order effect”.\n\nFirst child.\n\n\nWill automatically get more attention from parents, because there are no other children to divert attention, and the desire to be a “perfect parent” the first time around (before you realise this is a myth).\nThe child will commonly be: reliable, conscientious, controlling, achievers, perfectionists, competitive mindset.\n\n\nMiddle child.\n\n\nParents’ attitudes become more relaxed.\nThe middle child gets less attention than any other; first-born gets it at the start, then younger children afterwards.\nThe child will commonly be: people pleasers, rebellious, thrive on friendship, large social circle, peacemakers.\nA caveat to “middle child syndrome” is when the first and middle children are different sexes. Then they might be given attention in different ways.\n\n\nYoungest child.\n\n\nThe parents have loosened the reigns completely. The child tends to be: fun loving, uncomplicated, manipulative, outgoing, attention seeking, self-centred.\n\nWith an only child, there are similar characteristics to the first child but more extreme. They also tend to be mature for their age.\nThese aren’t hard and fast rules, and things like twins, adoption or large gaps between children can change the dynamics considerably.\nSo even if you feel like you are raising your children the same way, it’s impossible to do so."
  },
  {
    "objectID": "posts/learning/neuroscience_for_parents/neuroscience.html#the-golden-rules",
    "href": "posts/learning/neuroscience_for_parents/neuroscience.html#the-golden-rules",
    "title": "Neuroscience for Parents",
    "section": "5.4. The Golden Rules",
    "text": "5.4. The Golden Rules\nThe important ingredients for brain development are:\n\nSleep\nMovement\nDiet\n\nGolden rules:\n\nGive your child a framework - a set of rules\nWithin this framework, let them experiment\nAvoid small social traumas\n\nThe best way to deal with the primal brain is to take away the perceived danger. Communicate on the level of emotions, not always rationality.\nRemember the techniques to deal with stress: imagine looking back on this moment in 10 years time, and also imagine how it could have been worse."
  },
  {
    "objectID": "posts/business/pitching/pitching.html",
    "href": "posts/business/pitching/pitching.html",
    "title": "Pitching Notes",
    "section": "",
    "text": "Notes from “Pitch Anything” by Oren Klaff.\nSome of the book comes across as a bit incel sigma vibes, talking about alphas and betas. I don’t agree with it, but summarising it here for the parts that are interesting.\n\n\nFrames, AKA perspective AKA point of view, describe a person’s mental model of controlling the world. The person who owns the frame owns the conversation.\nInformation must penetrate through the following layers of the brain in order.\n\nSurvival - “crocodile brain”. Ignore anything that isn’t new, exciting, dangerous, unexpected. Needs big picture, high contrast points.\nSocial\nReason\n\nLogic won’t reach the “reason” part of the brain until the croc brain and social brain are satiated. Data is perceived as a threat by layer 1. the croc brain needs unexpected information that is concrete not abstract.\nSTRONG process:\n\nSet the frame\nTell the story\nReveal the intrigue\nOffer the prize\nNail the hook point\nGet the deal\n\n\n\n\nFrames are the mental structure that shape how you see the world. When frames collide, the stronger frame absorbs the weaker frame. If you have to explain your authority, power, position, leverage then you have the weaker frame.\nOpposing frames and how to counter them.\n\nPower frame - fuelled by status and ego\n\nDon’t react to our strengthen their status\nActs of denial/defiance - establish your own rituals of power rather than abiding by theirs.\nMildly shocking but not unfriendly. Humour is key.\n\nPrize frame\n\nMake yourself the prize. Make them qualify themselves to you. The money has to earn you, not the other way around.\nMake statements not “trial close” questions\n\nTime frame - occurs later in the interaction\n\nStop as soon as you are done OR have lost their attention. Continuing further signals desperation.\n\nAnalyst frame - stats, technical details\n\nIntrigue frame counters this\nThe brain can’t perform cold cognitions (analysis) at the same time as hot cognitions (excitement)\nTell a brief, relevant, intriguing story with tension involving you at the centre of it to break the cold cognition.\n\n\n\n\n\nGlobal status is a function of wealth, power and status.\nBeta traps enforce your lower status. E.g. making you wait, big dogs not showing up to meetings, small talk before meetings.\nSituational status is the temporary shift of status based on the situation. Create local star power to increase your situational status.\n\nIgnore power rituals, avoid beta traps, ignore their global status\nFrame control - small, friendly acts of defiance\nLocal star power - shift conversation to a topic where you are the domain expert\nPrize frame - position yourself as the reward, e.g. leave after a hard cut off time\nConfirm your status - make them say you’re the alpha\n\nShock -&gt; frame control -&gt; local star power -&gt; hook point -&gt; leave\n\n\n\nEvery pitch should tell a story.\nFour sections of the pitch:\n\nIntroduce yourself and the big idea (5 mins)\nExplain the budget and secret sauce (10 mins)\nOffer the deal (2 mins)\nStack hot cognition frames (3 mins)\n\n\n\nKeep the pitch short - 20 mins.\n\nAnnounce the time constraint at the start to appease the croc brain. Otherwise people don’t know how long they are trapped for and panic.\nDNA presentation by Crick and Watson was 5 mins\n\nIntro should be highlights of successes NOT a life story.\n\nThe impression you leave is based the average not the sum, so don’t dilute it.\n\nThe “why now?” frame. 3 market forces pattern:\n\nEconomic\nSocial\nTechnology\n\nTell the story of how your idea came to be.\n\nMovement is key to overcoming change blindness. Don’t just show 2 states, describe the transition between them.\nTrends, impacts of those trends, how have these opened a (temporary) market window (time frame).\nThis story places you as the hero at the centre of solving this problem (prize frame).\n\nIdea introduction pattern:\nFor [target customers]\n\nWho are dissatisfied with [current offerings in the market]\n \nMy idea/product is a [new idea or product category]\n \nThat provides [key problem/solution features]\n \nUnlike [the competing product]\n \nMy idea/product is [describe key features]\n\n\n\nTune the message to the croc brain of the target.\n\nSimplicity can come across as naive, but focus on describing concrete things like relationships between people. This is high-level enough that it will not engage cold cognitions or threaten the croc brain.\n\nAttention = Desire (anticipation of reward) + Tension (fear of loss, introduce consequences)\n\nAttention is high when novelty is high.\nPush/pull patterns to increase tension and desire\nThe pitch narrative is a series of tension loops.\nWhen the tension is released and attention is lost the pitch is over.\nDesire eventually becomes fear, so keep the pitch short.\n\nMust haves:\n\nNumbers and projections\n\nThis should demonstrate your skill at budgeting (a difficult, precise skill), not your skill at projecting (a simple, inaccurate skill).\n\nCompetition\n\nHow easy is it for new competitors to enter?\nHow easy is it for customers to switch TO you and AWAY from you?\n\nSecret sauce\n\nYour competitive advantage.\nPhrasing it this way frames you as the prize.\n\n\n\n\n\nDescribe what they get if they do business with you\n\nWhat you will deliver, when, and how.\nWhat are their roles, responsibilities and next steps.\n\n\n\n\n\nPropose something concrete and actionable in such a compelling way that the target wants to chase it. Don’t wait for the target to evaluate you on the spot at the end of the pitch as that triggers cold cognitions. Pile up the hot cognitions instead.\nA hot cognition is emotional rather than analytical.\n\nYou decide that you like something before you understand it.\nData is used to justify decisions after the fact.\nHot cognitions are generally unavoidable; you may be able to control the expression of emotion, but the initial experience is still there.\nHot cognitions tend to be instant and enduring.\nHot vs cold cognitions can be characterised by spinach vs chocolate - you know one is good for you but you want the other anyway.\nHot cognitions are “knowing” something through feeling it, cold cognitions are “knowing something through evaluating it.”\n\nStacking frames.\n\nWe want to stack frames to create “wanting”, an emotional response\nThis is not the same as getting the target to “like” us, as that is a slow, intellectual cold cognition.\nAppealing to the analytical brain can only trigger a “like”; we want to trigger a “want” in the croc brain.\n\nThe 4 frame stack of hot cognitions:\n\nThe intrigue frame\n\nA story where most important things are who it happened to and how the characters reacted to their situation.\nThe events don’t need to be extreme but the characters’ reactions should be.\nA narrative that feels correct in time with convey a strong sense of truth and accuracy.\n“Man in the jungle” formula - Put a man in the jungle; have beasts attack him; will he get to safety?; get the man to the edge of the jungle but don’t get him out of it (cliffhanger)\n\nThe prize frame\n\nPosition yourself as the most important party in the deal. Conviction is key.\n“I am the prize. You are trying to impress me. You are trying to win my approval.”\n\nThe time frame\n\nThis creates a scarcity bias that triggers fear that triggers action.\nExtreme time pressure feels forced and cutrate. There is a balance between pressure and fairness; there should be a real-world time constraint.\n\nThe moral authority frame\n\nOnce the frame stacking is complete, you have the target’s attention for about 30 seconds.\n\n\n\nValidation-seeking behaviour (i.e. neediness) is the number one deal killer.\n\nNeediness is a threat signal, it triggers fear and uncertainty in the target which triggers self-protection.\n\nAvoid “soft closes” aiming to seek validation, e.g. “So what do you think so far?”, “Do you still think this is a good deal?”\nCauses of neediness:\n\nAnxiety and insecurity turn to fear, so you resort to acceptance-seeking behaviour to confirm that you’re still “part of the pack”\nResponse to disappointment is to seek validation\nWe want something only the target can give us\nWe need cooperation from the target, and the lack of cooperation causes anxiety\nWe believe the target can make us feel good by accepting our offer\n\nCounteracting validation-seeking behaviours:\n\nWant nothing\nFocus only on things you do well\nAnnounce your intention to leave - time frame. When you finish the pitch, withdraw and deny your audience; this creates a prize frame.\n\nMantra: \"I don't need these people, they need me. I am the prize\".\nExample:\n\nTime frame - “The deal will be fully subscribed in the next 10 days”.\nPower frame - “We don’t need VC money but want a big name on cap sheet for later IPO”\nPrize frame - “Are you really the right investor for this? We need to know more about the relationships and value you can bring.”\n\n\n\n\nYou are not pushing, you are interacting with people using basic rules of social dynamics.\nCroc brain:\n\nBoring: Ignore it\nDangerous? Fight it or run\nComplicated? Radically summarise\n\nThe presenter’s problem boils down to deciding which information to include. It is not like an engineering problem where more information is better.\nThree key insights:\n\nAppeal to the croc brain\nControl frames\nUse humour - this is not to relieve tension but to signal that you are so confident that you’re able to play around a little\n\nIf the pitch stops being fun, you’re doing it wrong.\nSteps to learning the method:\n\nRecognise beta traps; anything designed to control your behaviour\nStep around beta traps\nIdentify social frames\nInitiate frame collisions with safe targets - always with good humour and “a twinkle in your eye”\nPractice the push/pull of small acts of defiance and denial\n\nThe only rule is that you make the rules that others follow."
  },
  {
    "objectID": "posts/business/pitching/pitching.html#the-method",
    "href": "posts/business/pitching/pitching.html#the-method",
    "title": "Pitching Notes",
    "section": "",
    "text": "Frames, AKA perspective AKA point of view, describe a person’s mental model of controlling the world. The person who owns the frame owns the conversation.\nInformation must penetrate through the following layers of the brain in order.\n\nSurvival - “crocodile brain”. Ignore anything that isn’t new, exciting, dangerous, unexpected. Needs big picture, high contrast points.\nSocial\nReason\n\nLogic won’t reach the “reason” part of the brain until the croc brain and social brain are satiated. Data is perceived as a threat by layer 1. the croc brain needs unexpected information that is concrete not abstract.\nSTRONG process:\n\nSet the frame\nTell the story\nReveal the intrigue\nOffer the prize\nNail the hook point\nGet the deal"
  },
  {
    "objectID": "posts/business/pitching/pitching.html#frame-control",
    "href": "posts/business/pitching/pitching.html#frame-control",
    "title": "Pitching Notes",
    "section": "",
    "text": "Frames are the mental structure that shape how you see the world. When frames collide, the stronger frame absorbs the weaker frame. If you have to explain your authority, power, position, leverage then you have the weaker frame.\nOpposing frames and how to counter them.\n\nPower frame - fuelled by status and ego\n\nDon’t react to our strengthen their status\nActs of denial/defiance - establish your own rituals of power rather than abiding by theirs.\nMildly shocking but not unfriendly. Humour is key.\n\nPrize frame\n\nMake yourself the prize. Make them qualify themselves to you. The money has to earn you, not the other way around.\nMake statements not “trial close” questions\n\nTime frame - occurs later in the interaction\n\nStop as soon as you are done OR have lost their attention. Continuing further signals desperation.\n\nAnalyst frame - stats, technical details\n\nIntrigue frame counters this\nThe brain can’t perform cold cognitions (analysis) at the same time as hot cognitions (excitement)\nTell a brief, relevant, intriguing story with tension involving you at the centre of it to break the cold cognition."
  },
  {
    "objectID": "posts/business/pitching/pitching.html#status",
    "href": "posts/business/pitching/pitching.html#status",
    "title": "Pitching Notes",
    "section": "",
    "text": "Global status is a function of wealth, power and status.\nBeta traps enforce your lower status. E.g. making you wait, big dogs not showing up to meetings, small talk before meetings.\nSituational status is the temporary shift of status based on the situation. Create local star power to increase your situational status.\n\nIgnore power rituals, avoid beta traps, ignore their global status\nFrame control - small, friendly acts of defiance\nLocal star power - shift conversation to a topic where you are the domain expert\nPrize frame - position yourself as the reward, e.g. leave after a hard cut off time\nConfirm your status - make them say you’re the alpha\n\nShock -&gt; frame control -&gt; local star power -&gt; hook point -&gt; leave"
  },
  {
    "objectID": "posts/business/pitching/pitching.html#pitching-the-big-idea",
    "href": "posts/business/pitching/pitching.html#pitching-the-big-idea",
    "title": "Pitching Notes",
    "section": "",
    "text": "Every pitch should tell a story.\nFour sections of the pitch:\n\nIntroduce yourself and the big idea (5 mins)\nExplain the budget and secret sauce (10 mins)\nOffer the deal (2 mins)\nStack hot cognition frames (3 mins)\n\n\n\nKeep the pitch short - 20 mins.\n\nAnnounce the time constraint at the start to appease the croc brain. Otherwise people don’t know how long they are trapped for and panic.\nDNA presentation by Crick and Watson was 5 mins\n\nIntro should be highlights of successes NOT a life story.\n\nThe impression you leave is based the average not the sum, so don’t dilute it.\n\nThe “why now?” frame. 3 market forces pattern:\n\nEconomic\nSocial\nTechnology\n\nTell the story of how your idea came to be.\n\nMovement is key to overcoming change blindness. Don’t just show 2 states, describe the transition between them.\nTrends, impacts of those trends, how have these opened a (temporary) market window (time frame).\nThis story places you as the hero at the centre of solving this problem (prize frame).\n\nIdea introduction pattern:\nFor [target customers]\n\nWho are dissatisfied with [current offerings in the market]\n \nMy idea/product is a [new idea or product category]\n \nThat provides [key problem/solution features]\n \nUnlike [the competing product]\n \nMy idea/product is [describe key features]\n\n\n\nTune the message to the croc brain of the target.\n\nSimplicity can come across as naive, but focus on describing concrete things like relationships between people. This is high-level enough that it will not engage cold cognitions or threaten the croc brain.\n\nAttention = Desire (anticipation of reward) + Tension (fear of loss, introduce consequences)\n\nAttention is high when novelty is high.\nPush/pull patterns to increase tension and desire\nThe pitch narrative is a series of tension loops.\nWhen the tension is released and attention is lost the pitch is over.\nDesire eventually becomes fear, so keep the pitch short.\n\nMust haves:\n\nNumbers and projections\n\nThis should demonstrate your skill at budgeting (a difficult, precise skill), not your skill at projecting (a simple, inaccurate skill).\n\nCompetition\n\nHow easy is it for new competitors to enter?\nHow easy is it for customers to switch TO you and AWAY from you?\n\nSecret sauce\n\nYour competitive advantage.\nPhrasing it this way frames you as the prize.\n\n\n\n\n\nDescribe what they get if they do business with you\n\nWhat you will deliver, when, and how.\nWhat are their roles, responsibilities and next steps."
  },
  {
    "objectID": "posts/business/pitching/pitching.html#stack-hot-cognition-frames-3-mins",
    "href": "posts/business/pitching/pitching.html#stack-hot-cognition-frames-3-mins",
    "title": "Pitching Notes",
    "section": "",
    "text": "Propose something concrete and actionable in such a compelling way that the target wants to chase it. Don’t wait for the target to evaluate you on the spot at the end of the pitch as that triggers cold cognitions. Pile up the hot cognitions instead.\nA hot cognition is emotional rather than analytical.\n\nYou decide that you like something before you understand it.\nData is used to justify decisions after the fact.\nHot cognitions are generally unavoidable; you may be able to control the expression of emotion, but the initial experience is still there.\nHot cognitions tend to be instant and enduring.\nHot vs cold cognitions can be characterised by spinach vs chocolate - you know one is good for you but you want the other anyway.\nHot cognitions are “knowing” something through feeling it, cold cognitions are “knowing something through evaluating it.”\n\nStacking frames.\n\nWe want to stack frames to create “wanting”, an emotional response\nThis is not the same as getting the target to “like” us, as that is a slow, intellectual cold cognition.\nAppealing to the analytical brain can only trigger a “like”; we want to trigger a “want” in the croc brain.\n\nThe 4 frame stack of hot cognitions:\n\nThe intrigue frame\n\nA story where most important things are who it happened to and how the characters reacted to their situation.\nThe events don’t need to be extreme but the characters’ reactions should be.\nA narrative that feels correct in time with convey a strong sense of truth and accuracy.\n“Man in the jungle” formula - Put a man in the jungle; have beasts attack him; will he get to safety?; get the man to the edge of the jungle but don’t get him out of it (cliffhanger)\n\nThe prize frame\n\nPosition yourself as the most important party in the deal. Conviction is key.\n“I am the prize. You are trying to impress me. You are trying to win my approval.”\n\nThe time frame\n\nThis creates a scarcity bias that triggers fear that triggers action.\nExtreme time pressure feels forced and cutrate. There is a balance between pressure and fairness; there should be a real-world time constraint.\n\nThe moral authority frame\n\nOnce the frame stacking is complete, you have the target’s attention for about 30 seconds."
  },
  {
    "objectID": "posts/business/pitching/pitching.html#eradicating-neediness",
    "href": "posts/business/pitching/pitching.html#eradicating-neediness",
    "title": "Pitching Notes",
    "section": "",
    "text": "Validation-seeking behaviour (i.e. neediness) is the number one deal killer.\n\nNeediness is a threat signal, it triggers fear and uncertainty in the target which triggers self-protection.\n\nAvoid “soft closes” aiming to seek validation, e.g. “So what do you think so far?”, “Do you still think this is a good deal?”\nCauses of neediness:\n\nAnxiety and insecurity turn to fear, so you resort to acceptance-seeking behaviour to confirm that you’re still “part of the pack”\nResponse to disappointment is to seek validation\nWe want something only the target can give us\nWe need cooperation from the target, and the lack of cooperation causes anxiety\nWe believe the target can make us feel good by accepting our offer\n\nCounteracting validation-seeking behaviours:\n\nWant nothing\nFocus only on things you do well\nAnnounce your intention to leave - time frame. When you finish the pitch, withdraw and deny your audience; this creates a prize frame.\n\nMantra: \"I don't need these people, they need me. I am the prize\".\nExample:\n\nTime frame - “The deal will be fully subscribed in the next 10 days”.\nPower frame - “We don’t need VC money but want a big name on cap sheet for later IPO”\nPrize frame - “Are you really the right investor for this? We need to know more about the relationships and value you can bring.”"
  },
  {
    "objectID": "posts/business/pitching/pitching.html#closing-thoughts",
    "href": "posts/business/pitching/pitching.html#closing-thoughts",
    "title": "Pitching Notes",
    "section": "",
    "text": "You are not pushing, you are interacting with people using basic rules of social dynamics.\nCroc brain:\n\nBoring: Ignore it\nDangerous? Fight it or run\nComplicated? Radically summarise\n\nThe presenter’s problem boils down to deciding which information to include. It is not like an engineering problem where more information is better.\nThree key insights:\n\nAppeal to the croc brain\nControl frames\nUse humour - this is not to relieve tension but to signal that you are so confident that you’re able to play around a little\n\nIf the pitch stops being fun, you’re doing it wrong.\nSteps to learning the method:\n\nRecognise beta traps; anything designed to control your behaviour\nStep around beta traps\nIdentify social frames\nInitiate frame collisions with safe targets - always with good humour and “a twinkle in your eye”\nPractice the push/pull of small acts of defiance and denial\n\nThe only rule is that you make the rules that others follow."
  },
  {
    "objectID": "posts/business/executive_presence/presence.html",
    "href": "posts/business/executive_presence/presence.html",
    "title": "Executive Presence",
    "section": "",
    "text": "Identify personal blocks.\n\n\nSelf-reflection is important, just beware of “the judgement zone”.\nYou are not the same person you were at 9 years old, 20 years old, 21 years old. You are always changing. Be compassionate towards yourself. Compassionate, curious and committed.\nIdentify your strengths and “stretches”; where you can grow. But these “stretch” areas change over time because you will improve and change.\n\n\n\nWhat is your habitual mind state? Do you see yourself as “less than”, shy, or some other negative trait?\n“The chains of habit are too light to be felt until they’re too heavy to be broken.”\n\n\n\nThere are several “errors in thinking” that cause us to be depressed:\n\nMinimise positives and maximise negatives.\nOvergeneralise.\nFuture telling. Assume you’re going to mess something up, which makes you more likely to mess it up.\nMind reading. You assume everyone thinks you’re terrible.\nYou “should” yourself. “I should be doing better”, “I should make no mistakes”, “I should be fluent in this”, etc.\n\nThis is a vicious cycle; once a few negative thoughts creep in, we might make a mistake and reinforce this view so more negative thoughts flood in.\nThe way to correct this is to think objectively. It can be helpful to do this in writing. Being objective doesn’t mean being blindly positive. Write down a negative thought and ask a close friend what they think about it - do they agree?\n\n\n\nThings that are making us seem less confident:\n\nRising intonation when things aren’t questions. Address this by imagining a big full stop at the end of your sentence.\nFiller words - “umm, err, like”. Stop and pause instead of using filler words.\nHigh pitched voice.\n\n\n\nRecord yourself speaking and identify which of the following vocal checklist applies to you:\n\nAiry or breathy\nSoft-apoken\nWhimsical\nFast\nSlurred or tired\nOver-articulated\nWarm\nRelaxed and comfortable\nHoarse\nHonky\nHyper-nasal\nHarsh\nStrained\nRaspy\nWobbly\nLoud\nChesty\nChild-like\nSultry\n\n\n\n\nWarm up your breathing:\n\nConnect to your breath. Notice your breath and let it drop down to your diaphragm.\nRelax your belly. Rub your belly with your hands.\nSend your breath down to your toes.\nBreathe in through nose and sigh out through mouth.\n\nWarm up your face:\n\nBring your voice forward to the front of your mouth. Roll your tongue forcefully behind your lips.\nBlow air through lips on a sigh. This should flap your lips.\n\nWarm up your lips:\n\nHum on a sigh. Feel the buzzing on your lips.\nSend the humming vibrations to a spot in front of you.\n\nWarm up your nasal resonators:\n\nCreate a bright “me” sound and push it out of your nose and the middle of your face.\nSend the sound through your cheekbones. Use your hands to touch your cheeks in rhythm with the “me” sound to help engage them.\n\nWarm up your chest resonators:\n\nConnect to your chest while making a “maaah” and “haah” sounds.\nThump on your chest as you do it to engage it.\n\nWarm up your articulators:\n\n“Topeka” x3 and “bodega” x3.\nAlternate the sounds. Play with pitch and rhythm.\n\nIntegrate your voice:\n\nSlide sound from head (nasal resonators) down to chest resonators, then out through mouth .\n“Me me me me me me my my my my ha hum ma”.\n\n\n\n\n\nExercise more, you’ll naturally stand up straighter.\nPilates and intentional breathing is helpful. Two useful Pilates exercises are “hundreds” and the bridge.\nThink of the core not just as the abs, but everything from the pelvic floor up to the neck."
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#assessing-yourself",
    "href": "posts/business/executive_presence/presence.html#assessing-yourself",
    "title": "Executive Presence",
    "section": "",
    "text": "Self-reflection is important, just beware of “the judgement zone”.\nYou are not the same person you were at 9 years old, 20 years old, 21 years old. You are always changing. Be compassionate towards yourself. Compassionate, curious and committed.\nIdentify your strengths and “stretches”; where you can grow. But these “stretch” areas change over time because you will improve and change."
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#your-mind",
    "href": "posts/business/executive_presence/presence.html#your-mind",
    "title": "Executive Presence",
    "section": "",
    "text": "What is your habitual mind state? Do you see yourself as “less than”, shy, or some other negative trait?\n“The chains of habit are too light to be felt until they’re too heavy to be broken.”"
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#cognitive-corrections",
    "href": "posts/business/executive_presence/presence.html#cognitive-corrections",
    "title": "Executive Presence",
    "section": "",
    "text": "There are several “errors in thinking” that cause us to be depressed:\n\nMinimise positives and maximise negatives.\nOvergeneralise.\nFuture telling. Assume you’re going to mess something up, which makes you more likely to mess it up.\nMind reading. You assume everyone thinks you’re terrible.\nYou “should” yourself. “I should be doing better”, “I should make no mistakes”, “I should be fluent in this”, etc.\n\nThis is a vicious cycle; once a few negative thoughts creep in, we might make a mistake and reinforce this view so more negative thoughts flood in.\nThe way to correct this is to think objectively. It can be helpful to do this in writing. Being objective doesn’t mean being blindly positive. Write down a negative thought and ask a close friend what they think about it - do they agree?"
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#your-voice",
    "href": "posts/business/executive_presence/presence.html#your-voice",
    "title": "Executive Presence",
    "section": "",
    "text": "Things that are making us seem less confident:\n\nRising intonation when things aren’t questions. Address this by imagining a big full stop at the end of your sentence.\nFiller words - “umm, err, like”. Stop and pause instead of using filler words.\nHigh pitched voice.\n\n\n\nRecord yourself speaking and identify which of the following vocal checklist applies to you:\n\nAiry or breathy\nSoft-apoken\nWhimsical\nFast\nSlurred or tired\nOver-articulated\nWarm\nRelaxed and comfortable\nHoarse\nHonky\nHyper-nasal\nHarsh\nStrained\nRaspy\nWobbly\nLoud\nChesty\nChild-like\nSultry\n\n\n\n\nWarm up your breathing:\n\nConnect to your breath. Notice your breath and let it drop down to your diaphragm.\nRelax your belly. Rub your belly with your hands.\nSend your breath down to your toes.\nBreathe in through nose and sigh out through mouth.\n\nWarm up your face:\n\nBring your voice forward to the front of your mouth. Roll your tongue forcefully behind your lips.\nBlow air through lips on a sigh. This should flap your lips.\n\nWarm up your lips:\n\nHum on a sigh. Feel the buzzing on your lips.\nSend the humming vibrations to a spot in front of you.\n\nWarm up your nasal resonators:\n\nCreate a bright “me” sound and push it out of your nose and the middle of your face.\nSend the sound through your cheekbones. Use your hands to touch your cheeks in rhythm with the “me” sound to help engage them.\n\nWarm up your chest resonators:\n\nConnect to your chest while making a “maaah” and “haah” sounds.\nThump on your chest as you do it to engage it.\n\nWarm up your articulators:\n\n“Topeka” x3 and “bodega” x3.\nAlternate the sounds. Play with pitch and rhythm.\n\nIntegrate your voice:\n\nSlide sound from head (nasal resonators) down to chest resonators, then out through mouth .\n“Me me me me me me my my my my ha hum ma”."
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#your-body",
    "href": "posts/business/executive_presence/presence.html#your-body",
    "title": "Executive Presence",
    "section": "",
    "text": "Exercise more, you’ll naturally stand up straighter.\nPilates and intentional breathing is helpful. Two useful Pilates exercises are “hundreds” and the bridge.\nThink of the core not just as the abs, but everything from the pelvic floor up to the neck."
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#everything-is-sales",
    "href": "posts/business/executive_presence/presence.html#everything-is-sales",
    "title": "Executive Presence",
    "section": "4.1. Everything is Sales",
    "text": "4.1. Everything is Sales\nEverything is selling even if you don’t realise it. If you are recommending a place to eat, or talking about yourself or selling a product, it’s the same idea.\nReframe selling as being in service to the other person. You’re doing them a favour.\nRemind yourself:\n\nYou have amazing gifts to share with the world\nPeople love to be inspired\nYour work is your service to other people"
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#find-your-spark",
    "href": "posts/business/executive_presence/presence.html#find-your-spark",
    "title": "Executive Presence",
    "section": "4.2. Find Your Spark",
    "text": "4.2. Find Your Spark\nWhat is an activity that gets you “in the zone”, in a state of flow? What is it about that activity that does it for you? What are the things that light you up?\nWhen you are in the zone, you are not self-doubting, overly critical, etc. Try to mimic that confidence when you’re not in a flow state; you know you are capable of it.\nI am in the zone when…\n\nIn the zone I am…\n\nAt work, I love to…\nWhen you’re communicating, e.g. giving an elevator pitch about yourself, it’s not just a list of facts you’re communicating. Think about what gets you in a flow state and get that across to the other person."
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#the-elevator-pitch",
    "href": "posts/business/executive_presence/presence.html#the-elevator-pitch",
    "title": "Executive Presence",
    "section": "4.3. The Elevator Pitch",
    "text": "4.3. The Elevator Pitch\nThe goal isn’t to information dump, it’s to spark curiosity so that the other person wants to carry on talking to you after the elevator ride is over.\n\nThe purpose of an elevator pitch isn’t to close the sale. The goal isn’t even to give a short, accurate, Wikipedia-standard description of you or your project. And the idea of using vacuous, vague words to craft a bland mission statement is dumb. No, the purpose of an elevator pitch is to describe a situation or solution so compelling that the person you’re with wants to hear more even after the elevator ride is over.\n\n- Seth Godin\nHow do you typically describe what you do?\nThink about what your spark is and then rewrite your elevator pitch with that at the centre. Then practice the new pitch.\nThe sign of a successful elevator pitch is that they ask you questions. You’ve hooked them and they want to know more."
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#listening",
    "href": "posts/business/executive_presence/presence.html#listening",
    "title": "Executive Presence",
    "section": "4.4. Listening",
    "text": "4.4. Listening\nHow to listen your way to more friends/sales:\n\nShift your attention from yourself (and what you plan to say next) to the other person\nBe present\nAsk questions - “what brings you here?” is open-ended enough to get to the heart of what they need not just what they’re doing.\nLet the conversation flow, don’t force it back to your topic / product\nBe generous - “Who can I help?”. Help others and they’ll want to help you."
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#how-to-identify-your-ideal-client",
    "href": "posts/business/executive_presence/presence.html#how-to-identify-your-ideal-client",
    "title": "Executive Presence",
    "section": "4.5. How to Identify Your Ideal Client",
    "text": "4.5. How to Identify Your Ideal Client\nLots of sales approaches try to cast too wide a net - “anybody is a potential client”. Adopt an abundance mindset - “there are so many potential clients out there that I can be picky”.\nAsk yourself the following questions to figure out your ideal client:\n\nWhich clients have you enjoyed working with the most? How would you describe them? What was your experience working together?\nWhat aspect of your work truly lights you up?\nWhat problems did they have that you solved? How did you impact their lives?\n\nThen write a client avatar for that ideal client:\n\nName\nAge\nProfession\nChallenge/Need/Frustration\nWhat are their fears?\nWhat are their desires and dreams?\nWhy do they need you?\nHow do they feel working with you?"
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#dealing-with-rejection",
    "href": "posts/business/executive_presence/presence.html#dealing-with-rejection",
    "title": "Executive Presence",
    "section": "4.6. Dealing with Rejection",
    "text": "4.6. Dealing with Rejection\nHow do you deal with rejection? You can’t control being rejected but you can control your response to it.\n\nDon’t take it personally.\nFeelings are not facts. Just because you feel like you failed, you performed badly, etc, doesn’t mean you are bad at what you do.\nPause before reacting.\n\nProcessing rejection:\n\nWrite. Brain dump your feelings.\nShower.\nFlick. If you have the urge to beat yourself up, put an elastic band on your wrist and flick yourself whenever you feel bad and move on."
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#picture-the-stage",
    "href": "posts/business/executive_presence/presence.html#picture-the-stage",
    "title": "Executive Presence",
    "section": "5.1. Picture the stage",
    "text": "5.1. Picture the stage\nAllow yourself to dream big. What is your ideal stage you would want to be on?\nOften the same limiting thoughts that tell us we aren’t good at presenting or public speaking are the same root cause as those stopping us from speaking our mind in meetings or asking for a promotion."
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#how-to-interview",
    "href": "posts/business/executive_presence/presence.html#how-to-interview",
    "title": "Executive Presence",
    "section": "5.2. How to Interview",
    "text": "5.2. How to Interview\nThink about what you love, your “why”. You don’t need a script, but you need to be able to articulate this.\nCommunicate how you do it. Think about a nice sound bite, especially if it’s a media interview. Short and sweet message.\nPay attention to what the other person is saying. Don’t be so focused on yourself that you don’t engage with the other person.\nPractice interviewing in lower stakes situations."
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#powerpoint-presentations",
    "href": "posts/business/executive_presence/presence.html#powerpoint-presentations",
    "title": "Executive Presence",
    "section": "5.3. PowerPoint Presentations",
    "text": "5.3. PowerPoint Presentations\nLess is more.\nWe remember stories, not information. Your job isn’t to dump information, it’s to tell a story.\n\nWhat is your story?\nThe targeted takeaway. What do you want the audience to remember? What action should they take?\nToss the script. Create thought bubbles around high level ideas that you can talk around them spontaneously. Instead of a script, it’s a journey through thoughts, like acts in a story. One thought bubble per slide keeps the message clear.\n\nPreparing your presentation:\n\nIdentify your goal. What should the audience feel or do?\nWhat stories illustrate your point?\nWhat images are absolutely necessary to support your point?\nWhat thought bubbles walk your audience through your message?\nWhat are the transitions to get from one thought bubble to the next?\n\nSuggested TED Talks:\n\nSusan Cain: The Power of Introverts\nBrene Brown: The Power of Vulnerability\nAmy Cuddy: Your Body Language Shapes Who You Are"
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#preparation-exercises",
    "href": "posts/business/executive_presence/presence.html#preparation-exercises",
    "title": "Executive Presence",
    "section": "5.4. Preparation exercises",
    "text": "5.4. Preparation exercises\n\nSmile\nBreathe\nEngage. Look at the audience. Look at the back row. Pick a friendly face; some may look disinterested but find the ones that are keen.\nPrepare. Use notes as necessary.\nVisualise. Imagine the presentation going well and the audience responding well.\nLet go. You may not see immediately whether people have taken your message on board and you may never see the impact directly. Trust that your message will have an impact. You can control your contribution but not the outcome."
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#dealing-with-anxiety",
    "href": "posts/business/executive_presence/presence.html#dealing-with-anxiety",
    "title": "Executive Presence",
    "section": "5.5. Dealing with Anxiety",
    "text": "5.5. Dealing with Anxiety\n\nDeep breathing. Physiological sighs.\nVisualisation. Picture the presentation and the audience’s positive reaction. Notice the details of the room.\nListen to relaxing sounds and music."
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#holding-the-neuro",
    "href": "posts/business/executive_presence/presence.html#holding-the-neuro",
    "title": "Executive Presence",
    "section": "5.6. Holding the Neuro",
    "text": "5.6. Holding the Neuro\nThe neurovascular points are the ridges above the eyes. They regulate the flow of blood in the body and bring blood to the thinking brain.\nWhen you are under stress, less blood goes to the frontal area of your brain. Rubbing the neurovascular points and temples can stimulate more blood flow. Do this while breathing deeply.\nA hand on the forehead and the other hand on the back of the neck can also help.\nThis can also help when processing trauma."
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#being-comfortable-on-camera",
    "href": "posts/business/executive_presence/presence.html#being-comfortable-on-camera",
    "title": "Executive Presence",
    "section": "6.1. Being Comfortable on Camera",
    "text": "6.1. Being Comfortable on Camera\nPractice in low stakes situations. FaceTime calls are the same basic idea as being on camera, so get used to it. Practice looking at the camera not the person; looking at a camera lens is weird and unusual so practice it."
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#types-of-videos",
    "href": "posts/business/executive_presence/presence.html#types-of-videos",
    "title": "Executive Presence",
    "section": "6.2. Types of Videos",
    "text": "6.2. Types of Videos\nSuggestions of business video content:\n\nProduct demonstrations\nAbout me\nNew product and service launches\nHomepage\nMeet the team\nBehind the scenes\nWeekly video blogs\nFrequently asked questions\nMarket updates\nReviews\n\nThink about the videos you like and dislike.\nTips:\n\nKnow your viewer\nKeep it short and sweet\nLove the lens\nUse calls to action\nPractice"
  },
  {
    "objectID": "posts/business/conflict_resolution/notes.html",
    "href": "posts/business/conflict_resolution/notes.html",
    "title": "Conflict Resolution",
    "section": "",
    "text": "There is a distinction between healthy confilct (e.g. a disagreement) and unhealthy conflict. Disagreement isn’t inherently a bad thing!\nRelationships break down when people see conflicts as win/lose (“I’m so assertive and everyone needs to do what’s best for me”) or lose/win (“I’m so scared of conflict that I’ll people please to avoid it”). Eventually, both lead to lose/lose situations as parties retaliate in their own ways.\nConflicts should be seen as opportunities for win/win.\n\n\n\nHarmony. Things are peaceful. Differences of opinion may happen, but they’re handled quickly and effectively.\nDiscomfort. You’re becoming aware in differences of values, ideas, needs, roles, goals, personalities, etc.\nDisagreement. Disagreements begin to feel more emotional. You feel vulnerable and threatened when there are differences of opinion.\nDiscord. You have tried unsuccessfully to work through disagreements, and now tensions are rising. You gather evidence to prove why you are right and they are wrong. Talking does not help as you are both trying to prove the other wrong.\nPolarisation. Emotions are high and objectivity is lost. This is unhealthy conflict. A change of tactics is required to resolve conflict, otherwise the party with more power/authority will impose their will on others.\nDisintegration. Point of no return. The relationship has broken down and is beyond repair.\n\n\n\n\nTrust is like a barometer of the health of a relationship.\nTrust is a necessary but not sufficient condition of a healthy relationship. Having trust doesn’t mean conflicts will never occur, but they are less likely to turn toxic and more likely to be handled in good faith.\n\nAct from personal values.\nMake and keep commitments.\nSeek to improve.\nAccept accountability. Apologise.\nBe curious and interested in others.\nPraise in public.\nListen to others and seek their input. Ensure all people have airtime.\nSeek solutions that are important to others, not just yourself.\n\n\n“We go fast when we walk alone. We go far when we walk together.”"
  },
  {
    "objectID": "posts/business/conflict_resolution/notes.html#phases-of-escalation",
    "href": "posts/business/conflict_resolution/notes.html#phases-of-escalation",
    "title": "Conflict Resolution",
    "section": "",
    "text": "Harmony. Things are peaceful. Differences of opinion may happen, but they’re handled quickly and effectively.\nDiscomfort. You’re becoming aware in differences of values, ideas, needs, roles, goals, personalities, etc.\nDisagreement. Disagreements begin to feel more emotional. You feel vulnerable and threatened when there are differences of opinion.\nDiscord. You have tried unsuccessfully to work through disagreements, and now tensions are rising. You gather evidence to prove why you are right and they are wrong. Talking does not help as you are both trying to prove the other wrong.\nPolarisation. Emotions are high and objectivity is lost. This is unhealthy conflict. A change of tactics is required to resolve conflict, otherwise the party with more power/authority will impose their will on others.\nDisintegration. Point of no return. The relationship has broken down and is beyond repair."
  },
  {
    "objectID": "posts/business/conflict_resolution/notes.html#building-trust",
    "href": "posts/business/conflict_resolution/notes.html#building-trust",
    "title": "Conflict Resolution",
    "section": "",
    "text": "Trust is like a barometer of the health of a relationship.\nTrust is a necessary but not sufficient condition of a healthy relationship. Having trust doesn’t mean conflicts will never occur, but they are less likely to turn toxic and more likely to be handled in good faith.\n\nAct from personal values.\nMake and keep commitments.\nSeek to improve.\nAccept accountability. Apologise.\nBe curious and interested in others.\nPraise in public.\nListen to others and seek their input. Ensure all people have airtime.\nSeek solutions that are important to others, not just yourself.\n\n\n“We go fast when we walk alone. We go far when we walk together.”"
  },
  {
    "objectID": "posts/business/conflict_resolution/notes.html#the-four-styles",
    "href": "posts/business/conflict_resolution/notes.html#the-four-styles",
    "title": "Conflict Resolution",
    "section": "2.1. The Four Styles",
    "text": "2.1. The Four Styles\nWe can categorise communication styles with respect to empathy (concern for others) vs assertiveness (concern for self).\n\n\n\nFour styles of communication\n\n\nThis gives rise to four communication styles:\n\nDominators. High assertiveness, low empathy. During conflict they handle their emotions by defending, arguing, lecturing, blaming, or attacking. Dominators don’t like to be wrong and they don’t like to lose. Their strategy is to convince, control or coerce other people.\nAccommodators. Low assertiveness, high empathy. Accommodators tend to put the opinions, needs and feelings of others ahead of their own; they are generally polite, easy to get along with, non-judgmental, and more self-aware than dominators or avoiders. During conflict, they defer to others and even give in. Over time, accommodators can grow resentful that their needs are unmet and act out in passive-aggressive ways.\nAvoiders. Low assertivenss, low empathy. They are usually easy going, independent, rational and emotionally detached. If things get tense, they try to pretend that everything is okay; their strategy is to leave issues alone and hope that they will go away. They suppress their feelings, use humor, rationalize and minimize. Because they have a hard time dealing with emotional issues, their relationships aren’t as deep; they don’t disclose their true selves to other people but seek to play it safe.\nCollaborators. High assertiveness, high empathy. They are able to share their own point of view, needs and so on but are also able to show empathy for the views and needs of others, responding in ways that allow all parties to “win.”\n\nCollaboration is what we aim for. In a healthy relationship, people are able to assert their own needs and opinions. Yet they are also sensitive to the needs and feelings of others. Understanding this balance is the essence of good communication, a healthy relationship, and the ability to resolve conflict productively.\nAn additional thought on this: does the “self” in “concern for self” expand and contract depending on the situation? If somebody acts more or less assertively whne around others, is that because the sense of self expands to include others in the group? Like a mama bear being more assertive to protect her baby, because her sense of self now includes the baby; she will be more assertive for “mama + baby” than she would have been for “mama” alone. Similarly when people behave differently when their spouse is around."
  },
  {
    "objectID": "posts/business/conflict_resolution/notes.html#common-combinations-of-commincation-styles-and-handling-conflicts",
    "href": "posts/business/conflict_resolution/notes.html#common-combinations-of-commincation-styles-and-handling-conflicts",
    "title": "Conflict Resolution",
    "section": "2.2. Common Combinations of Commincation Styles and Handling Conflicts",
    "text": "2.2. Common Combinations of Commincation Styles and Handling Conflicts\n\nDominate / Accommodate. Can work where one party is indifferent. The risk is the accommodator becomes resentful.\nAvoid / Avoid. Can be temporarily harmonious but by avoiding the lows they avoid the highs.\nDominate / Dominate. Lots of arguments, neither person listening or responding.\nDominate / Avoid. One person in control, the other indifferent.\nAccommodate / Accommodate. Both parties defer to each other, more concerned about the other’s needs, not concerned about or used to getting what they personally want."
  },
  {
    "objectID": "posts/business/conflict_resolution/notes.html#when-is-each-style-desirable",
    "href": "posts/business/conflict_resolution/notes.html#when-is-each-style-desirable",
    "title": "Conflict Resolution",
    "section": "2.3. When is Each Style Desirable?",
    "text": "2.3. When is Each Style Desirable?\nNobody is “always” one style. It depends on the situation at hand and the other party involved.\nGenerally collaboration is ideal, but in certain scenarios we may fall into other styles:\n\nDominate: when there is an emergency requiring decisive action, when protecting ourselves from others taking advantage, when our values are being violated.\nAccommodate: when preserving the relationship is more important than the issue at hand, when you need to buy time to respond, when you may be wrong or others may have better ideas, you want to give the other person a “win”.\nAvoid: when an issue seems trivial or unimportant, when the timing isn’t right, when the issue is outside of your control, when relationship damage outweighs the benefits of action."
  },
  {
    "objectID": "posts/business/conflict_resolution/notes.html#dialogue",
    "href": "posts/business/conflict_resolution/notes.html#dialogue",
    "title": "Conflict Resolution",
    "section": "3.1. Dialogue",
    "text": "3.1. Dialogue\nCollaboration becomes crucial as the topic of communication becomes sensitive or important, or as conflict is escalating.\nCollaboration is the only style that can address high stakes issues effectively. We succeed together or we fail together.\nDialogue is crucial for collaboration. Dialogue is defined as:\n\n“A participative process of communication in which people listen to understand each other’s point of view and then agree upon options to solve problems or resolve their disagreements”\n\nThere a key distinctions between arguments and dialogue.\n\n\n\nArgument\nDialogue\n\n\n\n\nConcerned with self\nConcerned with self and others\n\n\nAdversarial\nUnity\n\n\nIntent to win\nIntent to learn and explore\n\n\nListen to respond\nListen to understand\n\n\nPolarised positions\nMany sides\n\n\nOversimplify issues\nView nuances and complexities\n\n\nRight vs wrong\nDiscovery\n\n\nLook for confirming data\nLook for enlightenment"
  },
  {
    "objectID": "posts/business/conflict_resolution/notes.html#conflict-resolution-model",
    "href": "posts/business/conflict_resolution/notes.html#conflict-resolution-model",
    "title": "Conflict Resolution",
    "section": "3.2. Conflict Resolution Model",
    "text": "3.2. Conflict Resolution Model\nThere is usually a trigger for a conflict.\nThe intensity of the trigger depends on:\n\nRelationship context and culture - is there trust and goodwill?\nThe phases of escalation\n\n\n\n\nConflict resolution model"
  },
  {
    "objectID": "posts/business/conflict_resolution/notes.html#collusion-the-opposite-of-conflict-management",
    "href": "posts/business/conflict_resolution/notes.html#collusion-the-opposite-of-conflict-management",
    "title": "Conflict Resolution",
    "section": "4.1. Collusion: The Opposite of Conflict Management",
    "text": "4.1. Collusion: The Opposite of Conflict Management\nConflict can be thought of as “a circular and mutually reinforcing negative interaction”.\nYou are not responding to the behaviour of the other person, but your interpretation of their behaviour. We judge ourselves by our intentions and others by their actions.\n\n\n\nCollusion model\n\n\nYou need to break out of the collusive cycle to resolve conflict. Be curious about the other person’s interpretation: “the third story” in between yours and theirs.\nDialogue is key to breaking collusion, but you first need to be willing to challenge your interpretation."
  },
  {
    "objectID": "posts/business/conflict_resolution/notes.html#adopting-the-right-mindset",
    "href": "posts/business/conflict_resolution/notes.html#adopting-the-right-mindset",
    "title": "Conflict Resolution",
    "section": "4.2. Adopting the Right Mindset",
    "text": "4.2. Adopting the Right Mindset\n\n4.2.1. Responsibility\n\nLook at your contribution during collusion. It takes two to collude, and even if you’re not fully at fault, it’s rare that you’re 0% at fault. Can you take accountability for the negative parts you are responsible for?\nTake responsibility for yourself. See the collusion diagram - You can’t control the other person’s interpretations, feelings or behaviours. But you can control your own. The other person doesn’t cause you to behave a certain way; they may be a trigger, but they cannot cause it, that takes away your personal power.\nThink through three stories: yours, theirs, and the “third story” of a neutral observer.\n\n\n\n4.2.2. Dealing with Feelings\n“Avoiding feelings” doesn’t work because they will always leak in to discussions through tone of voice, facial expressions etc. It also makes it difficult to listen to others when focusing so hard on repressing emotions.\nFeelings are a symptom of conflict. Unfulfilled needs are the cause. You wouldn’t ignore an obvious physical symptom like a tumour, so don’t ignore feelings.\nLabel your own feelings. All are valid, don’t judge them as good or bad. Then you can understand your needs and which of these are not being met.\nThere are four possible ways that one could respond to negative messages:\n\nTake the message personally and accept it\nDefend yourself and retaliate\nBecome more aware of your own feelings and needs\nUnderstand the other person’s feelings and needs\n\nOption 4 is the best approach. Option 3 can be useful too. Options 1 and 2 are harmful.\nWhen sharing feelings, it is important to do so without judging or blaming. Start statements with “I” rather than “you”.\n\n\n4.2.3. Committing to Outcomes\nExamine your trust paradigm. Do you see people as adversaries or allies? The point isn’t to assume bad people don’t exist; some people are untrustworthy. But do you start from an assumption of mistrust or trust? Guilty until proven innocent or innocent until proven guilty?\nThe majority of disputes you have aren’t down to malice.\nWe have a tendency to assign negative emotions to others and positive to ourselves. Trust begets trust.\nCommit to collaboration. Disagreements should focus on issues, not relationships.\nBe clear about the outcomes you desire during negotiations. This doesn’t mean have a stubborn outcome in mind that you aren’t willing to budge from. Rather, the outcome is to collaborate on a win-win result."
  },
  {
    "objectID": "posts/business/conflict_resolution/notes.html#who-should-initiate-dialogue",
    "href": "posts/business/conflict_resolution/notes.html#who-should-initiate-dialogue",
    "title": "Conflict Resolution",
    "section": "4.3. Who Should Initiate Dialogue",
    "text": "4.3. Who Should Initiate Dialogue\nA common misconception is that it should be the person with greater authority.\nIt can be anyone with a vested interest in the outcome and some ability to influence the outcome.\nYou may also want to weigh the risk vs reward. Some conversations might not be worth having.\nTimes when it may be best to not initiate:\n\nYou’re upset and want to vent\nThe true conflict is internal - you’re actually angry about something else\nThe other person has no desire to resolve the conflict, and you don’t have the power to change their mind\nThere is a power/authority imbalance\n\nFour considerations when deciding whether to enter into dialogue:\n\nImpact. Short-term and long-term consequences\nRisk. Are you putting the relationship at risk by bringing up a topic? What could you potentially lose? What are the best, worst and most likely outcomes?\nReward. What do you have to gain by resolving this conflict?\nConfidence. How confident are you in initiating and facilitating dialogue?\n\nNot every conflict can be resolved:\n\nSome issues are protracted and the people involved may not see the value in dialogue.\nSome problems have no solutions."
  },
  {
    "objectID": "posts/business/conflict_resolution/notes.html#preparation",
    "href": "posts/business/conflict_resolution/notes.html#preparation",
    "title": "Conflict Resolution",
    "section": "5.1. Preparation",
    "text": "5.1. Preparation\nPutting yourself in the right frame of mind and deciding how to initiate dialogue.\nStep back from conflict, even if only briefly.\n\nReview these course materials rationally: nature of conflict, four communication styles, phases of escalation, collusion diagram and conflict resolution steps.\nWork through your thoughts and feelings.\nEvaluate your commitment to collaboration by answering the questions below.\n\nEvaluating your commitment to collaboration:\n\nWhat is your intent going in to this conversation? Is it to solve a problem, or to win, or to punish someone, or to survive an encounter?\nAm I willing to show respect (in language, tone, demeanour)?\nAm I committed to collaboration and win-win outcomes?\nAm I willing to take responsibility rather than blame others?\nAm I seeking shared understanding rather than trying to build my own case?\nAm I willing to be curious about the other party and understand their point of view?\nCan I listen and draw the other out?\nAm I concerned for us rather than just me?\nAm I willing to take the time necessary to arrive at a good outcome?"
  },
  {
    "objectID": "posts/business/conflict_resolution/notes.html#invitation",
    "href": "posts/business/conflict_resolution/notes.html#invitation",
    "title": "Conflict Resolution",
    "section": "5.2. Invitation",
    "text": "5.2. Invitation\nInviting the other person to problem solve together.\nInitiating the dialogue in a way that makes the other party feel safe sharing, thereby increasing the chances of a positive outcome. Safety, respect and openness. Allies not adversaries.\n96% of the time, if a conversation starts poorly, it will end poorly.\n\nUse a soft start up. This doesn’t mean you should beat around the bush, but it means avoiding criticism, contempt and accusations.\nShow respect. You can disagree but still be respectful.\nAvoid blame. Start with “I” rather than “you”; you own your feelings but you’re not responsible for the actions of others.\nUse repair attempts. This is throughout the conversation, not just at the start. Try to de-escalate and defuse tension. Listen more if the other is getting defensive, acknowledge or apologise for mistakes, restate your intent to collaborate, recognise something good about the other or concede a point they’re making, light humour. Don’t get so caught up in this issue that your ignore HOW you’re communicating.\n\nThe following sub-sections discuss some important skills for invitation.\n\n5.2.1. Soft Start Up\nSome examples of a soft start up to invite dialogue:\n\n“There’s something important that I’d like to get your thoughts on”\n“I’d like to share something that I’ve been thinking about lately”\n“I’d like to know your point of view about…”\n\n\n\n5.2.2. Leveling\nCreating a climate for dialogue in which everybody knows they will be treated in a fair way.\nLeveling statements are usually short and should include:\n\nState a concern without blame or hostility\nExpress a desire to understand the other person’s point of view\nLet them know you want to work things out together\n\n\n\n5.2.3. Clarifying Your Intent\nPoint out what you do and do not intend. Start with the don’t.\n\n“I don’t intend to place blame. I do intend to get back to a good working relationship.”\n\n\n\n5.2.4. Clarifying Your Concerns\nHighlight any concerns about entering into dialogue at the beginning.\n\n“I don’t want you to think our relationship will be harmed by bringing this up.”\n\n\n\n5.2.5. Stating Your Commitment to Collaboration\n\n“My intent is to be helpful and not divisive”."
  },
  {
    "objectID": "posts/business/conflict_resolution/notes.html#exploration",
    "href": "posts/business/conflict_resolution/notes.html#exploration",
    "title": "Conflict Resolution",
    "section": "5.3. Exploration",
    "text": "5.3. Exploration\nDevelop a shared understanding of everybody’s perspective before we try to solve the problem. Problem solving happens in the next step, this step is purely informational.\nExploration is a feedback loop of inquiry and advocacy.\nGoals:\n\nIdentify and understand the assumptions and perceptions of all parties\nTo search for complete view of reality\n\nTwo heads are better than one. No single person is smarter than everybody. Research shows groups come to better decisions than individuals if they draw everybody’s opinions out and don’t fall victim to groupthink.\nPotential mistakes in this phase:\n\nTrying to persuade or coerce others to your point of view\nMoving on to problem solving too quickly\n\n\n5.3.1. Inquiry\nInquiry is “encouraging others to disclose their point of view then listening with empathy”.\nStart with inquiry to demonstrate your intentions to listen and build trust.\nExamples:\n\n“What’s the situation as you see it?”\n“How do you see things?”\n“What are your thoughts?”\n\nThis is active not passive. Ask follow-on questions to draw them out. Acknowledge their point of view which is not the same as conceding or agreeing to it.\n\n“Here’s how it looks from your point of view”\n\nKeys to inquiry:\n\nNon-judgement\nSuspend your own point of view\nBe open and willing to learn\nKeep the responsibility on the speaker\n\nFour skills of inquiry:\n\nInviting. A statement that shows your willingness to listen.\n\n“What’s your POV?”\n“Could you tell me more?”\n“How do you see this differently?”\n\nClarifying. Asking questions that invite the speaker to expand. Don’t overdo it or it begins to feel like an interrogation.\n\n“What was your interpretation of that comment?”\n“Can you share more what you mean by that?”\n“What did you feel when that occurred?”.\n\nEmpathy. Understand the deeper meaning, not just the words.\nPriming. Making statements that represent your best guess when the other person shuts down or is struggling to articulate themselves.\n\n“I would guess that you’re opposed to what we’ve been discussing”\n\n\n\n\n5.3.2. Advocacy\nAdvocacy is “disclosing your own point of view”.\nDo this clearly but not dogmatically; show a willingness to be influenced.\n\n“This is the situation as I see it”\n“These are my thoughts”\n\nIt can be help to do this in two parts:\n\n“This is my point of view and this is how I arrived at it”\n\n\nGet right to the point. Don’t be vague or ramble.\nBe willing to share your feelings.\nAdvocate non-dogmatically. Share your opinion and be aware that there may be other valid opinions.\nEncourage commentary.\n\nUsually, inquiry is the more important skill over advocacy because people need to be drawn out and made to feel safe. The exception is when dealing with dominators. Here, it is important to acknowledge their point but focus on assertively advocating your own point. You may even need to say “Can you let me know you understand what I’m saying?” or “Can you repeat back to me what I’m saying?”\n\n\n5.3.3. Immediacy\nImmediacy is commenting on the process of communicating.\nThis is often necessary when you have done your best to understand the other’s point of view but they are unwilling to reciprocate.\n\n“Can we get back to the issue? It feels like the conversation has become hostile”\n\n\n\n5.3.4. Ground Rules\nParticularly helpful with groups.\n\nStay with the topic at hand and have one conversation at a time\nSeek what is right not who is right\nSeek involvement from all members\nShare opinions honestly with the intent to be constructive\nShare feedback with the intent of helpfulness, even when it is difficult to give\nBe open-minded to receive feedback, even when it is difficult to hear\nIf you disagree with the majority of the group, you have a responsibility to:\n\nMake sure you understand others’ point of view\nExplain your own point of view\nSuggest alternative ideas that will work as well or better\n\nKeep disagreements in the group\nWhat happens in the room stays in the room, aside from follow up actions and decisions\nBe patient with the process\nTry to have fun, keep a sense of humour"
  },
  {
    "objectID": "posts/business/conflict_resolution/notes.html#collaboration",
    "href": "posts/business/conflict_resolution/notes.html#collaboration",
    "title": "Conflict Resolution",
    "section": "5.4. Collaboration",
    "text": "5.4. Collaboration\nWorking together to find a win-win solution that solves everybody’s needs.\nIn some cases we don’t even need the collaboration step. The shared understanding built in the exploration phase is enough to “clear the air” and no more action is needed.\nThree steps to achieve solutions during collaboration:\n\nIdentify what is important to each party. Unmet needs are at the heart of conflict. People don’t fight over solutions, they fight over their needs. These needs may have been identified in the exploration phase, but it’s worth getting each party to explicitly state their needs in their own words so they can be confident that these will be considered in any proposed solution. “What is most important to you as we search for a solution?”, “What needs do you have that need to be met in order to know that we’ve solved the problem”\nBrainstorm alternatives. Just list the options, you’re not settling on any single option yet. See rules for brainstorming below.\nCome up with actions or solutions. Consensus is often helpful at this point. This is not the same as unanimity. Consensus means that everyone has contributed to the process of brainstorming solutions. Everyone can support the final solution even if it wasn’t their idea or their first choice. The decisions made by the group may well be different than those made if one person had been in complete control, and this is ok (and actively encouraged).\n\nRules for brainstorming:\n\nMake sure everyone understand the desired outcome.\nEach person shares ideas. Can be round robin or spontaneously.\nOne thought expressed at a time.\nNo criticism of any ideas.\nOutrageous ideas encouraged.\nNo discussion of the relative merits of ideas, only to clarify its meaning.\nBuild on each other’s ideas."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html",
    "href": "posts/software/snowflake/snowpro_notes.html",
    "title": "Snowflake: SnowPro Core",
    "section": "",
    "text": "The Snowsight interface is the GUI through which we interact with Snowflake.\nWhen querying a Snowflake table, a fully qualified table name means database_name + schema_name + table_name. For example, “DERIVED_DB.PUBLIC.TRADES_DATA”\nWorksheets are associated with a role.\nA warehouse is needed for compute to execute a query.\nSnowflake is a “self-managed cloud data platform”. It is cloud only. No on-premise option.\n“Self-managed” service means:\n\nNo hardware\nNo software\nNo maintenance\n\n“Data platform” means it can function as:\n\nData warehouse\nData lake - mix of structured and semi structured data\nData science - use your preferred language via Snowpark"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#multi-cluster-shared-disk",
    "href": "posts/software/snowflake/snowpro_notes.html#multi-cluster-shared-disk",
    "title": "Snowflake: SnowPro Core",
    "section": "2.1. Multi-Cluster Shared Disk",
    "text": "2.1. Multi-Cluster Shared Disk\nIn general, there are two approaches to designing a dsitributed data / compute platform: shared-disk and shared-nothing.\nShared-disk uses central data storage connected to multiple compute nodes.\n\nPros: simple, easy data management since their is only one database/disk\nCons: limited scalability (bottleneck of the central disk), single point of failure\n\nShared-nothing keeps each node independent. Each node is a separate processor, memory and disk.\n\nPros: scalability, availability\nCons: complicated, expensive\n\nSnowflake uses a hybrid approach: “multi-cluster shared-data”.\n\nThere is a single data repository like shared-disk.\nThere are multiple clusters or nodes that store a portion of the data locally, like shared-nothing.\n\nThis combines the pros of both: simplicity and scalability."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#layers-of-snowflake",
    "href": "posts/software/snowflake/snowpro_notes.html#layers-of-snowflake",
    "title": "Snowflake: SnowPro Core",
    "section": "2.2. Layers of Snowflake",
    "text": "2.2. Layers of Snowflake\nThere are three distinct layers of Snowflake:\n\nDatabase storage\n\nCompressed columnar storage.\nThis is stored as blobs in AWS, Azure, GCP etc.\nSnowflake abstracts this away so we just interact with it like a table.\nThis is optimised for OLAP (analytical purposes) which is read-heavy, rather than OLTP which is write-heavy.\n\nCompute\n\n“The muscle of the system”.\nQuery processing.\nQueries are processed using “virtual warehouses”. These are massive parallel processing compute clusters, e.g. EC2 on AWS.\n\nCloud services\n\n“The brain of the system”.\nCollection of services to manage and coordinate components, e.g. the S3 and EC2 instances used in the other two layers.\nThe cloud services layer also runs on a compute instance of the cloud provider and is completely handled by Snowflake.\nThis layer handles: authentication, access control, metadata management, infrastructure management, query parsing and optimisation. The query execution happens in the compute layer."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#loading-data-into-snowflake",
    "href": "posts/software/snowflake/snowpro_notes.html#loading-data-into-snowflake",
    "title": "Snowflake: SnowPro Core",
    "section": "2.3. Loading Data into Snowflake",
    "text": "2.3. Loading Data into Snowflake\nThis is covered more extensively in its own section, but this sub-section serves as a brief introduction.\nThe usual SQL commands can be used to create databases and tables.\nCREATE DATABASE myfirstdb\nALTER DATABASE myfirstdb RENAME firstdb\nCREATE TABLE loan_payments (\n    col1 string,\n    col2 string,\n);\nWe can specify a database to use with the USE DATABASE command to switch the active database. This avoids having to use the fully qualified table name everywhere.\nUSE DATABASE firstdb\n\nCOPY INTO loan_payments\nFROM s3/… -- The URL to copy from\nfile_format = (delimiter = “,”,\n               skip rows=1,\n               type=csv);"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#snowflake-editions",
    "href": "posts/software/snowflake/snowpro_notes.html#snowflake-editions",
    "title": "Snowflake: SnowPro Core",
    "section": "2.4. Snowflake Editions",
    "text": "2.4. Snowflake Editions\nThe different Snowflake editions vary by features and pricing. The feature matrix is available on the Snowflake docs.\n\nStandard\n\nComplete DWH, automatic data encryption, support for standard and special data types, time travel 1 day, disaster recovery for 7 days beyond time travel, network policies, federated auth and SSO, 24/7 support\n\nEnterprise\n\nMulti cluster warehouse, time travel 90 days, materialised views, search optimisation, column-level security, 24 hour early access to new releases\n\nBusiness critical\n\nAdditional security features such as customer managed encryption, support for data specific regulation, database failover and fallback\n\nVirtual private\n\nDedicated virtual servers and warehouse, dedicated metadata store. Isolated from all other snowflake accounts."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#compute-costs",
    "href": "posts/software/snowflake/snowpro_notes.html#compute-costs",
    "title": "Snowflake: SnowPro Core",
    "section": "2.5. Compute Costs",
    "text": "2.5. Compute Costs\n\n2.5.1. Overview of Cost Categories\nCompute costs and storage costs are decoupled and can be scaled separately. “Pay for what you need”.\n\nActive warehouses\n\nUsed for standard query processing.\nBilled per second (minimum 1 minute).\nDepends on size of warehouse, time and number of warehouses.\n\nCloud services\n\nBehind-the-scenes cloud service tasks.\nOnly charged if &gt;10% of warehouse consumption, which is not the case for most customers.\n\nServerless\n\nUsed for search optimisation and Snowpipe.\nThis is compute that is managed by snowflake, e.g. event-based processing.\n\n\nThese are charged in Snowflake credits.\n\n\n2.5.2. Calculating Number of Credits Consumed-\nThe warehouses consume the following number of credits per hour:\n\n\n\nWarehouse Size\nNumber of Credits\n\n\n\n\nXS\n1\n\n\nS\n2\n\n\nM\n4\n\n\nL\n8\n\n\nXL\n16\n\n\n4XL\n128\n\n\n\nCredits cost different amounts per edition. It also depends on the cloud provider (AWS) and region (US-East-1). Indicative costs for AWS US-East-1 are:\n\n\n\nEdition\n$ / Credit\n\n\n\n\nStandard\n2\n\n\nEnterprise\n3\n\n\nBusiness Critical\n4"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#storage-and-data-costs",
    "href": "posts/software/snowflake/snowpro_notes.html#storage-and-data-costs",
    "title": "Snowflake: SnowPro Core",
    "section": "2.6 Storage and Data Costs",
    "text": "2.6 Storage and Data Costs\n\n2.6.1. Storage Types and Costs\nMonthly storage costs are based on average storage used per month. Also depends on cloud provider and region. Cost is calculated AFTER Snowflake’s data compression.\nThere are two options for storage pricing:\n\nOn demand storage: Pay for what you use.\nCapacity storage: Pay upfront for defined capacity.\n\nTypically start with on demand until we understand our actual usage, then shift to capacity storage once this is stable.\n\n\n2.6.2. Transfer Costs\nThis depends on data ingress vs egress.\n\nData IN is free\n\nSnowflake wants to remove friction to getting your data in.\n\nData OUT is charged\n\nSnowflake wants to add friction to leaving.\nDepends on cloud provider and region. In-region transfers are free. Cross-region or cross-providers are charged."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#storage-monitoring",
    "href": "posts/software/snowflake/snowpro_notes.html#storage-monitoring",
    "title": "Snowflake: SnowPro Core",
    "section": "2.7. Storage Monitoring",
    "text": "2.7. Storage Monitoring\nWe can monitor storage for individual tables.\nSHOW TABLES gives general table storage stats and properties.\nWe get more detailed views with TABLE_STORAGE_METRICS. We can run this against the information schema or the account storage. These split the sizes into active bytes, time travel bytes and failsafe bytes.\nFor the information schema metrics:\nSELECT * FROM DB_NAME.INFORMATION_SCHEMA.TABLE_STORAGE_METRICS;\nFor the account admin metrics, this needs to use the correct account admin role USE ROLE ACCOUNTADMIN.\nSELECT * FROM SNOWFLAKE.ACCOUNT_USAGE.TABLE_STORAGE_METRICS;\nWe can also look at the Admin -&gt; Usage screen in the Snowflake GUI."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#resource-monitors",
    "href": "posts/software/snowflake/snowpro_notes.html#resource-monitors",
    "title": "Snowflake: SnowPro Core",
    "section": "2.8. Resource Monitors",
    "text": "2.8. Resource Monitors\nResource monitors help us control and monitor credit usage of individual warehouses and the entire account.\nWe can set a credit quota which limit the credits used per period. For example, the maximim number of credits that can be spent per month.\nWe can set actions based on when a percentage of the credit limit is reached. These percentages can be &gt;100%. There are three options for the choice of action:\n\nNotify\nSuspend and notify (but continue running tasks that have already started)\nSuspend immediately (aborting any running queries) and notify.\n\nWe set this using the Usage tab in the ACCOUNTADMIN role in the snowsight UI under Admin -&gt; Usage. Other roles can be granted MONITOR and MODIFY privileges.\nWe can select a warehouse then filter on different dimensions, for example, distinguishing storage vs compute vs data transfer costs.\nTo set up a new resource monitor, we give it:\n\nName\nCredit quota: how many credits to limit to\nMonitor type: specific warehouse, group of warehouses, or overall account\nSchedule\nActions"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#warehouses-and-multi-clustering",
    "href": "posts/software/snowflake/snowpro_notes.html#warehouses-and-multi-clustering",
    "title": "Snowflake: SnowPro Core",
    "section": "2.9. Warehouses and Multi Clustering",
    "text": "2.9. Warehouses and Multi Clustering\n\n2.9.1. Warehouse Properties\nThere are different types and sizes of warehouse and they can be multi-clustered.\nTypes: standard and snowpark-optimised (for memeory-intensive tasks like ML)\nSize: XS to XXL. Snowpark type is only M or bigger and consumes 50% more credits\nMulti-clustering is good for more queries, i.e. more concurrent users. We scale horizontally so there are multiple small warehouses rather than one big one. They can be in maximised mode (set size) or autoscaled mode (number of nodes scales between predefined min and max)\nThe autoscaler decides to add warehouses based on the queue, according to the scaling policy.\n\nStandard\n\nFavours starting extra clusters.\nStarts a new cluster as soon as there is a query queued.\nCluster shuts down after 2 to 3 successful checks. A “check” is when the load on the least used node could be redistributed to other nodes.\n\nEconomy\n\nFavours conserving credits.\nStarts a new cluster once the workload for the cluster would keep it running for &gt; 6 mins.\nCluster shuts down after 5-6 successful checks.\n\n\n\n\n2.9.2. Creating a Warehouse\nTo create a warehouse, we need to use the ACCOUNTADMIN, SECURTIYADMIN or SYSADMIN role.\nWarehouses can either be created through UI or SQL.\nCREATE WAREHOUSE my_wh\nWITH\nWAREHOUSE_SIZE = XSMALL\nMIN_CLUSTER_COUNT = 1\nMAX_CLUSTER_COUNT = 3\nAUTO_RESUME = TRUE\nAUTO_SUSPEND = 300\nCOMMENT = 'This is the first warehouse'\nWe can also ALTER or DROP a warehouse in SQL, just like we normally would with DROP TABLE.\nDROP WAREHOUSE my_wh;"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#snowflake-objects",
    "href": "posts/software/snowflake/snowpro_notes.html#snowflake-objects",
    "title": "Snowflake: SnowPro Core",
    "section": "2.10. Snowflake Objects",
    "text": "2.10. Snowflake Objects\nThere is a hierarchy of objects in Snowflake.\n\n\n\n\n\nflowchart TD\n\n\n  A(Organisation) --&gt; B1(Account 1)\n  A(Organisation) --&gt; B2(Account 2)\n\n\n  B1 --&gt; C1(Users)\n  B1 --&gt; C2(Roles)\n  B1 --&gt; C3(Databases)\n  B1 --&gt; C4(Warehouses)\n  B1 --&gt; C5(Other account objects)\n  \n  C3 --&gt; D1(Schemas)\n\n  D1 --&gt; E1(UDFs)\n  D1 --&gt; E2(Views)\n  D1 --&gt; E3(Tables)\n  D1 --&gt; E4(Stages)\n  D1 --&gt; E5(Other database objects)\n\n\n\n\n\n\nAn organisation (managed by ORGADMIN) can have multiple accounts (each managed by am ACCOUNTADMIN). These accounts might be by cloud region or department.\nWithin each account we have multiple account objects: users, roles, databases, warehouses, other objects.\nDatabases can have multiple schemas.\nSchemas can have multiples UDFs, views, tables, stages, other objects."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#snowsql",
    "href": "posts/software/snowflake/snowpro_notes.html#snowsql",
    "title": "Snowflake: SnowPro Core",
    "section": "2.11. SnowSQL",
    "text": "2.11. SnowSQL\nSnowSQL is used to connect to Snowflake via the command line. It needs to be installed on your local machine.\nWe can execute queries, load and unload data, etc."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#stages",
    "href": "posts/software/snowflake/snowpro_notes.html#stages",
    "title": "Snowflake: SnowPro Core",
    "section": "3.1. Stages",
    "text": "3.1. Stages\nStages are locations used to store data.\nFrom the stage, say an S3 bucket, we can load data from stage -&gt; database. Likewise, we can unload data from database -&gt; stage (S3 bucket).\nStages can be internal (managed by Snowflake) or external (managed by your cloud provider, eg AWS S3).\n\n3.1.1. Internal Stage\nAn internal stage is managed by Snowflake.\nWe upload data into an internal stage using the PUT command. By default, files are compressed with gzip and encrypted.\nWe load it into the database using the COPY INTO command. We can also unload using the COPY INTO command by varying the destination.\nThere are three types of stage:\n\nUser stage\n\nCan only be accessed by one user\nEvery user has one by default\nCannot be altered or dropped\nAccessed with @~\n\nTable stage\n\nCan only be accessed by one table\nCannot be altered or dropped\nUse this to load to a specific table\nAccessed with @%\n\nNamed stage\n\nCREATE STAGE to create your own\nThis is then just like any other database object, so you can modify it or grant privileges\nMost commonly used stage\nAccessed with @\n\n\nA typical use case for an internal stage is when we have a file on our local system that we want to load into Snowflake, but we don’t have an external cloud provider set up.\n\n\n3.1.2. External Stage\nAn external stage connects to an external cloud provider, such as an S3 bucket.\nWe create it with the CREATE STAGE command as with an internal stage. This creates a Snowflake object that we can modify and grant privileges to.\nCREATE STAGE stage_name \n  URL='s3://bucket/path/'\nWe can add CREDENTIALS argument but this would store them in plain text. A better practice is to pass a STORAGE_INTEGRATION argument that points to credentials.\nWe can also specify the FILE_FORMAT.\n\n\n3.1.3. Commands For Stages\nSome of the most common commands for stages:\n\nLIST\n\nList all files (and additional properties) in the stage.\n\nCOPY INTO\n\nLoad data into the stage, or unload data from the stage.\n\nSELECT\n\nQuery from stage\n\nDESC\n\nDescribe the stage. Shows the default values or arguments."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#copy-into",
    "href": "posts/software/snowflake/snowpro_notes.html#copy-into",
    "title": "Snowflake: SnowPro Core",
    "section": "3.2. COPY INTO",
    "text": "3.2. COPY INTO\nThis can bulk load or unload data.\nA warehouse is needed. Data transfer costs may apply if moving across regions or cloud providers.\n\n3.2.1. Loading Data\nLoad data from a stage to a table with:\nCOPY INTO table_name \nFROM stage_name\nWe can specify a file or list of files with the FILES argument.\nSupported file formats are:\n\ncsv (default)\njson\navro\norc\nparquet\nxml\n\nWe can also use the PATTERN argument to match a file pattern with wildcards, e.g. order*.csv\n\n\n3.2.2. Unloading Data\nUnloading data from the table to a stage uses the same syntax:\nCOPY INTO stage_name \nFROM table_name\nAs with loading, we can specify a file format with the FILE_FORMAT arg, or pass a reusable FILE_FORMAT object.\nCOPY INTO stage_name \nFROM table_name\nFILE_FORMAT = ( FORMAT_NAME = 'file_format_name' |\n                TYPE = CSV )"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#file-format",
    "href": "posts/software/snowflake/snowpro_notes.html#file-format",
    "title": "Snowflake: SnowPro Core",
    "section": "3.3. File Format",
    "text": "3.3. File Format\nIf the file format is not specified, it defaults to csv. You can see this and other default values by describing the stage with:\nDESC STAGE stage_name\nWe can overrule the defaults by specifying FILE_FORMAT argument in the COPY INTO command.\nA better practice is to use the file_format arg to pass a file_format object such as\nFILE_FORMAT = (TYPE = CSV)\nWe create this object with\nCREATE FILE FORMAT file_format_name\nTYPE = CSV\nFIELD_DELIMITER = ‘,’\nSKIP_HEADER = 1\nWe write this file format to a table like manage_db. Then we can reuse it in multiple places when creating the stage or table, loading or unloading data, etc."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#insert-and-update",
    "href": "posts/software/snowflake/snowpro_notes.html#insert-and-update",
    "title": "Snowflake: SnowPro Core",
    "section": "3.4. Insert and Update",
    "text": "3.4. Insert and Update\nInsert is the same as standard SQL:\nINSERT INTO table_name\nVALUES (1, 0.5, 'string')\nTo only insert specific columns:\nINSERT INTO table_name (col1, col2)\nVALUES (1, 0.5)\nINSERT OVERWRITE will truncate any existing data and insert only the given values. Use with caution! Any previous data is dropped, the table with only have the rows in this command.\nUpdate also works like standard SQL:\nUPDATE table_name\nSET col1=10\nWHERE col1=1\nTRUNCATE removes all of the values in the table.\nDROP removes the entire table object and its contents."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#storage-integration-object",
    "href": "posts/software/snowflake/snowpro_notes.html#storage-integration-object",
    "title": "Snowflake: SnowPro Core",
    "section": "3.5. Storage Integration Object",
    "text": "3.5. Storage Integration Object\nThis object stores a generated identity for external cloud storage.\nWe create it as a Snowflake object which constrains the allowed location and grant permissions to it in AWS, Azure etc."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#snowpipe",
    "href": "posts/software/snowflake/snowpro_notes.html#snowpipe",
    "title": "Snowflake: SnowPro Core",
    "section": "3.6. Snowpipe",
    "text": "3.6. Snowpipe\nThe discussion so far has focused on bulk loading, i.e. manual loading of a batch of data.\nSnowpipe is used for continuous data loading.\nA pipe is a Snowflake object. It loads data immediately when a file appears in blob storage. It triggers a predefined COPY command. This is useful when data needs to be available immediately.\nSnowpipe uses serverless features rather than warehouses.\nWhen files are uploaded to an S3 bucket, it sends an event notification to a serverless process which executes the copy command into the Snowflake database.\nCREATE PIPE pipe_name\nAUTO_INGEST = TRUE\nINGESTION = notification integration from cloud storage \nCOMMENT = string\nAS COPY INTO table_name\nFROM stage_name\nSnowpipe can be triggered by cloud messages or REST API. Cloud messages are for external stages only with that cloud provider. REST API can be internal or external stage.\n\nCost is based on “per second per core” of the serverless process.\nTime depends on size and number of files.\nIdeal file size is between 100-250 MB.\n\nSnowflake stores metadata about the file loading. Old history is retained for 14 days. The location of the pipe is stored in a schema in the database.\nThe schedule can be paused or resumed by altering the pipe.\nALTER PIPE pipe_name\nSET PIPE_EXECUTION_PAUSED = True"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#copy-options",
    "href": "posts/software/snowflake/snowpro_notes.html#copy-options",
    "title": "Snowflake: SnowPro Core",
    "section": "3.7. Copy Options",
    "text": "3.7. Copy Options\nThese are arguments we can pass to COPY INTO for loading and unloading. Some options only apply to loading and do not apply to unloading.\nThey are properties of the stage object, so if the arguments are not passed Snowflake will fall back to these default values.\n\n3.7.1. ON_ERROR\n\nData Type: String\nDescription: Only for data loading. How to handle errors in files.\nPossible Values:\n\nCONTINUE - Continue loading file if errors are found.\nSKIP_FILE - Skip loading this file if errors are found. This is the default for Snowpipe.\nSKIP_FILE_&lt;num&gt; - Skip if &gt;= num errors are found (absolute).\nSKIP_FILE_&lt;pct&gt;% - Skip if &gt;= pct errors are found (percentage).\nABORT_STATEMENT - Abort loading if an error is found. This is the default for bulk load.\n\n\n\n\n3.7.2. SIZE_LIMIT\n\nData Type: Int\nDescription: Maximum cumulative size, in bytes, to load. Once this amount of data has been loaded, skip any remaining files.\nPossible Values: Int bytes.\n\n\n\n3.7.3. PURGE\n\nData Type: Bool\nDescription: Remove files from the stage after they have been loaded.\nPossible Values: FALSE (default) | TRUE\n\n\n\n3.7.4. MATCH_BY_COLUMN_NAME\n\nData Type: String\nDescription: Load semi structured data by matching field names.\nPossible Values: NONE (default) | CASE_SENSITIVE | CASE_INSENSITIVE\n\n\n\n3.7.5. ENFORCE_LENGTH\n\nData Type: Bool\nDescription: If we have a varchar(10) field, how should we handle data that is too long?\nPossible Values:\n\nTRUE (default) - Raise an error\nFALSE - Automatically truncate strings\n\n\nTRUNCATECOLUMNS is an alternative arg that does the opposite.\n\n\n3.7.6. FORCE\n\nData Type: Bool\nDescription: If we have loaded this file before, should we load it again?\nPossible Values: False (default) | TRUE\n\n\n\n3.7.7. LOAD_UNCERTAIN_FILES\n\nData Type: Bool\nDescription: Should we load files if the load status is unknown?\nPossible Values: False (default) | TRUE\n\n\n\n3.7.8. VALIDATION_MODE\n\nData Type: String\nDescription: Validate the data instead of actually loading it.\nPossible Values:\n\nRETURN_N_ROWS - Validate the first N rows and returns them (like a SELECT statement would). If there is one or more errors in those rows, raise the first.\nRETURN_ERRORS - Return all errors in the file."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#validate",
    "href": "posts/software/snowflake/snowpro_notes.html#validate",
    "title": "Snowflake: SnowPro Core",
    "section": "3.8. VALIDATE",
    "text": "3.8. VALIDATE\nThe VALIDATE function validates the files loaded in a previous COPY INTO.\nReturns a list of errors from that bulk load. This is a table function, which means it returns multiple rows.\nSELECT * \nFROM TABLE(VALIDATE(table_name, JOB_ID =&gt; ‘_last’))\nWe can pass a query ID instead of _last to use a specific job run rather than the last run."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#unloading",
    "href": "posts/software/snowflake/snowpro_notes.html#unloading",
    "title": "Snowflake: SnowPro Core",
    "section": "3.9. Unloading",
    "text": "3.9. Unloading\nThe syntax for unloading data from a table into a stage is the same as loading, we just swap the source and target.\nCOPY INTO stage_name FROM table_name\nWe can unload specific rows or columns by using a SELECT statement:\nCOPY INTO stage_name \nFROM (SELECT col1, col2 FROM table_name)\nWe can pass a FILE_FORMAT object and HEADER args.\nWe can also specify the prefix or suffix for each file. By default the prefix is data_ and the suffix is _0, _1, etc.\nCOPY INTO stage_name/myprefix\nThis is the default behaviour to split the output into multiple files once MAX_FILE_SIZE is reached, setting an upper limit on the output. The SINGLE parameter can be passed to override this, to force the unloading task to keep the output to a single file without splitting.\nIf unloading to an internal stage, to get the data on your local machine use SnowSQL to run a GET command on the internal stage after unloading.\nYou can then use the REMOVE command to delete from the internal stage."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#transformations-and-functions",
    "href": "posts/software/snowflake/snowpro_notes.html#transformations-and-functions",
    "title": "Snowflake: SnowPro Core",
    "section": "4.1. Transformations and Functions",
    "text": "4.1. Transformations and Functions\nWe can specify transformations in the SELECT statement of the COPY command.\nThis can simplify ETL pipelines when performing simple transformations such as: column reordering, casting data types, removing columns, truncating strings to a certain length. We can also use a subset of SQL functions inside the COPY command. Supports most standard SQL functions defined in SQL:1999.\nSnowflake does not support more complex SQL inside the COPY command, such as FLATTEN, aggregations, GROUP BY, filtering with WHERE, JOINs.\nSupported functions:\n\nScalar functions. Return one value per row. E.g. DAYNAME\nAggregate functions. Return one value per group / table. E.g. MAX.\nWindow functions. Aggregate functions that return one value per row. E.g. SELECT ORDER_ID, SUBCATEGORY, MAX(amount) OVER (PARTITION BY SUBCATEGORY) FROM ORDERS;\nTable functions. Return multiple rows per input row. E.g. SELECT * FROM TABLE(VALIDATE(table_name, JOB_ID =&gt; ‘_last’))\nSystem functions. Control and information functions. E.g. SYSTEM$CANCEL_ALL_QUERIES\nUDFs. User-defined functions.\nExternal functions. Stored and executed outside of Snowflake."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#estimation-functions",
    "href": "posts/software/snowflake/snowpro_notes.html#estimation-functions",
    "title": "Snowflake: SnowPro Core",
    "section": "4.2. Estimation Functions",
    "text": "4.2. Estimation Functions\nExact calculations on large tables can be very compute-intensive or memory-intensive. Sometimes an estimate is good enough.\nSnowflake has some algorithms implemented out of the box that can give useful estimates with fewer resources.\n\n4.2.1. Number of Distinct Values - HLL()\nHyperLogLog algorithm.\nAverage error is ~1.6%.\nReplace\nCOUNT(DISTINCT (col1, col2, ...))\nwith\nHLL(col1, col2, ...)\nor\nAPPROX_COUNT_DISTINCT (col1, col2, ...)\n\n\n4.2.2. Frequent Values - APPROX_TOP_K()\nEstimate the most frequent values and their frequencies. Space-saving algorithm.\nUse the following command. The k argument is optional and defaults to 1. The counters argument is optional and specifies the maximum number of distinct values to track. If using this, we should use counters &gt;&gt; k.\nAPPROX_TOP_K (col1, k[optional], counters[optional])\n\n\n4.2.3. Percentile Values - APPROX_PERCENTILE()\nt-Digest algorithm.\nAPPROX_PERCENTILE (col1, percentile)\n\n\n4.2.4. Similarity of Two or More Data Sets - MINHASH & APPROXIMATE_SIMILARITY()\nThe full calculation uses the Jaccard similarity cofficient. This returns a value between 0 and 1 indicating similarity. \\[\nJ(A, B) = |(A \\cap B)| / |A \\cup B|\n\\]\nThe approximation is a two-step process that uses the MinHash algorithm to hash each table, then the APPROXIMATE_SIMILARITY function to estimate \\(J(A, B)\\).\nThe argument k in MINHASH is the number of hash functions to use. Higher k is more accurate but slower. We can pass individual column names instead oif *.\nSELECT MINHASH(k, *) AS mh FROM table_name;\nThe full approximation command is:\nSELECT APPROXIMATE_SIMILARITY(mh)\nFROM (\n    SELECT MINHASH(100, *) AS mh FROM mhtab1\n    UNION ALL\n    SELECT MINHASH(100, *) AS mh FROM mhtab2\n);"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#user-defined-functions-udf",
    "href": "posts/software/snowflake/snowpro_notes.html#user-defined-functions-udf",
    "title": "Snowflake: SnowPro Core",
    "section": "4.3. User-Defined Functions (UDF)",
    "text": "4.3. User-Defined Functions (UDF)\nThese are one of several ways of extending functionality with additional functions.\n(The other approaches are stored procedures and external functions, which are covered in the next sections.)\nUDFs support the following languages: SQL, Python, Java, JavaScript\n\n4.3.1. Defining a UDF\nWe can define a SQL UDF using create function.\nCREATE FUNCTION add_two(n int)\nreturns int\n    AS\n    $$\n    n+2\n    $$;\nDefining a UDF in Python is similar, we just need to specify the language and some other options.\nCREATE FUNCTION add_two(n int)\nreturns int\nlanguage Python\nruntime_version =‘3.8’\nhandler = ‘addtwo’ \n    AS\n    $$\n    def add_two(n):\n        return n+2\n    $$;\n\n\n4.3.2. Using a UDF\nWe just call the UDF from SnowSQL, for example\nSELECT add_two(3);\n\n\n4.3.3. Function Properties\nFunctions can be:\n\nScalar functions: Return one output row per input row.\nTabular functions: Return a table per input row.\n\nFunctions are schema-level objects in Snowflake. We can see them under Schema.Public.Functions.\nWe can manage access and grant privileges to functions, just like any other Snowflake object."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#stored-procedures",
    "href": "posts/software/snowflake/snowpro_notes.html#stored-procedures",
    "title": "Snowflake: SnowPro Core",
    "section": "4.4. Stored Procedures",
    "text": "4.4. Stored Procedures\nAnother way of extending functionality, like UDFs.\nUDF vs stored procedures: - UDF is typically used to calculate a value. It needs to return a value. No need to have access to the objects referenced in the function. - A stored procedure is typically used for database operations like INSERT, UPDATE, etc. It doesn’t need to return a value.\nCan rely on the caller’s or the owner’s access rights.\nProcedures are securable objects like functions, so we can grant access to them.\nSupported languages:\n\nSnowflake scripting - Snowflake SQL + procedural logic\nJavaScript\nSnowpark API - Python, Scala, Java\n\n\n4.4.1. Creating a Stored Procedure\nCREATE PROCEDURE find_min(n1 int, n2 int)\nreturns int\nlanguage sql\n    AS\n    BEGIN\n    IF (n1 &lt; n2)\n        THEN RETURN n1;\n        ELSE RETURN n2;\n    END IF;\n    END;\nStored procedures can be run with the caller’s rights or the user’s rights. This is defined with the stored procedure. By default, they run as owner but we can override this with:\nexecute as caller\n\n\n4.4.2. Calling a Stored Procedure\nCALL find_min(5,7)\nWe can reference dynamic values, such as variables in the user’s sessions, in stored procedures. - If argument is referenced in SQL, use :argname - If an object such as a table is referenced, use IDENTIFIER(:table_name)"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#external-functions",
    "href": "posts/software/snowflake/snowpro_notes.html#external-functions",
    "title": "Snowflake: SnowPro Core",
    "section": "4.5. External Functions",
    "text": "4.5. External Functions\nThese are user-defined functions that are stored and executed outside of Snowflake. Remotely executed code is referred to as a “remote service”.\nThis means it can reference third-party libraries, services and data.\nExamples of external API integrations are: AWS lambda function, Microsoft Azure function, HTTPS server.\nCREATE EXTERNAL FUNCTION my_func(string_col VAR_CHAR)\nreturns variant\napi_integration = azure_external_api_integration\nAS 'https://url/goes/here'\nSecurity-related information is stored in an API integration. This is a schema-level object, so it is securable and can be granted access to .\nAdvantages:\n\nCan use other languages\nAccess 3rd-party libraries\nCan be called from elsewhere, not just Snowflake, so we can have one central repository.\n\nLimitations:\n\nOnly scalar functions\nSlower performance - overhead of external functions\nNot shareable"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#secure-udfs-and-procedures",
    "href": "posts/software/snowflake/snowpro_notes.html#secure-udfs-and-procedures",
    "title": "Snowflake: SnowPro Core",
    "section": "4.6. Secure UDFs and Procedures",
    "text": "4.6. Secure UDFs and Procedures\nWe may want to hide certain information such as the function definition, or prevent users from seeing underlying data.\nWe just use the SECURE keyword.\nCREATE SECURE FUNCTION ...\nDisadvantages: - Lower query performance. The optimiser exposes some security risks, so this is locked down which restricts the optimisation options available therefore impacting performance lock this down the optimisations are restricted.\nWe should use it for sensitive data, otherwise the performance trade-off isn’t worthwhile."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#sequences",
    "href": "posts/software/snowflake/snowpro_notes.html#sequences",
    "title": "Snowflake: SnowPro Core",
    "section": "4.7. Sequences",
    "text": "4.7. Sequences\nSequences are typically used for DEFAULT values in CREATE TABLE statements. Sequences are not guaranteed to be gap-free.\nSequences are securable objects that we can grant privileges to.\nIt is a schema object like: Functions, Stored Procedures, Tables, File Formats, Stages, Stored Procedures, UDFs, Views, and Materialized Views.\nCREATE SEQUENCE my_seq\nSTART = 1\nINCREMENT = 1\nBoth START and INCREMENT default to 1.\nWe invoke a sequence with my_seq.nextval. For example:\nCREATE TABLE my_table(\n    id int DEFAULT my_seq.nextval,\n    first_name varchar\n    last_name varchar\n);\n\nINSERT INTO my_table(first_name, last_name)\nVALUES ('John', 'Cena'), ('Dwayne', 'Johnson'),('Steve','Austin');"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#semi-structured-data",
    "href": "posts/software/snowflake/snowpro_notes.html#semi-structured-data",
    "title": "Snowflake: SnowPro Core",
    "section": "4.8. Semi-structured Data",
    "text": "4.8. Semi-structured Data\n\n4.8.1. What is Semi-structured Data?\nSemi-structured data has no fixed schema, but contains tags/labels and has a nested structure.\nThis is in contrast to structured data like a table. Or unstructured data which is a free-for-all.\nSupported formats: json, xml, parquet, orc, avro\nSnowflake deals with unstructured data using 3 different data types:\n\nObject - think of this in the JavaScript sense, i.e. key:value pairs\nArray\nVariant - this can store data of any other type, including arrays and objects. Native support for semi-structured data.\n\nWe typically let Snowflake convert semi-structured data into a hierarchy of arrays and objects within a variant object.\nNulls and non-native strings like dates are cast to strings within a variant.\nVariant can store up to 16 MB uncompressed per row. If the data exceeds this, we will need to restructure or split the input.\nThe standard “ELT” approach (Extract, Load, Transform) for semi-structured data is to load the data as is, then transform it later once we’ve eyeballed it. This is a tweak of the classic ETL approach.\nWe often need to FLATTEN the data.\n\n\n4.8.2. Querying Semi-structured Data\nTo access elements of a VARIANT column, we use a :.\nFor example, if the raw_column column has a top-level key of heading1, we can query:\nSELECT raw_column:heading1\nFROM table_with_variant\nWe can also refer to columns by their position. So to access the first column:\nSELECT $1:heading1\nFROM table_with_variant\nTo access nested elements, use .:\nSELECT raw_column:heading1.subheading2\nFROM table_with_variant\nIf there is an array, we can access elements of the array with [index]. Arrays are 0-indexed, so to access the first element:\nSELECT raw_column:heading1.subheading2.array_field[0]\nFROM table_with_variant\nWe may also need to cast the types of the element with ::\nSELECT raw_column:heading1.subheading2.array_field[0]::VARCHAR\nFROM table_with_variant\n\n\n4.8.3. Flatten Hierarchical Data\nWe may want to flatten the hierarchies within semi-structured datainto a relational table. We do this with the FLATTEN table function.\nFLATTEN(INPUT =&gt; &lt;expression&gt;)\nNote that FLATTEN cannot be used inside a COPY command.\nWe can use it alongside the other keys of the data. For example:\nSELECT\n    RAW_FILE:id,\n    RAW_FILE:first_name,\n    VALUE as prev_company  -- this is the flatten array\nFROM \n    MY_DB.PUBLIC.JSON_RAW_TABLE,\n    TABLE(FLATTEN(input =&gt; RAW_FILE:previous_companies))\nThere is an implicit lateral join between the table and the flattened table function result set. We can make this explicit using the LATERAL FLATTEN keywords. The result is the same.\n\n\n4.8.4. Insert JSON Data\nWe use the PARSE_JSON function to import JSON data.\nSELECT PARSE_JSON(' { \"key1\": \"value1\", \"key2\": \"value2\" } ');\nWhen inserting variant data, we don’t use INSERT VALUES, we use a SELECT statement.\nINSERT INTO semi_structured_table_name\nSELECT PARSE_JSON(' { \"key1\": \"value1\", \"key2\": \"value2\" } ');"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#unstructured-data",
    "href": "posts/software/snowflake/snowpro_notes.html#unstructured-data",
    "title": "Snowflake: SnowPro Core",
    "section": "4.9. Unstructured Data",
    "text": "4.9. Unstructured Data\nUnstructured data is any data which does not fit in a pre-defined data model. E.g. video files, audio files, documents.\nSnowflake handles unstructured data using file URLs.\nSnowflake supports the following for both internal and external stages:\n\nAccess through URL in cloud storage\nShare file access URLs\n\n\n4.9.1. File URLs\nThere are three types of URL we can share:\n\n\n\n\n\n\n\n\n\nURL Type\nUse Case\nExpiry\nCommand to Return URL\n\n\n\n\nScoped URL\nEncoded URL with temporary access to a file but not the stage.\nExpires when results cache expires (currently 24 hours)\nBUILD_SCOPED_FILE_URL\n\n\nFile URL\nPermits prolonged access to a specified file.\nDoes not expire.\nBUILD_STAGE_FILE_URL\n\n\nPre-signed URL\nHTTPS URL used to access a file via a web browser.\nConfigurable expiration time for access token.\nGET_PRESIGNED_URL\n\n\n\nFor example, run the following SQL file function to create a scope URL:\nSELECT BUILD_SCOPED_FILE_URL(@stage_azure, 'Logo.png')\nFor a pre-signed URL, we also set the expiry time in seconds.\nSELECT GET_PRESIGNED_URL(@stage_azure, 'Logo.png', 60)\n\n\n4.9.2. Directory Tables\nA directory table stores metadata of staged files.\nThis is layered on a stage, rather than being a separate table object in a database.\nIt can be queried, with sufficient privileges on the stage, to retrieve file URLs to access files on the stage.\nIt needs to be enabled as it is not by default. We either do this in the CREATE STAGE command with\nCREATE STAGE stage_azure\nURL = &lt;'url'&gt;\nSTORAGE_INTEGRATION = integration\nDIRECTORY = (ENABLE = TRUE)\nor ALTER an existing stage:\nALTER STAGE stage_azure\nSET DIRECTORY = (ENABLE = TRUE)\nWe query this directory with:\nSELECT * FROM DIRECTORY(@stage_azure)\nThe first time querying, so will not see any results. The directory needs to be manually populated with:\nALTER STAGE stage_azure REFRESH\nWe may want to use the cloud provider’s event notification to set up an automatic refresh."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#data-sampling",
    "href": "posts/software/snowflake/snowpro_notes.html#data-sampling",
    "title": "Snowflake: SnowPro Core",
    "section": "4.10. Data Sampling",
    "text": "4.10. Data Sampling\nWhen developing views and queries against large databases, it may take a lot of time/compute to query against the entire database, making it slow to iterate. So we often want to work with a smaller subset of the data.\nUse cases:\n\nQuery development\nData analysis, estimates\n\nThere are two sampling methods:\n\nThe ROW / BERNOULLI method (both keywords give identical results). The following command will sample 10% of rows. We can optionally add a SEED(69) argument to get reproducible results.\n\nSELECT * FROM table\nSAMPLE ROW(10)\nSEED(69)\n\nThe BLOCK / SYSTEM method\n\nSELECT * FROM table\nSAMPLE BLOCK(10)\nSEED(69)\nComparison of row vs block sampling:\n\n\n\n\n\n\n\nROW\nBLOCK\n\n\n\n\nEvery row has probability \\(p\\) of being chosen\nEvery block (i.e. micro partition in block storage) has probability \\(p\\) of being chosen\n\n\nMore randomness\nMore efficient processing\n\n\nBetter for smaller tables\nBetter for larger tables"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#tasks",
    "href": "posts/software/snowflake/snowpro_notes.html#tasks",
    "title": "Snowflake: SnowPro Core",
    "section": "4.11. Tasks",
    "text": "4.11. Tasks\nTasks are used to schedule the execution of SQL statements or stored procedures. The are schema-level objects, so can be cloned.\nThey are often combined with streams to set up continuous ETL workflows.\nCREATE TASK my_task\n    WAREHOUSE = my_wh\n    SCHEDULE = '15 MINUTE'\n    AS\n    INSERT INTO my_table(time_col) VALUES (CURRENT_TIMESTAMP);\nIf we omit the WAREHOUSE argument, the task will run using Snowflake-managed compute.\nThe task is run using the privileges of the task owner.\nTo create a new task we need the following privileges: EXECUTE MANAGED TASK on account, CREATE TASK on schema and USAGE on warehouse.\nTo start/stop a task, we use the following commands. We always need to RESUME a task the first time we use it after creating it.\nALTER TASK my_task RESUME;\nALTER TASK my_task SUSPEND;\nAs well as setting up individual tasks, we can set up a DAG of interconnected tasks. We specify the dependencies using the AFTER keyword. DAGs are limited to 1000 tasks in total and 100 child tasks for a single node.\nCREATE TASK my_task_b\n    WAREHOUSE = my_wh\n    AFTER mytask_a\n    AS\n    ..."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#streams",
    "href": "posts/software/snowflake/snowpro_notes.html#streams",
    "title": "Snowflake: SnowPro Core",
    "section": "4.12. Streams",
    "text": "4.12. Streams\nA stream is an object which records Data Manipulation Language (DML) changes made to a table. This is change data capture. It is a schema-level object and can be cloned. The stream will be cloned when a database is cloned.\nIf a stream is set up on a table, the stream will record any inserts, deletes, updates to the table. We can then query the stream to see what has changed.\nTo create a stream:\nCREATE STREAM my_stream\nON TABLE my_table;\nWe can query from the stream. This will return any changed rows along with three metadata columns: METADATA$ACTION, METADATA$ISUPDATE, METADATA$ROW_ID.\nSELECT * FROM my_stream;\nConsuming a stream means we query its contents and then empty it. This is a typical use case in ETL work flows where we want to monitor for deltas and update another table. We do this by inserting into a target table from the stream:\nINSERT INTO target_table\n    SELECT col1, col2\n    FROM my_stream;\nThree types of stream:\n\nStandard: Insert, update, delete.\nAppend-only: Insert. Does not apply to external tables, only standard tables, directory tables, views.\nInsert-only: Insert. Only applies to external tables.\n\nA stream becomes stale when the offset is outside the data retention period of the table, i.e. unconsumed records in the stream are lost.\nThis determines how frequently a stream should be consumed. The STALE_AFTER column of DESCRIBE STREAM or SHOW STREAMS indicated when the stream is predicted to go stale.\nA stream extends the data retention period of the source table to 14 days by default. This is true for all snowflake editions. This is the MAX_DATA_EXTENION_TIME_IN_DAYS variable which defaults to 14 and can be increased to 90.\nTo trigger a task when a stream has data using WHEN SYSTEM$STREAM_HAS_DATA. For example:\nCREATE TASK my_task\n    WAREHOUSE = my_wh\n    SCHEDULE = '15 MINUTE'\n    WHEN SYSTEM$STREAM_HAS_DATA('my_stream')\n    AS\n    INSERT INTO my_table(time_col) VALUES (CURRENT_TIMESTAMP);"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#connectors-and-drivers",
    "href": "posts/software/snowflake/snowpro_notes.html#connectors-and-drivers",
    "title": "Snowflake: SnowPro Core",
    "section": "5.1. Connectors and Drivers",
    "text": "5.1. Connectors and Drivers\nSnowflake provides two interfaces:\n\nSnowsight web UI\nSnowSQL command line tool\n\nDrivers:\n\nGo\nJDBC\n.NET\nnode.js\nODBC\nPHP\nPython\n\nConnectors:\n\nPython\nKafka\nSpark\n\nPartner Connect allows us to create a trial account for third-party add-ons to Snowflake and integrate them with Snowflake.\nThey span many different categories and tools - BI, CI/CD, etc."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#snowflake-scripting",
    "href": "posts/software/snowflake/snowpro_notes.html#snowflake-scripting",
    "title": "Snowflake: SnowPro Core",
    "section": "5.2. Snowflake Scripting",
    "text": "5.2. Snowflake Scripting\nMostly used in stored procedures but can also be used for writing procedural code outside of this.\nWe can use@ if, case, for, repeat, while, loop\nThis is written in a “block”:\nDECLARE\nBEGIN \nEXCEPTION\nEND\nDECLARE and EXCEPTION are optional.\nThis is available in Snowsight. The classic UI or SnowSQL requires $$ around the block.\nObjects created in the block are available outside of it too. Variables created in the block can only be used inside it.\nThis is a similar syntax, but different from transactions which use BEGIN and END."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#snowpark",
    "href": "posts/software/snowflake/snowpro_notes.html#snowpark",
    "title": "Snowflake: SnowPro Core",
    "section": "5.3. Snowpark",
    "text": "5.3. Snowpark\nSnowpark API provides support for three programming languages: Python, Java, Scala.\nPython code converts to SQL which then queries Snowflake with serverless Snowflake engine.\nThis means there is no need to move data outside of Snowflake.\nBenefits:\n\nLazy evaluation\nPushdown - query is executed in Snowflake rather than unloading all data outside of Snowflake and then transforming\nUDFs can be defined inline"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#time-travel",
    "href": "posts/software/snowflake/snowpro_notes.html#time-travel",
    "title": "Snowflake: SnowPro Core",
    "section": "6.1. Time Travel",
    "text": "6.1. Time Travel\nWhat if someone drops a database or table accidentally? We need a backup to recover data. Time travel enables accessing historical data.\nWe can:\n\nQuery data that has been deleted or updated\nRestore dropped tables, schemas and databases\nCreate clones of tables, schemas and databases from a previous date\n\nWe can use a SQL query to access time travel data within a retention period. The AT keyword allows us to time travel.\nSELECT * \nFROM table\nAT (TIMESTAMP &gt;= timestamp)\nOr use OFFSET in seconds. So for 10 minutes:\nSELECT * \nFROM table \nAT (OFFSET &gt;= 10*60)\nAlternatively we can use BEFORE, where query_ID is the ID where we messed things up:\nSELECT * \nFROM table \nBEFORE (STATEMENT&gt;= query_id)\nGo to Activity -&gt; Query History in the Snowsight UI to get the correct query ID.\nIt is best practice to recover to an intermediate backup table first to confirm the recovered version is correct. E.g.\nCREATE OR REPLACE TABLE table_name_backup\nFROM table\nAT (TIMESTAMP &gt;= timestamp)"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#undrop",
    "href": "posts/software/snowflake/snowpro_notes.html#undrop",
    "title": "Snowflake: SnowPro Core",
    "section": "6.2. UNDROP",
    "text": "6.2. UNDROP\nWe can use the UNDROP keyword to recover objects.\nUNDROP TABLE table_name\nSame for SCHEMA or DATABASE.\nConsiderations.\n\nUNDROP fails if an object with the same name already exists.\nWe need ownership privileges to UNDROP an object.\n\nWhen working with time zones, it can be helpful to change the time zone to match your local time. This only affects the current session, not the whole account.\nALTER SESSION SET TIMEZONE = 'UTC'"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#retention-period",
    "href": "posts/software/snowflake/snowpro_notes.html#retention-period",
    "title": "Snowflake: SnowPro Core",
    "section": "6.3. Retention Period",
    "text": "6.3. Retention Period\nRetention period is the number of days for which historical data is preserved. This determines how far back we can time travel.\nThe retention period is configurable for table, schema, database or account.\nThe default DATA_RETENTION_TIME_IN_DAYS=1. If we want to disable time travel, we can set this to 0.\nWe can set this when we create a table, or alter it for an existing table:\nALTER TABLE table_name (\n    SET DATA_RETENTION_TIME_IN_DAYS=2\n)\nWe can set a minimum value at the account level:\nALTER ACCOUNT SET \nMIN_DATA_RETENTION_TIME_IN_DAYS=3\nThe account type determines the maximum retention period:\n\nStandard: up to 1 day\nEnterprise or higher: up to 90 days"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#fail-safe",
    "href": "posts/software/snowflake/snowpro_notes.html#fail-safe",
    "title": "Snowflake: SnowPro Core",
    "section": "6.4. Fail Safe",
    "text": "6.4. Fail Safe\nThis is for disaster recovery beyond time travel. This is not configurable and set to 7 days beyond time travel period for permanent tables.\nWe as users cannot access this data. We have to contact Snowflake support to restore it for us."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#storage-costs",
    "href": "posts/software/snowflake/snowpro_notes.html#storage-costs",
    "title": "Snowflake: SnowPro Core",
    "section": "6.5. Storage Costs",
    "text": "6.5. Storage Costs\nTime travel and fail safe contribute to storage costs. You only pay for what is modified.\nYou can see the breakdown in the Admin -&gt; Usage tab of the UI.\nYou can see how much storage is being used with:\nSELECT * \nFROM SNOWFLAKE_ACCOUNT_USAGE.TABLE_STORAGE_METRICS"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#table-types",
    "href": "posts/software/snowflake/snowpro_notes.html#table-types",
    "title": "Snowflake: SnowPro Core",
    "section": "6.6. Table Types",
    "text": "6.6. Table Types\nThe following table summarises the differences between the 3 types of table.\n\n\n\n\n\n\n\n\n\n\nTable Type\nCommand\nTime Travel Retention Period\nFail Safe\nUse Case\n\n\n\n\nPermanent\nCREATE TABLE\n0-90\nY\nStandard tables. Persist data until dropped.\n\n\nTransient\nCREATE TRANSIENT TABLE\n0-1\nN\nFor large tables that do not need to be protected. Persist data until dropped.\n\n\nTemporary\nCREATE TEMPORARY TABLE\n0-1\nN\nNon-permanent data just for this session.Only in session - data is deleted after the worksheet is closed.\n\n\n\nThese types are also available for other Snowflake objects: tables, stages, schema, database.\nIf a database is transient, so are all of its objects.\nTemporary table names do not clash with permanent or transient tables. The temporary table name takes precedence in the session and hides the others with the same name.\nIt is not possible to change the type of an existing object."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#zero-copy-cloning",
    "href": "posts/software/snowflake/snowpro_notes.html#zero-copy-cloning",
    "title": "Snowflake: SnowPro Core",
    "section": "7.1. Zero Copy Cloning",
    "text": "7.1. Zero Copy Cloning\nZero copy cloning makes it easy to copy an existing object in a storage-efficient way.\nFor example:\nCREATE TABLE table_name\nCLONE table_source\nWe can clone almost* any object: database, schema, table, stream, file format, sequence, task, stage, pipe.\n\n*the exceptions are:\n\npipes which can only be cloned if referencing an external stage\nstages which cannot be cloned for named internal stages\n\n\nWhen we clone a database or schema, all of its child objects are also cloned.\n\n\n\nZero Copy Clone\n\n\nIt’s called a “zero copy clone” because it does not actually copy the data at clone time. It is a metadata operation occurring in the cloud service layer.\nThe “copy” is a snapshot of the “original”. Both reference the same underlying micro-partitions in block storage. Think of it like pass-by-reference rather than pass-by-value.\nWhen modifying the “copy” Snowflake only stores the deltas, not the entire database again. The “original” and “copy” can be modified independently without affecting the other.\nA typical use case is to create backups for dev work.\nWe can clone from a time travel version of a table:\nCREATE TABLE table_new\nCLONE table_source\nBEFORE (TIMESTAMP &gt;= ‘2025-01-31 08:00:00’)"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#privileges",
    "href": "posts/software/snowflake/snowpro_notes.html#privileges",
    "title": "Snowflake: SnowPro Core",
    "section": "7.2. Privileges",
    "text": "7.2. Privileges\nThe privileges of the CHILD objects are inherited from the clone source if it is a container like a table or schema. But the privileges of the cloned object itself are NOT inherited. They need to be specified separately by the administrator.\nThe privileges required to clone an object depends on the object:\n\nTable: SELECT privileges\nPipe, stream, task: OWNER privileges\nAll other objects: USAGE privileges\n\nWhen we clone a table, its load history metadata is NOT copied. This means loaded data can be loaded again without causing a metadata clash."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#data-sharing",
    "href": "posts/software/snowflake/snowpro_notes.html#data-sharing",
    "title": "Snowflake: SnowPro Core",
    "section": "7.3. Data Sharing",
    "text": "7.3. Data Sharing\n\n7.3.1. What is a Data Share?\nData sharing can usually be quite complicated; managing multiple extracts of a common database and ensuring they are in sync.\nSnowflake separates storage vs compute. This allows us to share data without actually making a copy of it. As with zero copy cloning, this is a metadata operation, so uses the cloud service layer.\nWe grant access to the storage, and the customer provides their own compute resources.\nThis is available for all Snowflake pricing tiers.\nIn a one-way data share, Account 1 is the provider and Account 2 is the Consumer. Account 2 has a read-only view of the shared data. The provider is billed for the storage, whereas the consumer is billed for the compute costs of their queries.\n\n\n\nData Sharing One Way\n\n\nAn account can be simultaneously both a provider and consumer of data. You can even do this within the same account.\n\n\n\nData Sharing Two Way\n\n\nA share contains a single database. We can add multiple accounts to a share.\n\n\n7.3.2. How to Set Up a Data Share\nTo set up a data share we need to:\n\nCreate the share - requires ACCOUNTADMIN role or CREATE SHARE privileges.\n\nCREATE SHARE my_share;\n\nGrant privileges to the share\n\nGRANT USAGE ON DATABASE my_db TO_SHARE my_share;\nGRANT USAGE ON SCHEMA my_schema.my_db TO_SHARE my_share;\nGRANT SELECT ON TABLE my_table.my_schema.my_db TO_SHARE my_share;\n\nAdd consumer accounts\n\nALTER SHARE my_share ADD ACCOUNT = accountid123;\n\nThe consumer imports the share - requires ACCOUNTADMIN role, or IMPORE SHARE & CREATE TABLE privileges\n\nCREATE DATABASE new_db FROM SHARE my_share;\n\n\n7.3.3. What Can Be Shared\nWe can share: tables, external tables, secure views, secure materialised views, secure UDFs.\nThe share itself is a container containing one database, against which we can grant usage on scehma(s) and grant privileges to object(s). We can have one or more accounts that we grant access to.\nIt is a best practice to create secure views to avoid revealing unintended data. With an unsecured view, the user can see the command used to create the table, which could reveal the internal table used to create the view, columns not exposed to the user, aliases, etc.\nSnowflake will raise an error if trying to grant privileges to a standard view; they cannot be shared.\nIf sharing data with a lower tier Snowflake account, by default it won’t let you add a consumer account. We can override this by specifying SHARE_RESTRICTIONS=false.\nALTER SHARE my_share \nADD ACCOUNT accountid123\nSHARE_RESTRICTIONS = false;"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#database-replication",
    "href": "posts/software/snowflake/snowpro_notes.html#database-replication",
    "title": "Snowflake: SnowPro Core",
    "section": "7.4. Database Replication",
    "text": "7.4. Database Replication\nData shares are only possible within the same region and the same cloud provider.\nDatabase replication allows us to share data across different regions or on different cloud providers. It replicates a database between accounts within the same organisation. Also called “cross-region sharing”. This is available at all price tiers.\nThis incurs data transfer costs because it does actually copy the data across to a different location. The data and objects therefore need to be synchronised periodically.\nThe provider is referred to as the primary database and the consumer is the secondary database or a read-only replica.\nTo set up database replication we need to:\n\nEnable this at the account level with the ORGADMIN role.\n\nSELECT system$global_account_set_parameter(org_name.account_name), 'ENABLE_ACCOUNT_DATABASSE_REPLICATION', 'true');\n\nPromote a local database to primary database with ACCOUNTADMIN role.\n\nALTER DATABASE my_db ENABLE REPLICATION TO ACCOUNTS myorg.account2, myorg.account3;\n\nCreate a replica in the consumer account.\n\nCREATE DATABASE my_db AS REPLICA OF myorg.account1.my_db\n\nRefresh the database periodically. Ownership privileges are needed. We may want to set up a task to run this periodically.\n\nALTER DATABASE my_db REFRESH;"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#access-control",
    "href": "posts/software/snowflake/snowpro_notes.html#access-control",
    "title": "Snowflake: SnowPro Core",
    "section": "8.1. Access Control",
    "text": "8.1. Access Control\nThere are two approaches to access control:\n\nDiscretionary Access Control (DAC)\n\nEach object has an owner. That owner can grant access to the object.\n\nRole-based Access Control (RBAC)\n\nPrivileges are assigned to objects. Those privileges can be granted to roles. These roles can be assigned to users.\n\n\nWe do not assign privileges directly to users.\n\n\n\n\n\nflowchart LR\n\nA(Object) --&gt; B(Privilege) --&gt; C(Role) --&gt; D(User)\n\n\n\n\n\n\nKey concepts:\n\nSecurable object: An object that access can be granted to. Access is denied unless explicitly granted.\nPrivilege: A defined level of access.\nRole: The entity to which privileges are granted. Roles can be assigned to users or other roles. We can create a hierarchy of roles.\nUser: The identity associated with a person or program.\n\nWe can GRANT privileges to a role:\nGRANT my_privilege\nON my_object \nTO my_role\nAnd GRANT a role to a user.\nGRANT my_role\nTO user_id\nWe can also grant access to all tables in a schema:\nGRANT SELECT\nON ALL TABLES IN SCHEMA MARKETING_SALES\nTO ROLE MARKETING_ADMIN\nAnd also all future tables too:\nGRANT SELECT\nON FUTURE TABLES IN SCHEMA MARKETING_SALES\nTO ROLE MARKETING_ADMIN\nWe can REVOKE privileges in the same way:\nREVOKE privilege\nON object\nFROM role"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#roles",
    "href": "posts/software/snowflake/snowpro_notes.html#roles",
    "title": "Snowflake: SnowPro Core",
    "section": "8.2. Roles",
    "text": "8.2. Roles\nA role is an entity that we assign privileges to.\nRoles are then assigned to users. Multiple roles can be assigned to each user.\nWe need to grant access to all parent objects. So if we want to grant SELECT access to a table, we also need to grant USAGE access to the parent schema and database.\nEvery object is owned by one single role. Ownership can be transferred.\nThe SHOW ROLES command shows all available roles.\nThere is a “current role” used in every session. USE ROLE can be used in Snowflake worksheets or tasks.\n\n8.2.1. System-Defined Roles\nThese roles can’t be dropped. Additional privileges can be added to the roles, but not revoked.\nThe roles inherit from parent roles in the hierarchy.\nIt is best practice for custom roles to inherit from the SYSADMIN role.\n\nORGADMIN\n\nManage actions at the organisation level.\nCreate and view accounts.\n\nACCOUNTADMIN\n\nTop-level in hierarchy.\nShould only be grant to a limited number of users as it is the most powerful. It can manage all objects in the account.\nWe can create reader accounts and shares. Modify account level parameters including billing and resource monitors.\nContains SECURITYADMIN AND SYSADMIN roles.\n\nSECURITYADMIN\n\nManage any object grants globally.\nMANAGE GRANTS privilege.\nCreate, monitor and manage users and roles.\nInherits from USERADMIN. The difference with USERADMIN is SECURITYADMIN can manage grants GLOBALLY.\n\nSYSADMIN\n\nManages objects - it can create warehouses, databases and other objects.\nAll custom roles should be assigned to SYSADMIN so that SYSADMIN always remains a superset of all other roles and can manage any object by them.\n\nUSERADMIN\n\nFor user and role management.\nCREATE USER and CREATE ROLE privileges.\n\nPUBLIC\n\nAutomatically granted by default.\nGranted when no access control is needed. Objects can be owned but are available to everyone.\n\nCUSTOM\n\nCan be created by USERADMIN or higher.\nCREATE ROLE privilege. Should be assigned to SYSADMIN so that they can still manage all objects created by the custom role. Custom roles can be created by the database owner.\n\n\n\n\n8.2.2. Hierarchy of Roles\nThere is a hierarchy of roles.\n\n\n\n\n\nflowchart BT\n\nF(PUBLIC) --&gt; E(USERADMIN) --&gt; D(SECURITYADMIN) --&gt; A(ACCOUNTADMIN)\nC1(Custom Role 1) --&gt; B(SYSADMIN) --&gt; A(ACCOUNTADMIN) \nC3(Custom Role 3) --&gt; C2(Custom Role 2) --&gt; B(SYSADMIN)\n\n\n\n\n\n\nThis is similar to the hierarchy of objects that we’ve seen before in Section 2.10."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#privileges-1",
    "href": "posts/software/snowflake/snowpro_notes.html#privileges-1",
    "title": "Snowflake: SnowPro Core",
    "section": "8.3. Privileges",
    "text": "8.3. Privileges\nPrivileges define granular level of access to an object.\nWe can GRANT and REVOKE privileges. The owner of an object can manage its privileges. The SECURITYADMIN role has the global MANAGE GRANTS privilege, so can manage privileges on any object.\nWe can see all privileges in the Snowflake docs.\nSome of the important ones below.\nGlobal privileges:\n\nCREATE SHARE\nIMPORT SHARE\nAPPLY MASKING POLICY\n\nVirtual warehouse:\n\nMODIFY - E.g. resizing\nMONITOR - View executed queries\nOPERATE - Suspend or resume\nUSAGE - Use the warehouse to execute queries\nOWNERSHIP - Full control over the warehouse. Only one role can be an owner.\nALL - All privileges except ownership\n\nDatabases:\n\nMODIFY - ALTER properties of the database\nMONITOR - Use the DESCRIBE command\nUSAGE - Query the database and execute SHOW DATABASES command\nREFERENCE_USAGE - Use an object (usually secured view) to reference another object in a different database\nOWNERSHIP - Full control\nALL - Everything except ownership\nCREATE SCHEMA\n\nStages:\n\nREAD - Only for internal stages. Use GET, LIST, COPY INTO commands\nWRITE - Only for internal stages. Use PUT, REMOVE, COPY INTO commands\nUSAGE - Only for external stages. Equivalent of read and write.\nALL\nOWNERSHIP\n\nTables:\n\nSELECT\nINSERT\nUPDATE\nDELETE\nTRUNCATED\nDELETE\nALL\nOWNERSHIP\n\nWe can see the grants of a role with:\nSHOW GRANTS TO ROLE MARKETING_ADMIN\nWe can see which users are assigned to a role with\nSHOW GRANTS OF ROLE MARKETING_ADMIN"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#authentication",
    "href": "posts/software/snowflake/snowpro_notes.html#authentication",
    "title": "Snowflake: SnowPro Core",
    "section": "8.4. Authentication",
    "text": "8.4. Authentication\nAuthentication is proving that you are who you say you are.\nSnowflake uses Multi-Factor Authentication (MFA). This is powered by Duo, managed by Snowflake. MFA is available for all Snowflake editions and is supported by all Snowflake interfaces: web UI, SnowSQL, ODBC and JDBC, Python connectors.\nEnabled by default for accounts but requires users to enroll. Strongly recommended to use MFA for ACCOUNTADMIN.\nThe SECURITYADMIN or ACCOUNTADMIN roles can disable MFA for a user.\nWe can enable MFA token caching to reduce the number of prompts. This needs to be enabled; it isn’t by default. It makes the MFA token valid for 4 hours. Available for ODBC driver, JDBC driver and Python connector."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#sso",
    "href": "posts/software/snowflake/snowpro_notes.html#sso",
    "title": "Snowflake: SnowPro Core",
    "section": "8.5. SSO",
    "text": "8.5. SSO\nFederated authentication enables users to login via Single Sign-On (SSO).\nThe federated environment has two components:\n\nService provider: Snowflake\nExternal identity provider: Maintains credentials and authenticates users. Native support for Okta and Microsoft AD FS. Most SAML 2.0 compliant vendors are supported.\n\nWorkflow for Snowflake-initiated login:\n\nUser navigates to web UI\nChoose login via configured Identity Provider (IdP)\nAuthenticate via IdP credentials\nIdP sends a SAML response to Snowflake\nSnowflake opens a new session\n\nWorkflow for IdP-initiated login:\n\nUser navigates to IdP\nAuthenticate via IdP credentials\nSelect Snowflake as an application\nIdP sends a SAML response to Snowflake\nSnowflake opens a new session\n\nIdP has SCIM support. This is an open standard for automating user provisioning. We can create a user in the IdP and this provisions the user in Snowflake."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#key-pair-authentication",
    "href": "posts/software/snowflake/snowpro_notes.html#key-pair-authentication",
    "title": "Snowflake: SnowPro Core",
    "section": "8.6. Key-Pair Authentication",
    "text": "8.6. Key-Pair Authentication\nEnhanced security as an alternative to basic username and password.\nUser has a public key and a private key. This is the key-pair. Minimum key size is 2048-bit RSA key-pair.\nSetting this up:\n\nGenerate private key\nGenerate public key\nStore keys locally\nAssign public key to user. See command below.\nConfigure client to use key-pair authentication\n\nWe can set the public key for a user with:\nALTER USER my_user SET\nRSA_PUBLIC_KEY 'Fuak_shakfjsb';"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#column-level-security",
    "href": "posts/software/snowflake/snowpro_notes.html#column-level-security",
    "title": "Snowflake: SnowPro Core",
    "section": "8.7. Column-level Security",
    "text": "8.7. Column-level Security\nColumn-level security masks data in tables and views enforced on columns.\nThis is an enterprise edition feature.\n\n8.7.1. Dynamic Data Masking\nWith “dynamic data masking” the unmasked data is stored in the database, then depending on the role querying the data it is masked at runtime.\nWe can define a masking policy with:\nCREATE MASKING POLICY my_policy\nAS (col1 varchar) RETURNS varchar -&gt;\n    CASE\n    WHEN CURRENT ROLE IN (role_name)\n    THEN col1\n    ELSE “##-##”\n    END;\nWe apply a masking policy with:\nALTER TABLE my_table MODIFY COLUMN phone;\nWe can remove the policy with:\nUNSET MASKING POLICY my_policy;"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#external-tokenisation",
    "href": "posts/software/snowflake/snowpro_notes.html#external-tokenisation",
    "title": "Snowflake: SnowPro Core",
    "section": "8.8. External Tokenisation",
    "text": "8.8. External Tokenisation\nData is tokenised.\nThe benefit is the analytical value is preserved; we can still group the data and draw conclusions, it’s just anonymised. Sensitive data is protected.\nThe tokenisation happens in a pre-load step and the detokenisation happens at query runtime."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#row-level-security",
    "href": "posts/software/snowflake/snowpro_notes.html#row-level-security",
    "title": "Snowflake: SnowPro Core",
    "section": "8.9. Row-level Security",
    "text": "8.9. Row-level Security\nRow access policies allow us to determine which rows are visible to which users.\nThis is only available in the enterprise edition.\nRows are filtered at runtime on a given condition based on user or role.\nDefine the policy with:\nCREATE ROW ACCESS POLICY my_policy\nAS (col1 varchar) returns Boolean -&gt;\n    CASE\n    WHEN current_role() = 'EXAMPLE_ROLE'\n    AND col1 = 'value1' THEN true\n    ELSE false\n    END;\nApply the policy with:\nALTER TABLE my_table ADD ROW POLICY my_policy\nON (col1);"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#network-policies",
    "href": "posts/software/snowflake/snowpro_notes.html#network-policies",
    "title": "Snowflake: SnowPro Core",
    "section": "8.10. Network Policies",
    "text": "8.10. Network Policies\nNetwork policies allow us to restrict access to accounts based on the user’s IP address. We can also specify ranges of addresses.\nThis is available in all Snowflake editions.\nWe can either whitelist or blacklist IP addresses. If an address is in both, the blacklist takes precedence and the IP address is blocked.\nTo create a network policy we need the SECURITYADMIN role since this has the global CREATE NETWORK POLICY privilege.\nCREATE NETWORK POLICY my_network_policy\nALLOWED_IP_LIST = ('192.168.1.95', '192.168.1.113'),\nBLOCKED_IP_LIST = ('192.168.1.95')  -- Blacklist takes precedence\nTo apply this to an account, we again need the SECURITYADMIN role. This is a bit of an exception because this is a change to the account but we don’t need to be ACCOUNTADMIN.\nALTER ACCOUNT SET NETWORK_POLICY;\nWe can also alter a USER instead of an ACCOUNT if we have ownership of the user and the network policy."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#data-encryption",
    "href": "posts/software/snowflake/snowpro_notes.html#data-encryption",
    "title": "Snowflake: SnowPro Core",
    "section": "8.11. Data Encryption",
    "text": "8.11. Data Encryption\nAll data is encrypted at rest and in transit.\nThis is in all editions and happens automatically by default.\n\n8.11.1. Encryption at Rest\nRelevant to data in tables and internal stages. AES 256-bit encryption managed by Snowflake.\nNew keys are generated for new data every 30 days. Data generated with the old key will still use that old key. Old keys are preserved as long as there is still data that uses them; if not they are destroyed.\nWe can manually enable a feature to re-key all data every year. This is in the enterprise edition.\n\n\n8.11.2. Data in Transit\nThis is relevant for all Snowflake interfaces: WebUI, SnowSQL, JDBc, ODBC, Python Connector. TLS 1.2 end-to-end encryption.\n\n\n8.11.3. Other Encryption Features\nWe can additionally enable client-side encryption in external stages.\n“Tri-secret secure” enables customers to use their own keys. It is a business-critical edition feature. There is a composite master key which combines this customer-managed key with a Snowflake-managed key."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#account-usage-and-information-schema",
    "href": "posts/software/snowflake/snowpro_notes.html#account-usage-and-information-schema",
    "title": "Snowflake: SnowPro Core",
    "section": "8.12. Account Usage and Information Schema",
    "text": "8.12. Account Usage and Information Schema\nThese are functions we can use to query object metadata and historical data usage.\nThe ACCOUNT_USAGE schema is available in the SNOWFLAKE database, which is a shared database available to all accounts. The ACCOUNTADMIN can see everything. There are object metadata views like COLUMNS, and historical usage data views like COPY_HISTORY. The data is not real-time; there is a lag of 45 mins to 3 hours depending on the view. Retention period is 365 days. This does include dropped objects so can include a DELETED column.\nREADER_ACCOUNT_USAGE lets us query object metadata and historical usage data for reader accounts. It is more limited.\nThe INFORMATION_SCHEMA is a default schema available in all databases. This is read-only data about the parent database and account-level information. The output depends on which privileges you have. There is a lot of data returned so it can return messages saying to filter the query down. Shorter retention period: 7 days to 6 months. This is real-time, there is no delay. Does not include dropped objects."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#release-process",
    "href": "posts/software/snowflake/snowpro_notes.html#release-process",
    "title": "Snowflake: SnowPro Core",
    "section": "8.13. Release Process",
    "text": "8.13. Release Process\nReleases are weekly with no downtime.\n\nFull releases: New features, enhancements, fixes, behaviour changes\n\nBehaviour changes are monthly (i.e. every 4 weekly releases)\n\nPatch releases: Bugfixes\n\nSnowflake operate a three stage release approach:\n\nDay 1: early access for enterprise accounts who request this\nDay 1/2: regular access for standard accounts\nDay 2: final access for enterprise or higher accounts"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#query-profile",
    "href": "posts/software/snowflake/snowpro_notes.html#query-profile",
    "title": "Snowflake: SnowPro Core",
    "section": "9.1. Query Profile",
    "text": "9.1. Query Profile\nThis is a graphical representation of the query execution. This helps us understand the components of the query and optimise its performance.\n\n9.1.1. Accessing Query History\nThere are three ways to view the query history:\n\nIn Snowsight. This is in the Activity -&gt; Query History tab, or the Query tab in a worksheet for the most recent query.\nInformation schema\n\nSELECT * FROM TABLE(INFORMATION_SCHEMA.QUERY_HISTORY())\nORDER BY START_TIME;\n\nAccount usage schema\n\nSELECT * FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY;\n\n\n9.1.2. The Graphical Representation of a Query\nThe “Operator Tree” is a graphical representation of the “Operator Types”, which are components of query processing. We can also see the data flow, i.e. the number of records processed. The is also a percentage on each node indicating the percentage of overall execution time.\nThe “Profile Overview” tab gives details of the total time taken. The “Statistics” tab shows number of bytes scanned, percentage from cache, number of paritions scan.\n\n\n9.1.3. Data Spilling\nIt also shows “data spilling”, when the data does not fit in memory and is therefore written to the local storage disk. This is called “spilling to disk”. If the local storage is filled, the data is further spilled to remote cloud storage. This is called “spilling to cloud”.\nTo reduce spilling, we can either run a smaller query or use a bigger warehouse."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#caching",
    "href": "posts/software/snowflake/snowpro_notes.html#caching",
    "title": "Snowflake: SnowPro Core",
    "section": "9.2. Caching",
    "text": "9.2. Caching\nThere are three mechanisms for caching:\n\nResult cache\nData cache\nMetadata cache\n\nIf the data is not in a cache, it is retrieved from storage, i.e. the remote disk.\n\n9.2.1. Result Cache\nStores the results of a query. This happens in the cloud service layer.\nThe result is very fast and avoids re-execution.\nThis can only happen if:\n\nThe table data and micro-partitions have not changed,\nThe query does not use UDFs or enternal functions\nSufficient privileges and results are still available (cache is valid for 24 hours)\n\nResult caching is enabled by default, but can be disabled using the USE_CACHED_RESULT parameter.\nThe result cache is purged after 24 hours. If the query is re-run, it resets the purge timer, up to a maximum of 31 days.\n\n\n9.2.2. Data Cache\nThe local SSD disk of the virtual warehouse. This cannot be shared with other warehouses.\nIt improves the performance of subsequent queries which use the same underlying data. We can therefore improve performance and costs by using the same warehouse for queries on similar data.\nThe data cache is purged if the warehouse is suspended or resized.\n\n\n9.2.3. Metadata cache\nStores statistics for tables and columns in the cloud services layer.\nThis is also known as the “metadata store”. The virtual private Snowflake edition allows for a dedicated metadata store.\nThe metadata cache stores results about the data, such as range of values in a micro-partition, count rows, count distinct values, max/min values. We can then query these without using the virtual warehouse.\nThis is used for functions like DESCRIBE and other system-defined functions."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#micro-partitions",
    "href": "posts/software/snowflake/snowpro_notes.html#micro-partitions",
    "title": "Snowflake: SnowPro Core",
    "section": "9.3. Micro Partitions",
    "text": "9.3. Micro Partitions\nThe data is cloud storage is stored in micro partitions.\nThese are chunks of data, usually about 50-500MB in uncompressed size (although they are compressed by Snowflake). The most efficient compression algorithm for each micro partition is used independently.\nThis is specific to Snowflake’s architecture.\nData is stored in a columnar form. We can improve read queries by only selecting required columns.\nMicro partitions are immutable - they cannot be changed once created. New data will be stored in new micro partitions, so the partitions depend on the order of insertion.\n\n9.3.1. Partition Pruning\nMicro partitions allow for very granular partition pruning. When we run a query on a table, we can eliminate unnecessary partitions.\nSnowflake stores metadata per micro partition. E.g. range of values, number of distinct values, and other properties for query optimisation.\nSnowflake optimises queries by using the metadata to determine if a micro partition needs to be read, so it can avoid reading all of the data.\n\n\n9.3.2. Clustering Keys\nClustering keys let us influence how the data is partitioned. This helps improve query performance by improving partition pruning. Grouping similar rows in micro-partitions also generally improves column compression.\nClustering a table on a specific column redistributes the data in the micro-partitions.\nSnowflake stored metadata about the micro-partitions in a table:\n\nNumber of micro-partitions\nOverlapping micro-partitions: Number of partitions with overlapping values\nClustering depth: Average depth of overlapping columns for a specific column\n\nA table with micro-partitions that are optimal are said to be in a “constant state”.\n\n\n9.3.3. Reclustering\nReclustering does not update immediately, it happens during periodic reclustering.\nOnce we define the clustering keys, Snowflake handles the automatic reclustering in the cloud services layer (serverless). This only adjusts the micro-partitions that would benefit from reclustering.\nNew partitions are created and the old partitions marked as deleted. They are not immediately deleted, they remain accessible for the time travel period, which incurs storage costs.\nThis incurs costs:\n\nServerless costs: Credit consumption of reclustering\nStorage costs: Old partitions are maintained for time travel\n\nClustering is therefore not appropriate for every table. It depends on the use case; does the usage (and improved query performance) justify the cost?\nThe columns which would benefit most from clustering:\n\nLarge number of micro-partitions\n\nIn the extreme case, if a table only had one micro-partition there would be no point pruning or reclustering.\n\nFrequently used in WHERE, JOIN, ORDER BY\nGoldilocks amount of cardinality\n\nIf there are only a small number of possible values, e.g. True/False, then pruning would not be very effective\nIf there are too many unique values, e.g. timestamps, then there would not be an efficient grouping of values within a micro-partition\n\n\n\n\n9.3.4. Defining Clustering Keys\nA cluster key can be added at any time.\nWhen clustering on multiple columns, it is best practice to cluster on the lowest cardinality first and go in order of increasing cardinality. In the example below, col1 has fewer unique values than col5.\nDefining a clustering key:\nALTER TABLE table_name \nCLUSTER BY (col1, col5);\nWe can use expressions in the clustering key. A common use case is converting timestamps to dates.\nALTER TABLE table_name \nCLUSTER BY (DATE(timestamp));\nWe can also define the clustering key at the point of creating the table.\nCREATE TABLE table_name\nCLUSTER BY (col1, col5);\nWe can remove clustering keys with:\nALTER TABLE table_name \nDROP CLUSTER KEY;\n\n\n9.3.5. Systems Functions for Clustering\nFor info on clustering on a table, we can run this command to get a json result.\nSYSTEM$CLUSTERING_INFORMATION('my_table', '(col1, col3)')\nThis contains info on: total_partition_count, total_constant_partition_count, average_overlaps, average_depth, partition_depth_histogram.\nThe notes key contains Snowflake info/warning messages, such as when a clustering key has high cardinality."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#search-optimization-service",
    "href": "posts/software/snowflake/snowpro_notes.html#search-optimization-service",
    "title": "Snowflake: SnowPro Core",
    "section": "9.4. Search Optimization Service",
    "text": "9.4. Search Optimization Service\nThis can improve performance of lookup and analytical queries that use many predicates for filtering.\nIt does this by adding a search access path.\nThe queries that benefit most from this are:\n\nSelective point lookup: return very few rows\nEquality or IN predicates\nSubstring and regex searches\nSelective geospatial functions\n\nThis is in the enterprise edition. It is maintained by Snowflake once setup for a table. It runs in the cloud service layer, so incurs serverless costs and additional storage costs.\nWe can add this to a table with the following command. We need either OWNERSHIP privileges on the table or the ADD SEARCH OPTIMIZATION privilege on the schema.\nALTER TABLE mytable\nADD SEARCH OPTIMIZATION;\nAdding it to the entire table is equivalent to specifying ON EQUALITY(*). We can be more specific about the columns we optimise:\nALTER TABLE mytable\nADD SEARCH OPTIMIZATION ON GEO(mycol);\nWe can remove search optimization with\nALTER TABLE mytable\nDROP SEARCH OPTIMIZATION;"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#materialised-views",
    "href": "posts/software/snowflake/snowpro_notes.html#materialised-views",
    "title": "Snowflake: SnowPro Core",
    "section": "9.5. Materialised Views",
    "text": "9.5. Materialised Views\nThese can improve performance issues of views. This is useful for complex queries that are run frequently.\nAvailable in enterprise edition.\nWe would typically create a regular view for a frequently run query. If this is compute-intensive, we can create a materialised view which pre-computes the result and stores it in a physical table.\nThis differs to a regular Snowflake table because the underlying data may change. The materialised view is updated automatically by Snowflake using the cloud storage layer. This incurs serverless costs and additional storage costs.\nIt is best practice to start small with materialised views and incrementally use more. Resource monitors can’t control Snowflake-managed warehouses.\nWe can see info on materialized views using:\nSELECT * FROM TABLE(INFORMATION_SCHEMA.MATERIALIZED_VIEW_REFRESH_HISTORY());\nTo create a materialised view:\nCREATE MATERIALIZED VIEW V_1 AS\n    SELECT * FROM table1 WHERE c1=200;\nConstraints on materialised views:\n\nOnly query 1 table - no joins or self-joins\nCan’t query other views or materialized views\nNo window functions, UDFs or HAVING clauses\nSome aggregate functions aren’t allowed\n\nA typical use case is for queries on external tables which can be slow.\nWe can pause and resume materialized views. This will stop automatic updates to save credit consumption.\nALTER MATERIALIZED VIEW v_1 SUSPEND;\nALTER MATERIALIZED VIEW v_1 RESUME;\nWe can drop materialized views that we no longer need.\nDROP MATERIALIZED VIEW v_1;"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#warehouse-considerations",
    "href": "posts/software/snowflake/snowpro_notes.html#warehouse-considerations",
    "title": "Snowflake: SnowPro Core",
    "section": "9.6. Warehouse Considerations",
    "text": "9.6. Warehouse Considerations\nWarehouses can be altered to improve performance.\n\nResizing: Warehouses can be resized when queries are running, but the new warehouse size will only affect future queries. The warehouse can also be resized when the warehouse is suspended.\nScale up for more complex queries.\nScale out for more frequent queries. We may want to enable auto-scaling to automate this.\nDedicated warehouse: Isolate workload of specific users/team. Helpful to direct specific type of workload to a particular warehouse. Best practice to enable auto-suspend and auto-resume on dedicated warehouses."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html",
    "href": "posts/software/software_architecture/software_architect_notes.html",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "Notes from “The Complete Guide to Becoming a Software Architect” Udemy course\n\n\nA developer knows what can be done, an architect knows what should be done. How do we use technology to meet business requirements.\nGeneral system requirements:\n\nFast\nSecure\nReliable\nEasy to maintain\n\nArchitect’s need to know how to code for:\n\nArchitecture’s trustworthiness\nSupport developers\nRespect of developers\n\n\n\n\n\nUnderstand the business - Strengths, weaknesses, compettions, growth strategy.\nDefine the system’s goals - Goals are not requirements. Goals describe the effect on the organisation, requirements describe what the system should do.\nWork for your client’s clients - Prioritise the end user.\nTalk to the right people with the right language - What is the thing that really matters to the person I’m talking to?\n\nProject managers care about how things affect deadlines, developers care about the technologies used, CEOs care about business continuity and bottom line.\n\n\n\n\nUnderstand the system requirements - What the system should do.\nUnderstand the non-functional requirements - Technical and service level attributes, e.g. number of users, loads, volumes, performance.\nMap the components - Understand the system functionality and communicate this to your client. Completely non-technical at this point, no mention of specific technologies. A diagram of high level components helps.\nSelect the technology stack - Backend, frontend, data store.\nDesign the architecture\nWrite the architecture document\nSupport the team\n\nInclude developers in non-functional requirements and architecture design for two reasons:\n\nLearn about unknown scenarios early\nCreate ambassadors\n\n\n\n\nThe 2 types of requirements: functional and non-functional.\n\n\n“What the system should do”\n\nBusiness flows\nBusiness services\nUser interfaces\n\n\n\n\n“What the system should deal with”\n\nPerformance - latency and throughput\n\nLatency - How long does it take to perform a single task?\nThroughput - How many tasks can be performed in a given time unit?\n\nLoad - Quantity of work without crashing. Determines the availability of the system. Always look at peak (worst case) numbers.\nData volume - How much data will the system accumulate over time?\n\nData required on day one\nData growth (say, annually)\n\nConcurrent users - How many users will be using the system? This includes “dead times” which differentiates it from the load requirement. Rule of thumb is concurrent_users = load * 10\nSLA - Service Level Agreement for uptime.\n\nThe non-functional requirements are generally the more important in determining the architecture. The client will generally need guiding towards sensible values, otherwise they just want as much load as possible, as much uptime as possible etc.\n\n\n\n\nThe application type should be established early based on the use case and expected user interaction.\n\nWeb apps\n\nServe html pages.\nUI; user-initiated actions; large scale; short, focused actions.\nRequest-response model.\n\nWeb API\n\nServe data (often JSON).\nData retrieval and storage; client-initiated actions; large scale; short, focused actions.\nRequest-response model.\n\nMobile\n\nRequire user interaction; frontend for web API; location-based.\n\nConsole\n\nNo UI; limited interation; long-running processes; short actions for power users.\nRequire technical knowledge.\n\nService\n\nNo UI; managed by service manager; long-running processes.\n\nDesktop\n\nAll resources on the PC; UI; user-centric actions.\n\nFunction-as-a-service\n\nAWS lambda\n\n\n\n\n\nConsiderations:\n\nAppropriate for the task\nCommunity - e.g. stack overflow activity\nPopularity - google trends over 2 years\n\n\n\nCovers web app, web API, console, service.\nOptions: .NET, Java, node.js, PHP, Python\n\n\n\n\n\n\nFigure 1: Backend Tech\n\n\n\n\n\n\nCovers:\n\nWeb app - Angular, React\nMobile - Native (Swift, Java/Kotlin), Xamarin, React Native\nDesktop - depends on target OS\n\n\n\n\nSQL - small, structured data\n\nRelational tables\nTransactions\n\nAtomicity\nConsistency\nIsolation\nDurability\n\nQuerying language is universal\n\nNoSQL - huge, unstructured or semi-structured data\n\nEmphasis on scale and performance.\nSchema-less, with entities stored as JSON.\nEventual consistency - data can be temporarily inconsistent\nNo universal querying language\n\n\n\n\n\nQuality attributes that describe technical capabilities to fulfill the non-functional requirements. Non-functional requirements map to quality attributes, which are designed in the architecture.\n\nScalability - Adding computing resources without any interruption. Scale out (add more compute instances) is preferred over scale up (increase CPU, RAM on the existing instance). Scaling out introduces redundancy and is not limited by hardware.\nManageability - Know what’s going on and take action accordingly. The question “who reports the problems” determines if a system is manageable. The system should flag problems, not the suer.\nModularity - A system that is built from blcoks that can be changed or replaced without affecting the whole system.\nExtensibility - Functionality of the system can be extended without modifying existing code.\nTestability - How easy is it to test the application. Independent modules and methods and single responsibility principle aid testability.\n\n\n\n\nA software component is a piece of code that runs in a single process. Distributed systems are composed of independent software components deployed on separate processes/servers/containers.\nComponent architecture relates to the lower level details, whereas system architecture is the higher level view of how the components fit together to achieve the non-functional requirements.\n\n\nLayers represent horizontal functionality:\n\nUI/SI - user interface or service interface (i.e. an API), authentication\nBusiness logic - validation, enrichment, computation\nData access layer - connection handling, transaction handling, querying/saving data\n\nCode can flow downwards by one layer only. It can never skip a layer and can never go up a layer. This enforces modularity.\nA service might sit across layers, for example logging sits across the 3 layers described above. In this case, the logging service is called a “sross-cutting concern”.\nEach layer should be independent of the implementation of other layers. Layers should handle exceptions from those below them, log the error and then throw a more generic exception, so that there is no reference to other layers’ implementation.\nLayers are part of the same process. This is different from tiers, which are processes that are distributed across a network.\n\n\n\nAn interface declares the signature of an implementation. This means that each implementation must implement the methods described. The abstract base class in Python is an example of this.\nClose coupling should be avoided; “new is glue”.\n\n\n\n\nSingle responsibility principle: Each class, module or method should have exactly one responsibility.\nOpen/closed principle: Software should be open for extension but closed for modification. Can be implemented using class inheritance.\nLiskov substitution principle: If S is a subtype of T, then objects of type T can be replaced with objects of type S without altering the program. This looks similar to polymorphism but is stricter; not only should the classes implement the same methods, but the behaviour of those methods should be the same too, there should be no hidden functionality performed by S that is not performed by T or vice versa.\nInterface segregation principle: Many client-specific interfaces are preferable to one general purpose interface.\nDependency inversion principle: High-level modules should depend on abstractions rather than concrete implementations. Relies on dependency injection, where one object supplies the dependencies of another object. A factory method determines which class to load and returns an instance of that class. This also makes testing easier, as you can inject a mock class rather than having to mock out specific functionality.\n\n\n\n\n\nStructure - case, underscores\nContent - class names should be nouns, methods should be imperative verbs\n\n\n\n\nSome best practices:\n\nOnly catch exceptions if you are going to do something with it\nCatch specific exceptions\nUse try-catch on the smallest code fragments possible\nLayers should handle exceptions raised by layers below them, without exposing the specific implementation of the other layer. Catch, log, re-raise a more generic error.\n\nPurposes of logging:\n\nTrack errors\nGather data\n\n\n\n\n\nA collection of general, reusable solutions to common software design problems. Popularised in the book “Design patterns: elements of reusable object-oriented software”.\n\nThe factory pattern: Creating objects without specifying the exact class of the object. Avoids strong coupling between classes. Create an interface, then a factory method returns an instance of a class which implements that interface. If the implementation needs to change, we only need to change the factory method.\nThe repository pattern: Modules which don’t work with the data store should be oblivious to the type of data store. This is similar to the concept of the Data Access Layer. Layers are for architects, design patterns are for developers. They both seek to solve the same issue.\nThe facade pattern: Creating a layer of abstraction to mask complex actions. The facade does not create any new functionality, it is just a wrapper integrating existing modular functions together.\nThe command pattern: All the action’s information is encapsulated within an object.\n\n\n\n\nThe architecture design is the big picture that should answer the following:\n\nHow will the system work under heavy load?\nWhat will happen if the system crashes at a critical moment?\nHow complicated is it to update?\n\nThe architecture should:\n\nDefine components\nDefine how components communicate\nDefine the system’s quality attributes\n\nMake the correct choice as early as possible\n\n\nEnsuring the services are not strongly tied to other services. Avoid the “spiderweb” - when the graph of connections between services is densely connected.\nPrevents coupling of platforms and URLs.\nTo avoid URL coupling between services, two options are:\n\n“Yellow pages” directory\nGateway\n\nThe “yellow pages” contains a directory of all service URLs. If service A wants to query service D, A queries the yellow pages for D’s URL, then uses that URL to query D. This means that service A does not hardcode any information about service D. If D’s URLs change, then only the yellow pages need to be updated. Services only need to know the yellow pages directory’s URL.\n\n\n\n\n\n\nFigure 2: Yellow Pages\n\n\n\nThe gateway acts as a middleman. It holds a mapping table of all URLs Service A queries the gateway, which in turn queries service E. Service A doesn’t need to know about E or any other services. Services only need to know the gateway’s URL.\n\n\n\n\n\n\nFigure 3: Gateway\n\n\n\n\n\n\nThe application’s state is stored in only 2 places:\n\nThe data store\nThe user interface\n\nStateless architecture is preferred in virtually all circumstances.\nIn a stateful architecture, when a user logs in, the login service retrieves the user details from the database and stored them for future use. Data is stored in code.\nIf the login request was routed to server A, the user’s details are stored there. If the user later tried to add itemsto the cart and their cart service request is routed to server B, then their user details will not exist as those are stored on server A.\nDisadvantages of stateful:\n\nLack of scalability\nLack of redundancy\n\n\n\n\n\n\n\nFigure 4: Stateful\n\n\n\nIn a stateless architecture, no data is stored in the service itself. This means the behaviour will be the same regardless of which server a request was routed to.\n\n\n\n\n\n\nFigure 5: Stateless\n\n\n\n\n\n\nCaches store data locally to avoid retrieving the same data from the database multiple times. It trades reliability of data (it is stored in volatile memory) for improved performance.\nA cache should store data that is frequently accessed and rarely modified.\nTwo types of cache:\n\nIn-memory cache - Cache stored in memory on a single service\n\nPros:\n\nBest performance\nCan store any objects\nEasy to use\n\nCons:\n\nSize is limited by the process’s memory\nCan grow stale/inconsistent if the service is scaled out to multiple servers\n\n\nDistributed cache - Cache is independent of the services and can be accessed by all servers\n\nPros:\n\nSupports scaled out servers\nFailover capabilities\nStorage is unlimited\n\nCons:\n\nSetup is more complex\nOften only support primitive data types\nWorse performance than in-memory cache\n\n\n\n\n\n\nMessaging methods can be evaluated on these criteria:\n\nPerformance\nMessage size\nExecution model\nFeedback (handshaking) and reliability\nComplexity\n\nMessaging methods include:\n\nREST API\nHTTP Push\nQueue\nFile-based and database-based methods\n\n\n\nUniversal standard for HTTP-based systems.\nUseful for traditional web apps.\n\n\n\n\n\n\nFigure 6: REST API\n\n\n\n\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nVery fast\n\n\nMessage size\nSame as HTTP protocol limitations - GET 8KB, POST ~10MB\n\n\nExecution model\nRequest/response - ideal for quick, short actions\n\n\nFeedback\nImmediate feedback via response codes\n\n\nComplexity\nExtremely easy\n\n\n\n\n\n\nA client subscribes to the service waiting for an event. When that event occurs, the server notifies the client.\nFor real-time messaging, these often use web sockets to maintain the subscription connection, rather than a traditional request/response model.\nUseful for chat or monitoring.\n\n\n\n\n\n\nFigure 7: HTTP Push\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nVery fast\n\n\nMessage size\nLimited, few KB\n\n\nExecution model\nWeb socket connection / long polling\n\n\nFeedback\nNone (fire and forget)\n\n\nComplexity\nExtremely easy\n\n\n\n\n\n\nThe queue sits between two (or more) services. If service A wants to send a message to service B, A places the message in the queue and B periodically pulls from the queue.\nThis ensures messages will be handled exactly once and in the order received.\nUseful for complex systems with lots of data, when order and reliability are important.\n\n\n\n\n\n\nFigure 8: Queue\n\n\n\n\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nSlow - push/poll time and database persistence\n\n\nMessage size\nUnlimited but best practice to use small messages\n\n\nExecution model\nPolling\n\n\nFeedback\nVery reliable but feedback depends on the monitoring in place to ensure messages make it to the queue and get executed\n\n\nComplexity\nRequires training and setup to maintain a queue engine\n\n\n\n\n\n\nSimilar to queue, but rather han placing messages in a queue it places them in a file directory or database. There is no guarantee that messages are processed once and only once.\nIf multiple services are polling the same file folder, then when a new file is added we can get two issues:\n\nFile locked\nDuplicate processing\n\nSimilar use case to queues, but queues are generally preferred.\n\n\n\n\n\n\nFigure 9: File-based Messaging\n\n\n\n\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nSlow - push/poll time and database persistence\n\n\nMessage size\nUnlimited\n\n\nExecution model\nPolling\n\n\nFeedback\nVery reliable but feedback depends on the monitoring\n\n\nComplexity\nRequires training and setup to maintain the filestore or database\n\n\n\n\n\n\n\n\n\nCreate a central logging service that all other services call to write to a central database. This solves the problem where each microservice has its own log format, data and location. For example, some might be in files, SQL database, NoSQL database, etc.\nImplementation can be via an API or polling folders that each of the services write to.\n\n\n\n\n\n\nFigure 10: Central Logging Service\n\n\n\n\n\n\nCorrelation ID is an identifier attached to the beginning of a user flow and is attached to any action taken by that user, so that if there is an error at any point the user flow can be traced back through the different services’ logs.\n\n\n\n\n\nExternal considerations can affect architecture and design decisions.\n\nDeadlines\nDev team skills - New technologies can introduce uncertainty, delays and low quality\nIT support - Assign who will support the product from the outset. This should not be developers.\nCost - Build vs buy. Estimate cost vs value. Spend money to save time, don’t spend time to save money.\n\n\n\n\nThis should describe the basic elements of the system:\n\nTechnology stack\nComponents\nServices\nCommunication between components and services\n\nNo development should begin before the document is complete.\nGoals of the document:\n\nDescribe what should be developed and how\nList the functional and non-functional requirements\n\nAudience:\n\nEveryone involved with the system: project manager, CTO, QA leader, developers\nSections for management appear first as they are unlikely to read the whole document\nQA lead can begin preparing test infrastructure ahead of time\n\nFormat: UML (universal modeling language) is often used. It visualises the system’s design with concepts and diagrams. However, this assumes the audience is familiar with UML which is often not the case.\n\nKeep the contents as simple as possible\nUse plain English\nVisualise using whatever software is comfortable and appropriate\n\nStructure:\n\nBackground\nRequirements\nExecutive summary\nArchitecture overview\nComponents drill-down\n\n\n\nThis section validates your point of view and instils confidence that you understand the project.\nOne page for all team and management.\nDescribe the system from a business POV:\n\nThe system’s role\nReasons for replacing the old system\nExpected business impact\n\n\n\nFunctional and non-functional requirements - what should the system do and deal with? This section validates your understadning of the requirements. The requirements dictate the architecture so this section sets the scene for the chosen architecture.\nOne page for all team and management.\nThis should be a brief, bullet point list of requirements with no more than 3 lines on each.\n\n\n\n\nProvide a high-level view of the architecture.\nManagers will not read the whole document; &lt;3 pages for management.\n\nUse charts and diagrams\nWrite this after the rest of the document\nUse technical terms sparsely and only well known ones\nBe concise and don’t repeat yourself\n\n\n\n\nProvide a high-level view of the architecture in technical terms. Do not deep dive into specific components\n&lt;10 pages for developers and QA lead.\n\nGeneral description: type and major non-functional requirements\nHigh-level diagram of logical components, no specifics about hardware or technologies\nDiagram walkthrough: describe the various parts and their roles\nTechnology stack: iff there is a single stack include it here, otherwise include it in the components drill down section\n\n\n\n\nDetailed description of each component.\nUnlimited length, for developers and QA lead.\nFor each component:\n\nComponent’s role\nTechnology stack: Data store, backend, frontend\nComponent’s architecture: Describe the API (URL, logic, response code, comments). Describe the layers. Mention design patterns here.\nDevelopment instructions: Specific development guidelines\n\nBe specific and include rationale behind decisions.\n\n\n\n\nThese architecture patterns solve specific problems but add complexity to the system, so the costs and benefits should be compared.\n\n\nAn architecture in which functionalities are implemented as separate, loosely coupled services that interact with each other using a standard lightweight protocol.\nProblems with monolithic services:\n\nA single exception can crash the whole process\nUpdates impact all components\nLimited to one development platform/language\nUnoptimised compute resources\n\nWith microservices, each service is independent of others so can be updated separately, use a different platform, and be optimised separately.\nAn example of splitting a monolithic architecture into microservices:\n\n\n\n\n\n\nFigure 11: Monolith Example\n\n\n\nAs a microservice architecture, this becomes:\n\n\n\n\n\n\nFigure 12: Microservice Example\n\n\n\nProblems with microservices:\n\nComplex monitoring of all services and their interactions\nComplex architecture\nComplex testing\n\n\n\n\nStoring the deltas to each entity rather than updating it. The events can then be “rebuilt” from the start to give a view of the state at any given point in time.\nUse when history matters.\n\n\n\n\n\n\nFigure 13: Event Sourcing\n\n\n\nPros:\n\nTracing history\nSimple data model\nPerformance\nReporting\n\nCons:\n\nNo unified view - need to rebuild all events from the start to see the current state\nStorage usage\n\n\n\n\nCommand query responsibility segregation. Data storage and data retrieval are two separate databases, with a sync service between the two.\nThis integrates nicely with event sourcing, where events (deltas) are stored in one database and the current state is periodically built and stored in the retrieval database.\n\n\n\n\n\n\nFigure 14: CQRS\n\n\n\nPros:\n\nUseful with high-frequency updates that require near real-time querying\n\nCons:\n\nComplexity - need 2 databases, a sync service, ETL between the storage and retrieval database\n\n\n\n\n\nThe architect is often not the direct line manager of the developers implementing the software. You ultimately work with people, not software. Influence without authority is key.\n\nListening - assume you are not the smartest person in the room, collective wisdom is always better.\nDealing with criticism\n\nGenuine questioning: Provide facts and logic, be willing to go back and check again\nMocking: Don’t attack back, provide facts and logic\n\nBe smart not right\nOrganisational politics - be aware of it but don’t engage in it\nPublic speaking - define a goal, know your audience, be confident, don’t read, maintain eye contact\nLearning - blogs (DZone, InfoQ, O’Reilly), articles, conferences\n\n\n\n\n\n“The Complete Guide to Becoming a Software Architect” Udemy course\nDesign patterns\n\n\n\n\nFigure 1: Backend Tech\nFigure 2: Yellow Pages\nFigure 3: Gateway\nFigure 4: Stateful\nFigure 5: Stateless\nFigure 6: REST API\nFigure 7: HTTP Push\nFigure 8: Queue\nFigure 9: File-based Messaging\nFigure 10: Central Logging Service\nFigure 11: Monolith Example\nFigure 12: Microservice Example\nFigure 13: Event Sourcing\nFigure 14: CQRS"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#what-is-a-software-architect",
    "href": "posts/software/software_architecture/software_architect_notes.html#what-is-a-software-architect",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "A developer knows what can be done, an architect knows what should be done. How do we use technology to meet business requirements.\nGeneral system requirements:\n\nFast\nSecure\nReliable\nEasy to maintain\n\nArchitect’s need to know how to code for:\n\nArchitecture’s trustworthiness\nSupport developers\nRespect of developers"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#the-architects-mindset",
    "href": "posts/software/software_architecture/software_architect_notes.html#the-architects-mindset",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "Understand the business - Strengths, weaknesses, compettions, growth strategy.\nDefine the system’s goals - Goals are not requirements. Goals describe the effect on the organisation, requirements describe what the system should do.\nWork for your client’s clients - Prioritise the end user.\nTalk to the right people with the right language - What is the thing that really matters to the person I’m talking to?\n\nProject managers care about how things affect deadlines, developers care about the technologies used, CEOs care about business continuity and bottom line."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#the-architecture-process",
    "href": "posts/software/software_architecture/software_architect_notes.html#the-architecture-process",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "Understand the system requirements - What the system should do.\nUnderstand the non-functional requirements - Technical and service level attributes, e.g. number of users, loads, volumes, performance.\nMap the components - Understand the system functionality and communicate this to your client. Completely non-technical at this point, no mention of specific technologies. A diagram of high level components helps.\nSelect the technology stack - Backend, frontend, data store.\nDesign the architecture\nWrite the architecture document\nSupport the team\n\nInclude developers in non-functional requirements and architecture design for two reasons:\n\nLearn about unknown scenarios early\nCreate ambassadors"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#system-requirements",
    "href": "posts/software/software_architecture/software_architect_notes.html#system-requirements",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "The 2 types of requirements: functional and non-functional.\n\n\n“What the system should do”\n\nBusiness flows\nBusiness services\nUser interfaces\n\n\n\n\n“What the system should deal with”\n\nPerformance - latency and throughput\n\nLatency - How long does it take to perform a single task?\nThroughput - How many tasks can be performed in a given time unit?\n\nLoad - Quantity of work without crashing. Determines the availability of the system. Always look at peak (worst case) numbers.\nData volume - How much data will the system accumulate over time?\n\nData required on day one\nData growth (say, annually)\n\nConcurrent users - How many users will be using the system? This includes “dead times” which differentiates it from the load requirement. Rule of thumb is concurrent_users = load * 10\nSLA - Service Level Agreement for uptime.\n\nThe non-functional requirements are generally the more important in determining the architecture. The client will generally need guiding towards sensible values, otherwise they just want as much load as possible, as much uptime as possible etc."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#application-types",
    "href": "posts/software/software_architecture/software_architect_notes.html#application-types",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "The application type should be established early based on the use case and expected user interaction.\n\nWeb apps\n\nServe html pages.\nUI; user-initiated actions; large scale; short, focused actions.\nRequest-response model.\n\nWeb API\n\nServe data (often JSON).\nData retrieval and storage; client-initiated actions; large scale; short, focused actions.\nRequest-response model.\n\nMobile\n\nRequire user interaction; frontend for web API; location-based.\n\nConsole\n\nNo UI; limited interation; long-running processes; short actions for power users.\nRequire technical knowledge.\n\nService\n\nNo UI; managed by service manager; long-running processes.\n\nDesktop\n\nAll resources on the PC; UI; user-centric actions.\n\nFunction-as-a-service\n\nAWS lambda"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#select-technology-stack",
    "href": "posts/software/software_architecture/software_architect_notes.html#select-technology-stack",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "Considerations:\n\nAppropriate for the task\nCommunity - e.g. stack overflow activity\nPopularity - google trends over 2 years\n\n\n\nCovers web app, web API, console, service.\nOptions: .NET, Java, node.js, PHP, Python\n\n\n\n\n\n\nFigure 1: Backend Tech\n\n\n\n\n\n\nCovers:\n\nWeb app - Angular, React\nMobile - Native (Swift, Java/Kotlin), Xamarin, React Native\nDesktop - depends on target OS\n\n\n\n\nSQL - small, structured data\n\nRelational tables\nTransactions\n\nAtomicity\nConsistency\nIsolation\nDurability\n\nQuerying language is universal\n\nNoSQL - huge, unstructured or semi-structured data\n\nEmphasis on scale and performance.\nSchema-less, with entities stored as JSON.\nEventual consistency - data can be temporarily inconsistent\nNo universal querying language"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#the--ilities",
    "href": "posts/software/software_architecture/software_architect_notes.html#the--ilities",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "Quality attributes that describe technical capabilities to fulfill the non-functional requirements. Non-functional requirements map to quality attributes, which are designed in the architecture.\n\nScalability - Adding computing resources without any interruption. Scale out (add more compute instances) is preferred over scale up (increase CPU, RAM on the existing instance). Scaling out introduces redundancy and is not limited by hardware.\nManageability - Know what’s going on and take action accordingly. The question “who reports the problems” determines if a system is manageable. The system should flag problems, not the suer.\nModularity - A system that is built from blcoks that can be changed or replaced without affecting the whole system.\nExtensibility - Functionality of the system can be extended without modifying existing code.\nTestability - How easy is it to test the application. Independent modules and methods and single responsibility principle aid testability."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#components-architecture",
    "href": "posts/software/software_architecture/software_architect_notes.html#components-architecture",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "A software component is a piece of code that runs in a single process. Distributed systems are composed of independent software components deployed on separate processes/servers/containers.\nComponent architecture relates to the lower level details, whereas system architecture is the higher level view of how the components fit together to achieve the non-functional requirements.\n\n\nLayers represent horizontal functionality:\n\nUI/SI - user interface or service interface (i.e. an API), authentication\nBusiness logic - validation, enrichment, computation\nData access layer - connection handling, transaction handling, querying/saving data\n\nCode can flow downwards by one layer only. It can never skip a layer and can never go up a layer. This enforces modularity.\nA service might sit across layers, for example logging sits across the 3 layers described above. In this case, the logging service is called a “sross-cutting concern”.\nEach layer should be independent of the implementation of other layers. Layers should handle exceptions from those below them, log the error and then throw a more generic exception, so that there is no reference to other layers’ implementation.\nLayers are part of the same process. This is different from tiers, which are processes that are distributed across a network.\n\n\n\nAn interface declares the signature of an implementation. This means that each implementation must implement the methods described. The abstract base class in Python is an example of this.\nClose coupling should be avoided; “new is glue”.\n\n\n\n\nSingle responsibility principle: Each class, module or method should have exactly one responsibility.\nOpen/closed principle: Software should be open for extension but closed for modification. Can be implemented using class inheritance.\nLiskov substitution principle: If S is a subtype of T, then objects of type T can be replaced with objects of type S without altering the program. This looks similar to polymorphism but is stricter; not only should the classes implement the same methods, but the behaviour of those methods should be the same too, there should be no hidden functionality performed by S that is not performed by T or vice versa.\nInterface segregation principle: Many client-specific interfaces are preferable to one general purpose interface.\nDependency inversion principle: High-level modules should depend on abstractions rather than concrete implementations. Relies on dependency injection, where one object supplies the dependencies of another object. A factory method determines which class to load and returns an instance of that class. This also makes testing easier, as you can inject a mock class rather than having to mock out specific functionality.\n\n\n\n\n\nStructure - case, underscores\nContent - class names should be nouns, methods should be imperative verbs\n\n\n\n\nSome best practices:\n\nOnly catch exceptions if you are going to do something with it\nCatch specific exceptions\nUse try-catch on the smallest code fragments possible\nLayers should handle exceptions raised by layers below them, without exposing the specific implementation of the other layer. Catch, log, re-raise a more generic error.\n\nPurposes of logging:\n\nTrack errors\nGather data"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#design-patterns",
    "href": "posts/software/software_architecture/software_architect_notes.html#design-patterns",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "A collection of general, reusable solutions to common software design problems. Popularised in the book “Design patterns: elements of reusable object-oriented software”.\n\nThe factory pattern: Creating objects without specifying the exact class of the object. Avoids strong coupling between classes. Create an interface, then a factory method returns an instance of a class which implements that interface. If the implementation needs to change, we only need to change the factory method.\nThe repository pattern: Modules which don’t work with the data store should be oblivious to the type of data store. This is similar to the concept of the Data Access Layer. Layers are for architects, design patterns are for developers. They both seek to solve the same issue.\nThe facade pattern: Creating a layer of abstraction to mask complex actions. The facade does not create any new functionality, it is just a wrapper integrating existing modular functions together.\nThe command pattern: All the action’s information is encapsulated within an object."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#system-architecture",
    "href": "posts/software/software_architecture/software_architect_notes.html#system-architecture",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "The architecture design is the big picture that should answer the following:\n\nHow will the system work under heavy load?\nWhat will happen if the system crashes at a critical moment?\nHow complicated is it to update?\n\nThe architecture should:\n\nDefine components\nDefine how components communicate\nDefine the system’s quality attributes\n\nMake the correct choice as early as possible\n\n\nEnsuring the services are not strongly tied to other services. Avoid the “spiderweb” - when the graph of connections between services is densely connected.\nPrevents coupling of platforms and URLs.\nTo avoid URL coupling between services, two options are:\n\n“Yellow pages” directory\nGateway\n\nThe “yellow pages” contains a directory of all service URLs. If service A wants to query service D, A queries the yellow pages for D’s URL, then uses that URL to query D. This means that service A does not hardcode any information about service D. If D’s URLs change, then only the yellow pages need to be updated. Services only need to know the yellow pages directory’s URL.\n\n\n\n\n\n\nFigure 2: Yellow Pages\n\n\n\nThe gateway acts as a middleman. It holds a mapping table of all URLs Service A queries the gateway, which in turn queries service E. Service A doesn’t need to know about E or any other services. Services only need to know the gateway’s URL.\n\n\n\n\n\n\nFigure 3: Gateway\n\n\n\n\n\n\nThe application’s state is stored in only 2 places:\n\nThe data store\nThe user interface\n\nStateless architecture is preferred in virtually all circumstances.\nIn a stateful architecture, when a user logs in, the login service retrieves the user details from the database and stored them for future use. Data is stored in code.\nIf the login request was routed to server A, the user’s details are stored there. If the user later tried to add itemsto the cart and their cart service request is routed to server B, then their user details will not exist as those are stored on server A.\nDisadvantages of stateful:\n\nLack of scalability\nLack of redundancy\n\n\n\n\n\n\n\nFigure 4: Stateful\n\n\n\nIn a stateless architecture, no data is stored in the service itself. This means the behaviour will be the same regardless of which server a request was routed to.\n\n\n\n\n\n\nFigure 5: Stateless\n\n\n\n\n\n\nCaches store data locally to avoid retrieving the same data from the database multiple times. It trades reliability of data (it is stored in volatile memory) for improved performance.\nA cache should store data that is frequently accessed and rarely modified.\nTwo types of cache:\n\nIn-memory cache - Cache stored in memory on a single service\n\nPros:\n\nBest performance\nCan store any objects\nEasy to use\n\nCons:\n\nSize is limited by the process’s memory\nCan grow stale/inconsistent if the service is scaled out to multiple servers\n\n\nDistributed cache - Cache is independent of the services and can be accessed by all servers\n\nPros:\n\nSupports scaled out servers\nFailover capabilities\nStorage is unlimited\n\nCons:\n\nSetup is more complex\nOften only support primitive data types\nWorse performance than in-memory cache\n\n\n\n\n\n\nMessaging methods can be evaluated on these criteria:\n\nPerformance\nMessage size\nExecution model\nFeedback (handshaking) and reliability\nComplexity\n\nMessaging methods include:\n\nREST API\nHTTP Push\nQueue\nFile-based and database-based methods\n\n\n\nUniversal standard for HTTP-based systems.\nUseful for traditional web apps.\n\n\n\n\n\n\nFigure 6: REST API\n\n\n\n\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nVery fast\n\n\nMessage size\nSame as HTTP protocol limitations - GET 8KB, POST ~10MB\n\n\nExecution model\nRequest/response - ideal for quick, short actions\n\n\nFeedback\nImmediate feedback via response codes\n\n\nComplexity\nExtremely easy\n\n\n\n\n\n\nA client subscribes to the service waiting for an event. When that event occurs, the server notifies the client.\nFor real-time messaging, these often use web sockets to maintain the subscription connection, rather than a traditional request/response model.\nUseful for chat or monitoring.\n\n\n\n\n\n\nFigure 7: HTTP Push\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nVery fast\n\n\nMessage size\nLimited, few KB\n\n\nExecution model\nWeb socket connection / long polling\n\n\nFeedback\nNone (fire and forget)\n\n\nComplexity\nExtremely easy\n\n\n\n\n\n\nThe queue sits between two (or more) services. If service A wants to send a message to service B, A places the message in the queue and B periodically pulls from the queue.\nThis ensures messages will be handled exactly once and in the order received.\nUseful for complex systems with lots of data, when order and reliability are important.\n\n\n\n\n\n\nFigure 8: Queue\n\n\n\n\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nSlow - push/poll time and database persistence\n\n\nMessage size\nUnlimited but best practice to use small messages\n\n\nExecution model\nPolling\n\n\nFeedback\nVery reliable but feedback depends on the monitoring in place to ensure messages make it to the queue and get executed\n\n\nComplexity\nRequires training and setup to maintain a queue engine\n\n\n\n\n\n\nSimilar to queue, but rather han placing messages in a queue it places them in a file directory or database. There is no guarantee that messages are processed once and only once.\nIf multiple services are polling the same file folder, then when a new file is added we can get two issues:\n\nFile locked\nDuplicate processing\n\nSimilar use case to queues, but queues are generally preferred.\n\n\n\n\n\n\nFigure 9: File-based Messaging\n\n\n\n\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nSlow - push/poll time and database persistence\n\n\nMessage size\nUnlimited\n\n\nExecution model\nPolling\n\n\nFeedback\nVery reliable but feedback depends on the monitoring\n\n\nComplexity\nRequires training and setup to maintain the filestore or database\n\n\n\n\n\n\n\n\n\nCreate a central logging service that all other services call to write to a central database. This solves the problem where each microservice has its own log format, data and location. For example, some might be in files, SQL database, NoSQL database, etc.\nImplementation can be via an API or polling folders that each of the services write to.\n\n\n\n\n\n\nFigure 10: Central Logging Service\n\n\n\n\n\n\nCorrelation ID is an identifier attached to the beginning of a user flow and is attached to any action taken by that user, so that if there is an error at any point the user flow can be traced back through the different services’ logs."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#external-considerations",
    "href": "posts/software/software_architecture/software_architect_notes.html#external-considerations",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "External considerations can affect architecture and design decisions.\n\nDeadlines\nDev team skills - New technologies can introduce uncertainty, delays and low quality\nIT support - Assign who will support the product from the outset. This should not be developers.\nCost - Build vs buy. Estimate cost vs value. Spend money to save time, don’t spend time to save money."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#architecture-document",
    "href": "posts/software/software_architecture/software_architect_notes.html#architecture-document",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "This should describe the basic elements of the system:\n\nTechnology stack\nComponents\nServices\nCommunication between components and services\n\nNo development should begin before the document is complete.\nGoals of the document:\n\nDescribe what should be developed and how\nList the functional and non-functional requirements\n\nAudience:\n\nEveryone involved with the system: project manager, CTO, QA leader, developers\nSections for management appear first as they are unlikely to read the whole document\nQA lead can begin preparing test infrastructure ahead of time\n\nFormat: UML (universal modeling language) is often used. It visualises the system’s design with concepts and diagrams. However, this assumes the audience is familiar with UML which is often not the case.\n\nKeep the contents as simple as possible\nUse plain English\nVisualise using whatever software is comfortable and appropriate\n\nStructure:\n\nBackground\nRequirements\nExecutive summary\nArchitecture overview\nComponents drill-down\n\n\n\nThis section validates your point of view and instils confidence that you understand the project.\nOne page for all team and management.\nDescribe the system from a business POV:\n\nThe system’s role\nReasons for replacing the old system\nExpected business impact\n\n\n\nFunctional and non-functional requirements - what should the system do and deal with? This section validates your understadning of the requirements. The requirements dictate the architecture so this section sets the scene for the chosen architecture.\nOne page for all team and management.\nThis should be a brief, bullet point list of requirements with no more than 3 lines on each.\n\n\n\n\nProvide a high-level view of the architecture.\nManagers will not read the whole document; &lt;3 pages for management.\n\nUse charts and diagrams\nWrite this after the rest of the document\nUse technical terms sparsely and only well known ones\nBe concise and don’t repeat yourself\n\n\n\n\nProvide a high-level view of the architecture in technical terms. Do not deep dive into specific components\n&lt;10 pages for developers and QA lead.\n\nGeneral description: type and major non-functional requirements\nHigh-level diagram of logical components, no specifics about hardware or technologies\nDiagram walkthrough: describe the various parts and their roles\nTechnology stack: iff there is a single stack include it here, otherwise include it in the components drill down section\n\n\n\n\nDetailed description of each component.\nUnlimited length, for developers and QA lead.\nFor each component:\n\nComponent’s role\nTechnology stack: Data store, backend, frontend\nComponent’s architecture: Describe the API (URL, logic, response code, comments). Describe the layers. Mention design patterns here.\nDevelopment instructions: Specific development guidelines\n\nBe specific and include rationale behind decisions."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#advanced-architecture-topics",
    "href": "posts/software/software_architecture/software_architect_notes.html#advanced-architecture-topics",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "These architecture patterns solve specific problems but add complexity to the system, so the costs and benefits should be compared.\n\n\nAn architecture in which functionalities are implemented as separate, loosely coupled services that interact with each other using a standard lightweight protocol.\nProblems with monolithic services:\n\nA single exception can crash the whole process\nUpdates impact all components\nLimited to one development platform/language\nUnoptimised compute resources\n\nWith microservices, each service is independent of others so can be updated separately, use a different platform, and be optimised separately.\nAn example of splitting a monolithic architecture into microservices:\n\n\n\n\n\n\nFigure 11: Monolith Example\n\n\n\nAs a microservice architecture, this becomes:\n\n\n\n\n\n\nFigure 12: Microservice Example\n\n\n\nProblems with microservices:\n\nComplex monitoring of all services and their interactions\nComplex architecture\nComplex testing\n\n\n\n\nStoring the deltas to each entity rather than updating it. The events can then be “rebuilt” from the start to give a view of the state at any given point in time.\nUse when history matters.\n\n\n\n\n\n\nFigure 13: Event Sourcing\n\n\n\nPros:\n\nTracing history\nSimple data model\nPerformance\nReporting\n\nCons:\n\nNo unified view - need to rebuild all events from the start to see the current state\nStorage usage\n\n\n\n\nCommand query responsibility segregation. Data storage and data retrieval are two separate databases, with a sync service between the two.\nThis integrates nicely with event sourcing, where events (deltas) are stored in one database and the current state is periodically built and stored in the retrieval database.\n\n\n\n\n\n\nFigure 14: CQRS\n\n\n\nPros:\n\nUseful with high-frequency updates that require near real-time querying\n\nCons:\n\nComplexity - need 2 databases, a sync service, ETL between the storage and retrieval database"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#soft-skills",
    "href": "posts/software/software_architecture/software_architect_notes.html#soft-skills",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "The architect is often not the direct line manager of the developers implementing the software. You ultimately work with people, not software. Influence without authority is key.\n\nListening - assume you are not the smartest person in the room, collective wisdom is always better.\nDealing with criticism\n\nGenuine questioning: Provide facts and logic, be willing to go back and check again\nMocking: Don’t attack back, provide facts and logic\n\nBe smart not right\nOrganisational politics - be aware of it but don’t engage in it\nPublic speaking - define a goal, know your audience, be confident, don’t read, maintain eye contact\nLearning - blogs (DZone, InfoQ, O’Reilly), articles, conferences"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#references",
    "href": "posts/software/software_architecture/software_architect_notes.html#references",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "“The Complete Guide to Becoming a Software Architect” Udemy course\nDesign patterns\n\n\n\n\nFigure 1: Backend Tech\nFigure 2: Yellow Pages\nFigure 3: Gateway\nFigure 4: Stateful\nFigure 5: Stateless\nFigure 6: REST API\nFigure 7: HTTP Push\nFigure 8: Queue\nFigure 9: File-based Messaging\nFigure 10: Central Logging Service\nFigure 11: Monolith Example\nFigure 12: Microservice Example\nFigure 13: Event Sourcing\nFigure 14: CQRS"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html",
    "href": "posts/software/aws/aws_saa_notes.html",
    "title": "AWS Solutions Architect",
    "section": "",
    "text": "Regions are geographic locations, e.g. europe-west-3, us-east-1, etc.\nHow should we choose a region?\n\nCompliance - data governance rules may require data within a certain location\nProximity to reduce latency\nAvailable services vary by region\nPricing varies by region\n\nEach region can have multiple Availability Zones. There are usually between 3 and 6, e.g. ap-southeast-2a, ap-southeast-2b and ap-southeast-2c.\nEach AZ contains multiple data centers with redundant power, networking and connectivity.\nThere are multiple Edge Locations/Points of Presence; 400 locations around the world.\n\n\n\nSome services are global: IAM, Route 53, CloudFront, WAF\nMost are region-scoped: EC2, Elastic Beanstalk, Lambda, Rekognition\nThe region selector is in the top right. The service selector in top left, or alternatively use search bar.\n\n\n\nClick on Billing and Cost Management in the top right of the screen.\nThis needs to first be activated for administrator IAM users. From the root account: Account (top right) -&gt; IAM user and role access to billing information -&gt; tick the Activate IAM Access checkbox.\n\nBills tab - You can see bills per service and per region.\nFree Tier tab - Check what the free tier quotas are, and your current and forecasted usage.\nBudgets tab - set a budget. Use a template -&gt; Zero spend budget -&gt; Budget name and email recipients. This will alert as soon as you spend any money. There is also a monthly cost budget for regular reporting."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#aws-global-infrastructure",
    "href": "posts/software/aws/aws_saa_notes.html#aws-global-infrastructure",
    "title": "AWS Solutions Architect",
    "section": "",
    "text": "Regions are geographic locations, e.g. europe-west-3, us-east-1, etc.\nHow should we choose a region?\n\nCompliance - data governance rules may require data within a certain location\nProximity to reduce latency\nAvailable services vary by region\nPricing varies by region\n\nEach region can have multiple Availability Zones. There are usually between 3 and 6, e.g. ap-southeast-2a, ap-southeast-2b and ap-southeast-2c.\nEach AZ contains multiple data centers with redundant power, networking and connectivity.\nThere are multiple Edge Locations/Points of Presence; 400 locations around the world."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#tour-of-the-console",
    "href": "posts/software/aws/aws_saa_notes.html#tour-of-the-console",
    "title": "AWS Solutions Architect",
    "section": "",
    "text": "Some services are global: IAM, Route 53, CloudFront, WAF\nMost are region-scoped: EC2, Elastic Beanstalk, Lambda, Rekognition\nThe region selector is in the top right. The service selector in top left, or alternatively use search bar."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#aws-billing",
    "href": "posts/software/aws/aws_saa_notes.html#aws-billing",
    "title": "AWS Solutions Architect",
    "section": "",
    "text": "Click on Billing and Cost Management in the top right of the screen.\nThis needs to first be activated for administrator IAM users. From the root account: Account (top right) -&gt; IAM user and role access to billing information -&gt; tick the Activate IAM Access checkbox.\n\nBills tab - You can see bills per service and per region.\nFree Tier tab - Check what the free tier quotas are, and your current and forecasted usage.\nBudgets tab - set a budget. Use a template -&gt; Zero spend budget -&gt; Budget name and email recipients. This will alert as soon as you spend any money. There is also a monthly cost budget for regular reporting."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#overview",
    "href": "posts/software/aws/aws_saa_notes.html#overview",
    "title": "AWS Solutions Architect",
    "section": "2.1. Overview",
    "text": "2.1. Overview\nIdentity and access management. This is a global service.\nThe root account is created by default. It shouldn’t be used or shared; just use it to create users.\nUsers are people within the org and can be grouped. Groups cannot contain other groups. A user can belong to multiple groups (or none, but this is generally not best practice)."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#permissions",
    "href": "posts/software/aws/aws_saa_notes.html#permissions",
    "title": "AWS Solutions Architect",
    "section": "2.2. Permissions",
    "text": "2.2. Permissions\nUsers or groups can be assigned policies which are specified as a JSON document.\nLeast privilege principle means you shouldn’t give a user more permissions than they need."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#creating-users-and-groups",
    "href": "posts/software/aws/aws_saa_notes.html#creating-users-and-groups",
    "title": "AWS Solutions Architect",
    "section": "2.3. Creating Users and Groups",
    "text": "2.3. Creating Users and Groups\nIn the IAM dashboard, there is a Users tab.\nThere is a Create User button. We give them a user name and can choose a password (or autogenerate a password if this is for another user).\nThen we can add permissions directly, or create a group and add the user.\nTo create a group, specify the name and permissions policy.\nTags are optional key-value pairs we can add to assign custom metadata to different resources.\nWe can also create an account alias in IAM to simplify the account sign in, rather than having to remember the account ID.\nWhen signing in to the AWS console, you can choose to log in as root user or IAM user."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#iam-policies",
    "href": "posts/software/aws/aws_saa_notes.html#iam-policies",
    "title": "AWS Solutions Architect",
    "section": "2.4. IAM Policies",
    "text": "2.4. IAM Policies\nPolicies can be attached to groups, or assigned as inline policies to a specific user. Groups are best practice.\nComponents of JSON document:\n\nVersion: Policy language version (date)\nId: Identifier for the policy\nStatement: Specifies the permissions\n\nEach statement consists of:\n\nSid: Optional identifier for the statement\nEffect: “Allow” or “Deny”\nPrincipal: The account/user/role that this policy applies to\nAction: List of actions that this policy allows or denies\nResource: What the actions apply to, eg a bucket\nCondition: Optional, conditions when this policy should apply\n\n“*” is a wildcard that matches anything."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#mfa",
    "href": "posts/software/aws/aws_saa_notes.html#mfa",
    "title": "AWS Solutions Architect",
    "section": "2.5. MFA",
    "text": "2.5. MFA\nPassword policy can have different settings: minimum length, specific characters, password expiration, prevent password re-use.\nMulti-factor authentication requires the password you know and the device you own to log in.\nA hacker needs both to compromise the account.\nMFA devices:\n\nVirtual MFA devices - Google Authenticator, Authy. Support for multiple tokens on a single device.\nUniversal 2nd Factor Security Key (U2F) - eg YubiKey. Support for multiple root and IAM users on a single security key.\n\nHardware key fob MFA device\nHardware key fob MFA device for AWS GovCloud"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#access-keys",
    "href": "posts/software/aws/aws_saa_notes.html#access-keys",
    "title": "AWS Solutions Architect",
    "section": "2.6. Access Keys",
    "text": "2.6. Access Keys\nThere are 3 approaches to access AWS:\n\nManagement console (web UI) - password + MFA\nCommand line interface (CLI) - access keys\nSoftware Developer Kit (SDK) - access keys\n\nAccess keys are generated through the console and managed by the user. Access Key ID is like a username. Secret access key is like a password. Do not share access keys.\nAWS CLI gives programmatic access to public APIs of AWS. It is open source. Configure access keys in the CLI using aws configure.\nAWS SDK is for language-specific APIs."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#aws-cloudshell",
    "href": "posts/software/aws/aws_saa_notes.html#aws-cloudshell",
    "title": "AWS Solutions Architect",
    "section": "2.7. AWS CloudShell",
    "text": "2.7. AWS CloudShell\nAccess using the terminal icon in the toolbar next to the search bar.\nThis is an alternative to using your own terminal to access the AWS CLI. It is a cloud-based terminal.\nYou can pass --region to a command to run in a region other than the region selected in the AWS console.\nCloudShell has a file system attached so we can upload and download files."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#iam-roles-for-services",
    "href": "posts/software/aws/aws_saa_notes.html#iam-roles-for-services",
    "title": "AWS Solutions Architect",
    "section": "2.8. IAM Roles for Services",
    "text": "2.8. IAM Roles for Services\nSome AWS services can perform actions on your behalf. To do so, they need the correct permissions, which we can grant with an IAM role.\nFor example, EC2 instance roles, Lambda Function roles, CloudFormation roles.\nIn IAM, select Roles. Choose AWS Service and select the use case, e.g. EC2. Then we attach a permissions policy, such as IAMReadOnlyAccess."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#iam-security-tools",
    "href": "posts/software/aws/aws_saa_notes.html#iam-security-tools",
    "title": "AWS Solutions Architect",
    "section": "2.9. IAM Security Tools",
    "text": "2.9. IAM Security Tools\n\nIAM Credentials Report. Account-level report on all users and their credentials.\nIAM Access Advisor. User-level report on the service permissions granted to a user and when they were last accessed. This can help to see unused permissions to enforce principle of least privilege. This is in the Access Advisor tab under Users in IAM."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#iam-guidelines-and-best-practices",
    "href": "posts/software/aws/aws_saa_notes.html#iam-guidelines-and-best-practices",
    "title": "AWS Solutions Architect",
    "section": "2.10 IAM Guidelines and Best Practices",
    "text": "2.10 IAM Guidelines and Best Practices\n\nDon’t use root account except for account set up\nOne physical user = One AWS user\nAssign users to groups and assign permissions to groups\nCreate a strong password policy and use MFA\nUse Roles to give permissions to AWS services\nUse Access Keys for programmatic access via CLI and SDK\nAudit permissions using credentials report and access advisor\nNever share IAM users or access keys"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#summary",
    "href": "posts/software/aws/aws_saa_notes.html#summary",
    "title": "AWS Solutions Architect",
    "section": "2.11. Summary",
    "text": "2.11. Summary\n\nUsers map to a physical user\nGroups contain users. They can’t contain other groups.\nPolicies are JSON documents denoting the permissions for a user / group\nRoles grant permissions for AWS services like EC2 instances\nSecurity use MFA and password policy\nProgrammatic use of services via CLI or SDK. Access keys manage permissions for these.\nAudit usage via credentials report or access advisor"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#ec2-overview",
    "href": "posts/software/aws/aws_saa_notes.html#ec2-overview",
    "title": "AWS Solutions Architect",
    "section": "3.1. EC2 Overview",
    "text": "3.1. EC2 Overview\nElastic Compute Cloud used for infrastructure-as-a-service.\nEncompasses a few different use cases:\n\nRenting virtual machines (EC2)\nStoring data on virtual drives (EBS)\nDistributing load across machines (ELB)\nScaling services using an auto-scaling group (ASG)\n\nSizing and configuration options:\n\nOS\nCPU\nRAM\nStorage - This can be network-attached (EBS and EFS) or hardware (EC2 Instance Store)\nNetwork Card - Speed of card and IP address\nFirewall rules - Security group\nBootstrap script - Configure a script to run at first launch using and EC2 User Data script. This runs as the root user so has sudo access.\n\nThere are different instance types that have different combinations of the configuration options above."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#creating-an-ec2-instance",
    "href": "posts/software/aws/aws_saa_notes.html#creating-an-ec2-instance",
    "title": "AWS Solutions Architect",
    "section": "3.2. Creating an EC2 Instance",
    "text": "3.2. Creating an EC2 Instance\n\nSpecify a “name” tag for the instance and any other optional tags.\nChoose a base image. OS.\nChoose an instance type.\nKey pair. This is optional and allows you to ssh into your instance.\nConfigure network settings. Public IP address, checkboxes to allow ssh access, http access\nConfigure storage amount and type. Delete on termination is an important selection to delete the EBS volume once the corresponding EC2 instance is terminated.\nThe “user data” box allows us to pass a bootstrap shell script.\nCheck the summary and click Launch Instance.\n\nThe Instance Details tab tells you the Instance ID, public IP address (to access from the internet) and the private IP address (to access from within AWS).\nWe can stop an instance to keep the storage state of the attached EBS volume but without incurring any more EC2 costs. The public IP address might change it stopping and starting. The private IP address stays the same.\nAlternatively, we can terminate it completely."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#ec2-instance-types",
    "href": "posts/software/aws/aws_saa_notes.html#ec2-instance-types",
    "title": "AWS Solutions Architect",
    "section": "3.3. EC2 Instance Types",
    "text": "3.3. EC2 Instance Types\nThere are several families of instances: general purpose, compute-optimised, memory-optimised, accelerated computing, storage-optimised.\nSee the AWS website for an overview of all instances. There is also a handy comparison website here.\nThe naming convention is: \\[\nm5.large\n\\]\n\nm is the instance class\n5 is the generation (AWS releases new versions over time)\nlarge is the size within the class\n\nThe use cases for each of the instance types:\n\nGeneral purpose is for generic workloads like web servers. Balance between compute, memory and networking.\nCompute-optimized instances for tasks that require good processors, such as batch processing, HPC, scientific modelling.\nMemory-optimized instances for large RAM, e.g. in-memory databases and big unstructured data processing.\nStorage-optimised instances for tasks that require reading and writing a lot of data from lcoal storage, e.g. high-frequency transaction processing, cache for in-memory databases, data warehouse."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#security-groups",
    "href": "posts/software/aws/aws_saa_notes.html#security-groups",
    "title": "AWS Solutions Architect",
    "section": "3.4. Security Groups",
    "text": "3.4. Security Groups\nSecurity groups control how traffic is allowed into or out of EC2 instances. They act as a “firewall” on EC2 instances.\nSecurity groups only contain allow rules. Security groups can reference IP addresses or other security groups.\nThey regulate:\n\nAccess to ports\nAuthorised IP ranges (IPv4 and IPv6)\nInbound and outbound network\n\nBy default, any inbound traffic is blocked and any outbound traffic is allowed.\nA security group can be attached to multiple instances. It is locked down to a (region, VPC) combination.\nThe security group exists “outside” of the EC2 instance, so if traffic is blocked then the instance will never see it.\n\nAny time you get a timeout when trying to access your EC2 instance, it’s almost always a result of the security rule.\nIf the application gives “connection refused” then it’s an application error.\nIt can be helpful to keep a security group specifically for SSH access\n\nAccess security groups under:\nEC2 -&gt; Network & Security -&gt; Security Groups\nWe can set the type of connection, the port and the IP address/range.\nA security group can reference other security groups, i.e. “allow traffic from any other EC2 instance which has Security Group A or Security Group B attached to it”. This saves us from having to reference IP addresses all the time, which can be handy when these are not static.\nTypical ports to know:\n\n21 - FTP, file transfer protocol\n22 - SSH or SFTP (because SFTP uses SSH), secure shell and secure FTP\n80 - HTTP, access unsecured websites\n443 - HTTPS, access secured websites\n3389 - RDP, Remote Desktop Protocol, SSH equivalent for Windows"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#connecting-to-instances",
    "href": "posts/software/aws/aws_saa_notes.html#connecting-to-instances",
    "title": "AWS Solutions Architect",
    "section": "3.5. Connecting to Instances",
    "text": "3.5. Connecting to Instances\nSSH works for Linux, Mac or Windows &gt; 10. Putty works for all versions of Windows. EC2 Instance Connect works for all operating systems.\n\n3.5.1. Linux via SSH\nSSH allows you to control a remote machine using the command line.\nYou need you pem / ppm file for your secure keys. The EC2 instance needs to allow inbound connections for SSH access.\nssh EC2-&lt;username&gt;@&lt;public IP address&gt;\nWe can pass the file path for the key with the argument -i path/to/file.pem\n\n\n3.5.2. EC2 Instance Connect\nThis opens a terminal in browser. No security keys are required since it generates temporary keys.\nThis relies on SSH behind the scenes, so the correct security groups for SSH need to be allowed on the EC2 instance.\nUse the EC2 Instance Connect tab in the EC2 section for your running instance."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#ec2-instance-roles",
    "href": "posts/software/aws/aws_saa_notes.html#ec2-instance-roles",
    "title": "AWS Solutions Architect",
    "section": "3.6. EC2 Instance Roles",
    "text": "3.6. EC2 Instance Roles\nNever enter your IAM details on an EC2 instance as this would be available to anybody else who can access the instance. Poor security practices!\nInstead, we use EC2 instance roles.\nIn the tab for the instance tab, we can do this with:\nAction -&gt; Security -&gt; Modify IAM Role\nThen select a role to attach to the instance."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#ec2-instance-purchase-options",
    "href": "posts/software/aws/aws_saa_notes.html#ec2-instance-purchase-options",
    "title": "AWS Solutions Architect",
    "section": "3.7. EC2 Instance Purchase Options",
    "text": "3.7. EC2 Instance Purchase Options\n\n3.7.1. Purchase Options\nMore common: - Spot: short workloads, cheap but can be terminated. Not suitable for critical jobs or databases. - On-demand: short uninterrupted workloads, pay per second - Reserved: long workloads like a database. 1 or 3 years. Convertible reserved instances allow you to change the instance type over the reserved period. - Savings plan: 1 to 3 year commitment to an amount of USAGE rather than committing to a specific instance size or OS.\nLess common: - Dedicated hosts: book an entire physical server and control instance placement. Most expensive. Useful to meet compliance requirements, or where you have Bring Your Own Licence (BYOL) software. - Dedicated instances: no other customers share your hardware. No control over instance placement, so the physical hardware might move after stopping and starting. May share hardware with other instances in the same account. - Capacity reservations: reserve capacity in your AZ for any duration. No time commitment and no billing discounts. You’re charged on demand rates whether you run the instance or not. Suitable for short term interrupted workloads that need to be in a specific AZ.\n\n\n3.7.2. IPv4 Charges\nThere is a $0.005 per hour charge for all public IPv4 in your account.\nThere is a free tier for the EC2 service. There is no free tier for any other service.\nThere is no charge for IPv6 addresses. But this does not work for all ISPs.\nAWS Public IP Insights Service under Billing is helpful to see these costs.\n\n\n3.7.3. Spot Instances\nDiscount of up to 90% compared to on demand instances.\nYou define the max spot price you are willing to pay, and you get the instance for as long as the current price is less than your max price. The hourly spot price varies by offer and capacity. If the current price rises above your max, you have a 2 minute grace period to stop or terminate your instance.\n“Spot Block” is a strategy to block a spot instance for a specified time frame (1-6 hours). They are no longer available but could potentially still come up on the exam.\nA spot request consists of:\n\nMax price\nDesired number of instances\nLaunch specification - AMI\nRequest type: one-time or persistent. A persistent request will automatically request more spot instances whenever any are terminated, for as long as the spot request is valid.\nValid from and until\n\nSpot Instance Requests can only be cancelled if they are open, active or disabled. Canceling a spot request does not terminate the instance. You need to cancel the request then terminate the instance, to ensure a persistent request does not launch another."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#spot-fleets",
    "href": "posts/software/aws/aws_saa_notes.html#spot-fleets",
    "title": "AWS Solutions Architect",
    "section": "3.7.4. Spot Fleets",
    "text": "3.7.4. Spot Fleets\nA spot fleet is a set of spot instances + optional on-demand instances.\nIt will try to meet the target capacity within the price constraints. You specify the launch pool: instance type, OS and availability zone. You can have multiple launch pools so the fleet can choose the best. It will stop launching instances either when it reaches target capacity or max cost.\nThere are several strategies for allocating spot instances:\n\nlowestPrice: from the pool with lowest price\ndiversified: distributed across pools for better availability\ncapacityOptimized: from the pool with optimal capacity\npriceCapacityOptimized (recommended): pool with highest capacity first, then lowest price"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#private-vs-public-ip",
    "href": "posts/software/aws/aws_saa_notes.html#private-vs-public-ip",
    "title": "AWS Solutions Architect",
    "section": "4.1. Private vs Public IP",
    "text": "4.1. Private vs Public IP\nThere are two types of IP in networking: IPv4 and IPv6. v4 is most commonly used, v6 is for IoT.\nPublic IP means the machine can be identified on the internet. It must be unique across the whole web.\nPrivate IP means the machine can only be located on the private network. It must be unique across that private network. Only a specified range of IP addresses can be used as private addresses. Machines connect to the internet using an internet gateway (a proxy)."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#elastic-ip",
    "href": "posts/software/aws/aws_saa_notes.html#elastic-ip",
    "title": "AWS Solutions Architect",
    "section": "4.2. Elastic IP",
    "text": "4.2. Elastic IP\nWhen you start and stop an EC2 instance it can change its IP. If you need this to be fixed, you can use elastic IP which will get reused for future instances if that one gets terminated. You can only have 5 elastic IP addresses in your account.\nIt is best practice to avoid elastic IP addresses as they often are a symptom of bad design choices. Instead, use a random public IP and register a DNS name to it. Or alternatively, use a load balancer and don’t use a public IP."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#placement-groups",
    "href": "posts/software/aws/aws_saa_notes.html#placement-groups",
    "title": "AWS Solutions Architect",
    "section": "4.3. Placement Groups",
    "text": "4.3. Placement Groups\nPlacement groups allow you to control the EC2 instance placement strategy. You don’t get direct access to / knowledge of the hardware, but you can specify one of three strategies:\n\nCluster - cluster instances into a low latency group in a single AZ. High performance but high risk; low latency and high bandwidth. Useful for big data jobs that need to complete quickly.\nSpread - spread instances across different hardware (max 7 instances per AZ). Useful for critical applications as the risk of all instances simultaneously failing is minimised. But the max instance count limits the size of the job.\nPartition - Spread instances across many different sets of partitions within an AZ. Each partition represents a physical rack of hardware. Max 7 partitions per AZ, but each partition can have many instances. Useful for applications with hundreds of instances or more, like Hadoop.\n\n\nCreating a Placement Group\nTo create a placement group:\nEC2 -&gt; Network & Security -&gt; Placement Groups -&gt; Create Placement Group\nGive it a name, e.g. my-critical-group, then select one of the three strategies.\nTo launch an instance in a group:\nClick Launch Instances -&gt; Advanced Settings -&gt; Placement Group Name"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#elastic-network-interfaces",
    "href": "posts/software/aws/aws_saa_notes.html#elastic-network-interfaces",
    "title": "AWS Solutions Architect",
    "section": "4.4. Elastic Network Interfaces",
    "text": "4.4. Elastic Network Interfaces\n\n4.4.1. What is an ENI?\nElastic Network Interfaces (ENIs) represent a virtual network card in a VPC. They are bound to a specific AZ.\nEach ENI can have the following attributes:\n\nOne private IPv4, plus one or more secondary IPv4 addresses\nOne Elastic IPv4 per private IPv4\nOne public IPv4\nOne or more security groups\nOne MAC address\n\nAn ENI can be created and then attached to EC2 instances on the fly. This makes them useful for failover, as the ENI from the failed instance can be moved to its replacement to keep the IP addresses consistent.\nAnother use case is for deployments. We have the current version of the application running on instance A with an ENI. This is accessible by its IP address. Then we can run the new version of the application of instance B. When we are ready to deploy, move the ENI to instance B.\n\n\n4.4.2. Creating an ENI\nClick on the Instance in the UI and see the Network Interfaces section.\nUnder the Network & Security tab we can see Network Interfaces. We can create ENIs here.\nSpecify: description, subnet, Private IPv4 address (auto assign), attach a Security Group.\n\n\n4.4.3 Attaching an ENI to an Instance\nOn the Network Interfaces UI, Actions -&gt; Attach. Select the instance.\nMore on ENIs: https://aws.amazon.com/blogs/aws/new-elastic-network-interfaces-in-the-virtual-private-cloud/"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#ec2-hibernate",
    "href": "posts/software/aws/aws_saa_notes.html#ec2-hibernate",
    "title": "AWS Solutions Architect",
    "section": "4.5. EC2 Hibernate",
    "text": "4.5. EC2 Hibernate\n\n4.5.1. Why Hibernate?\nWhen we stop an instance, the data on disk (EBS) is kept intact until the next start. When we start it again, the OS boots up and runs the EC2 User Data script. This can take time.\nEC2 Hibernate is a way of reducing boot time. When the instance is hibernated, the RAM state is saved to disk (EBS). When the instance is started again, it loads the RAM state from disk. This avoids having to boot up and initialise the instance from scratch.\nUse cases:\n\nLong-running processing\nServices that take time to initialise\n\nAn instance can not be hibernated for more than 60 days. The instance RAM size must be less than 150GB and the EBS root volume large enough to store it.\n\n\n4.5.2. Enable Hibernation on an Instance\nWe can enable hibernation when creating an instance under Advanced Details, there is a “Stop - Hibernate Behaviour” drop down that we can enable.\nUnder Storage, the EBS volume must be encrypted and larger than the RAM.\nTo then hibernate a specific instance, on the Instance Summary select Instance State -&gt; Hibernate Instance."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#ebs",
    "href": "posts/software/aws/aws_saa_notes.html#ebs",
    "title": "AWS Solutions Architect",
    "section": "5.1. EBS",
    "text": "5.1. EBS\n\n5.1.1. What is an EBS Volume?\nAn Elastic Block Storage (EBS) volume is a network drive that you can attach to your instance. Think of it like a “cloud USB stick”. They allow us to persist data even after an instance is terminated. EBS volumes have a provisioned capacity: size in GB and IOPS.\nEach EBS volume can only be mounted to one EC2 instance at a time, and are bound to a specific AZ. To move a volume across an AZ, you need to snapshot it. Each EC2 instance can have multiple EBS volumes.\nThere is a “delete on terminate” option. By default, this is on for the root volume but not any additional volumes. We can control this in the AWS console or CLI.\n\n\n5.1.2. Creating an EBS Volume on an Instance\nWe can see existing volumes under\nEC2 -&gt; Elastic Block Store -&gt; Volumes\nWe can select Create Volume. We then choose volume type, size, AZ (same as instance).\nThis makes the volume available. We can then attach the volume using Actions -&gt; Attach Volume. The Volume State will now be “In-use” instead of “Available”.\n\n\n5.1.3. EBS Snapshots\nA snapshot is a backup of your EBS volume at a point in time.\nIt is recommended, but not necessary, to detach the volume from an instance.\nWe can copy snapshots across AZ and regions.\nFeatures:\n\nEBS Snapshot Archive. Moving the snapshot to an archive tier is much cheaper (75%) but then takes 24-72 hours to restore.\nRecycle bin. There are setup rules to retain deleted snapshots so they can be restored after deletion. Retention period is 1 day to 1 year.\nFast snapshot restore (FSR). Force full initialisation of snapshot to have no latency. This costs more.\n\n\n\n5.1.4. EBS Features Hands On\nCreate an EBS volume:\nElastic Block Store UI, Actions -&gt; Create Snapshot -&gt;  Add a description\nSee snapshots:\nEBS -&gt; Snapshots tab\nCopy it to another region:\nRight-click volume -&gt; Copy Snapshot -&gt; Select the description and destination region\nRecreate a volume from a snapshot:\nSelect the snapshot -&gt; Actions -&gt; Create Volume From Snapshot -&gt;  Select size and AZ\nArchive the snapshot:\nSelect the volume -&gt; Actions -&gt; Archive Snapshot\nRecover a snapshot after deletion:\nRecycle Bin -&gt; Select the snapshot -&gt; Recover\n\n\n5.1.5. EBS Volume Types\nEBS volumes are characterised by: size, throughput and IOPS.\nTypes of EBS volumes:\n\ngp2/gp3 - General purpose SSD. The newer gp3 options allow size and IOPS to be varied independently, for the older gp2 types they were linked.\nio1/io2 Block Express - High throughput low latency SSD. Support EBS Multi Attach.\nst1 - Low cost HDD for frequently accessed data.\nsc1 - Lowest cost HDD for infrequently accessed data.\n\nOnly the SSD options can be used as boot volumes.\n\n\n5.1.6. EBS Multi Attach\nAttach the same EBS volume to multiple EC2 instances (up to 16) in the same AZ.\nOnly available for io1 and io2 EBS volume types. You must use a file system that is cluster-aware.\nFor use cases with higher application availability in clustered applications, or where applications must manage concurrent write operations.\n\n\n5.1.7. EBS Encryption\nWhen you create an encrypted EBS volume you get:\n\nData at rest is encrypted inside the volume\nData in flight is encrypted between the volume and the instance\nSnapshots are encrypted\nVolumes created from the snapshot are encrypted\n\nEncryption and decryption is all handled by AWS. The latency impact is minimal. It uses KMA (AES-256) keys.\nCopying an unencrypted snapshot allows encryption:\n\nCreate an EBS snapshot of the volume.\nEncrypt the snapshot using copy.\nCreate a new EBS volume from the snapshot. This will be encrypted.\nAttach the encrypted volume to the original instance."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#ami",
    "href": "posts/software/aws/aws_saa_notes.html#ami",
    "title": "AWS Solutions Architect",
    "section": "5.2. AMI",
    "text": "5.2. AMI\nAmazon Machine Image (AMI) is the customisation of an EC2 instance. It defines the OS, installed software, configuration, monitoring, etc. AMIs are built for a specific region.\nPutting this in the AMI rather than the boot script results in a faster boot time since the software is prepackaged.\nWe can launch EC2 instances from:\n\nA public AMI (provided by AWS)\nAn AMI from the AWS Marketplace (provided by a third-party)\nYour own AMI\n\nCreate an AMI from a running instance that we have customised to our liking:\nRight-click the instance -&gt; Images and Templates -&gt; Create Image\nSee the AMI:\nEC2 UI -&gt; Images -&gt; AMIs\nLaunch an instance from an AMI\nSelect the AMI -&gt; Launch Instance from AMI"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#ec2-instance-store",
    "href": "posts/software/aws/aws_saa_notes.html#ec2-instance-store",
    "title": "AWS Solutions Architect",
    "section": "5.3. EC2 Instance Store",
    "text": "5.3. EC2 Instance Store\nEBS volumes are network drives, which gives adequate but potentially slow read/write.\nEC2 Instance Store is a physical disk attached to the server that is running the EC2 instance.\nThey give better I/O performance but are ephemeral, the data is lost if the instance is stopped or the hardware fails.\nGood for cache or temporary data."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#elastic-file-system-efs",
    "href": "posts/software/aws/aws_saa_notes.html#elastic-file-system-efs",
    "title": "AWS Solutions Architect",
    "section": "5.4. Elastic File System (EFS)",
    "text": "5.4. Elastic File System (EFS)\n\n5.4.1. What is EFS?\nEFS is a managed Network File System (NFS) that can be mounted on multiple EC2 instances. The EC2 instances can be in multiple AZs.\nHighly available, scalable, but more expensive. It scales automatically and you pay per GB. You don’t need to plan the capacity in advance.\nA security group is required to control access to EFS. It is compatible with Linux AMIs only, not Windows. It is a POSIX (Linux-ish) file system with the standard API.\nUses cases: content management, web serving, data sharing, Wordpress.\n\n\n5.4.2. Performance Modes\n\nEFS Scale - This gives thousands of concurrent NFS clients for &gt;10GB/s of throughput.\nPerformance modes - This can be set to general purpose for latency-sensitive use cases, or Max I/O for higher throughput at the expense of higher latency.\nThroughput mode - This can be set to bursting which scales throughput with the total storage used, provisioned which sets a fixed throughput, or elastic which scales throughput depending on the demand (ie the requests received)\n\n\n\n5.4.3. Storage classes\nStorage tiers are a lifecycle management feature to move files to cheaper storage after N days. You can implement lifecycle policies to automatically move files between tiers based on the number of days since it was last accessed.\n\nStandard. For frequently accessed files.\nInfrequent access (EFS-IA). There is a cost to retrieve files, but lower cost to store.\nArchive. For rarely accessed data.\n\nThere are two different availability options:\n\nRegional. Multi-AZ within a region, good for production.\nOne zone. Only one AZ with backup enabled by default. Good for dev."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#ebs-vs-efs",
    "href": "posts/software/aws/aws_saa_notes.html#ebs-vs-efs",
    "title": "AWS Solutions Architect",
    "section": "5.5. EBS vs EFS",
    "text": "5.5. EBS vs EFS\nEBS volumes are attached to one instance (mostly, apart from multi-attach) and are locked at the AZ level.\nEFS can be mounted to hundreds of instances across AZs. It is more expensive, but storage tiers can help reduce this.\nInstance Store is attached to a specific instance, and is lost when that instance goes down."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#scalability-and-availability",
    "href": "posts/software/aws/aws_saa_notes.html#scalability-and-availability",
    "title": "AWS Solutions Architect",
    "section": "6.1. Scalability and Availability",
    "text": "6.1. Scalability and Availability\nScalability means an application can adapt to handle greater loads.\n\nVertical scalability. Increase the size of a single instance. The scaling limit is often a hardware limit. “Scale up and down”.\nHorizontal scalability. Also called elasticity. Distribute across more instances. “Scale out and in”.\n\nHigh availability is the ability to survive a data center loss. This often comes with horizontal scale. Run the application across multiple AZs."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#elb",
    "href": "posts/software/aws/aws_saa_notes.html#elb",
    "title": "AWS Solutions Architect",
    "section": "6.2. ELB",
    "text": "6.2. ELB\n\n6.2.1. Load balancing\nLoad balancers are servers that forward traffic to multiple servers downstream.\nBenefits:\n\nSpread the load across downstream instances\nPerform health checks on instances and handle downstream failures\nExpose a single point of access (DNS) to your application\nProvide SSL termination\nSeparate public traffic from private traffic\nHigh availability across zones\n\nElastic Load Balancer (ELB) is a managed load balancer.\nHealth checks are done on a port and route, and return a 200 status if it can be reached.\nThere are four kinds of managed load balancer:\n\nClassic load balancer (Deprecated)\nApplication load balancer\nNetwork load balancer\nGateway load balancer\n\nSome can be set up as internal (private) or external (public).\n\n\n6.2.2. Security Groups\nUsers can connect via HTTP/HTTPS from anywhere. So the security groups typically allow inbound TCP connections on ports 80 and 443.\nThe security groups for the downstream EC2 instances then only needs to allow inbound connections from the load balancer, i.e. one specific source. This means we can forbid users from connecting directly to the instance and force them to go via the load balancer.\n\n\n6.2.3. Application Load Balancer (ALB)\nThese are layer 7 load balancers, meaning they take HTTP requests. They support HTTP/2 and WebSocket, and can also redirect from HTTP to HTTPS.\nYou get a fixed hostname, i.e. XXX.region.elb.amazonaws.com. This is helpful to get a fixed IP to connect to instances which are being created and destroyed, where IP addresses are normally changing constantly. The application servers don’t see the IP of the client directly, but this is inserted as a header X-forwarded-for. We also headers for the port and protocol.\nUse cases are microservices and container-based applications (e.g. ECS). One load balancer can route traffic between multiple applications. There is a port mapping feature to redirect to a dynamic port in ECS.\nThey can route requests to multiple HTTP applications across machines (called target groups) or multiple applications on the same machine (e.g. containers).\nRouting options:\n\nBy path in URL - e.g. /users endpoint, /blog endpoint\nBy hostname in URL - e.g. one.example.com and two.example.com\nBy query string headers - e.g. /id=123&orders=True\n\nALB can route to multiple target groups. Health checks are at the target group level. Target groups can be:\n\nEC2 instances\nECS tasks\nLambda functions\nPrivate IP addresses\n\n\n\n6.2.4. Network Load Balancer (NLB)\nThese are layer 4 load balancers, meaning they route TCP and UDP traffic. Ultra-low latency and can handle millions of requests per second. NLB has one static IP per AZ.\nTarget groups can be:\n\nEC2 instances\nPrivate IP addresses\nApplication load balancers. You may want the NLB for a static IP, routing to an ALB for the http routing rules.\n\nHealth checks support TCP, HTTP and HTTPS protocols.\n\n\n6.2.5. Gateway Load Balancer (GWLB)\nThis is a layer 3 load balancer, meaning it routes IP packets.\nThis is useful when we want to route traffic via a target group of a 3rd party network virtual appliance (e.g. a firewall) before it reaches our application.\nUser traffic &gt; GWLB &gt; Firewall &gt; GWLB &gt; Our application \nIt uses the GENEVE protocol on port 6081.\nTarget groups can be:\n\nEC2 instances\nPrivate IP addresses\n\n\n\n6.2.6. Sticky Sessions\nStickiness means a particular client is always routed to the same instance behind the load balancer. This means the user doesn’t lose their session data. It does this via a cookie which has an expiration date.\nOverusing sticky sessions can result in imbalanced loads, since they’re constraining the load balancer to direct traffic to instances that may not be optimal.\nApplication-based cookies. Two options for this: - A custom cookie is generated by the target. The cookie name must be specified for each target group and cannot be one of the reserved keywords: AWSALB, AWSALBAPP, AWSALBTG. - An application cookie is generated by the load balancer. The cookie name is always AWSALBAPP.\nDuration-based cookies. This is generated by the load balancer. The cookie name is always AWSALB for ALB (or AWSELB for CLB).\nELB UI -&gt; Target Groups -&gt; Select a target group -&gt; Edit Target Group \n-&gt; Turn On Stickiness -&gt; Select cookie type and duration\n\n\n6.2.7. Cross-Zone Load Balancing\nWith cross-zone load balancing, each load balancer will distribute requests evenly across all registered instances in all AZs, regardless of which zone the request came from.\nWithout cross-zone load balancing, there can be big disparities between load in different AZs.\nCross-zone load balancing is enabled by default for ALB and CLB, and won’t charge for data transfer between AZs. It is disabled by default for NLB and GWLB. These will charge for inter-AZ data transfer if you enable it.\n\n\n6.2.8. Connection Draining\nThe load balancer allows time to complete “in-flight requests” while the instance is registering or unhealthy. The load balancer stops sending new requests to the EC2 instance which is de-registering.\nThe is called connection draining for CLB, and deregistration delay for ALB and NLB.\nIt can be 0-3600 seconds. By default it is 300 seconds. Disable connection draining by setting it to 0."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#ssl-certificates",
    "href": "posts/software/aws/aws_saa_notes.html#ssl-certificates",
    "title": "AWS Solutions Architect",
    "section": "6.3. SSL Certificates",
    "text": "6.3. SSL Certificates\n\n6.3.1. SSL and TLS\nAn SSL certificate allows in-flight encryption - traffic between clients and load balancer is encrypted. They have an expiration date (that you set) and must be renewed. Public SSL certificates are issued by Certificate Authorities (CA) like GoDaddy, GlobalSign etc.\nTLS certificates are actually used in practice, but the name SSL has stuck.\n\nSSL = Secure Sockets Layer\nTLS = Transport Layer Security (a newer version of SSL)\n\nThe load balancer uses a default X.509 certificate, but you can upload your own. AWS Certificate Manager (ACM) allows you to manage these certificates.\n\n\n6.3.2. SNI\nClients can use Server Name Indication (SNI) to specify the hostname they reach.\nSNI solves the problem of loading multiple SSL certificates on one web server. We may have a single load balancer serving two websites: www.example.com and www.company.com\nEach of these websites has an SSL certificate uploaded to the load balancer. When a client request comes in, it indicates which website it wants to reach and the load balancer will use the corresponding SSL certificate.\nThis works for ALB, NLB or CloudFront.\nELB UI -&gt; Select a load balancer -&gt; Add a listener -&gt; Select the default SSL/TLS certificate"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#auto-scaling-groups-asg",
    "href": "posts/software/aws/aws_saa_notes.html#auto-scaling-groups-asg",
    "title": "AWS Solutions Architect",
    "section": "6.4. Auto Scaling Groups (ASG)",
    "text": "6.4. Auto Scaling Groups (ASG)\n\n6.4.1. What is an ASG?\nThe goal of an ASG is to scale out/in (add/remove EC2 instances) to match load. It ensures we have a minimum and maximum number of instances running.\nIf running an ASG connected to a load balancer, any EC2 instances created will be part of that load balancer.\nASG is free, you only pay for the underlying EC2 instances.\nYou need to create a ASG Launch Template.\n\nAMI and instance type\nEC2 user data\nEBS volumes\nSecurity groups\nSSH key pair\nIAM roles for EC2 instances\nNetwork and subnet information\nLoad balancer information\n\nThe ASG has a minimum, maximum and initial size as well as a scaling policy.\nIt is possible to scale the ASG based on CloudWatch alarms.\n\n\n6.4.2. Scaling Policies\nDynamic scaling:\n\nTarget tracking scaling. Keep a certain metric, e.g. ASG CPU, to stay at 40%\nSimple / step scaling. When a CloudWatch alarm is triggered, e.g. CPU &gt; 70%, add 2 units.\n\nScheduled scaling:\n\nAnticipate scaling based on known usage patterns. E.g. market open.\n\nPredictive scaling:\n\nContinuously forecast load and schedule scaling accordingly.\n\nGood metrics to scale on:\n\nCPUUtilization\nRequestCountPerTarget\nAverage Network In/Out\nApplication-specific metrics\n\n\n\n6.4.3. Scaling Cooldown\nAfter a scaling activity happens, there is a cooldown period (default 300 seconds) where the ASG will not launch or remove any more instances while it waits for metrics to stabilise.\nUsing a ready-to-use AMI means the EC2 instances start quicker, allowing you to use a shorter cooldown and be more reactive."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#rds",
    "href": "posts/software/aws/aws_saa_notes.html#rds",
    "title": "AWS Solutions Architect",
    "section": "7.1. RDS",
    "text": "7.1. RDS\n\n7.1.1 What is Relational Database Service?\nRelational Database Service (RDS) is a managed DB using SQL as a query language.\nSupported database engines: Postgres, MySQL, MariaDB, Oracle, Microsoft SQL Server, IBM DB2, Aurora (AWS proprietary database).\nInstead of RDS, we could run our own EC2 instance with a database inside. The benefit of RDS is that it is a managed service, so you get:\n\nAutomated provisioning and OS patching\nContinuous backups and point-in-time restore\nMonitoring dashboards\nRead replicas\nMulti-AZ setup for disaster recovery\nMaintenance windows for upgrades\nHorizontal and vertical scaling capabilities\nStorage backed by EBS\n\nBut the downside is you can’t SSH into the underlying instances.\n\n\n7.1.2. Storage Auto-Scaling\nRDS will increase your DB instance automatically as you run out of free space. You set a Maximum Storage Threshold.\nThis will automatically modify storage if:\n\nFree storage is less than 10%\nLow-storage lasts at least 5 mins\n6 hours have passed since the last notification\n\n\n\n7.1.3. Read Replicas\nRead replicas allow better read scalability. Read replicas are obviously read-only, so only support SELECT statements.\nWe can create up to 15 read replicas. They can be within AZ, cross-AZ or cross-region. The replication is asynchronous so they are eventually consistent. Replicas can be promoted to their own DB.\nApplications must update their connection string to use the read replicas.\nUse cases may be if you have an existing production application, and now you want to add a reporting application without affecting performance of the existing process.\nNetwork costs:\n\nIf the read replicas are in the same region, there is no network costs for the data transfer.\nThere is a network cost for cross-region read replicas.\n\n\n\n7.1.4. Multi-AZ\nThis is typically for disaster recovery.\nThis is a synchronous replication.\nThere is one DNS name, and the application will automatically failover to the standby database if the master database goes down. No manual intervention is required. This increases availability.\nAside from the disaster case, no traffic is normally routed to the standby database. It is only for failovers, not scaling.\nYou can set up read replicas as multi-AZ for disaster recovery.\nSingle-AZ to multi-AZ is a zero downtime operation, the DB does not stop. We just click “modify” on the database.\nInternally, what happens is: a snapshot it taken, a new DB is restored from this snapshot in a new AZ, synchronisation is established between the two databases.\n\n\n7.1.5. RDS Custom\nThis is a managed Oracle and Microsoft SQL Server database with full admin access for OS and database customisation. Usually these are managed by RDS.\nIt allows us to configure the OS and settings, and access the underlying EC2 instance using SSH or SSM Session Manager.\n\n\n7.1.6. RDS Proxy\nAn RDS Proxy pools and shares incoming connections together resulting in fewer connections to the database. Think of it like a load balancer for the database.\nThis is useful when you have multiples instances scaling in and out that might connect to your database then disappear and leave lingering connections open. For example, when using Lambda functions.\nIt is serverless and supports autoscaling. It reduces failover time by up to 66%. It supports both RDS and Aurora, including most flavours of SQL.\nNo code changes are required for most apps, just point the connection details to the proxy rather than the database directly. Authentication is via IAM using credentials stored in AWS Secrets Manager. The RDS Proxy can only be accessed from inside the VPC, it is never publicly accessible."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#amazon-aurora",
    "href": "posts/software/aws/aws_saa_notes.html#amazon-aurora",
    "title": "AWS Solutions Architect",
    "section": "7.2. Amazon Aurora",
    "text": "7.2. Amazon Aurora\n\n7.2.1. What is Aurora?\nAurora is a proprietary database from AWS with compatibility with Postgres and MySQL.\nAurora is “cloud-optimised” with faster read/write performance and less lag when creating read replicas. Storage grows automatically. Failover is instantaneous.\nIt stores 6 copies of your data across 3 AZ: 4 out of 6 copies are needed for writes, and 3 out of 6 for reads.\nStorage is striped across hundreds of volumes. There is self-healing with peer-to-peer replication.\n\nOne master Aurora instance takes writes. There is automated failover within 30 seconds if the master instance goes down.\n\nMaster + up to 15 read replicas serve reads. You can set up auto-scaling for read replicas. There is support for cross-region replication.\n\nThe aurora DB cluster. You don’t interact with any instance directly; they can scale and be removed so the connection URL would be constantly changing. Instead there is a writer endpoint that always points to the master instance. There is a read endpoint which points to a load balancer that directs your query to a read replica.\n\n\n\nAurora Cluster Readers and Writers\n\n\n\n\n7.2.2. Advanced Concepts\nAuto scaling. Read replicas scale based on CPU usage or number of connections breaching a user-defined threshold.\nCustom endpoint. Define a subset of the read replicas as a custom endpoint. This means we can route traffic for jobs that we know are database-intensive, like analytical queries, to a subset of the instances without affecting the performance on the other read replicas.\nAurora serverless. Automated database instantiation and auto-scaling based on actual usage.\nGood for infrequent, intermittent or unpredictable workloads. No capacity planning needed, you pay per second of usage.\nThe client connects to a proxy fleet, which is like a load balancer that directs requests to Aurora instances that are scaled in the background.\nGlobal Aurora. Cross-region replicas are useful for disaster recovery. Aurora Global Database is the recommended approach.\nYou create 1 primary regions for read/write. You can then have up to 5 secondary read-only regions. Replication lag is &lt;1 second. Up to 16 read replicas per secondary region.\nPromoting another region in the event of disaster recovery has a Recovery Time Objective (RTO) &lt; 1 minute.\nAurora Machine Learning. Add ML-based predictions to your application via SQL. Supported on SageMaker or Amazon Comprehend.\nBabelfish for Aurora PostgreSQL. Babelfish allows Aurora PostgreSQL to understand commands targeted for Microsoft SQL Server (written in T-SQL). It automatically translates between these flavours of SQL to make migration easier."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#backups-and-monitoring",
    "href": "posts/software/aws/aws_saa_notes.html#backups-and-monitoring",
    "title": "AWS Solutions Architect",
    "section": "7.3. Backups and Monitoring",
    "text": "7.3. Backups and Monitoring\n\n7.3.1. RDS\nThere are automated backups:\n\nFull backup daily during the backup window.\nTransaction logs backed up every 5 mins. This gives the ability to do a point-in-time restore.\n1-35 days of retention. Can be disabled by setting to 0.\n\nManual DB snapshots are triggered by the user and can be retained as long as you want.\nA use case for this is for an infrequently used database. A stopped RDS database will still incur storage costs. If you intend to stop it for a long time, you can snapshot it then restore it later.\n\n\n7.3.2. Aurora\nAutomate backups are retained for 1-35 days. Cannot be disabled. Point-in-time recovery for any point in that timeframe.\nManual DB snapshots. Triggered by user and retained for as long as you want.\n\n\n7.3.3. Restore Options\n\nRestore an RDS / Aurora backup or snapshot to create a new database.\nRestore a MySQL RDS database from S3. Create a backup of your on-premises database, store it in S3, the restore the backup file on to a new instance running MySQL.\nRestore a MySQL Aurora cluster from S3. Same as for RDS, except the on-premises backup must be created using Percona XtraBackup.\n\n\n\n7.3.4 Aurora Database Cloning\nCreate a new Aurora DB cluster from an existing one. An example use case is cloning a production database into dev and staging.\nIt is faster than doing a snapshot+restore. It uses the copy-on-write protocol. Initially the clone uses the same data volume as the original cluster, then when updates are made to the cloned DB cluster additional storage is allocated and data is copied to be separated."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#encryption",
    "href": "posts/software/aws/aws_saa_notes.html#encryption",
    "title": "AWS Solutions Architect",
    "section": "7.4. Encryption",
    "text": "7.4. Encryption\nApplies to both RDS and Aurora.\nAt rest encryption. Database master and read replicas are encrypted using AWS KMS. This must be defined at launch time. If master is not encrypted then the read replicas cannot be encrypted. If you want to encrypt and unencrypted database, you need to take a snapshot of it and restore a new database with encryption set up at launch time.\nIn flight encryption. RDS and Aurora are TLS-ready by default. Applications must use the provided AWS TLS root certificates on the client side.\nAuthentication can be via IAM or by the standard username/password used to connect to databases. Security groups can also be used to control access. Audit logs can be enabled and sent to CloudWatch Logs for longer retention."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#elasticache",
    "href": "posts/software/aws/aws_saa_notes.html#elasticache",
    "title": "AWS Solutions Architect",
    "section": "7.5. ElastiCache",
    "text": "7.5. ElastiCache\n\n7.5.1. What is ElastiCache?\nElastiCache is a managed Redis or Memcached. Analogous to how RDS is a managed SQL database. It is managed, meaning AWS takes care of OS maintenance, configuration, monitoring, failure recovery, backups, etc.\nCaches are in-memory databases with low latency. They reduce the load on your database for read-intensive workloads.\nThis can help make your application stateless. For example, when the user logs in, their session is written to the cache. If their workload is moved to another instance, their session can be retrieved from the cache.\nIt does, however, require significant changes to your application’s code. Instead of querying the database directly, we need to:\n\nQuery the cache. If we get a cache hit, use that result.\nIf we get a cache miss, read from the database directly.\nThen write that result to the cache ready for the next query.\n\nWe also need to define a cache invalidation strategy to ensure the data in the cache is not stale.\n\n\n7.5.2. Redis vs Memcached\nRedis replicates whereas Memcached shards.\nRedis:\n\nMulti-AZ with auto-failover\nRead replicas to scale for high availability\nAOF persistence\nBackup and restore features\nSupports sets and sorted sets. Sorted sets allow for things like real-time leaderboards\n\nMemcached:\n\nMulti-node for partitioning (sharding) data\nNo replication (therefore not high availability)\nNot persistent\nBackup and restore available for the serverless option only\nMulti-threaded architecture\n\n\n\n7.5.3. ElastiCache Security\nElastiCache supports IAM authentication for Redis. IAM policies on ElastiCache are only used for AWS API-level security.\nFor Memcached, it needs to be username/password. Memcached supports SASL-based authentication.\nWith Redis AUTH you can set a password/token when you create a cluster, which provides an extra level of security on top of security groups. It supports SSL in-flight encryption.\nCommon patterns for ElastiCache.\n\nLazy loading - All the read data is cached, but data in the cache may become stale.\nWrite through - Data is inserted/updated in the cache any time it is written to the DB. Ensures no stale data.\nSession store - Using the cache to store temporary session data, and using TTL to determine cache validation.\n\n\n\n7.5.4. Common Port Numbers\nUseful port numbers to know:\n\n21 - FTP\n22 - SFTP, SSH\n80 - HTTP\n443 - HTTPS\n\nCommon database ports:\n\n5432 - PostgreSQL, Aurora\n3306 - MySQL, MariaDB, Aurora\n1433 - MySQL Server\n1521 - Oracle RDS"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#dns",
    "href": "posts/software/aws/aws_saa_notes.html#dns",
    "title": "AWS Solutions Architect",
    "section": "8.1. DNS",
    "text": "8.1. DNS\n\n8.1.1. What is DNS?\nDomain Name System (DNS) translates human-friendly hostnames into the machine-friendly IP address. E.g. www.google.com -&gt; 172.217.18.36\nThere is a hierarchical naming structure separated by ., e.g. \nwww.example.com\napi.example.com\nTerminology:\n\nDomain registrar: Amazon Route 53, GoDaddy\nDNS Records: A, AAAA, CNAME\nZone file: contains DNS records\nName server: Server that resolves DNS queries\nTop-Level Domain (TLD): .com, .gov, .org\nSecond-Level Domain (SLD): google.com, amazon.com\n\n\n\n\nComponents of a URL\n\n\n\n\n8.1.2. How DNS Works\nYour web browser sends a request for www.example.com to a Local DNS Server managed by your company or ISP. This routes to a Root DNS Server managed by ICANN, which resolves the top-level domain (.com) and gives the corresponding IP address for that part. The browser then sends a request to the TLD DNS Server managed by ICANN which resolves the second-level domain. The browser then sends a request to the SLD DNS Server managed by Amazon Registrar etc, and that gives the IP address of the requested website.\n\n\n\nDNS Routing"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#route-53-1",
    "href": "posts/software/aws/aws_saa_notes.html#route-53-1",
    "title": "AWS Solutions Architect",
    "section": "8.2. Route 53",
    "text": "8.2. Route 53\nRoute 53 is a fully managed authoritative DNS and Domain Regstrar. You can also check the health of your resources. Authoritative means the customer (you) can update the DNS records.\n“53” is a reference to the traditional DNS port.\n\n8.2.1. Records\nYou define records which define how you want to route traffic for a domain. Each record contains:\n\nDomain/subdomain name - example.com\nRecord type - A/AAAA/CNAME/NS\nValue - 12.34.56.78\nRouting policy - how Route53 responds to queries\nTTL - how long this record is cached at DNS resolvers\n\nRecord types:\n\nA - maps a hostname to IPv4\nAAAA - maps a hostname to IPv6\nCNAME - maps a hostname to another hostname. The target is a domain which must have an A or AAAA record.\nNS - Name servers for the hosted zone. These are the DNS names/IP addresses for the servers that can respond to queries for your hosted zone.\n\n\n\n8.2.2. Hosted Zones\nA hosted zone is a container for records that define how traffic is routed to a domain and its subdomains.\n\nPublic hosted zones - contain records specifying how to route traffic on the internet (i.e. public domain names)\nPrivate hosted zones - contain records specifying how to route traffic in your VPC (i.e. private domain names)\n\nIt costs $0.50 per month per hosted zone.\n\n\n\nPublic vs Private Hosted Zones\n\n\nRoute 53 UI -&gt; Domains -&gt; Registered Domains \n-&gt; Choose a domain name -&gt; Choose duration, auto renew \n-&gt; Checkout -&gt; Contact info\nNow in Hosted Zones you will see the DNS records created for your domain.\n\n\n8.2.3. Creating a Record\nRoute 53 -&gt; Hosted Zones -&gt; Create Record\nSpecify the record details discussed previously.\nWe can take the IP addresses from our EC2 instances, load balancers, etc and route to these as we wish.\n\n\n8.2.4. TTL\nTime To Live (TTL) on a record tells the client how long it should cache the record (i.e. the IP address) before requesting it again. TTL is mandatory for all records except Alias records.\nA high TTL (e.g. 24 hours) results in less traffic to Route 53, but clients might possibly have outdated records. A low TTL results in more traffic to Route 53 (and therefore higher costs) but records are more up to date and so easier to change.\nA common strategy when you know you are changing your DNS record soon is to temporarily lower the TTL close to the switchover.\n\n\n8.2.5. CNAME vs Alias Records\nMany AWS resources, like ELB and CloudFront, expose an AWS hostname, e.g. blabla.us-east-1.elb.amazonaws.com and we want to map it to myapp.domain.com.\nCNAME records allow a hostname to point to any other hostname, but only for a non-root domain.\nAlias records point a host name to an AWS resource, and this works for root domains and non-root domains. It is free and has a native health check. It automatically recognises changes in the resource’s IP address. It is always of type A or AAAA, i.e. IPv4 or IPv6. AWS sets the TTL so you can’t set this manually.\nValid Alias record targets: ELB, CloudFront Distributions, API Gateway, Elastic Beanstalk environments, S3 Websites (not buckets), VPC Interface Endpoints, Global Accelerator, Route 53 record in the same hosted zone.\nYou cannot set an Alias record for an EC2 DNS name."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#routing-policies",
    "href": "posts/software/aws/aws_saa_notes.html#routing-policies",
    "title": "AWS Solutions Architect",
    "section": "8.3. Routing Policies",
    "text": "8.3. Routing Policies\nRouting policies define how Route 53 responds to DNS queries.\nIt isn’t “routing” in the sense of a load balancer; DNS does not route any traffic, it just responds to DNS queries.\nRoute 53 supports several routing policies:\n\nSimple. Route traffic to a single resource. We can specify multiple values for the same record. The client will pick one of the IP addresses at random. Cannot use with health checks.\nWeighted. Control the percentage of requests that go to each resource. We assign each record a relative weight; the DNS records must have the same name and type. Can be used with health checks.\nLatency-based. Redirect to the resource with the lowest latency, based on traffic between users and AWS. Can be associated with health checks.\nFailover. There is a health checker associated with the primary instance. If this passes, Route 53 returns its IP address to route traffic to it. If unhealthy, it fails over to another instance and returns that IP address.\nGeolocation. Routing is based on the user’s location. This is subtly different from the latency-based policy. We can specify location by continent, country or US state. There should be a “default” record in case of no match. Can be used with health checks.\nGeo-proximity. Route traffic to your resources based on the geographic location of users and resources. It is like a continuous equivalent of the discrete binning of the geolocation policy. We shift more traffic to resources based on the defined bias - a value between -99 to 99. Resources can be AWS resources (specify AWS region) or non-AWS resources (specify latitude and longitude).\nIP-based routing. Routing is based of the clients’ IP addresses. You provide a list of CIDRs for your clients and the corresponding endpoints/locations. These are user-IP-to-endpoint mappings. An example is you route users from a particular ISP to a specify endpoint.\nMulti-value. Used when routing traffic to multiple resources; Route 53 returns multiple values. Can be associated with health checks and will only return IP addresses of healthy instances.\n\nIt is not a replacement for an ELB. It doesn’t necessarily distribute load evenly, it just gives clients more options and lets them choose."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#health-checks",
    "href": "posts/software/aws/aws_saa_notes.html#health-checks",
    "title": "AWS Solutions Architect",
    "section": "8.4 Health Checks",
    "text": "8.4 Health Checks\nHTTP Health Checks are for public resources. We can use them to get automatic DNS failover.\nWe can use health checks to:\n\nMonitor resources\nMonitor other health checks. This is called a Calculated Health Check. We can use OR, AND or NOT logic to combine the results of multiple health checks, or specify at least N checks must pass.\nMonitor CloudWatch Alarms. This gives us a workaround to use them for private resources. The health checkers are outside the VPC so cannot access the endpoint directly. Create a CloudWatch Metric with and associated CloudWatch Alarm, and the health checker monitors the alarm.\n\nRoute 53 UI -&gt; Health Checks -&gt; Create health check\nThere are 15 global health checkers in different regions. They will periodically send HTTP requests to /health and if &gt;18% of them receive a 2xx or 3xx status code the endpoint is healthy. The health checker can use the text in the first 5120 bytes of the response.\nYou can customise the range of regions to use. You must configure your resource to allow incoming requests from the Route 53 health checker IP range."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#domain-registrar-vs-dns-service",
    "href": "posts/software/aws/aws_saa_notes.html#domain-registrar-vs-dns-service",
    "title": "AWS Solutions Architect",
    "section": "8.5. Domain Registrar vs DNS Service",
    "text": "8.5. Domain Registrar vs DNS Service\nYou buy/register your domain name with a Domain Registrar by paying an annual fee. The Domain Registrar usually provides you with a DNS service to manage your DNS records. Examples of domain registars are GoDaddy, Amazon Registrar.\nBut you don’t have to stick with the same service provider. You could buy a domain from GoDaddy and use Route53 to manage your DNS records. You can create a hosted zone in Route 53 and specify the custom Nameservers to do this."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#instantiating-applications-quickly",
    "href": "posts/software/aws/aws_saa_notes.html#instantiating-applications-quickly",
    "title": "AWS Solutions Architect",
    "section": "9.1. Instantiating Applications Quickly",
    "text": "9.1. Instantiating Applications Quickly\nEC2 instances, RDS databases and other resources take time to boot up. Some common patterns to speed up boot time:\n\nEC2 instances. Use a golden AMI which already has your applications and dependencies installed. Launch your instance from this AMI. You may have some user data or other dynamic data. Use a bootstrap script for these. A hybrid approach is to put as much static logic as possible into the golden AMI and keep the bootstrap script lean.\nRDS databases. Restore from a snapshot to quickly recover/resume without having to do lots of slow inserts.\nEBS volumes. Restore from a snapshot. The disk will already be formatted and have the correct data."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#elastic-beanstalk",
    "href": "posts/software/aws/aws_saa_notes.html#elastic-beanstalk",
    "title": "AWS Solutions Architect",
    "section": "9.2. Elastic Beanstalk",
    "text": "9.2. Elastic Beanstalk\nMany applications will have the same architecture: an ALB with an ASG scaling out the EC2 instances with an RDS database.\nThe pain points are around managing infrastructure and configuring all of the services each time. Ideally, we would have a single way of doing this rather than repeating the same steps every time.\nElastic Beanstalk is a managed service to handle all of this. It automatically handles scaling, load balancing, health monitoring, configuration. You still have control to configure the resources.\nBeanstalk is free, but you pay for the underlying resources (EC2, RDS, ALB, etc).\nComponents:\n\nApplication - a collection of Elastic Beanstalk components (environments, versions, configurations)\nApplication Version - an iteration of your application code\nEnvironment - a collection of AWS resources running an application version. You can create multiple environments for dev, staging, prod, etc. There are web server and worker environment tiers\n\nCreate application -&gt; Upload version -&gt; Launch environment -&gt; Manage environment (upload another version to update it)\nBeanstalk supports a lot of languages."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#web-server-environment-vs-worker-environment",
    "href": "posts/software/aws/aws_saa_notes.html#web-server-environment-vs-worker-environment",
    "title": "AWS Solutions Architect",
    "section": "9.3. Web Server Environment vs Worker Environment",
    "text": "9.3. Web Server Environment vs Worker Environment\n\n\n\nWeb server env vs worker env"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#deployment-modes",
    "href": "posts/software/aws/aws_saa_notes.html#deployment-modes",
    "title": "AWS Solutions Architect",
    "section": "9.4. Deployment Modes",
    "text": "9.4. Deployment Modes\nSingle instance vs high availability\n\n\n\nSingle instance vs high availability"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#security",
    "href": "posts/software/aws/aws_saa_notes.html#security",
    "title": "AWS Solutions Architect",
    "section": "10.2. Security",
    "text": "10.2. Security\nCan be:\n\nUser-based. IAM policies\nResource-based. Bucket policies (allows cross-account access), Object Access Control List (ACL) for finer grained control, Bucket ACL.\n\nUse bucket policies to grant public access, grant cross-account access, or force encryption for objects at upload. We can use bucket settings or account settings to block public access, if we know that nothing should ever be public so we want to make sure nobody accidentally sets a policy that is too open.\nThe policies are OR based - a user can access a resource if either the IAM policy or the resource policy allows it, as long as there is not a specific deny.\nObjects can be encrypted within S3 as an extra layer of security.\nPolicies are defined with a JSON. Important keys are:\n\nResources: buckets and objects that this policy applies to\nEffect: allow or deny\nActions: the API actions to allow or deny, like GetObject, ListObjects etc\nPrincipal: the account/user to apply the policy to"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#s3-website",
    "href": "posts/software/aws/aws_saa_notes.html#s3-website",
    "title": "AWS Solutions Architect",
    "section": "10.3. S3 Website",
    "text": "10.3. S3 Website\nS3 can host static websites.\nWe must enable public read access on the bucket, otherwise users will get 403 errors."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#s3-versioning",
    "href": "posts/software/aws/aws_saa_notes.html#s3-versioning",
    "title": "AWS Solutions Architect",
    "section": "10.4. S3 Versioning",
    "text": "10.4. S3 Versioning\nYou can version files in S3. If we upload a file with the same key, it will increment the version number.\nThis needs to be enabled at the bucket-level. It is best practice to use versioning for backup and roll back.\n\nEnabling versioning on an existing bucket will result in version=null for existing objects.\nRemoving versioning on a bucket will not delete the previous versions on existing objects.\n\nWe can delete the newest version if we want to roll back to the previous version.\nWhen we delete an object, we are actually updating it with a “delete marker” version. We can then rollback the delete marker to recover our file."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#replication",
    "href": "posts/software/aws/aws_saa_notes.html#replication",
    "title": "AWS Solutions Architect",
    "section": "10.5. Replication",
    "text": "10.5. Replication\nCross-Region Replication (CRR) and Same-Region Replication (SRR) are used to asynchronously copy data from one bucket to another. They can be in different accounts.\nVersioning must be enabled in source and destination buckets, and S3 must have the required IAM permissions.\nUse cases:\n\nCRR. Compliance, lower latency access\nSRR. Log aggregation, sync dev vs prod environments\n\nAfter you enable replication, only new objects are replicated. To also replicate the history, use S3 Batch Replication. Deletes with delete markers are replicated, but unversioned “permanent” deletes are not replicated to avoid replicating malicious deletes.\nYou cannot chain replication from Bucket A -&gt; B -&gt; C."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#s3-storage-classes",
    "href": "posts/software/aws/aws_saa_notes.html#s3-storage-classes",
    "title": "AWS Solutions Architect",
    "section": "10.6. S3 Storage Classes",
    "text": "10.6. S3 Storage Classes\nDurability represents how often you will lose an object in storage. S3 has 11 9s durability, so if you store 10 million objects you will lose a single object on average once every 10000 years.\nThis is the same for all storage classes.\nAvailability is how the uptime of a service. This varies depending on the storage class.\nStorage classes:\n\nS3 Standard: general purpose, infrequent access, one zone infrequent access\nS3 Glacier: instance retrieval, flexible retrieval, deep archive\nS3 Intelligent Tiering: automatically move objects between tiers based on lifecycle policies."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#lifecycle-rules",
    "href": "posts/software/aws/aws_saa_notes.html#lifecycle-rules",
    "title": "AWS Solutions Architect",
    "section": "10.7. Lifecycle Rules",
    "text": "10.7. Lifecycle Rules\nYou can transition objects between storage classes. This can be done manually, or automated through lifecycle rules.\n\nTransition actions. Configure objects to transition to another storage class. E.g. standard-IA 60 days after creation or glacier for archiving after 6 months.\nExpiration actions. Configure objects to expire (delete) after a specified period. Can be used to delete old versions of files, called “non-current versions”, if versioning is enabled. Can also be used to delete incomplete multi-part uploads after a certain time has elapsed.\n\nRules can be created for a certain prefix, or for certain tags.\nAmazon S3 Analytics provides storage classes analysis. It gives daily reports with recommendations for transition actions for standard and standard-IA storage classes."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#s3-requester-pays",
    "href": "posts/software/aws/aws_saa_notes.html#s3-requester-pays",
    "title": "AWS Solutions Architect",
    "section": "10.8. S3 Requester Pays",
    "text": "10.8. S3 Requester Pays\nGenerally, the owner of the bucket pays for storage and data transfer costs.\nWith Requester Pays buckets, the requester pays the data transfer costs associated with their request. The requester must be authenticated in AWS (they cannot be anonymous).\nThis is useful when sharing large amounts of data with other accounts."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#s3-event-notifications",
    "href": "posts/software/aws/aws_saa_notes.html#s3-event-notifications",
    "title": "AWS Solutions Architect",
    "section": "10.9. S3 Event Notifications",
    "text": "10.9. S3 Event Notifications\nAn event can be an object being created, removed, restored, replicated.\nYou can filter on the object names, e.g. *.jpg\nS3 Event Notifications can then be sent to other AWS resources, e.g. EC2, to trigger a downstream workflow. The notifications typically deliver in seconds but can take a minute or more.\nThe S3 service needs to have the appropriate access policy. To send event notifications to SNS, it needs an SNS Resource Access Policy. For SQS, it needs an SQS resource policy. And for Lambda functions, it needs a Lambda resource policy. Do this in the AWS console for the appropriate service (SNS, SQS, Lambda).\nAll events, regardless of whether they are going to SNS, SQS, Lambda etc, go via Amazon EventBridge. From here, you can set rules to go to over 18 AWS services as destinations.\nEventBridge has filtering options, multiple destinations, and archive and replay capabilities."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#s3-performance",
    "href": "posts/software/aws/aws_saa_notes.html#s3-performance",
    "title": "AWS Solutions Architect",
    "section": "10.10 S3 Performance",
    "text": "10.10 S3 Performance\nYou can get at least 3500 PUT, COPY, POST, DELETE requests and 5500 GET/HEAD requests per second per prefix in a bucket.\nThe prefix is everything between the bucket and the file name. Remember S3 isn’t really a file system, it’s an object store, so the prefix is just a long string that happens to have some slashes in, it isn’t actually a hierarchy.\nThere are no limits to the number of prefixes you can have in a bucket.\nMulti-part upload is recommended for files &gt; 100 MB and required for files &gt; 5GB The file is divided into parts and uploaded in parallel.\nS3 transfer acceleration increases transfer speed by transferring the file to an AWS edge location and then forwarding it to the target region. So rather than uploading directly to an AZ that is far away, you can upload to your nearest location which is quicker and then transfer between regions using AWS’s private network which is fast. Minimise the time spent on public networks and maximise the time on private networks.\nS3 Byte-range fetches parallelises GET requests by requesting specific byte ranges. This also gives better resilience in the case of failures. It can also be used to retrieve partial data, e.g. just the head of a file."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#s3-batch-operations",
    "href": "posts/software/aws/aws_saa_notes.html#s3-batch-operations",
    "title": "AWS Solutions Architect",
    "section": "10.11 S3 Batch Operations",
    "text": "10.11 S3 Batch Operations\nPerform bulk operations on existing S3 objects with a single request. A job consists of a list of objects, the action to perform, optional parameters. You can use S3 Inventory to get the object list and use Athena to filter the list.\nS3 Batch Operations will manage retries, progress tracking, completion notifications and generate reports.\nCommon use cases: modify object metadata, copy objects between buckets, encrypt objects, modify tags, restore objects from S3 Glacier, invoke a Lambda function to perform custom actions on each object."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#s3-storage-lens",
    "href": "posts/software/aws/aws_saa_notes.html#s3-storage-lens",
    "title": "AWS Solutions Architect",
    "section": "10.12 S3 Storage Lens",
    "text": "10.12 S3 Storage Lens\nAnalyse storage across the entire organisation. It can identify anomalies and cost efficiencies.\nIt aggregates data for an organisation, or specific accounts, regions, buckets or prefixes.\nYou can use this to create a dashboard or export metrics as a CSV file to an S3 bucket. There is a default dashboard and you can create your own custom dashboards.\nAvailable metrics:\n\nSummary metrics. Storage bytes, object count\nCost-optimisation metrics. Non-current version storage bytes, incomplete multi part upload storage bytes.\nData protection metrics. Version-enabled bucket count, MFA enabled bucket count, KMS-enabled bucket count, cross-region replication rule count.\nAccess management metrics. Object ownership bucket owner enforce bucket count.\nEvent metrics. Insights for S3 Event notifications, number of buckets with S3 events enabled.\nPerformance metrics. Transfer acceleration enabled bucket count.\nActivity metrics. Number of requests, split get get vs put vs list etc, bytes downloaded.\nStatus code metrics. Count of 200 status codes, 403, 404 etc.\n\nSome metrics are free and some are paid. Under the free tier, metrics are available for 14 days. For paid, it is available for 15 months and automatically published to CloudWatch."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#object-encryption",
    "href": "posts/software/aws/aws_saa_notes.html#object-encryption",
    "title": "AWS Solutions Architect",
    "section": "11.1. Object Encryption",
    "text": "11.1. Object Encryption\nObject encryption can either be server-side or client-side.\n\nServer-Side Encryption (SSE). Can manage keys using S3-managed keys (SSE-S3), KMS (SSE-KMS) or customer-provided keys (SSE-C).\nClient-Side Encryption.\n\nSSE-S3 is enabled by default for new buckets and objects. SSE-KMS allows usage statistics of keys to be tracked using CloudTrail. Each write and read using these keys will could towards your KMS APi usage. This can result in throttling if uploading lots of data. SSE-C requires the key to be provided in the header directly when uploading or downloading data. You can only use this from the CLI, not from the AWS console UI.\nThe option used is specified in the header when uploading the object.\nFor client-side encryption, the customer manages the keys and encryption themselves. Client must encrypt data before sending it to S3 and decrypt data when retrieving it. You handle this yourself and don’t need to specify this in the S3 encryption settings.\nEncryption in transit is also called SSL/TLS. Amazon S3 exposes two endpoints: HTTPS which is encrypted in transit and HTTP which is not. HTTPS is recommended.\nEncryption in transit and/or server-side encryption can be enforced using a bucket policy. Bucket policies are evaluated before “Default Encryption“ settings for the bucket."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#cors",
    "href": "posts/software/aws/aws_saa_notes.html#cors",
    "title": "AWS Solutions Architect",
    "section": "11.2. CORS",
    "text": "11.2. CORS\nCross-Origin Resource Sharing (CORS).\nOrigin = scheme (protocol) + host (domain) + port\nUsing https://www.example.com The port is implied by HTTPS (443) or HTTP (80). The scheme is www and the host is example.com\nSo these have the same origin:\n\nhttp://www.example.com/app1\nhttp://www.example.com/app2\n\nThese have different origins:\n\nhttp://www.example.com\nhttp://api.example.com\n\nCORS is a web-browser based mechanism to allow requests to other origins while visiting the main origin. The other origin must allow for requests using CORS Headers.\n\n\n\nCORS example\n\n\nWe make a request to a web server, and it routes us to another server in a different region, for example to retrieve some images on the page.\nOn S3, if a client makes a cross-origin request on our S3 bucket, we need to enable the correct CORS headers. For example, if we are hosting a static website on S3 which contains images stored on a different S3 bucket; the image bucket must have the correct CORS headers enabled to fulfil the request. You can allow a specific region or * for any region."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#mfa-delete",
    "href": "posts/software/aws/aws_saa_notes.html#mfa-delete",
    "title": "AWS Solutions Architect",
    "section": "11.3. MFA Delete",
    "text": "11.3. MFA Delete\nWe can force users to authenticate with MFA when doing potentially destructive actions on the bucket, e.g. permanently deleting an object version or SuspendVersioning on the bucket. Only the bucket owner (root account) can enable/disable MFA Delete.\nMFA won’t be required to enable versioning or list deleted versions."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#s3-access-logs",
    "href": "posts/software/aws/aws_saa_notes.html#s3-access-logs",
    "title": "AWS Solutions Architect",
    "section": "11.4 S3 Access Logs",
    "text": "11.4 S3 Access Logs\nFor audit purposes, you can log access requests to a particular S3 bucket, whether authorised or denied.\nThe logs are written to another S3 bucket; the target logging bucket must be in the same region. Never set the logging bucket to be the monitoring bucket! Otherwise you will create an infinite loop and rack up AWS costs."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#pre-signed-url",
    "href": "posts/software/aws/aws_saa_notes.html#pre-signed-url",
    "title": "AWS Solutions Architect",
    "section": "11.5. Pre-signed URL",
    "text": "11.5. Pre-signed URL\nUsers given a pre-signed URL inherit the permissions of the user that generated it for GET/PUT requests.\nThey can be generated in the AWS console, CLU or SDK. The expiration time can be 1 minute to 168 hours.\nWithin S3 console -&gt; Find the object -&gt; Object actions -&gt; Share with pre-signed URL"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#s3-glacier-vault-lock",
    "href": "posts/software/aws/aws_saa_notes.html#s3-glacier-vault-lock",
    "title": "AWS Solutions Architect",
    "section": "11.6. S3 Glacier Vault Lock",
    "text": "11.6. S3 Glacier Vault Lock\nThis allows us to adopt a Write Once Read Many (WORM) model, which is helpful for compliant and data retention.\nWe create a Vault Lock Policy, which means the object cannot be deleted by anyone, and the policy itself cannot be edited."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#s3-object-lock",
    "href": "posts/software/aws/aws_saa_notes.html#s3-object-lock",
    "title": "AWS Solutions Architect",
    "section": "11.7. S3 Object Lock",
    "text": "11.7. S3 Object Lock\nS3 Object Lock is a similar idea but less restrictive. Again you can adopt a WORM model, but it is for specific objects for a specified amount of time. So you can block object deletion. The retention period must be set when creating the lock.\nThere are two retention modes:\n\nCompliance. Object versions cannot be overwritten or deleted by anyone user, even the root user. The retention modes cannot be edited once set. This is similar to vault lock and is the strictest setting.\nGovernance. Most users cannot overwrite or delete objects or change the lock settings, but some users have special permissions to override this.\n\nA third option is legal hold which protects an object indefinitely, independent of the retention period. Only users with the s3:PutObjectLegalHold IAM permission can add or remove legal holds on objects."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#s3-access-points",
    "href": "posts/software/aws/aws_saa_notes.html#s3-access-points",
    "title": "AWS Solutions Architect",
    "section": "11.8. S3 Access Points",
    "text": "11.8. S3 Access Points\nThis is a more granular control over permissions at the prefix level rather than the bucket policy.\nSay we have a bucket which has folders for multiple departments: finance, sales, analytics. We want to make sure each department can only access their folder. We could define a complicated bucket policy to enforce this.\nA convenient alternative is to define a Finance Access Point for that folder, a Sales Access Point for that folder, etc.\nEach access policy looks similar to a bucket policy — it is a JSON document with the same keys — but it applies to a prefix. Each access point has its own DNS name and access point policy.\nWe can restrict the access point to only be accessible from within the VPC. You must create a VPC Endpoint to connect to the Access Point, and define a VPC Endpoint Policy that allows access to the Access Point and target bucket."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#s3-object-lambda",
    "href": "posts/software/aws/aws_saa_notes.html#s3-object-lambda",
    "title": "AWS Solutions Architect",
    "section": "11.9. S3 Object Lambda",
    "text": "11.9. S3 Object Lambda\nThis allows us to transform an object as it is loaded.\nSay you want multiple versions of a file: the original, a redacted version for customers, an enriched version for the sales department.\nWe could store 3 different variations of each file. But this is an inefficient use of storage.\nWe can create a lambda function for each transformation and apply it at read time before it reaches the user. Only one S3 bucket is needed, on which we create an S3 Access Point and multiple S3 Object Lambda Access Points."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#cloudfront-overview",
    "href": "posts/software/aws/aws_saa_notes.html#cloudfront-overview",
    "title": "AWS Solutions Architect",
    "section": "12.1 CloudFront Overview",
    "text": "12.1 CloudFront Overview\nCloudFront is a Content Delivery Network (CDN). It improves read performance by caching content at the edges, at 216 points of presence globally. I this protects against DDoS attacks."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#origin",
    "href": "posts/software/aws/aws_saa_notes.html#origin",
    "title": "AWS Solutions Architect",
    "section": "12.2. Origin",
    "text": "12.2. Origin\nCloudFront can have different origins:\n\nS3 bucket. For distributing files and caching them at the edge. Secured using Origin Access Control (OAC).\nVPC Origin. For applications hosted in VPC private subnets - ALB, EC2 instances. CloudFront creates a VPC Origin inside the VPC and communicates with that. The old deprecated method was to create a public EC2 instance or ALB and attach a security group that only allowed access from the public IP addresses of the edge location; this is more error prone and less secure.\nCustom origin. Any public HTTP backend.\n\nThe client requests data from the edge location. If the data is cached, return it. Otherwise, fetch it from the origin and cache it at that edge location ready for any future requests.\nCloudFront differs from S3 Cross-Region Replication, but naively seems similar in principle. CloudFront caches across the global edge network (there is no region selection) and only caches files for the TTL (short lived). Good for static content that must be highly available.\nS3 Cross-Region Replication must be set up for each region you want replication to happen in, and files are updated in near real-time. Good for dynamic content that needs to be available in a small number of regions at low latency."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#cloudfront-geo-restriction",
    "href": "posts/software/aws/aws_saa_notes.html#cloudfront-geo-restriction",
    "title": "AWS Solutions Architect",
    "section": "12.3. CloudFront Geo Restriction",
    "text": "12.3. CloudFront Geo Restriction\nYou can restrict who can access your distribution based on location. You define an Allowlist or Blocklist to allow/block access to content is the user is in a particular country. The country is determined using a 3rd party Geo-IP database.\nA use case is to enforce copyright laws based on country."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#price-classes",
    "href": "posts/software/aws/aws_saa_notes.html#price-classes",
    "title": "AWS Solutions Architect",
    "section": "12.4. Price Classes",
    "text": "12.4. Price Classes\nThe cost of data out per edge location varies.\nYou can reduce the number of edge locations for cost reduction. There are three price classes:\n\nPrice Class All. All regions, best performance but most expensive.\nPrice Class 200. Most regions but excluding the most expensive.\nPrice Class 100. Only the cheapest regions."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#cache-invalidation",
    "href": "posts/software/aws/aws_saa_notes.html#cache-invalidation",
    "title": "AWS Solutions Architect",
    "section": "12.5. Cache Invalidation",
    "text": "12.5. Cache Invalidation\nIf you update the content on the backend origin, the CloudFront edge location will only get the refreshed content after the TTL expires.\nYou can force an entire or partial cache refresh by performing a CloudFront Invalidation. You pass in a file path. This can be all files * or a specific folder /images/*.\nIt essentially tells the edge location that the content isn’t there, so the next user that requests the data will get a cache miss and go to the origin. Note that it doesn’t refresh the data per se, it invalidates the data and it’s only when the next user makes a request that the edge location retrieves the updated content."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#aws-global-accelerator",
    "href": "posts/software/aws/aws_saa_notes.html#aws-global-accelerator",
    "title": "AWS Solutions Architect",
    "section": "12.6 AWS Global Accelerator",
    "text": "12.6 AWS Global Accelerator\nThe problem AWS Global Accelerator is solving is you have an application deployed in one region, say an ALB, but your traffic is global. So some users in faraway locations have many hops before reaching our content. We want to go through the AWS network as soon as possible to minimise latency.\n\n12.6.1. Unicast IP vs Anycast IP\nUnicast is what we’re typically familiar with. Each server holds one IP address.\nWith Anycast IP, all servers hold the same IP address, and the client is routed to the nearest one. This is how AWS Global Accelerator works.\n\n\n12.6.2. How Global Accelerator Works\nThere is a server at each edge location. Anycast IP sends traffic directly to the nearest edge location. The traffic is then routed along AWS’s private network which is faster than over public internet.\nTwo Anycast IP addresses are created for you application. It works with Elastic IP, EC2 instances, ALB, NLB, and can be public or private. It is helpful for security too because there are only two IP addresses that clients need to whitelist, but with the benefit of being globally available.\n\n\n12.6.3. Global Accelerator vs CloudFront\nCloudFront is caching data. A server at the edge location stores a cached version of your data and serves this to clients.\nGlobal Accelerator is simply routing traffic through AWS’s private network, there is no caching. The clients request is routed to the nearest edge location then directly to the origin through the AWS network."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#aws-snowball",
    "href": "posts/software/aws/aws_saa_notes.html#aws-snowball",
    "title": "AWS Solutions Architect",
    "section": "13.1. AWS Snowball",
    "text": "13.1. AWS Snowball\nSnowball is a device that allows you to collect and process data at the edge, and migrate data in and out of AWS. You receive physical Snowball device that you upload data on to locally and then ship it to AWS.\nWhen performing data migrations of large amounts of data or over a slow connection (over a week of transfer time), AWS Snowball is recommended.\nAnother use case is edge computing, where we want to process data at an edge location (e.g. a truck or a ship) that has limited internet or compute power.\nIn the UI, you specify the your shipping address and the S3 bucket you want the data uploaded to once AWS receive it, and they will post you a Snowball device.\nA common user story is you want to upload data from the Snowball device directly in to Glacier. This is not possible, so the workaround is to upload into S3 and have a lifecycle policy on the bucket which transitions the data into Glacier."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#amazon-fsx",
    "href": "posts/software/aws/aws_saa_notes.html#amazon-fsx",
    "title": "AWS Solutions Architect",
    "section": "13.2 Amazon FSx",
    "text": "13.2 Amazon FSx\nFSx allows you to launch third-party file systems on AWS as a fully managed service.\nAnalogous to how RDS allows you to run third party databases like MySQL, PostgreSQL etc as a managed service. FSx is the equivalent for file systems.\n\nFSx for Windows File Server is a fully managed shared drive. It supports Active Directory integration. Despite it being windows, it can also be mounted on Linux EC2 instances. There are SSD or HDD storage options. Data is backed up to S3 daily.\nFSx for Lustre is a parallel distributed file system. The name is derived from “Linux cluster”. It is used for high performance computing (HPC), ML, video processing. Storage options can be SSD or HDD. There is integration with S3 so you can read S3 as if it were a file system and write data back to S3.\nFSx for NetApp ONTAP. Scaled automatically and there is point in time instantaneous cloning which is helpful for testing new workloads.\nFSx for OpenZFS. Managed OpenZFS file system. Good for snapshots, compression and low cost, but no data duplication.\n\nThere are two file system deployment options for FSx:\n\nScratch file system. Temporary storage. Data is not replicated but throughput is high.\nPersistent file system. Long term storage replicated within the same AZ. Failed files are replace within minutes."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#storage-gateway",
    "href": "posts/software/aws/aws_saa_notes.html#storage-gateway",
    "title": "AWS Solutions Architect",
    "section": "13.3. Storage Gateway",
    "text": "13.3. Storage Gateway\nHybrid cloud for storage. Some infrastructure on-premises and some on cloud. This can be due to: long cloud migrations, security requirements, compliance requirements, IT strategy\nAWS Storage Gateway is a way to expose S3 on premises.\nThe cloud native options are:\n\nBlock store. EBS, EC2 instance store\nFile storage. EFS, FSx\nObject storage. S3, Glacier\n\nTypes of Storage Gateway:\n\nS3 file gateway. Allows our on premises to access our S3 bucket. The application server communicates with S3 file gateway via NFS or SMB. S3 file gateway communicates with S3 via HTTPS. The most recently used data is cached in the file gateway. Data can then be transferred to Glacier using a lifecycle policy. IAM roles must be created for each gateway.\nFSx file gateway. Local cache for frequently accessed data.\nVolume gateway. Block storage backed by EBS snapshots. Cached volumes allow low latency access to most recent data. Entire data set is stored on premise and there are scheduled backups to S3.\nTape gateway. For companies with backup processes using physical tapes. With tape gateway, companies can use the same process but in the cloud.\nStorage gateway - hardware appliance. A physical appliance that Amazon post to you which works with file gateway, volume gateway, tape gateway."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#aws-transfer-family",
    "href": "posts/software/aws/aws_saa_notes.html#aws-transfer-family",
    "title": "AWS Solutions Architect",
    "section": "13.4. AWS Transfer Family",
    "text": "13.4. AWS Transfer Family\nA managed service for FTP file transfers in and out of S3. Supports FTP, FTPS, SFTP.\nPay per provisioned endpoint per hour + data transfer in GB."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#aws-datasync",
    "href": "posts/software/aws/aws_saa_notes.html#aws-datasync",
    "title": "AWS Solutions Architect",
    "section": "13.5. AWS DataSync",
    "text": "13.5. AWS DataSync\nMove large amounts of data to and from:\n\nOn-premises -&gt; AWS\nAWS -&gt; AWS\n\nCan sync to: S3, EFS, FSx. It can work in either direction.\nReplication tasks can be scheduled hourly, daily, weekly; it is not continuous/ instantaneous. File permissions and metadata are preserved - DataSync is the only option that does this.\nAWS Snowcone comes with a DataSync agent pre-installed. It can be used on premises to sync to the cloud."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#comparison-of-storage-options",
    "href": "posts/software/aws/aws_saa_notes.html#comparison-of-storage-options",
    "title": "AWS Solutions Architect",
    "section": "13.6. Comparison of Storage Options",
    "text": "13.6. Comparison of Storage Options\n\nS3: Object Storage\nS3 Glacier: Object Archival\nEBS volumes: Network storage for one EC2 instance at a time\nInstance Storage: Physical storage for your EC2 instance (high IOPS)\nEFS: Network File System for Linux instances, POSIX filesystem\nFSx for Windows: Network File System for Windows servers\nFSx for Lustre: High Performance Computing Linux file system\nFSx for NetApp ONTAP: High OS Compatibility\nFSx for OpenZFS: Managed ZFS file system\nStorage Gateway: S3 & FSx File Gateway, Volume Gateway (cache & stored), Tape Gateway\nTransfer Family: FTP, FTPS, SFTP interface on top of Amazon S3 or Amazon EFS\nDataSync: Schedule data sync from on-premises to AWS, or AWS to AWS\nSnowcone / Snowball / Snowmobile: to move large amount of data to the cloud, physically\nDatabase: for specific workloads, usually with indexing and querying"
  },
  {
    "objectID": "posts/software/data_structures_algos/algorithms/algorithms.html",
    "href": "posts/software/data_structures_algos/algorithms/algorithms.html",
    "title": "Algorithms",
    "section": "",
    "text": "The mathematical explanation is given in terms of the upper bound of the growth rate of a function. Very briefly: if a function \\(g(x)\\) grows no faster than \\(f(x)\\) then \\(g\\) is a member of \\(O(f)\\). This isn’t particularly helpful for intuitive understanding though.\nThe fundamental question is:\n\nIf there are \\(N\\) data elements, how many steps will the algorithm take?\n\nThis helps us understand how the performance of the algorithm changes as the data increases.\nBasically, count the number of steps the algorithm takes as a function of N (and M if it involves two arrays), then drop any constants and only keep the “worst” term since we consider the worst case scenario.\nBe aware, though, that this doesn’t necessarily mean a lower complexity algorithm will always be faster in practice for every use case. Take for exmaple algorithm A, which always takes 100 steps regardless of input size so is \\(O(1)\\), and algorithm B which scales linearly with the input so in \\(O(N)\\).\nIf we apply these to an array with 10 elements, A will take 100 steps and B will take 10 steps. So the “worse” algorithm can perform better for small data.\nWhen an algorithm is \\(O(log(N))\\), the log is implicitly \\(log_2\\). These come up regularly in algorithms where we divide and conquer, as in the binary search, because we half the data with each iteration. If you see an algorithm has a log term in its complexity, that’s generally a clue that there is some binary split happening somewhere.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nDefinition: Recursion\nSee “Recursion”.\n\n\nRecursion is when a function calls itself.\nIn any case where you can use a loop, you could also write it recursively. I’m not saying you should, but you can.\nEvery recursive function should have a base case (or multiple base cases) where it does not recurse, to prevent it from entering an infinite loop.\n\n\nThere is a knack to reading recursive functions. Start from the base case and work backwards.\n\nIdentify the base case and walk through it\nIdentify the “second-to-last” case and walk through it\nRepeat for the next-to-last case before that and walk through it\n\n\ndef factorial(number):\n    \"\"\"Recursively calculate the factorial of a number.\"\"\"\n    # The base case\n    if number == 1:\n        return 1\n    # The recursive bit\n    return number * factorial(number - 1)\n\n\nfactorial(6)\n\n720\n\n\n\n\n\nThe computer uses a call stack to keep track of the functions to call. When we enter a new recursion, we push a function call on to the stack, and when we finish executing we pop it from the call stack.\nIf we don’t write appropriate base cases, the recursive function can loop infinitely, leading to stack overflow.\n\n\n\nUse recursion when the depth of the problem is unknown or arbitrary.\nIf we have a problem where we want to go through nested structures but we don’t know ahead of time how deep they go, we can’t solve this using regular loops but we can with recursion.\nFor example, if we want to traverse a directory and each of its subdirectories, and each of their subdirectories, etc.\n\n\n\nRecursive algorithms are useful for categories of problems where:\n\nThe goal is to repeatedly execute a task\nThe problem can be broken into subproblems, which are versions of the same problem but with a smaller input.\n\n\n\nIf we are modifying the data structure, say an array, in place, we can pass it as an extra parameter to the recursive function so that it can be passed up and down the call stack.\nThis is often useful for the repeatedly execute category of problems.\n\n\n\nRecursion is useful when coupled with a new, slightly unintuitive, mental model of top-down problem solving.\n\nImagine the function you’re writing has already been implemented\nIdentify the subproblem, often the next step along\nSee what happens when you call the function on the subproblem\nGo from there until you reach a base case\n\nThis is often useful for the subproblem category of problems.\n\n\n\n\n\nThe idea behind dynamic programming is similar to recursion: break the problem down into smaller subproblems. For dynamic programming problems, the subproblems are typically overlapping subproblems. We also want to remove any duplicate calls.\n\n\nA common problem with recursive approaches is that we end up calculating the same function multiple times in the call stack. This can lead to some horrendous complexities like \\(O(N!)\\) or \\(O(2^N)\\).\nThe key difference to recursion is we only solve each subproblem once and store its result. This way, we can just look it up for any other calls that require it. This storing of intermediate calculations is called memoization.\nWe will need to pass this memoised object as an extra parameter and modify it in place, as noted in the recusion section.\n\n\n\nDitch recursion and just use a loop.\nThis is technically another way of “solving a problem that can be solved using recursion but without making duplciate calls” - which is what dynamic programming essentially is. In this case, we do it by removing recursion altogether.\n\n\n\n\nOften the primary focus of an algorithm is its speed, characterised by its time complexity - how many steps does the algorithm take for an input of N elements?\nBut another useful dimension to analyse is its space complexity - how much memory does the algorithm consume for an input of N elements?\nIt is important to note that space complexity generally only considers the new data that the algorithm is generating, not the original input. The extra space consumed by the new data is called auxiliary space.\n(However, some textbooks do include the original data in the space complexity, so it’s important to check the convention being used.)\n\n\nA recursive function takes up a unit of memory for each call that it makes.\nEach time it is called, it adds an item (the function call itself and any additional parameters) on the call stack. So to understand its space complexity, we need to determine how big the call stack can get at its peak, i.e. how many recursive calls it makes.\n\n\n\n\nThe first step towards optimising your code is understanding its current complexity.\n\n\nWhat is the best complexity you can imagine?\nAlso called the best-conceivable runtime.\nFor example, if a problem requires processing every unit of a list, then you will have to visit all \\(N\\) elements, so the best you can do is probably \\(O(N)\\). You won’t necessarily be able to achieve this, but it gives you an indication of the potential.\n\nDetermine your current algorithm’s Big O\nDetermine the best-imaginable Big O\nIf they’re different, try optimising\n\nAnother mental trick that can be helpful is to pick a really fast Big and ask yourself “If someone told me they had an algorithm to achieve this Big O, would I believe them?”\n\n\n\nAsk yourself: “If I could magically look up a desired piece of information in \\(O(1)\\) time, could I make my algorithm faster?”\nIf so, you may be able to bring in an additional data structure to accommodate this look up. Often this is a hash table.\n\n\n\nStart with several smaller cases of the problem and work through the answers by hand.\nDoes a pattern emerge that might generalise to bigger cases?\n\n\n\nA greedy algorithm, at each step, chooses what appears to be the current best option.\nThis local optimisation doesn’t necessarily find the global optimal solution. But it can be sueful in situations where finding the absolute best option is not necessary or practical.\n\n\n\nIf the input data were stored as a different data structure, would it make the problem easier?\nIn some cases, it can be worthwhile performing a pre-compute step to convert the data structure if it then allows faster algorithms to be run.\n\n\n\n\n\nA common sense guide to data structures and algorithms, Jay Wengrow."
  },
  {
    "objectID": "posts/software/data_structures_algos/algorithms/algorithms.html#big-o",
    "href": "posts/software/data_structures_algos/algorithms/algorithms.html#big-o",
    "title": "Algorithms",
    "section": "",
    "text": "The mathematical explanation is given in terms of the upper bound of the growth rate of a function. Very briefly: if a function \\(g(x)\\) grows no faster than \\(f(x)\\) then \\(g\\) is a member of \\(O(f)\\). This isn’t particularly helpful for intuitive understanding though.\nThe fundamental question is:\n\nIf there are \\(N\\) data elements, how many steps will the algorithm take?\n\nThis helps us understand how the performance of the algorithm changes as the data increases.\nBasically, count the number of steps the algorithm takes as a function of N (and M if it involves two arrays), then drop any constants and only keep the “worst” term since we consider the worst case scenario.\nBe aware, though, that this doesn’t necessarily mean a lower complexity algorithm will always be faster in practice for every use case. Take for exmaple algorithm A, which always takes 100 steps regardless of input size so is \\(O(1)\\), and algorithm B which scales linearly with the input so in \\(O(N)\\).\nIf we apply these to an array with 10 elements, A will take 100 steps and B will take 10 steps. So the “worse” algorithm can perform better for small data.\nWhen an algorithm is \\(O(log(N))\\), the log is implicitly \\(log_2\\). These come up regularly in algorithms where we divide and conquer, as in the binary search, because we half the data with each iteration. If you see an algorithm has a log term in its complexity, that’s generally a clue that there is some binary split happening somewhere."
  },
  {
    "objectID": "posts/software/data_structures_algos/algorithms/algorithms.html#recursion",
    "href": "posts/software/data_structures_algos/algorithms/algorithms.html#recursion",
    "title": "Algorithms",
    "section": "",
    "text": "Tip\n\n\n\nDefinition: Recursion\nSee “Recursion”.\n\n\nRecursion is when a function calls itself.\nIn any case where you can use a loop, you could also write it recursively. I’m not saying you should, but you can.\nEvery recursive function should have a base case (or multiple base cases) where it does not recurse, to prevent it from entering an infinite loop.\n\n\nThere is a knack to reading recursive functions. Start from the base case and work backwards.\n\nIdentify the base case and walk through it\nIdentify the “second-to-last” case and walk through it\nRepeat for the next-to-last case before that and walk through it\n\n\ndef factorial(number):\n    \"\"\"Recursively calculate the factorial of a number.\"\"\"\n    # The base case\n    if number == 1:\n        return 1\n    # The recursive bit\n    return number * factorial(number - 1)\n\n\nfactorial(6)\n\n720\n\n\n\n\n\nThe computer uses a call stack to keep track of the functions to call. When we enter a new recursion, we push a function call on to the stack, and when we finish executing we pop it from the call stack.\nIf we don’t write appropriate base cases, the recursive function can loop infinitely, leading to stack overflow.\n\n\n\nUse recursion when the depth of the problem is unknown or arbitrary.\nIf we have a problem where we want to go through nested structures but we don’t know ahead of time how deep they go, we can’t solve this using regular loops but we can with recursion.\nFor example, if we want to traverse a directory and each of its subdirectories, and each of their subdirectories, etc.\n\n\n\nRecursive algorithms are useful for categories of problems where:\n\nThe goal is to repeatedly execute a task\nThe problem can be broken into subproblems, which are versions of the same problem but with a smaller input.\n\n\n\nIf we are modifying the data structure, say an array, in place, we can pass it as an extra parameter to the recursive function so that it can be passed up and down the call stack.\nThis is often useful for the repeatedly execute category of problems.\n\n\n\nRecursion is useful when coupled with a new, slightly unintuitive, mental model of top-down problem solving.\n\nImagine the function you’re writing has already been implemented\nIdentify the subproblem, often the next step along\nSee what happens when you call the function on the subproblem\nGo from there until you reach a base case\n\nThis is often useful for the subproblem category of problems."
  },
  {
    "objectID": "posts/software/data_structures_algos/algorithms/algorithms.html#dynamic-programming",
    "href": "posts/software/data_structures_algos/algorithms/algorithms.html#dynamic-programming",
    "title": "Algorithms",
    "section": "",
    "text": "The idea behind dynamic programming is similar to recursion: break the problem down into smaller subproblems. For dynamic programming problems, the subproblems are typically overlapping subproblems. We also want to remove any duplicate calls.\n\n\nA common problem with recursive approaches is that we end up calculating the same function multiple times in the call stack. This can lead to some horrendous complexities like \\(O(N!)\\) or \\(O(2^N)\\).\nThe key difference to recursion is we only solve each subproblem once and store its result. This way, we can just look it up for any other calls that require it. This storing of intermediate calculations is called memoization.\nWe will need to pass this memoised object as an extra parameter and modify it in place, as noted in the recusion section.\n\n\n\nDitch recursion and just use a loop.\nThis is technically another way of “solving a problem that can be solved using recursion but without making duplciate calls” - which is what dynamic programming essentially is. In this case, we do it by removing recursion altogether."
  },
  {
    "objectID": "posts/software/data_structures_algos/algorithms/algorithms.html#space-complexity",
    "href": "posts/software/data_structures_algos/algorithms/algorithms.html#space-complexity",
    "title": "Algorithms",
    "section": "",
    "text": "Often the primary focus of an algorithm is its speed, characterised by its time complexity - how many steps does the algorithm take for an input of N elements?\nBut another useful dimension to analyse is its space complexity - how much memory does the algorithm consume for an input of N elements?\nIt is important to note that space complexity generally only considers the new data that the algorithm is generating, not the original input. The extra space consumed by the new data is called auxiliary space.\n(However, some textbooks do include the original data in the space complexity, so it’s important to check the convention being used.)\n\n\nA recursive function takes up a unit of memory for each call that it makes.\nEach time it is called, it adds an item (the function call itself and any additional parameters) on the call stack. So to understand its space complexity, we need to determine how big the call stack can get at its peak, i.e. how many recursive calls it makes."
  },
  {
    "objectID": "posts/software/data_structures_algos/algorithms/algorithms.html#code-optimisation-techniques",
    "href": "posts/software/data_structures_algos/algorithms/algorithms.html#code-optimisation-techniques",
    "title": "Algorithms",
    "section": "",
    "text": "The first step towards optimising your code is understanding its current complexity.\n\n\nWhat is the best complexity you can imagine?\nAlso called the best-conceivable runtime.\nFor example, if a problem requires processing every unit of a list, then you will have to visit all \\(N\\) elements, so the best you can do is probably \\(O(N)\\). You won’t necessarily be able to achieve this, but it gives you an indication of the potential.\n\nDetermine your current algorithm’s Big O\nDetermine the best-imaginable Big O\nIf they’re different, try optimising\n\nAnother mental trick that can be helpful is to pick a really fast Big and ask yourself “If someone told me they had an algorithm to achieve this Big O, would I believe them?”\n\n\n\nAsk yourself: “If I could magically look up a desired piece of information in \\(O(1)\\) time, could I make my algorithm faster?”\nIf so, you may be able to bring in an additional data structure to accommodate this look up. Often this is a hash table.\n\n\n\nStart with several smaller cases of the problem and work through the answers by hand.\nDoes a pattern emerge that might generalise to bigger cases?\n\n\n\nA greedy algorithm, at each step, chooses what appears to be the current best option.\nThis local optimisation doesn’t necessarily find the global optimal solution. But it can be sueful in situations where finding the absolute best option is not necessary or practical.\n\n\n\nIf the input data were stored as a different data structure, would it make the problem easier?\nIn some cases, it can be worthwhile performing a pre-compute step to convert the data structure if it then allows faster algorithms to be run."
  },
  {
    "objectID": "posts/software/data_structures_algos/algorithms/algorithms.html#references",
    "href": "posts/software/data_structures_algos/algorithms/algorithms.html#references",
    "title": "Algorithms",
    "section": "",
    "text": "A common sense guide to data structures and algorithms, Jay Wengrow."
  },
  {
    "objectID": "posts/software/kubernetes/kubernetes_notes.html",
    "href": "posts/software/kubernetes/kubernetes_notes.html",
    "title": "Kubernetes",
    "section": "",
    "text": "Kubernetes is an open-source system (not single library) for automating the deployment, scaling and management of containerised applications, independent of the specific cloud provider\nManual deployment of containers is hard to maintain and error-prone.\n\nSecurity and configuration concerns.\nContainers crash and need to be replaced. We don’t want to have to manually monitor this and intervene.\nNeed to scale containers for traffic spikes.\nincoming traffic should be evenly distributed.\n\nServices like ECS can help with some of these, such as replacing crashed containers and autoscaling. With a load balancer, it can distribute incoming traffic. But that locks us in to that cloud provider.\n\n\n\n“K8s” is the de facto standard for container orchestration. It helps with tasks like automatic deployment, scaling and load balancing, and management.\nWe define the Kubernetes configuration which determines the desired architecture, number of containers etc. We can then pass this standardised, generic config to any cloud provider. We can also include config specific to a cloud provider without having to rewrite the entire config.\nWhat isn’t Kubernetes?\n\nIt isn’t an alternative to a cloud provider.\nIt isn’t a specific service from a particular cloud provider.\nIt isn’t a single piece of software, it’s a collection of tools and concepts.\n\nKubernetes is like Docker Compose but for multiple machines.\n\n\n\n\n\n\nKubernetes Cluster Architecture\n\n\nA pod holds one or more containers. It is the smallest unit in Kubernetes.\nA pod runs on a worker node. A node is essentially a machine or virtual instance. Multiple pods can run on the same. Ode.\nThere is also a proxy on each worker node, which configures the network traffic.\nMultiple pods can be created and removed to scale your app.\nThere is a a master node. This contains a Control Plane which handles the creation, removal and replacement of worker nodes. As a user, you don’t interact with the worker nodes directly. You define the desired end state and interact with the master node, which then handles the implementation.\nThe group of master nodes and all worker nodes is called a cluster. This then sends instructions to the cloud provider API to create all of the services required to realise the desired implementation.\nKubernetes does not manage your infrastructure for you. You still need to create the instances for the master node and worker nodes, and any other resources like load balancers and file systems. You also need to install kubelet and other Kubernetes services on those nodes yourself.\nMany cloud providers have managed services to make this set up easier. But this is a service provided by the specific cloud providers, not natively by Kubernetes.\n\n\n\nThink of a worker node as one machine or virtual instance. What happens on the worker nodes (e.g. creating a pod) is managed by the master node.\nThe worker node contains one or more pods. Each pod hosts one or more application containers and their associated resources (such as volumes). Pods are created and managed by K8s via the master node.\nThe worker node can contain multiple pods.\n\nDocker, kubelet and kube-proxyneed to be installed on the worker node.\nKubelet handles the communication between the worker node and master node.\nKube-proxy handles the network communication.\n\n\n\n\nThe master node needs to run the API server. This is the counterpart to kubelet running on the worker nodes, and allows the master node to communicate with the workers.\nThere is a scheduler which watches the pods and selects worker nodes to run new pods on.\nKube-Controller-Manager watches the worker nodes and ensures the correct number of pods are running. There is also a Cloud-Controller-Manager which is the same thing but for a specific cloud provider.\nServices are a group of pods with a unique IP address that can interact with the outside world."
  },
  {
    "objectID": "posts/software/kubernetes/kubernetes_notes.html#why-do-we-need-kubernetes",
    "href": "posts/software/kubernetes/kubernetes_notes.html#why-do-we-need-kubernetes",
    "title": "Kubernetes",
    "section": "",
    "text": "Kubernetes is an open-source system (not single library) for automating the deployment, scaling and management of containerised applications, independent of the specific cloud provider\nManual deployment of containers is hard to maintain and error-prone.\n\nSecurity and configuration concerns.\nContainers crash and need to be replaced. We don’t want to have to manually monitor this and intervene.\nNeed to scale containers for traffic spikes.\nincoming traffic should be evenly distributed.\n\nServices like ECS can help with some of these, such as replacing crashed containers and autoscaling. With a load balancer, it can distribute incoming traffic. But that locks us in to that cloud provider."
  },
  {
    "objectID": "posts/software/kubernetes/kubernetes_notes.html#what-is-kubernetes",
    "href": "posts/software/kubernetes/kubernetes_notes.html#what-is-kubernetes",
    "title": "Kubernetes",
    "section": "",
    "text": "“K8s” is the de facto standard for container orchestration. It helps with tasks like automatic deployment, scaling and load balancing, and management.\nWe define the Kubernetes configuration which determines the desired architecture, number of containers etc. We can then pass this standardised, generic config to any cloud provider. We can also include config specific to a cloud provider without having to rewrite the entire config.\nWhat isn’t Kubernetes?\n\nIt isn’t an alternative to a cloud provider.\nIt isn’t a specific service from a particular cloud provider.\nIt isn’t a single piece of software, it’s a collection of tools and concepts.\n\nKubernetes is like Docker Compose but for multiple machines."
  },
  {
    "objectID": "posts/software/kubernetes/kubernetes_notes.html#cluster-architecture-and-concepts",
    "href": "posts/software/kubernetes/kubernetes_notes.html#cluster-architecture-and-concepts",
    "title": "Kubernetes",
    "section": "",
    "text": "Kubernetes Cluster Architecture\n\n\nA pod holds one or more containers. It is the smallest unit in Kubernetes.\nA pod runs on a worker node. A node is essentially a machine or virtual instance. Multiple pods can run on the same. Ode.\nThere is also a proxy on each worker node, which configures the network traffic.\nMultiple pods can be created and removed to scale your app.\nThere is a a master node. This contains a Control Plane which handles the creation, removal and replacement of worker nodes. As a user, you don’t interact with the worker nodes directly. You define the desired end state and interact with the master node, which then handles the implementation.\nThe group of master nodes and all worker nodes is called a cluster. This then sends instructions to the cloud provider API to create all of the services required to realise the desired implementation.\nKubernetes does not manage your infrastructure for you. You still need to create the instances for the master node and worker nodes, and any other resources like load balancers and file systems. You also need to install kubelet and other Kubernetes services on those nodes yourself.\nMany cloud providers have managed services to make this set up easier. But this is a service provided by the specific cloud providers, not natively by Kubernetes."
  },
  {
    "objectID": "posts/software/kubernetes/kubernetes_notes.html#worker-nodes",
    "href": "posts/software/kubernetes/kubernetes_notes.html#worker-nodes",
    "title": "Kubernetes",
    "section": "",
    "text": "Think of a worker node as one machine or virtual instance. What happens on the worker nodes (e.g. creating a pod) is managed by the master node.\nThe worker node contains one or more pods. Each pod hosts one or more application containers and their associated resources (such as volumes). Pods are created and managed by K8s via the master node.\nThe worker node can contain multiple pods.\n\nDocker, kubelet and kube-proxyneed to be installed on the worker node.\nKubelet handles the communication between the worker node and master node.\nKube-proxy handles the network communication."
  },
  {
    "objectID": "posts/software/kubernetes/kubernetes_notes.html#master-node",
    "href": "posts/software/kubernetes/kubernetes_notes.html#master-node",
    "title": "Kubernetes",
    "section": "",
    "text": "The master node needs to run the API server. This is the counterpart to kubelet running on the worker nodes, and allows the master node to communicate with the workers.\nThere is a scheduler which watches the pods and selects worker nodes to run new pods on.\nKube-Controller-Manager watches the worker nodes and ensures the correct number of pods are running. There is also a Cloud-Controller-Manager which is the same thing but for a specific cloud provider.\nServices are a group of pods with a unique IP address that can interact with the outside world."
  },
  {
    "objectID": "posts/software/kubernetes/kubernetes_notes.html#interacting-with-a-deployment-object",
    "href": "posts/software/kubernetes/kubernetes_notes.html#interacting-with-a-deployment-object",
    "title": "Kubernetes",
    "section": "2.1. Interacting with a Deployment Object",
    "text": "2.1. Interacting with a Deployment Object\nMinikube is a tool to make it easy to run Kubernetes locally, by running a cluster in a VM on your local machine.\nYou will need the kubectl tool installed. This gives commands to the master node, which in turn controls the worker nodes. Think of it like the president commanding the general to command his soldiers to attack.\nKubernetes works with Objects: pods, deployments, services, volumes, etc. Objects can be created imperatively or declaratively.\nA pod has a cluster-internal IP address by default. Containers within a pod can communicate with each other via localhost.\nPods are ephemeral. When they are started, stopped or replaced, any local data is lost. Just like containers, which makes sense because they are just a thin wrapper around a container.\nThe Deployment object is what we interact with as users, not pods directly. You generally create a Deployment object which defines the desired target state. Then K8s will create the required pods and other objects. Deployments can be paused, deleted and rolled back. They can also be scaled (and auto-scaled).\nOn local machine, we send commands to the cluster. We create a deployment object with kubectl create. The image we specify needs to exist on the cluster, not the local machine that we are sending commands from. The image should be in a registry like DockerHub.\nkubectl create deployment my-first-kube - -image=repo/my-docker-image\nWe can see the status of the deployment with:\nkubectl get deployments\nWe can see the status of pods with:\nkubectl get pods\nIf we’re running locally, we can see our deployment with:\nminikube dashboard\nWe can expose pods to the cluster or externally using a Service object. Pods have an internal IP address, but Services have a static IP address we can use to externally access it from anywhere.\nWe create a service by exposing a deployment:\nkubectl expose deployment my-first-kube --type=LoadBalancer --port=8080\nWe can see the service with:\nkubectl get services\nExpose an external IP address when running locally with:\nminikube service my-first-kube\nTo manually scale a deployment object, we can set the number of replicas we want. We can scale up and down by setting the number of replicas higher or lower than it currently is.\nkubectl scale deployment/my-first-kube --replicas=3"
  },
  {
    "objectID": "posts/software/kubernetes/kubernetes_notes.html#updating-deployments",
    "href": "posts/software/kubernetes/kubernetes_notes.html#updating-deployments",
    "title": "Kubernetes",
    "section": "2.2. Updating Deployments",
    "text": "2.2. Updating Deployments\nWe can update the image in a deployment object by setting the new image. The new image should have a more recent tag, as this will trigger Kubernetes to download the newer image.\nkubectl set image deployments/my-first-kube old-image-name=repo/new-image-name\nWe can check the status of the updated deployment with:\nkubectl rollout status deployment/my-first-kube\nTo undo the latest deployment:\nkubectl rollout undo deployment/my-first-kube\nWe can see the rollout history with:\nkubectl rollout history deployment/my-first-kube\nThis will give a list of revisions. We can see details for a particular revision, say the 2nd revision, with:\nkubectl rollout history deployment/my-first-kube --revision=2\nIf we want to revert to a specific revision:\nkubectl rollout undo deployment/my-first-kube - -to-revision=2"
  },
  {
    "objectID": "posts/software/kubernetes/kubernetes_notes.html#imperative-vs-declarative-approach",
    "href": "posts/software/kubernetes/kubernetes_notes.html#imperative-vs-declarative-approach",
    "title": "Kubernetes",
    "section": "2.3. Imperative vs Declarative Approach",
    "text": "2.3. Imperative vs Declarative Approach\nThe imperative approach so far requires entering commands into the command line in the correct order. The declarative approach allows us to define the configuration which can then be reused.\nIt is analogous to how we could create docker containers in the command line (imperative), or define a docker-compose.yml file with all of the configuration and simply run this (declarative).\nOnce we have the configuration file defined, we just need to run:\nkubectl apply -f config.yaml\nThis applies the config file to the connected cluster. If we change the config file, we can run the command again and Kubernetes will make the appropriate changes to get to the target state."
  },
  {
    "objectID": "posts/software/kubernetes/kubernetes_notes.html#kubernetes-config-files",
    "href": "posts/software/kubernetes/kubernetes_notes.html#kubernetes-config-files",
    "title": "Kubernetes",
    "section": "2.4. Kubernetes Config Files",
    "text": "2.4. Kubernetes Config Files\nThese are the config files used in the declarative approach.\n\n2.4.1. Defining a Deployment Object\napiVersion: apps/v1  # See K8sdocs for the latest version\nkind: Deployment\nmetadata:\n  name: my-second-app\nspec:\n  replicas: 3\n  # A selector is needed to specify which pods should be controlled by this deployment\n  selector:\n    matchLabels: \n      app: second-app\n      tier: backend\n    # As an alternative to matching labels, we can also match on an expression. \n    # Key is still referring to the key of a label\n    # Operator can be In, NotIn, Exist, DoesNotExist\n    matchExpressions:\n      - {key: app, operator: In, values: [second-app, first-app]}\n    \n  template:  \n    # Define the pod object. \n    # The template of a Deployment always defines a Pod, so you don’t need to (and can’t) specify kind: Pod\n    metadata:\n      labels:\n        # Key:Value can be whatever you like. It is used for the Deployment selector to identify the pod\n        app: second-app\n        tier: backend\n    spec:\n      containers:\n      # List of containers\n        - name: container1\n           image: repo/my-image-name:latest\n           # We can define a specific path and frequency for Kubernetes to check if this container is alive \n           livenessProbe: \n             httpGet:\n               path: /some/specific/path\n               port: 8080\n           periodSeconds: 10\n           initialDelaySeconds: 5\n\n\n2.4.2. Defining a Service Object\napiVersion: v1\nkind: Service\nmetadata:\n  name: backend\nspec:\n  selector: \n    app: second-app\n  ports:\n    - protocol : TCP\n       port: 80\n       target port: 8080\n  type: LoadBalancer\nWe apply the config the same way, with kubectl apply.\nWe can delete all of the resources that were created by a particular config file using\nkubectl delete -f=deployment.yaml \nWe can also delete by label. You should pass the kinds of objects (e.g. deployments, services) that you want to delete.\nkubectl delete deployments -l key=value\n\n\n2.4.3. Multiple Configs\nIf you have multiple configurations, such as a Service and a Deployment, you can put them all in one file. Just separate each section with three dashes ---\nThe objects will be created top to bottom. It’s best practice to put the service before the deployment since it references the pods created later. The service will “watch” for any pods created which match its selector.\nSome other parameters of the deployment config that may be helpful:\n\nimagePullPolicy - we can set this to always pull the latest image, even if we don’t explicitly specify a tag.\nlivenessProbe - how Kubernetes should check the health of the container. Is there a specific path to send a request to?"
  },
  {
    "objectID": "posts/software/kubernetes/kubernetes_notes.html#kubernetes-volumes",
    "href": "posts/software/kubernetes/kubernetes_notes.html#kubernetes-volumes",
    "title": "Kubernetes",
    "section": "3.1. Kubernetes Volumes",
    "text": "3.1. Kubernetes Volumes\nKubernetes supports a variety of types/drivers of volumes: local volumes (on nodes) or cloud-provider specific volumes. This is extra functionality added by Kubernetes, not supported by plain Docker.\nThe lifetime of the volume depends on the lifetime of its pod. The volume survives container restarts, but not pod deletion.\nWe define the volumes inside the pod specification in the yaml file. We also need to specify which containers are linked to which pods using the volumeMounts key.\n\n3.1.1. The emptyDir Volume Type\nThe emptyDir volume type creates a new empty directory when the pod starts. It keeps the directory alive as long as the pod is alive. Containers can write to this directory. It will survive container restarts but not pod restarts.\nUnder the spec key:\nspec:\n  containers:\n    - name\n       …\n       volumeMounts:\n         - mountPath: /app/story\n            name: story-volume\n  volumes:\n  - name: my-first-volume \n     # We specify the TYPE of volume \n     emptyDir: {}\n     # We create a host path volume at a specific location on the host machine. Create the directory if it doesn’t already exist\n     hostPath: \n       path: /data\n       type: DirectoryOrCreate\n\n\n3.1.2. The hostPath Volume Type\nThe hostPath volume type creates a folder on the host machine. It is a bit like a bind mount in Docker. We could use it if we wanted to share some existing data.\nThe host path volume is node-specific, so won’t share data across replicas on different machines.\n\n\n3.1.3. The CSI Volume Type\nThe Container Storage Interface (CSI) volume type is a generic interface that different cloud providers are compatible with. Different cloud providers then integrate with this, so we can plug into AWS, GCP, Azure etc, or build our own integration."
  },
  {
    "objectID": "posts/software/kubernetes/kubernetes_notes.html#persistent-volumes",
    "href": "posts/software/kubernetes/kubernetes_notes.html#persistent-volumes",
    "title": "Kubernetes",
    "section": "3.2. Persistent Volumes",
    "text": "3.2. Persistent Volumes\nThe previous volume types do not scale to multiple pods. If we replicate our pods over multiple machines, they won’t share data. “Normal” volumes are node-independent but not pod-independent. The data is lost if the whole node is removed.\nPersistent volumes are pod-independent and node-independent. This is appropriate for long-term data. The volume is detached from the pod, including being detached from the pod lifecycle. We define the persistent volumes once, and can then use it across multiple deployments. The data will survive the pod or node being removed.\nThe storage is not on the nodes, it is in the cloud storage.\n\n\n\nPersistent Volumes and Claims\n\n\n\n3.2.1 Defining a Persistent Volume\nIn a new file, say pv.yaml, we can define the persistent volume.\napiVersion: c1\nkind: PersistentVolume\nmetadata:\n  name: my-pv\nspec:\n  capacity:\n    storage: 4Gi\n  # We can use block storage or file system storage \n  volumeMode: Filesystem\n  # Kubernetes lets you define different storage classes, but by default gives you the standard class\n  storageClassName: standard\n  # How many nodes (one or many) can claim this volume, and can they write or only read?\n  accessModes:\n    - ReadWriteOnce\n    - ReadOnlyMany\n    - ReadWriteMany\n  hostPath:\n    path: /data\n    type: DirectoryOrCreate\n\n\n3.2.2. Creating a Persistent Volume Claim\nWe configure a claim in a new yaml file, then we can assign this claim to a pod so that it claims the persistent volumes.\napiVersion: v1\nkind: PersistentVolume\nmetadata: \n  name: my-pv-claim\nspec:\n  # Static provisioning is when we specify the exact volume to claim by name\n  # We can also do dynamic provisioning where we specify the required properties of the volume and Kubernetes will find an appropriate volume in our cluster\n  volumeName: my-pv\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: standard\n  # How much storage will this claim use?\n  resources:\n    requests:\n      storage: 1Gi\nWe can then assign this claim to a pod under the volumes key of the pod spec:\n  volumes:\n  - name: my-persistent-volume \n     persistentVolumeClaim: \n       claimName: my-pv-claim\nThen we can use kubectl to apply each of the updated yaml files to our cluster.\nWe can see the persistent volumes and claims currently running by using:\nkubectl get pv\nkubectl get pvc"
  },
  {
    "objectID": "posts/software/kubernetes/kubernetes_notes.html#environment-variables",
    "href": "posts/software/kubernetes/kubernetes_notes.html#environment-variables",
    "title": "Kubernetes",
    "section": "3.3. Environment Variables",
    "text": "3.3. Environment Variables\n\n3.3.1. Using Environment Variables\nUnder the container key of a pod specification, you can add an env key with a list of key-value pairs.\ncontainers:\n  env:\n    - name: STORY_FOLDER\n       value: “story”\nYou can then use this env variable in your app.\n\n\n3.3.2. Using an Environment File\nWe can store environment variables in another yaml file of kind ConfigMap. This can then be reused across multiple pods and deployments.\nIn a file environment.yaml:\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: data-store-env\ndata:\n  folder: “story”\nThen we can point to that file in our deployment.yaml file.\ncontainers:\n  env:\n    - name: STORY_FOLDER\n       # We can refer to a ConfigMap\n       valueFrom:\n         configMapKeyRef:\n           name: data-store-env\n           # Which key to we want to pull out of the env file to use as the STORY_FOLDER value?\n           key: folder\nSee this blog post for storing secrets: https://phoenixnap.com/kb/kubernetes-secrets"
  },
  {
    "objectID": "posts/software/kubernetes/kubernetes_notes.html#pod-internal-communication",
    "href": "posts/software/kubernetes/kubernetes_notes.html#pod-internal-communication",
    "title": "Kubernetes",
    "section": "4.1. Pod-Internal Communication",
    "text": "4.1. Pod-Internal Communication\nFor containers in the same pod, we can use localhost to communicate between the pods. We need to expose the port of the container when creating the Dockerfile. This is similar to internal communication between containers when using docker compose."
  },
  {
    "objectID": "posts/software/kubernetes/kubernetes_notes.html#pod-to-pod-communication",
    "href": "posts/software/kubernetes/kubernetes_notes.html#pod-to-pod-communication",
    "title": "Kubernetes",
    "section": "4.2. Pod-to-Pod Communication",
    "text": "4.2. Pod-to-Pod Communication\n\n4.2.1. Using IP address\nFor pods in the same cluster, we can create a service with type=ClusterIP. We would need to get the IP address of the service and use this in our code. Kubernetes has a convenient environment variable to save the leg work here. If we have a service called “auth-service”, for example, it will automatically create an environment variable called AUTH_SERVICE_SERVICE_HOST. So we can use this in place of the IP address in our code.\n\n\n4.2.2 Using DNS\nYou can also pass the name of the service as the URL. We also need to suffix it with the namespace, which is “default” by default.\nE.g. auth-service.default\nThis is automatically generated by Kubernetes.\n\n\n4.2.3 Best Practices\nYou should normally have different containers in different pods, unless they are very tightly coupled together.\nSo we need to create a service per pod to allow them to communicate. The DNS approach is the most common method, rather than the IP address environment variable."
  },
  {
    "objectID": "posts/software/kubernetes/kubernetes_notes.html#communicating-with-the-outside-world",
    "href": "posts/software/kubernetes/kubernetes_notes.html#communicating-with-the-outside-world",
    "title": "Kubernetes",
    "section": "4.3. Communicating with the Outside World",
    "text": "4.3. Communicating with the Outside World\nCreate a service with type=LoadBalancer."
  },
  {
    "objectID": "posts/software/kubernetes/kubernetes_notes.html#deployment-options",
    "href": "posts/software/kubernetes/kubernetes_notes.html#deployment-options",
    "title": "Kubernetes",
    "section": "5.1. Deployment Options",
    "text": "5.1. Deployment Options\nThere are several approaches:\n\nDeploy on your own data centre\nDeploy manually on a cloud provider. Tools like kops can help make this more straightforward.\nUse a managed service from a cloud provider like AWS EKS."
  },
  {
    "objectID": "posts/software/kubernetes/kubernetes_notes.html#aws-eks",
    "href": "posts/software/kubernetes/kubernetes_notes.html#aws-eks",
    "title": "Kubernetes",
    "section": "5.2. AWS EKS",
    "text": "5.2. AWS EKS\nThis is Elastic Kubernetes Service. It is analogous to how we used Elastic Container Service (ECS) when using plain Docker containers.\nWe use the UI to create an EKS cluster.\n\nWe need to give EKS permissions to create new resources, e.g. EC2 instances, EFS storage, etc. In IAM, create a new role with appropriate permissions.\nWe also need to create a network for our EKS cluster. We can do this using a template in CloudFormation.\n\nOn our computer from which we issue kubectl commands, we want it to be able to talk to the EKS cluster. Whether using EKS or not, this is configured in a config file on your machine located at User/.kube/config\nYou can update this to speak to EKS. The most straightforward way to update this is using the AWS CLI. We’ll need to create an access key in IAM for this.\nRun aws configure to set up the CLI to be able to make changes on your machine.\nThen run\naws eks --region us-east-2 update-kubeconfig --name my-cluster-name\n\n5.2.1. Add Worker Nodes\nIn the EKS console, click Add Node Group. We again need to assign an appropriate role to these worker nodes so that they can write log files, connect to other services, etc.\nWe attach an IAM role for this. Relevant policies are EKSWorkerNode, CNI_Policy, EC2ContainerRegistryPolicy\n\n\n5.2.2. Adding a Volume\nWe can add EFS storage as a volume. We make the persistent volume type=CSI.\nThere are third party libraries that make the interface easier to deal with, e.g. aws-efs-csi-driver is an open source package to make it simple to deal with. If using this, you will also need to add a section to your volume’s config.yaml to define the new storage class that this provides. The snippet can be copied from the driver’s repo.\nIn the EC2 section, create a security group that has inbound NFS connections allowed with the IPv4 CIDR of our VPC.\nNow in EFS, we can click Create File System. Make sure it’s in the same VPC. Under customise, remove the default security groups and add the new security groups created previously.\nThen the usual K8s process of creating a persistent volume config, a claim config, and assigning the claim to a pod."
  },
  {
    "objectID": "posts/software/react/7_debugging/post.html",
    "href": "posts/software/react/7_debugging/post.html",
    "title": "React: Debugging",
    "section": "",
    "text": "We’ve done stylin’, now time for profilin’. Woo!\n\n\nDebug react apps using the browser console.\nThis gives the stack trace that raised the error, and the file and line number on which it was raised.\n\n\n\nUse the Sources tab (in chrome) to use the debugger.\nYou’ll see the directory structure of your project. You can click a line number to set a breakpoint and pause execution there, then observe variable values or step through execution.\nYou can also achieve the same by placing a debugger() line in the code.\n\n\n\nStrictMode is a React component which you can wrap any other component in, including the root App component.\nimport { StrictMode } from 'react';\nIt causes React to render every component twice in development mode. This can help surface errors that may not be obvious in normal execution.\n\n\n\nThis is a browser extension. It adds 2 new tabs to the console view in the browser window: profiler and components.\nComponents shows your component tree and highlights these in the browser. It also gives information about that component such as props. You can edit those props values in the browser. You can also see hooks and their state values.\n\n\n\n\nSection 7 of “React: The Complete Guide” Udemy course"
  },
  {
    "objectID": "posts/software/react/7_debugging/post.html#browser-console",
    "href": "posts/software/react/7_debugging/post.html#browser-console",
    "title": "React: Debugging",
    "section": "",
    "text": "Debug react apps using the browser console.\nThis gives the stack trace that raised the error, and the file and line number on which it was raised."
  },
  {
    "objectID": "posts/software/react/7_debugging/post.html#debugger",
    "href": "posts/software/react/7_debugging/post.html#debugger",
    "title": "React: Debugging",
    "section": "",
    "text": "Use the Sources tab (in chrome) to use the debugger.\nYou’ll see the directory structure of your project. You can click a line number to set a breakpoint and pause execution there, then observe variable values or step through execution.\nYou can also achieve the same by placing a debugger() line in the code."
  },
  {
    "objectID": "posts/software/react/7_debugging/post.html#strict-mode",
    "href": "posts/software/react/7_debugging/post.html#strict-mode",
    "title": "React: Debugging",
    "section": "",
    "text": "StrictMode is a React component which you can wrap any other component in, including the root App component.\nimport { StrictMode } from 'react';\nIt causes React to render every component twice in development mode. This can help surface errors that may not be obvious in normal execution."
  },
  {
    "objectID": "posts/software/react/7_debugging/post.html#react-developer-tools",
    "href": "posts/software/react/7_debugging/post.html#react-developer-tools",
    "title": "React: Debugging",
    "section": "",
    "text": "This is a browser extension. It adds 2 new tabs to the console view in the browser window: profiler and components.\nComponents shows your component tree and highlights these in the browser. It also gives information about that component such as props. You can edit those props values in the browser. You can also see hooks and their state values."
  },
  {
    "objectID": "posts/software/react/7_debugging/post.html#references",
    "href": "posts/software/react/7_debugging/post.html#references",
    "title": "React: Debugging",
    "section": "",
    "text": "Section 7 of “React: The Complete Guide” Udemy course"
  },
  {
    "objectID": "posts/software/react/29_testing/post.html",
    "href": "posts/software/react/29_testing/post.html",
    "title": "React: Testing",
    "section": "",
    "text": "We should run tests at different levels of granularity:\n\nUnit tests\nIntegration tests\nEnd-to-end tests\n\nWe need some build tools to help us run tests.\nJest and React Testing Library are useful for unit and integration tests, which are the main focus of this page. End-to-end testing can be done with tools like Selenium or Cypress.\n\n\nJest is a testing framework that allows us to run JavaScript tests.\nThe test function creates a unit test. It takes a name of the test and an anonymous function which runs the test code.\nThe expect function then defines some behaviour to assert. For pure JavaScript util functions, this is all we need.\nFor React components, we need to test the rendered component with the help of React Testing Library.\n\n\n\nReact testing library is a library that lets us simulate rendered components and assert characteristics of them.\nThe render function is used to simulate the dom to render a component in a test. The screen function is used to get properties from the simulated DOM, eg screen.getByText to assert a particular passage of text appears on the screen.\nThe get functions return an error if the object does not exist. The query functions return null instead. The latter is useful if we want to test when something should NOT be rendered.\nThe getByRole function is useful to pick out specific elements. See available roles here.\nThe userEvent object from React Testing Library simulates user actions like click or hover, so we can test interactive behaviour of components.\n\n\n\n\nThe general pattern for testing is: Arrange, Act, Assert.\n\nArrange: Render the component.\nAct: Any user events or interaction (if applicable for the test).\nAssert: Check the desired output is in the DOM.\n\n\n\n\nWe may want to group tests for related features/components together for readability.\nUse the describe function to define a test suite.\nIt takes a test suite description string and anonymous function as argumens. Then each of the tests are defined within it.\n\n\n\nThe get and query functions attempt to retrieve elements from the DOM immediately, as soon as the component is rendered. This is not the behaviour we want for components with asynchronous elements which may take time to fetch data.\nThe findByRole function, and related “find” functions, return a promise which will wait and re-attempt to find the element before failing. This allows us to test async code. You can pass optional args to the find functions to set how long they should wait, when to retry etc.\nWe generally want to avoid sending external requests as part of unit tests, so this is more relevant to integration or end-to-end tests.\nFor unit tests, we can mock the results.\n\n\n\nWe want to mock out external calls like fetch when we run them is our unit tests.\nWe can overwrite the window.fetch method:\nwindow.fetch = jest.fn();\nwindow.fetch.mockResolvedValueOnce({\n    json: async() =&gt; {\n        // The mock values\n        [{id: 1, text: 'example text goes here'}] \n    }\n});\n\n\n\n\nSection 29 of “React: The Complete Guide” Udemy course\nJest\nReact Testing Library\nReact Testing Library with custom hooks"
  },
  {
    "objectID": "posts/software/react/29_testing/post.html#what-to-test-and-how",
    "href": "posts/software/react/29_testing/post.html#what-to-test-and-how",
    "title": "React: Testing",
    "section": "",
    "text": "We should run tests at different levels of granularity:\n\nUnit tests\nIntegration tests\nEnd-to-end tests\n\nWe need some build tools to help us run tests.\nJest and React Testing Library are useful for unit and integration tests, which are the main focus of this page. End-to-end testing can be done with tools like Selenium or Cypress.\n\n\nJest is a testing framework that allows us to run JavaScript tests.\nThe test function creates a unit test. It takes a name of the test and an anonymous function which runs the test code.\nThe expect function then defines some behaviour to assert. For pure JavaScript util functions, this is all we need.\nFor React components, we need to test the rendered component with the help of React Testing Library.\n\n\n\nReact testing library is a library that lets us simulate rendered components and assert characteristics of them.\nThe render function is used to simulate the dom to render a component in a test. The screen function is used to get properties from the simulated DOM, eg screen.getByText to assert a particular passage of text appears on the screen.\nThe get functions return an error if the object does not exist. The query functions return null instead. The latter is useful if we want to test when something should NOT be rendered.\nThe getByRole function is useful to pick out specific elements. See available roles here.\nThe userEvent object from React Testing Library simulates user actions like click or hover, so we can test interactive behaviour of components."
  },
  {
    "objectID": "posts/software/react/29_testing/post.html#the-3-as-of-testing",
    "href": "posts/software/react/29_testing/post.html#the-3-as-of-testing",
    "title": "React: Testing",
    "section": "",
    "text": "The general pattern for testing is: Arrange, Act, Assert.\n\nArrange: Render the component.\nAct: Any user events or interaction (if applicable for the test).\nAssert: Check the desired output is in the DOM."
  },
  {
    "objectID": "posts/software/react/29_testing/post.html#test-suites",
    "href": "posts/software/react/29_testing/post.html#test-suites",
    "title": "React: Testing",
    "section": "",
    "text": "We may want to group tests for related features/components together for readability.\nUse the describe function to define a test suite.\nIt takes a test suite description string and anonymous function as argumens. Then each of the tests are defined within it."
  },
  {
    "objectID": "posts/software/react/29_testing/post.html#testing-asynchronous-code",
    "href": "posts/software/react/29_testing/post.html#testing-asynchronous-code",
    "title": "React: Testing",
    "section": "",
    "text": "The get and query functions attempt to retrieve elements from the DOM immediately, as soon as the component is rendered. This is not the behaviour we want for components with asynchronous elements which may take time to fetch data.\nThe findByRole function, and related “find” functions, return a promise which will wait and re-attempt to find the element before failing. This allows us to test async code. You can pass optional args to the find functions to set how long they should wait, when to retry etc.\nWe generally want to avoid sending external requests as part of unit tests, so this is more relevant to integration or end-to-end tests.\nFor unit tests, we can mock the results."
  },
  {
    "objectID": "posts/software/react/29_testing/post.html#using-mocks",
    "href": "posts/software/react/29_testing/post.html#using-mocks",
    "title": "React: Testing",
    "section": "",
    "text": "We want to mock out external calls like fetch when we run them is our unit tests.\nWe can overwrite the window.fetch method:\nwindow.fetch = jest.fn();\nwindow.fetch.mockResolvedValueOnce({\n    json: async() =&gt; {\n        // The mock values\n        [{id: 1, text: 'example text goes here'}] \n    }\n});"
  },
  {
    "objectID": "posts/software/react/29_testing/post.html#references",
    "href": "posts/software/react/29_testing/post.html#references",
    "title": "React: Testing",
    "section": "",
    "text": "Section 29 of “React: The Complete Guide” Udemy course\nJest\nReact Testing Library\nReact Testing Library with custom hooks"
  },
  {
    "objectID": "posts/software/react/1_getting_started/post.html",
    "href": "posts/software/react/1_getting_started/post.html",
    "title": "React: A Gentle Introduction",
    "section": "",
    "text": "React is a Javascript library for building user interfaces. It is less cumbersome and error-prone than using vanilla JS.\nCode sandbox is an in-browser environment to experiment with UIs. As an example, the same page is implemented in pure Javascript and React. The latter is much easier to follow, modularise and requires less boilerplate.\n\nWith React, you write declarative code: you define the goal, not the steps to get there.\nWith vanilla JS, you write imperative code, defining the steps, not the goal.\n\nA build tool (like Vite or Next.js) is necessary because the Javascript (specifically the JSX) must be transformed. React uses JSX which allows us to “mix” HTML and JS, so that we can define layout and functionality in the same place. This isn’t natively supported by the browser, so a build tool transforms this to pure html and JS.\n\n\n\nKey concepts in React are: components, JSX, props, and state.\n\n\nComponents are a core concept. They bundle html, CSS and JS into reusable blocks.\nIn vanilla JS, the JavaScript and HTML are in different files, so it can be hard to follow what needs to be changed where. Related code lives together, which is a key benefit of React and component style coding.\nJSX is a JavaScript syntax extension that allows us to write HTML in JavaScript files. This is not natively supported by browsers, so requires transformation by the build system, such as Vite.\nThe build process (of some but not all build tools) relies on the jsx file extension to indicate a JSX file that needs transformation. The browser does not care, as it never sees (and cannot read) these jsx files directly. Similarly, some build processes require the file extension in the import statements but others don’t.\nComponents must:\n\nStart with an upper case letter - so they do not clash with built-ins like header\nReturn a renderable object\n\nReact creates a component tree for your app. Your components do not end up in the source code directly. The build process traverses the tree until each component is resolved into built ins, and then these appear in the source code.\n\n\n\nUse curly braces to indicate dynamic values in JSX.\nIdeally declare constants rather than having complicated inline expressions.\nImages should be exported then the dynamic value passed as the src of the image. This prevents the image being lost in the build process if the build ignores files with certain extensions.\nimport myImage from './assets/exampleImage.png'\n\n&lt;img src={myImage}/&gt;\nDynamic values can be passed to components as props. React components take a single argument called props, which is an object of key:value pairs passed to the component.\nIf you have an object of props to pass, you can use the spread operator to avoid writing them out individually. Also use object destructuring inside the component to pick out the variables.\n\n\n\nIt is good practice that each component is in its own file. File name should match the component name and be the default export.\nAlso split out style CSS files and keep these alongside the component. CSS files need to be imported by each component file that uses it.\nThe styles are NOT automatically scoped to the component that uses them. They will apply to all components with that name. For example, if you apply header styling to a custom Header component, it will also apply to the built in header html component.\n\n\n\nThe children prop is passed by all components and it is the value between the component tags. It can be used for HTML tag-style syntax.\nWe can react to events by passing a function to onClick or similar. In vanilla JS, we would need to select the element and add an event listener, but react is declarative. We can define the handleClick function inside the component so that it has access to the component’s props and state. We can pass functions as props. This is useful as we can pass state setter functions down to nested components. This should be a pointer to the function, not the executed function itself, e.g. handleClick NOT handleClick()\nIf we want to modify the args that we pass to the function in onClick, use an anonymous arrow function () =&gt; handleSelect(arg) That doesn’t actually get executed until onClick is called.\nWe can set default values of props by putting the default value in the function signature.\n\n\n\nBy default, React components only execute once, even if an internal variable changes. You have to “tell” React to execute something again. This is where state comes in useful. React checks if UI updates are needed by comparing old output with new and applying the difference. So we use states rather than regular variables to indicate that a re-render is required if the state changes. State is essentially a special registered variable that react handles differently. If the state of a component changes, that component and its children in the component tree re-render.\nThe useState function is a “hook”. Hooks must be declared in the top level of a component function, they can’t be nested in internal functions such as event handlers, and they also can’t be declared outside of functions. It returns an array of two elements, the state value and a setter. A default state value can be passed to use state. The setter “schedules” an update, but that isn’t necessarily immediate. So you can see unexpected things when logging a value after the setter in code, where the logged value is still the “old” state value because the UI update has been scheduled but not completed yet.\nIf setting a state value based on its previous value, pass it as a function. For example, if on a button click we want to invert the value of a Boolean, use\nsetIsEditing((editing) =&gt; {!editing})\nNOT\nsetIsEditing(!isEditing)\nThis is because React schedules when to change state, it doesn’t necessarily do it immediately. So you could get unexpected behaviour. But when passed as a function, this triggers a re-render, similarly to how a hook does.\n\n\n\n\n\n\nOne option is to use JSX with a ternary expression. It is valid for null to be used in place of a component.\n{ selectedState ? ComponentA : ComponentB }\nAn alternative is the and operator which can also be used for this. In JavaScript, if the first term is truthy then it returns the second term, which is what we want.\n{ selectedState && ComponentA }\nA third option is to save the component as a variable and conditionally reassign it.\n// The default component\nlet tabContent = &lt;p&gt;Please select a topic&lt;/p&gt;\n\nif (selectedTopic) {\n    tabContent = (\n        // The component displayed if a topic is selected\n        &lt;div&gt;\n            &lt;h3&gt;selectedTopic.title&lt;/h3&gt;\n             &lt;p&gt;selectedTopic.description&lt;/p&gt;\n        &lt;/div&gt;\n    )\n}\n\n\n\nWe can set className as a JSX expression and use a ternary expression. For example, check if the button is selected and apply a different className depending on whether it’s selected.\n\n\n\nJSX is capable of outputting lists of renderable components. We do this by using map over the array:\nmyArray.map((item) =&gt; (&lt;Component item={item}/&gt;))\nAdd a key prop to the Component which uniquely identifies the item to avoid warnings raised by React.\n\n\n\n\nThe following sections are a collection of less essential topics but provide useful background in React.\n\n\nYou don’t NEED JSX, but it makes life easier.\nThe following is JSX, which is easy to read but requires a build transformation process.\n&lt;div id=\"content\"&gt;\n    &lt;p&gt;Hello World!&lt;/p&gt;\n&lt;/div&gt;\nThe alternative in plain JavaScript is to manipulate the DOM directly. It’s not as clear. But it avoids the need for a build process because it IS valid JavaScript supported by the browser.\nReact.createElement(\n    'div',\n    { id: 'content'},\n    React.createElement(\n        'p',\n        null,\n        'Hello World!'\n    )\n)\n\n\n\nA JavaScript function must return one value, it cannot return multiple values. This is true of React components, since they are really just syntactic sugar around JavaScript functions.\nSo if we have two or more sibling components being returned, we must wrap them in a parent component. Naively, we could just use a div, but this adds an unnecessary extra component to our tree.\nAn alternative is to use Fragment. This can be imported from react and used as a parent component without actually creating any new component when built. In newer versions of react, we can skip the import and just use empty opening and closing tags &lt;&gt; &lt;/&gt; to create a Fragment.\n\n\n\nRemember, React will re-render a component and all its child components when a state changes. So if a state is too high up the component tree, it will cause unnecessary re-rendering of many other components.\nThis is an indication that a component needs to be split out and its state managed lower down the tree.\n\n\n\nReact doesn’t auto-forward props to nested components.\nWe can forward an arbitrary number of props without having to write out each manually. Use the rest operator …extraProps in the function signature. Then use the spread operator (same syntax) to pass them to the inner component that you want …extraProps.\nThis is like **kwargs in Python.\n\n\n\nWe’ve seen how we can pass the special children prop to pass arbitrary JSX to our components.\nWhat if we wanted 2 or more slots in our component with arbitrary content? Components are ultimately just JavaScript code, so we can pass the components for the other slot as a prop (and if there are multiple siblings we can wrap them in a fragment).\n\n\n\nWe can pass a prop (with capital letter since it will be used as a custom component) which we can then vary in the nested component. For example, pass a prop called ButtonContainer which can be:\n\na string for a built in type like “menu”\na function for a custom component like Section (without calling it or using angled brackets)\n\n\n\n\nNot all content needs to go in components. Remember you can modify index.html directly if there is static content that makes more sense there.\n\n\n\nAny files (typically images) stored in the public directory of the root of the project are made publicly available, so anybody can navigate to them.\nIf you want files to be private until used on the website, store them in a folder in src, usually src/assets. Anything in src is not publicly accessible.\n\n\n\n\n\n\nIf we have an input tag, we can manage the user input value as a new state playerName.\nWe use the onChange prop of the input to handle this. We pass a handleChange function to it that takes the event (from the user input) and updates our state based on it.\nfunction handleChange (event) {\n    setPlayerName(event.target.value)\n    }\nWe then pass the value and onChange to the input\n&lt;input value={playerName} onChange={handleChange} /&gt;\nThis technique of passing a value to the input then allowing it to change the value is called a two-way binding.\n\n\n\nWhen your state is an object or array (an array is just a subset of object in JavaScript) then you should create a deep copy of it before altering its value. Objects are passed by reference so you may see unintended side effects otherwise if you try to update the object directly. Use the spread operator to copy to a new variable.\n\n\n\nIf two or more sibling components all need access to the same state, the state should be handled by the closest ancestor component.\n\n\n\nDon’t have two different states in different places which refer to the same state / data. Potential for conflicts and bugs.\nAlso avoid using logic based on state A for the setter logic of state B. For the same reason we use the functional form of the setter when it’s based on previous values; the state may be outdated as it is scheduled to updated but not yet recalculated and rendered.\n\n\n\nManage as few states as possible and pass them as props where needed, then derive any further states from those instead of managing another set of states.\n\n\n\nIf you initialise a state as an object or array, remember that JavaScript passes these by reference.\nSo if you then modify that array, you are modifying the initial array. If you later want to reset the state by setting the state back to that initial array, it won’t work because the original was mutated.\nTo work around this, take a deep copy using the spread operator. If it is a nested array, map for each inner array.\n[…initialArray.map(array =&gt; […array])]\n\n\n\nIf lifting up the state breaks the modularity of the nested components or would trigger the parent function to rerender unnecessarily, avoid lifting.\n\n\n\n\n\nSections 1, 3 and 4 of “React: The Complete Guide” Udemy course"
  },
  {
    "objectID": "posts/software/react/1_getting_started/post.html#what-is-react",
    "href": "posts/software/react/1_getting_started/post.html#what-is-react",
    "title": "React: A Gentle Introduction",
    "section": "",
    "text": "React is a Javascript library for building user interfaces. It is less cumbersome and error-prone than using vanilla JS.\nCode sandbox is an in-browser environment to experiment with UIs. As an example, the same page is implemented in pure Javascript and React. The latter is much easier to follow, modularise and requires less boilerplate.\n\nWith React, you write declarative code: you define the goal, not the steps to get there.\nWith vanilla JS, you write imperative code, defining the steps, not the goal.\n\nA build tool (like Vite or Next.js) is necessary because the Javascript (specifically the JSX) must be transformed. React uses JSX which allows us to “mix” HTML and JS, so that we can define layout and functionality in the same place. This isn’t natively supported by the browser, so a build tool transforms this to pure html and JS."
  },
  {
    "objectID": "posts/software/react/1_getting_started/post.html#key-react-concepts",
    "href": "posts/software/react/1_getting_started/post.html#key-react-concepts",
    "title": "React: A Gentle Introduction",
    "section": "",
    "text": "Key concepts in React are: components, JSX, props, and state.\n\n\nComponents are a core concept. They bundle html, CSS and JS into reusable blocks.\nIn vanilla JS, the JavaScript and HTML are in different files, so it can be hard to follow what needs to be changed where. Related code lives together, which is a key benefit of React and component style coding.\nJSX is a JavaScript syntax extension that allows us to write HTML in JavaScript files. This is not natively supported by browsers, so requires transformation by the build system, such as Vite.\nThe build process (of some but not all build tools) relies on the jsx file extension to indicate a JSX file that needs transformation. The browser does not care, as it never sees (and cannot read) these jsx files directly. Similarly, some build processes require the file extension in the import statements but others don’t.\nComponents must:\n\nStart with an upper case letter - so they do not clash with built-ins like header\nReturn a renderable object\n\nReact creates a component tree for your app. Your components do not end up in the source code directly. The build process traverses the tree until each component is resolved into built ins, and then these appear in the source code.\n\n\n\nUse curly braces to indicate dynamic values in JSX.\nIdeally declare constants rather than having complicated inline expressions.\nImages should be exported then the dynamic value passed as the src of the image. This prevents the image being lost in the build process if the build ignores files with certain extensions.\nimport myImage from './assets/exampleImage.png'\n\n&lt;img src={myImage}/&gt;\nDynamic values can be passed to components as props. React components take a single argument called props, which is an object of key:value pairs passed to the component.\nIf you have an object of props to pass, you can use the spread operator to avoid writing them out individually. Also use object destructuring inside the component to pick out the variables.\n\n\n\nIt is good practice that each component is in its own file. File name should match the component name and be the default export.\nAlso split out style CSS files and keep these alongside the component. CSS files need to be imported by each component file that uses it.\nThe styles are NOT automatically scoped to the component that uses them. They will apply to all components with that name. For example, if you apply header styling to a custom Header component, it will also apply to the built in header html component.\n\n\n\nThe children prop is passed by all components and it is the value between the component tags. It can be used for HTML tag-style syntax.\nWe can react to events by passing a function to onClick or similar. In vanilla JS, we would need to select the element and add an event listener, but react is declarative. We can define the handleClick function inside the component so that it has access to the component’s props and state. We can pass functions as props. This is useful as we can pass state setter functions down to nested components. This should be a pointer to the function, not the executed function itself, e.g. handleClick NOT handleClick()\nIf we want to modify the args that we pass to the function in onClick, use an anonymous arrow function () =&gt; handleSelect(arg) That doesn’t actually get executed until onClick is called.\nWe can set default values of props by putting the default value in the function signature.\n\n\n\nBy default, React components only execute once, even if an internal variable changes. You have to “tell” React to execute something again. This is where state comes in useful. React checks if UI updates are needed by comparing old output with new and applying the difference. So we use states rather than regular variables to indicate that a re-render is required if the state changes. State is essentially a special registered variable that react handles differently. If the state of a component changes, that component and its children in the component tree re-render.\nThe useState function is a “hook”. Hooks must be declared in the top level of a component function, they can’t be nested in internal functions such as event handlers, and they also can’t be declared outside of functions. It returns an array of two elements, the state value and a setter. A default state value can be passed to use state. The setter “schedules” an update, but that isn’t necessarily immediate. So you can see unexpected things when logging a value after the setter in code, where the logged value is still the “old” state value because the UI update has been scheduled but not completed yet.\nIf setting a state value based on its previous value, pass it as a function. For example, if on a button click we want to invert the value of a Boolean, use\nsetIsEditing((editing) =&gt; {!editing})\nNOT\nsetIsEditing(!isEditing)\nThis is because React schedules when to change state, it doesn’t necessarily do it immediately. So you could get unexpected behaviour. But when passed as a function, this triggers a re-render, similarly to how a hook does."
  },
  {
    "objectID": "posts/software/react/1_getting_started/post.html#dynamic-content",
    "href": "posts/software/react/1_getting_started/post.html#dynamic-content",
    "title": "React: A Gentle Introduction",
    "section": "",
    "text": "One option is to use JSX with a ternary expression. It is valid for null to be used in place of a component.\n{ selectedState ? ComponentA : ComponentB }\nAn alternative is the and operator which can also be used for this. In JavaScript, if the first term is truthy then it returns the second term, which is what we want.\n{ selectedState && ComponentA }\nA third option is to save the component as a variable and conditionally reassign it.\n// The default component\nlet tabContent = &lt;p&gt;Please select a topic&lt;/p&gt;\n\nif (selectedTopic) {\n    tabContent = (\n        // The component displayed if a topic is selected\n        &lt;div&gt;\n            &lt;h3&gt;selectedTopic.title&lt;/h3&gt;\n             &lt;p&gt;selectedTopic.description&lt;/p&gt;\n        &lt;/div&gt;\n    )\n}\n\n\n\nWe can set className as a JSX expression and use a ternary expression. For example, check if the button is selected and apply a different className depending on whether it’s selected.\n\n\n\nJSX is capable of outputting lists of renderable components. We do this by using map over the array:\nmyArray.map((item) =&gt; (&lt;Component item={item}/&gt;))\nAdd a key prop to the Component which uniquely identifies the item to avoid warnings raised by React."
  },
  {
    "objectID": "posts/software/react/1_getting_started/post.html#going-deeper",
    "href": "posts/software/react/1_getting_started/post.html#going-deeper",
    "title": "React: A Gentle Introduction",
    "section": "",
    "text": "The following sections are a collection of less essential topics but provide useful background in React.\n\n\nYou don’t NEED JSX, but it makes life easier.\nThe following is JSX, which is easy to read but requires a build transformation process.\n&lt;div id=\"content\"&gt;\n    &lt;p&gt;Hello World!&lt;/p&gt;\n&lt;/div&gt;\nThe alternative in plain JavaScript is to manipulate the DOM directly. It’s not as clear. But it avoids the need for a build process because it IS valid JavaScript supported by the browser.\nReact.createElement(\n    'div',\n    { id: 'content'},\n    React.createElement(\n        'p',\n        null,\n        'Hello World!'\n    )\n)\n\n\n\nA JavaScript function must return one value, it cannot return multiple values. This is true of React components, since they are really just syntactic sugar around JavaScript functions.\nSo if we have two or more sibling components being returned, we must wrap them in a parent component. Naively, we could just use a div, but this adds an unnecessary extra component to our tree.\nAn alternative is to use Fragment. This can be imported from react and used as a parent component without actually creating any new component when built. In newer versions of react, we can skip the import and just use empty opening and closing tags &lt;&gt; &lt;/&gt; to create a Fragment.\n\n\n\nRemember, React will re-render a component and all its child components when a state changes. So if a state is too high up the component tree, it will cause unnecessary re-rendering of many other components.\nThis is an indication that a component needs to be split out and its state managed lower down the tree.\n\n\n\nReact doesn’t auto-forward props to nested components.\nWe can forward an arbitrary number of props without having to write out each manually. Use the rest operator …extraProps in the function signature. Then use the spread operator (same syntax) to pass them to the inner component that you want …extraProps.\nThis is like **kwargs in Python.\n\n\n\nWe’ve seen how we can pass the special children prop to pass arbitrary JSX to our components.\nWhat if we wanted 2 or more slots in our component with arbitrary content? Components are ultimately just JavaScript code, so we can pass the components for the other slot as a prop (and if there are multiple siblings we can wrap them in a fragment).\n\n\n\nWe can pass a prop (with capital letter since it will be used as a custom component) which we can then vary in the nested component. For example, pass a prop called ButtonContainer which can be:\n\na string for a built in type like “menu”\na function for a custom component like Section (without calling it or using angled brackets)\n\n\n\n\nNot all content needs to go in components. Remember you can modify index.html directly if there is static content that makes more sense there.\n\n\n\nAny files (typically images) stored in the public directory of the root of the project are made publicly available, so anybody can navigate to them.\nIf you want files to be private until used on the website, store them in a folder in src, usually src/assets. Anything in src is not publicly accessible."
  },
  {
    "objectID": "posts/software/react/1_getting_started/post.html#more-on-states",
    "href": "posts/software/react/1_getting_started/post.html#more-on-states",
    "title": "React: A Gentle Introduction",
    "section": "",
    "text": "If we have an input tag, we can manage the user input value as a new state playerName.\nWe use the onChange prop of the input to handle this. We pass a handleChange function to it that takes the event (from the user input) and updates our state based on it.\nfunction handleChange (event) {\n    setPlayerName(event.target.value)\n    }\nWe then pass the value and onChange to the input\n&lt;input value={playerName} onChange={handleChange} /&gt;\nThis technique of passing a value to the input then allowing it to change the value is called a two-way binding.\n\n\n\nWhen your state is an object or array (an array is just a subset of object in JavaScript) then you should create a deep copy of it before altering its value. Objects are passed by reference so you may see unintended side effects otherwise if you try to update the object directly. Use the spread operator to copy to a new variable.\n\n\n\nIf two or more sibling components all need access to the same state, the state should be handled by the closest ancestor component.\n\n\n\nDon’t have two different states in different places which refer to the same state / data. Potential for conflicts and bugs.\nAlso avoid using logic based on state A for the setter logic of state B. For the same reason we use the functional form of the setter when it’s based on previous values; the state may be outdated as it is scheduled to updated but not yet recalculated and rendered.\n\n\n\nManage as few states as possible and pass them as props where needed, then derive any further states from those instead of managing another set of states.\n\n\n\nIf you initialise a state as an object or array, remember that JavaScript passes these by reference.\nSo if you then modify that array, you are modifying the initial array. If you later want to reset the state by setting the state back to that initial array, it won’t work because the original was mutated.\nTo work around this, take a deep copy using the spread operator. If it is a nested array, map for each inner array.\n[…initialArray.map(array =&gt; […array])]\n\n\n\nIf lifting up the state breaks the modularity of the nested components or would trigger the parent function to rerender unnecessarily, avoid lifting."
  },
  {
    "objectID": "posts/software/react/1_getting_started/post.html#references",
    "href": "posts/software/react/1_getting_started/post.html#references",
    "title": "React: A Gentle Introduction",
    "section": "",
    "text": "Sections 1, 3 and 4 of “React: The Complete Guide” Udemy course"
  },
  {
    "objectID": "posts/software/sql/notes.html",
    "href": "posts/software/sql/notes.html",
    "title": "SQL",
    "section": "",
    "text": "Structured Query Language (SQL) is used to manage and query data stored in a Relational Database Management System (RDMBS). Essentially, put data in tables and get it out again.\nAn RDMBS organises data into tables with defined schemas (column names and data types).\nSQL is a declarative language. You specify what you want to happen, not how to achieve it. The SQL query engine optimises how the query run internally, e.g. what order to execute commands, what indexes to use.\n\n\n\nThere are different “flavours” of SQL. For example, MySQL, PostegreSQL, SQLite, SQL Server. These are often, incorrectly, used interchangeably with SQL. SQL is a general, high-level language for querying RDMBSes.\nThe “flavours” are each a specific RDBMS which you query with the corresponding language. Postgres is an RDBMS which you can query by writing PostgreSQL. MySQL is and RDBMS which you can query by writing MySQL. Etc. In practice, the difference is pretty minimal. If you can use one, you’ll learn the others pretty quickly.\n\n\n\nWe use PostgreSQL and its associated Postgres RDBMS.\nSome of the advantages of PostgreSQL:\n\nPopularity: One of the most popular behind MySQL\nOpen Source: BSD-style license is not too restrictive\nExtensible: Postgres has extensions like PostGIS for geospatial data, etc\nANSI Compliant: American National Standards Institute (ANSI) define standards and PostgreSQL mostly conforms to these. One of the least quirky flavours of SQL."
  },
  {
    "objectID": "posts/software/sql/notes.html#sql-and-rdbms",
    "href": "posts/software/sql/notes.html#sql-and-rdbms",
    "title": "SQL",
    "section": "",
    "text": "Structured Query Language (SQL) is used to manage and query data stored in a Relational Database Management System (RDMBS). Essentially, put data in tables and get it out again.\nAn RDMBS organises data into tables with defined schemas (column names and data types).\nSQL is a declarative language. You specify what you want to happen, not how to achieve it. The SQL query engine optimises how the query run internally, e.g. what order to execute commands, what indexes to use."
  },
  {
    "objectID": "posts/software/sql/notes.html#sql-flavours",
    "href": "posts/software/sql/notes.html#sql-flavours",
    "title": "SQL",
    "section": "",
    "text": "There are different “flavours” of SQL. For example, MySQL, PostegreSQL, SQLite, SQL Server. These are often, incorrectly, used interchangeably with SQL. SQL is a general, high-level language for querying RDMBSes.\nThe “flavours” are each a specific RDBMS which you query with the corresponding language. Postgres is an RDBMS which you can query by writing PostgreSQL. MySQL is and RDBMS which you can query by writing MySQL. Etc. In practice, the difference is pretty minimal. If you can use one, you’ll learn the others pretty quickly."
  },
  {
    "objectID": "posts/software/sql/notes.html#postgresql",
    "href": "posts/software/sql/notes.html#postgresql",
    "title": "SQL",
    "section": "",
    "text": "We use PostgreSQL and its associated Postgres RDBMS.\nSome of the advantages of PostgreSQL:\n\nPopularity: One of the most popular behind MySQL\nOpen Source: BSD-style license is not too restrictive\nExtensible: Postgres has extensions like PostGIS for geospatial data, etc\nANSI Compliant: American National Standards Institute (ANSI) define standards and PostgreSQL mostly conforms to these. One of the least quirky flavours of SQL."
  },
  {
    "objectID": "posts/software/sql/notes.html#select",
    "href": "posts/software/sql/notes.html#select",
    "title": "SQL",
    "section": "2.1. SELECT",
    "text": "2.1. SELECT\nUse SELECT to read specific columns (or all with *) from a given table.\nSELECT column1, column2\nFROM table_name;\nWe can optional use LIMIT to return a set number of rows. This can be helpful if we’re querying a massive table that might be a huge query.\nSELECT *\nFROM table_name\nLIMIT 10\nWe can use the AS keyword to alias a column name.\nSELECT column1, column2 AS skibidi  -- using a stupid alias\nFROM table_name;\nWe can also add comments with -- as above."
  },
  {
    "objectID": "posts/software/sql/notes.html#where",
    "href": "posts/software/sql/notes.html#where",
    "title": "SQL",
    "section": "2.2. WHERE",
    "text": "2.2. WHERE\nUse WHERE to filter the result on a specific condition. Conditions can be: =, !=, &lt;, &gt;, &lt;=, &gt;=\nSELECT column1, column2\nFROM table_name\nWHERE condition;"
  },
  {
    "objectID": "posts/software/sql/notes.html#combining-conditions",
    "href": "posts/software/sql/notes.html#combining-conditions",
    "title": "SQL",
    "section": "2.3. Combining Conditions",
    "text": "2.3. Combining Conditions\nUse logical operators AND, OR, NOT to chain multiple conditions.\nSELECT *\nFROM table\nWHERE condition1\n  AND condition2\n  AND NOT condition3;"
  },
  {
    "objectID": "posts/software/sql/notes.html#between",
    "href": "posts/software/sql/notes.html#between",
    "title": "SQL",
    "section": "2.4. BETWEEN",
    "text": "2.4. BETWEEN\nThe BETWEEN operator can also be used as a condition, and is equivalent to a combination of &gt;= AND &lt;=. Note that both sides are inclusive.\nFor example, the following BETWEEN condition:\nSELECT column1, column2\nFROM table_name\nWHERE column1 BETWEEN 0 AND 100;\nis equivalent to\nSELECT column1, column2\nFROM table_name\nWHERE column1 &gt;= 0 AND column2 &lt;= 100;"
  },
  {
    "objectID": "posts/software/sql/notes.html#in",
    "href": "posts/software/sql/notes.html#in",
    "title": "SQL",
    "section": "2.5. IN",
    "text": "2.5. IN\nThe IN operator is another implicit combined condition. It saves us the hassle of writing out multiple OR conditions.\nFor example, the following IN condition:\nSELECT column1, column2\nFROM table_name\nWHERE column1 (1, 2);\nis equivalent to\nSELECT column1, column2\nFROM table_name\nWHERE column1 = 1 OR column1 = 2;"
  },
  {
    "objectID": "posts/software/sql/notes.html#like",
    "href": "posts/software/sql/notes.html#like",
    "title": "SQL",
    "section": "2.6. LIKE",
    "text": "2.6. LIKE\nThe LIKE operator is another implicit condition. Similarly to IN, it save us the hassle of writing out multiple OR conditions.\nIt allows us to match patterns using the wildcards _ (to represent a single character) or % (to represent arbitrary number of characters).\nThe LIKE command is case-sensitive. The ILIKE command is a case-insensitive variant (Insensitive LIKE)\nSELECT product_id,\n       manufacturer,\n       drug\nFROM pharmacy_sales\nWHERE drug LIKE '%Relief%';\nWe can use multiple underscores to match a specific number of unknown characters. For example, this will match “a” followed by any 3 characters.\nWHERE word LIKE 'a___'\nThe wildcards can be at multiple points in the pattern, e.g.\nWHERE word LIKE 'f_c_'"
  },
  {
    "objectID": "posts/software/sql/notes.html#order-by",
    "href": "posts/software/sql/notes.html#order-by",
    "title": "SQL",
    "section": "2.7. ORDER BY",
    "text": "2.7. ORDER BY\nThe order of rows saved in the database is not guaranteed. Executing the same SELECT twice in a row can give a different ordering.\nIf we want a specific order, we can specify the ORDER BY column(s).\nSELECT column1, column2\nFROM table_name\nORDER BY column1;\nBy default, this is in ascending order (ASC). We can pass DESC to instead return items in descending order. This can be column-specific.\nSELECT column1, column2\nFROM table_name\nORDER BY column1 ASC, column2 DESC;\nWe can also pass the column numbers rather than names.\nSELECT policy_holder_id, call_category, call_received\nFROM callers\nORDER BY 1,3 DESC;\nWe can use ORDER BY in conjunction with LIMIT where we need the top N highest/lowest results. We can also use OFFSET to skip a number of results.\nFor example, we can skip the first 10 rows and then return the next 5, so the following query returns the 11th-15th ordered results.\nSELECT *\nFROM callers\nORDER BY call_received DESC\nOFFSET 10\nLIMIT 5;"
  },
  {
    "objectID": "posts/software/sql/notes.html#aggregate-functions",
    "href": "posts/software/sql/notes.html#aggregate-functions",
    "title": "SQL",
    "section": "3.1. Aggregate Functions",
    "text": "3.1. Aggregate Functions\nWe can aggregate data with SUM, MIN, MAX, AVG, COUNT.\nSELECT COUNT(*)\nFROM table_name;"
  },
  {
    "objectID": "posts/software/sql/notes.html#group-by",
    "href": "posts/software/sql/notes.html#group-by",
    "title": "SQL",
    "section": "3.2. GROUP BY",
    "text": "3.2. GROUP BY\nThe aggregate functions can be run on the entire table as above. But they come into their own when grouping by particular fields.\nWe can GROUP BY one or more columns.\nSELECT\ncategory,\n    SUM(spend) AS total_spend\nFROM product_spend\nGROUP BY category;"
  },
  {
    "objectID": "posts/software/sql/notes.html#having",
    "href": "posts/software/sql/notes.html#having",
    "title": "SQL",
    "section": "3.3. HAVING",
    "text": "3.3. HAVING\nSuppose we want to filter the data on the aggregated value. For example, in the previous example, we want to only return categories with total_spend &gt; 10.\nNaively, we might try to use WHERE. But WHERE filters individual rows. Trying this will give some variation of the following error message\n\naggregate functions are not allowed in WHERE\n\nThe HAVING clause is essentially the analog of WHERE, but operates on grouped data rather than individual rows.\nSELECT ticker, AVG(open)\nFROM stock_prices\nGROUP BY ticker\nHAVING AVG(open) &gt; 200;"
  },
  {
    "objectID": "posts/software/sql/notes.html#distinct",
    "href": "posts/software/sql/notes.html#distinct",
    "title": "SQL",
    "section": "3.4. DISTINCT",
    "text": "3.4. DISTINCT\nThe DISTINCT keyword can specify that only rows where the column(s) are distinct. If we pass multiple columns, we will get all of the distinct pairs (or tuples in the general case) of those columns.\nSELECT DISTINCT col1, col2\nFROM table_name;\nDISTINCT can be combined with aggregate functions, typically COUNT.\nSELECT COUNT(DISTINCT user_id)\nFROM trades;"
  },
  {
    "objectID": "posts/software/sql/notes.html#arithmetic",
    "href": "posts/software/sql/notes.html#arithmetic",
    "title": "SQL",
    "section": "3.5. Arithmetic",
    "text": "3.5. Arithmetic\nWe can use standard mathematical operations +, -, /, *, ^, %\nSELECT salary + bonus AS total_compensation\nFROM employees;\nWe have the modulus operator % which returns the remainder of a division. This is often helpful in problems where we need to find odd or even values.\nSELECT *\nFROM measurements\nWHERE measurement_num % 2 = 1\nThese operations follow the usual BODMAS rule (or PEMDAS if you’re an asshole)."
  },
  {
    "objectID": "posts/software/sql/notes.html#mathematical-functions",
    "href": "posts/software/sql/notes.html#mathematical-functions",
    "title": "SQL",
    "section": "3.6. Mathematical Functions",
    "text": "3.6. Mathematical Functions\nThe following built-in maths functions are useful:\n\nABS() - absolute value\nCEIL() - round up\nFLOOR() - round down\nROUND(column_name, N) - round to N decimal places\nPOWER(column_name, exponent) - equivalent to column_name ^ exponent\nMOD(column_name, divisor) - equivalent tocolumn_name % divisor`"
  },
  {
    "objectID": "posts/software/sql/notes.html#division",
    "href": "posts/software/sql/notes.html#division",
    "title": "SQL",
    "section": "3.7. Division",
    "text": "3.7. Division\nDivision is SQL can be deceptively tricky. Naively, we might think we just do col1 / col2, job done.\nBut in practice, we can get weird results depending on the data types of the numerator or denominator.\n\n\n\nInput\nSQL Output\nExpected\n\n\n\n\nSELECT 10/4\n2\n2.5\n\n\nSELECT 10/2\n5\n5\n\n\nSELECT 10/6\n1\n1.6666666667\n\n\nSELECT 10.0/4\n2.5000000000000000\n2.5\n\n\nSELECT 10/3.0\n3.3333333333333333\n3.333333333\n\n\n\nWe can coerce values to floats by:\n\nUsing the CAST(column_name AS FLOAT) function\nMultiplying * 1.0\nExplicitly using types with ::\n\nSELECT \n  CAST(10 AS DECIMAL)/4,\n  CAST(10 AS FLOAT)/4,\n  10 * 1.0 / 4\n  10::DECIMAL / 4"
  },
  {
    "objectID": "posts/software/sql/notes.html#nulls",
    "href": "posts/software/sql/notes.html#nulls",
    "title": "SQL",
    "section": "3.8. Nulls",
    "text": "3.8. Nulls\nA NULL value indicates the absence of a value. Missing data is different to data which is populated but empty, like a 0 or an empty string.\nWe can identify null and non-null values with IS NULL and IS NOT NULL.\nSELECT *\nFROM goodreads\nWHERE book_title IS NULL;\nThe COALESCE keyword allows us to pass multiple inputs and return the first non-null value. We can pass multiple columns, or a mix of columns and a hard-coded default value. This makes it useful to fill nulls. Think of it like the pandas fillna method.\nSELECT COALESCE(book_rating, 0)  -- fill NULL values with 0\nFROM goodreads;\nWe can also use the IFNULL keyword to fill null values.\nSELECT \n  book_title, \n  IFNULL(book_rating, 0) AS rated_books  - fill NULL values with 0\nFROM goodreads;\nIn the above examples, IFNULL and COALESCE are interchangeable. In general, use COALESCE when checking multiple columns, e.g. COALESCE(col1, col2, col3). If only checking one column, IFNULL is more concise.\nThe above examples are to replace null with values. We can do the opposite – conditionally replace values with nulls – using the NULLIF command. NULLIF(expr1, expr2) will return NULL if the two expressions are equal."
  },
  {
    "objectID": "posts/software/sql/notes.html#case",
    "href": "posts/software/sql/notes.html#case",
    "title": "SQL",
    "section": "3.9. CASE",
    "text": "3.9. CASE\nThe CASE statement is used to create new columns, categorize data, or perform calculations based on specified conditions.\nSELECT\n  column_1,\n  column_2, \n  CASE \n    WHEN condition_1 THEN result_1\n    WHEN condition_2 THEN result_2\n    ELSE result_3 -- If condition_1 and condition_2 are not met, return result_3 in ELSE clause\n  END AS column_3_name -- Give your new column an alias\nFROM table_1;  \nWe can also use CASE inside a WHERE clause to filter rows based on specific conditions.\nSELECT\n  column_1,\n  column_2\nFROM table_1\nWHERE CASE \n    WHEN condition_1 THEN result_1\n    WHEN condition_2 THEN result_2\n    ELSE result_3 -- If condition_1 and condition_2 are not met, return result_3 in ELSE clause\n  END; \nAs a concrete example of filtering, we may want to filter based on number of followers, where the threshold for followers depends on the platform\nSELECT \n  actor, \n  character, \n  platform\nFROM marvel_avengers\nWHERE \n  CASE \n    WHEN platform = 'Instagram' THEN followers &gt;= 500000\n    WHEN platform = 'Twitter' THEN followers &gt;= 200000\n    ELSE followers &gt;= 100000\n  END;\nWe can also use aggregate functions like COUNT, AVG, SUM around a CASE statement to only include rows which meet a certain criteria.\nSELECT\n  platform,\n  SUM(CASE \n    WHEN engagement_rate &gt;= 8.0 THEN followers\n    ELSE 0\n  END) AS high_engagement_followers_sum,\n  SUM(CASE \n    WHEN engagement_rate &lt; 8.0 THEN followers\n    ELSE 0\n  END) AS low_engagement_followers_sum\nFROM marvel_avengers\nGROUP BY platform;"
  },
  {
    "objectID": "posts/software/sql/notes.html#join",
    "href": "posts/software/sql/notes.html#join",
    "title": "SQL",
    "section": "3.10. JOIN",
    "text": "3.10. JOIN\nJoining multiple tables is the bread and butter of relational databases. We specify the tables to JOIN and the keys to join ON. We optionally define the type of join; this defaults to INNER if not specified.\nSELECT *\nFROM artists\nJOIN songs\n  ON artists.artist_id = songs.artist_id;\nThere are 4 types of join:\n\nINNER JOIN - Returns only the rows with matching values from both tables.\nLEFT JOIN - Returns all the rows from the left table and the matching rows from the right table. NULL values where there is no match in the right table.\nRIGHT JOIN - Returns all the rows from the right table and the matching rows from the left table. NULL values where there is no match in the left table.\nFULL OUTER JOIN - Returns all rows from either table. Where there is no match in either left or right, return a NULL value.\n\nWe can perform conditional joins where we filter the tables as we join them. This avoids the need to join two, potentially large tables, then filter the very large result after joining.\nThis example will filter the orders table before joining it.\nSELECT \n  g.book_title, \n  o.quantity\nFROM goodreads AS g\nINNER JOIN orders AS o \n  ON g.book_id = o.book_id\n    AND o.quantity &gt; 2;\nSimilarly, we can also use CASE statements in the JOIN to apply the CASE on the source table before joining, rather than on the potentially larger result set. This may be particularly helpful when we also want to filter on that CASE statement before joining."
  },
  {
    "objectID": "posts/software/sql/notes.html#datetimes",
    "href": "posts/software/sql/notes.html#datetimes",
    "title": "SQL",
    "section": "3.11. Datetimes",
    "text": "3.11. Datetimes\nDates and timestamps are handled as specific data types in SQL.\nThe following functions are useful for getting the current date, time or datetime: - CURRENT_DATE - CURRENT_TIME - CURRENT_TIMESTAMP or NOW()\nWe can use comparison operators =, !=, &gt;, &lt; to compare datetimes. Aggregate functions like MIN and MAX also work.\nSELECT *\nFROM messages\nWHERE sent_date &gt;= '2022-08-10 00:00:00';\nWe can extract parts of the date with either EXTRACT or DATE_PART. These are equivalent, and can be used to extract year, month, day, hour, minute.\nSELECT \n  EXTRACT(YEAR FROM sent_date) AS extracted_year,\n  DATE_PART('year', sent_date) AS part_year\nFROM messages;\nWe can truncate a datetime to a specified granularity with DATE_TRUNC.\nSELECT \n  DATE_TRUNC('day', sent_date) AS truncated_to_day\nFROM messages;\nWe can add and subtract datetimes using INTERVAL.\nSELECT \n  sent_date + INTERVAL '2 days' AS add_2days,\n  sent_date - INTERVAL '10 minutes' AS minus_10mins\nFROM messages;\nTimestamps can be converted to strings with a specified format using TO_CHAR.\nSELECT \n  TO_CHAR(sent_date, 'YYYY-MM-DD HH:MI:SS') AS formatted_iso8601\nFROM messages;\nThere are different formatting options for the string.\n\n\n\n\n\n\n\n\nFormat Name\nFormat\nExample\n\n\n\n\nISO 8601 Date and Time\n‘YYYY-MM-DD HH24:MI:SS’\n‘2023-08-27 14:30:00’\n\n\nDate and Time with 12-hour Format\n‘YYYY-MM-DD HH:MI:SS AM’\n‘2023-08-27 02:30:00 PM’\n\n\nLong Month Name, Day and Year\n‘Month DDth, YYYY’\n‘August 27th, 2023’\n\n\nShort Month Name, Day and Year\n‘Mon DD, YYYY’\n‘Aug 27, 2023’\n\n\nDay, Month, and Year\n‘DD Month YYYY’\n‘27 August 2023’\n\n\nDay of the Month\n‘Month’\n‘August’\n\n\nDay of the Week\n‘Day’\n‘Saturday’\n\n\n\nStrings can be converted to timestamps by casting. There are two equivalent syntaxes. For dates we can use ::DATE or TO_DATE(). For timestamps we can use ::TIMESTAMP or TO_TIMESTAMP().\nSELECT \n  sent_date::DATE AS casted_date,\n  TO_DATE('2023-08-27', 'YYYY-MM-DD') AS converted_to_date,\n  sent_date::TIMESTAMP AS casted_timestamp,\n  TO_TIMESTAMP('2023-08-27 10:30:00', 'YYYY-MM-DD HH:MI:SS') AS converted_to_timestamp\nFROM messages;"
  },
  {
    "objectID": "posts/software/sql/notes.html#cte-vs-subquery",
    "href": "posts/software/sql/notes.html#cte-vs-subquery",
    "title": "SQL",
    "section": "4.1. CTE vs Subquery",
    "text": "4.1. CTE vs Subquery\nA Common Table Expression (CTE) is like a query within a query using a WITH statement.\nA Subquery is a query within a query using parentheses.\n\n4.1.1. CTE\nWe declare CTEs at the beginning of the query, which can help break down more complex queries to improve readability.\n-- Start of a CTE\nWITH genre_revenue_cte AS (\n  SELECT\n    genre,\n    SUM(concert_revenue) AS total_revenue\n  FROM concerts\n  GROUP BY genre\n)\n-- End of a CTE\n\nSELECT\n  g.genre,\n  g.total_revenue,\n  AVG(c.concert_revenue) AS avg_concert_revenue\nFROM genre_revenue_cte AS g\nINNER JOIN concerts AS c \n  ON g.genre = c.genre\nWHERE c.concert_revenue &gt; g.total_revenue * 0.5\nGROUP BY g.genre, g.total_revenue;\nWe can reuse the same CTE result multiple times, which can avoid redundant calculations.\nThey also allow for recursive queries.\nWITH recursive_cte AS (\n  SELECT \n    employee_id, \n    name, \n    manager_id\n  FROM employees\n  WHERE manager_id = @manager_id\n  \n  UNION ALL\n  \n  SELECT \n    e.employee_id, \n    e.name, \n    e.manager_id\n  FROM employees AS e\n  INNER JOIN recursive_cte AS r -- The RECURSIVE CTE is utilized here within the main CTE.\n    ON e.ManagerID = r.employee_id\n)\n\nSELECT * \nFROM recursive_cte;\n\n\n4.1.2. Subquery\nSubqueries are generally favoured where we need the result from some other small query. Think of it like lambda functions compared to defining a function in python; if the inner query is small then a subquery is more readable.\nSELECT artist_name\nFROM concerts\nWHERE concert_revenue &gt; (\n  SELECT AVG(concert_revenue) FROM concerts);\nIn a similar vein, we can use subqueries to create and aggregate columns on the fly without sacrificing readability.\nSELECT \n  artist_name, \n  genre, \n  concert_revenue,\n  (SELECT AVG(concert_revenue) FROM concerts) AS avg_concert_revenue,\n  (SELECT MAX(concert_revenue) FROM concerts) AS max_concert_revenue\nFROM concerts;\nThey can be useful for filtering on conditions in another query.\nSELECT artist_name\nFROM concerts\nWHERE artist_id IN (\n  SELECT artist_id FROM concert_revenue WHERE concert_revenue &gt; 500000);\nCorrelated subqueries are when we want to query one table based on a dynamic result in another (or itself). For example, return fields for the highest-grossing concerts of each genre.\nSELECT \n  artist_name, \n  genre, \n  concert_revenue\nFROM concerts AS c1\nWHERE concert_revenue = (\n  SELECT MAX(concert_revenue)\n  FROM concerts AS c2\n  WHERE c1.genre = c2.genre\n);"
  },
  {
    "objectID": "posts/software/sql/notes.html#window",
    "href": "posts/software/sql/notes.html#window",
    "title": "SQL",
    "section": "4.2. WINDOW",
    "text": "4.2. WINDOW\nWe have already calculated aggregates over the whole table or certain groups.\nWindow functions allow us to create our own partitions, or “virtual windows”, and perform cumulative aggregates over these.\nFor example, if we want a cumulative total of revenue for each product type:\nSELECT\n  spend,\n   SUM(spend) OVER (\n     PARTITION BY product\n     ORDER BY transaction_date) AS running_total\n  FROM product_spend;\nThe PARTITION BY clause defines our virtual windows. We can partition by multiple fields. ORDER BY determines the ordering for a running total. SUM(spend) OVER defines what aggreagtion we perform per partition. The OVER keyword is required for window function.\nCommon aggregates are COUNT, SUM, AVG, MIN, MAX, FIRST_VALUE, LAST_VALUE.\nThe distinction between PARTITION BY vs GROUP BY can feel subtle. Think of it like .rolling() vs .groupby() in pandas.\n\nGROUP BY normally reduces the number of rows returned by grouping them up and calculating averages or sums for each group.\nPARTITION BY does not affect the number of rows returned, but it changes how a window function’s result is calculated."
  },
  {
    "objectID": "posts/software/sql/notes.html#ranking",
    "href": "posts/software/sql/notes.html#ranking",
    "title": "SQL",
    "section": "4.3. Ranking",
    "text": "4.3. Ranking\nThere are several ranking functions – RANK, DENSE_RANK, ROW_NUMBER – we can use to ranks rows based on specific criteria. This assigns numbers indicating the position of the data within a certain “window”.\nThe syntax for all of them follows the same pattern:\nSELECT \n  RANK() / DENSE_RANK() / ROW_NUMBER() OVER ( -- Compulsory expression\n    PARTITION BY partitioning_expression -- Optional expression\n    ORDER BY order_expression) -- Compulsory expression\nFROM table_name;\nThe differences between the ranking functions:\n\nROW_NUMBER(): Essentially numbers the rows in order.\nRANK(): Tied values are given the same rank, skipping subsequent ranks which leaves gaps in the sequence.\nDENSE_RANK(): Tied values are given the same rank, without skipping subsequent ranks so there are no gaps in the sequence."
  },
  {
    "objectID": "posts/software/sql/notes.html#leadlag",
    "href": "posts/software/sql/notes.html#leadlag",
    "title": "SQL",
    "section": "4.4. Lead/Lag",
    "text": "4.4. Lead/Lag\nThe time series window functions LAG and LEAD allow us to access data from rows before or after the current row.\nLEAD(column_name, offset) OVER (  -- Compulsory expression\n  PARTITION BY partition_column -- Optional expression\n  ORDER BY order_column) -- Compulsory expression\nThis can “feel” similar to the OFFSET function. The difference is OFFSET skips rows in the output, resulting in a different result set. LAG accesses previous rows within the same result set, allowing for row-wise comparisons within a window. OFFSET wouldn’t work in a window function."
  },
  {
    "objectID": "posts/software/sql/notes.html#self-joins",
    "href": "posts/software/sql/notes.html#self-joins",
    "title": "SQL",
    "section": "4.5. Self Joins",
    "text": "4.5. Self Joins\nWe can join a table to itself to match on the data within the table.\nAs an example, the query below will take each book in the table and give suggestions of other books in the same genre.\nSELECT\n  b1.genre,\n  b1.book_title AS current_book,\n  b2.book_title AS suggested_book\nFROM goodreads AS b1\nINNER JOIN goodreads AS b2\n  ON b1.genre = b2.genre\nWHERE b1.book_id != b2.book_id  -- Don't suggest the same book\nORDER BY b1.book_title;"
  },
  {
    "objectID": "posts/software/sql/notes.html#set-operations---union-intersect-except",
    "href": "posts/software/sql/notes.html#set-operations---union-intersect-except",
    "title": "SQL",
    "section": "4.6. Set operations - UNION, INTERSECT, EXCEPT",
    "text": "4.6. Set operations - UNION, INTERSECT, EXCEPT\nThe set operation UNION allows us to combine data vertically, i.e. appending columns.\nIn the same way that JOINs allow us to combine data horizontally, i.e. appending rows.\nFor all set operations – UNION, INTERSECT, EXCEPT – the number of columns, data types, and order of columns must match between the two SELECT statements.\n\n\n\nJOIN vs UNION. From datalemur.com\n\n\nThe UNION keyword gives all unique rows, UNION ALL retains duplicate rows.\nSELECT col1, col2\nFROM table1\nUNION ALL\nSELECT col1, col2\nFROM table2;\n\n\n\nUNION vs UNION ALL. From datalemur.com\n\n\nThe INTERSECT command gives the intersection between two sets, i.e. the common rows present in both.\nSELECT order_id\nFROM orders\nWHERE quantity &gt;= 2\nINTERSECT\nSELECT order_id\nFROM deliveries\nWHERE delivery_status = 'Delivered';\n\n\n\nINTERSECT. From datalemur.com\n\n\nEXCEPT gives all the unique rows in A that are not present in B.\nSELECT ingredient\nFROM recipe_1\nEXCEPT\nSELECT ingredient\nFROM recipe_2;\n\n\n\nEXCEPT. From datalemur.com"
  },
  {
    "objectID": "posts/software/sql/notes.html#sql-code-best-practices",
    "href": "posts/software/sql/notes.html#sql-code-best-practices",
    "title": "SQL",
    "section": "4.7. SQL Code Best Practices",
    "text": "4.7. SQL Code Best Practices\n\nUPPERCASE for keywords:\nlowercase or snake_case for names\nDescriptive and concise aliases\nConsistent formatting and indentation\nAvoid SELECT *, explicitly specify columns\nUse JOINs explicitly for clarity\n\nRather than relying on FROM table1, table2 syntax\nSpecify the type of join (INNER, LEFT, RIGHT, OUTER)\n\nFormat dates consistently - YYYY-MM-DD\nComment wisely\n\nUse -- for inline comments and /* ... */ for multiline"
  },
  {
    "objectID": "posts/software/sql/notes.html#execution-order",
    "href": "posts/software/sql/notes.html#execution-order",
    "title": "SQL",
    "section": "4.8. Execution Order",
    "text": "4.8. Execution Order\nThe query engine for the database handles the order of execution of commands in the query. This is helpful to understand what happens in the database under the hood so we can optimise our queries better.\n\n\n\n\n\n\n\n\nClause\nOrder\nDescription\n\n\n\n\nFROM / JOIN\n1\nThe query begins with the FROM clause, where the database identifies the tables involved and accesses the necessary data.\n\n\nWHERE\n2\nThe database applies the conditions specified in the WHERE clause to filter the data retrieved from the tables in the FROM clause.\n\n\nGROUP BY\n3\nIf a GROUP BY clause is present, the data is grouped based on the specified columns, and aggregation functions (such as SUM(), AVG(), COUNT()) are applied to each group.\n\n\nHAVING\n4\nThe HAVING clause filters the aggregated data based on specified conditions.\n\n\nSELECT / DISTINCT\n5\nThe SELECT clause defines the columns to be included in the final result set.\n\n\nORDER BY\n6\nIf an ORDER BY clause is used, the result set is sorted according to the specified columns.\n\n\nLIMIT / OFFSET\n7\nIf LIMIT or OFFSET clause is present, the result set is restricted to the specified number of rows and optionally offset by a certain number of rows."
  },
  {
    "objectID": "posts/software/sql/notes.html#string-functions",
    "href": "posts/software/sql/notes.html#string-functions",
    "title": "SQL",
    "section": "4.9. String Functions",
    "text": "4.9. String Functions\nString functions are useful for cleaning and manipulating text data.\nWe can change case with UPPER and LOWER.\nSELECT \n  UPPER(text_col) AS upper_case_text,\n  LOWER(text_col) AS lower_case_text\nFROM table_name;\nWe can extract the first or last N characters of a string with LEFT(col, N) and RIGHT(col, N). If N is greater than the length of the string, it will return the whole string.\nSELECT \n  LEFT(text_col, 5) AS left_substring,\n  RIGHT(text_col, 5) AS right_substring\nFROM table_name;\nWe can calculate the LENGTH of a string.\nSELECT \n  LENGTH(text_col) AS text_length\nFROM table_name;\nWe can find the index of a substring within a larger string using POSITION(substring IN string). This returns 0 if the substring is not found.\nSELECT POSITION('substring' IN text_col) AS position_result\nFROM table_name;\nWhite spaces (and other characters) can be trimmed from the left, right or both sides using LTRIM, RTRIM, BTRIM. They can each take an optional second argument specifying the character to trim. The TRIM command removes spaces from both sides of the string; essentially a shorthand for BTRIM(text_col, ' ').\nSELECT \n  TRIM('     Spiderman') AS full_trim,\n  LTRIM('Iron Man', 'Iron ') AS left_trim,\n  RTRIM('Scarlet Witch', ' Witch') AS right_trim,\n  BTRIM('   Falcon   ', ' ') AS combination_trim1,\n  BTRIM('...Iron Man...', '.') AS combination_trim2\nFROM marvel_avengers;\nWe can combine multiple string fields with CONCAT.\nSELECT \n    CONCAT(col1, col2)\nFROM table_name;\nWe can also “concatenate with separator” using CONCAT_WS. So these two queries would be equivalent.\nSELECT \n    CONCAT(col1, '-', col2),\n    CONCAT_WS('-', col1,col2)\nFROM table_name;\nWe can extract a SUBSTRING from a larger string using the following syntax: SUBSTRING(string, start_position, length [optional]). The start_position argument can be a negative index, meaning it counts from the end of the string. The length argument is optional. If not provided it will return the rest of the string.\nSELECT \n    SUBSTRING(col1, 2),  -- From second chracter onwards\n    SUBSTRING(col1, 2, 5)  -- 5 characters, starting from the 2nd\nFROM table_name;\nWe can split text into segments based on a specific delimiter using SPLIT_PART(string, delimiter, part_number).\nSELECT \n  SPLIT_PART('Spider-Man', '-', 1) AS split_part_1, -- Extracting the first part: 'Spider'\n  SPLIT_PART('Spider-Man', '-', 2) AS split_part_2, -- Extracting the second part: 'Man'\n  SPLIT_PART('Black Widow', ' ', -1) AS split_part_3 -- Extracting the last part: 'Widow'\nFROM marvel_avengers;"
  },
  {
    "objectID": "posts/ml/timeseries/timeseries.html",
    "href": "posts/ml/timeseries/timeseries.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "Time series data has certain properties that make it different from other tabular or sequential data:\n\nOrdering is important: The order of data points is important so you need to be wary of information leakage. For example, unless you really know what you’re doing and have a good reason, you probably shouldn’t shuffle your train/test split.\nLook-ahead bias: Related to the above. We often want to forecast future data points, so we need to make sure our model is not inadvertently looking ahead.\nIrregular sampling: The data is not always at regular intervals. for example, tick-level financial data or heart beats in medical data.\nInformative sampling: The presence/timing of a sample contains information in and of itself. For example, more ticks in a short time window indicates more trading activity, or more heart beats in a time period indicates unusual activity. Resampling to a regular interval risks losing this information.\n\n\n\n\nA time series may exhibit a trend over time. That is to say, it’s rolling average is monotonically increasing/decreasing.\nWe can model this with a simple linear relationship w.r.t. time \\(t\\): \\[\ntarget = a t + b\n\\]\nIf the trend is non-linear, we can transform the time variable so we can still apply linear models. For example, if we think the trend is quadratic with time, we can pass \\(t\\) and \\(t^2\\) as independent variables to a linear model: \\[\ntarget = a t^2 + b t + c\n\\]\nWe may want to split the trend and residual components of the data and model them separately. If the residuals are stations (more on this later) then there are more models that would be applicable.\nA moving average term can be useful to eyeball changes in trend.\n\nplot_ma_df = df[['BTCUSD']].copy()\nplot_ma_df['100_day_MA'] = plot_ma_df['BTCUSD'].rolling(100).mean()\n\nplot_ma_df.plot()\n\n                                                \n\n\n\n\n\nSeasonality is when there are regular, periodic changes in the mean of a time series. They often happen at “human-interpretable” intervals, e.g. daily, weekly, monthly, etc.\nA seasonal plot can help identify such seasonality. If we suspect day-of-week seasonality, we can plot the day of week vs target value to see if there is a common behaviour.\nThis time series doesn’t actually exhibit any strong seasonality, but let’s see how we’d check.\n\nplot_seasonal_df = df[['BTCUSD']].copy()\nplot_seasonal_df['day_of_week'] = plot_seasonal_df.index.dayofweek\n\nplot_seasonal_df.tail(50).set_index('day_of_week').plot()\n\n                                                \n\n\nSeasonal indicators are binary features that represent the seasonality level of interest.\nFor example, if we believed there was weekly seasonality, we could one-hot encode each day of the week as a feature.\n\nplot_seasonal_df['Monday'] = (plot_seasonal_df['day_of_week'] == 0) * 1.\nplot_seasonal_df['Tuesday'] = (plot_seasonal_df['day_of_week'] == 1) * 1.\nplot_seasonal_df['Wednesday'] = (plot_seasonal_df['day_of_week'] == 2) * 1.\nplot_seasonal_df['Thursday'] = (plot_seasonal_df['day_of_week'] == 3) * 1.\nplot_seasonal_df['Friday'] = (plot_seasonal_df['day_of_week'] == 4) * 1.\nplot_seasonal_df['Saturday'] = (plot_seasonal_df['day_of_week'] == 5) * 1.\n\nplot_seasonal_df\n\n\n\n\n\n\n\n\nBTCUSD\nday_of_week\nMonday\nTuesday\nWednesday\nThursday\nFriday\nSaturday\n\n\ntimestamp\n\n\n\n\n\n\n\n\n\n\n\n\n2018-01-01\n13657.200195\n0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2018-01-02\n14982.099609\n1\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n2018-01-03\n15201.000000\n2\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n2018-01-04\n15599.200195\n3\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n2018-01-05\n17429.500000\n4\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-07-05\n30514.166016\n2\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n2023-07-06\n29909.337891\n3\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n2023-07-07\n30342.265625\n4\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n2023-07-08\n30292.541016\n5\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n2023-07-09\n30280.958984\n6\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n2016 rows × 8 columns\n\n\n\nWe can decompose a timeseries into trend + seasonal + residual components.\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndecomp = seasonal_decompose(df['BTCUSD'], model='additive', filt=None, period=None, two_sided=False, extrapolate_trend=0)\nfig = decomp.plot()\n\n\n\n\n\n\n\n\nFourier analysis can be useful in determining frequencies of seasonality. More on this later.\nIn brief, we can plot the periodogram to determine the strength of different frequencies.\n\n# From https://www.kaggle.com/code/ryanholbrook/seasonality\ndef plot_periodogram(ts, detrend='linear', ax=None):\n    from scipy.signal import periodogram\n    fs = pd.Timedelta(\"365D\") / pd.Timedelta(\"1D\")\n    freqencies, spectrum = periodogram(\n        ts,\n        fs=fs,\n        detrend=detrend,\n        window=\"boxcar\",\n        scaling='spectrum',\n    )\n    if ax is None:\n        _, ax = plt.subplots()\n    ax.step(freqencies, spectrum, color=\"purple\")\n    ax.set_xscale(\"log\")\n    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104])\n    ax.set_xticklabels(\n        [\n            \"Annual (1)\",\n            \"Semiannual (2)\",\n            \"Quarterly (4)\",\n            \"Bimonthly (6)\",\n            \"Monthly (12)\",\n            \"Biweekly (26)\",\n            \"Weekly (52)\",\n            \"Semiweekly (104)\",\n        ],\n        rotation=30,\n    )\n    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n    ax.set_ylabel(\"Variance\")\n    ax.set_title(\"Periodogram\")\n    return ax\n\n\nplot_periodogram(df['BTCUSD'])\n\n\n\n\n\n\n\n\n\n\n\n\nA stationary time series is one whose properties do not depend on the time at which the series is observed\n\nFrom Forecasting: Principles and Practice\nIn other words, the mean and variance do not change over time.\nIn the context of financial time series, it is often the case that price is non-stationary, but returns (the lag-1 difference) is stationary.\nThe Augmented Dickey-Fuller test provides a test statistic to quantify stationarity. \nUsing the BTCUSD time series as an example, the price series is definitely not stationary. We can see this from a plot, but the ADF test corroborates this with a p-value of 0.58.\n\nfrom statsmodels.tsa.stattools import adfuller\n\nadf_test = adfuller(df['BTCUSD'])\nadf_results = {\n    \"ADF Statistic\": adf_test[0],\n    \"p-value\": adf_test[1],\n    \"Critical Values\": adf_test[4],\n}\n\nadf_results\n\n{'ADF Statistic': -1.4058608330365159,\n 'p-value': 0.5794631585252685,\n 'Critical Values': {'1%': -3.4336386745240652,\n  '5%': -2.8629927557359443,\n  '10%': -2.5675433856598793}}\n\n\nNext we take the differences. This is stationary, with a tiny p-value.\n\nbtc_rets = df['BTCUSD'].diff().dropna()\n\nadf_test = adfuller(btc_rets)\nadf_results = {\n    \"ADF Statistic\": adf_test[0],\n    \"p-value\": adf_test[1],\n    \"Critical Values\": adf_test[4],\n}\n\nadf_results\n\n{'ADF Statistic': -7.742323245600769,\n 'p-value': 1.0538877703747789e-11,\n 'Critical Values': {'1%': -3.433643643742798,\n  '5%': -2.862994949652858,\n  '10%': -2.5675445538118042}}"
  },
  {
    "objectID": "posts/ml/timeseries/timeseries.html#considerations-for-time-series-data",
    "href": "posts/ml/timeseries/timeseries.html#considerations-for-time-series-data",
    "title": "Time Series Analysis",
    "section": "",
    "text": "Time series data has certain properties that make it different from other tabular or sequential data:\n\nOrdering is important: The order of data points is important so you need to be wary of information leakage. For example, unless you really know what you’re doing and have a good reason, you probably shouldn’t shuffle your train/test split.\nLook-ahead bias: Related to the above. We often want to forecast future data points, so we need to make sure our model is not inadvertently looking ahead.\nIrregular sampling: The data is not always at regular intervals. for example, tick-level financial data or heart beats in medical data.\nInformative sampling: The presence/timing of a sample contains information in and of itself. For example, more ticks in a short time window indicates more trading activity, or more heart beats in a time period indicates unusual activity. Resampling to a regular interval risks losing this information."
  },
  {
    "objectID": "posts/ml/timeseries/timeseries.html#trend",
    "href": "posts/ml/timeseries/timeseries.html#trend",
    "title": "Time Series Analysis",
    "section": "",
    "text": "A time series may exhibit a trend over time. That is to say, it’s rolling average is monotonically increasing/decreasing.\nWe can model this with a simple linear relationship w.r.t. time \\(t\\): \\[\ntarget = a t + b\n\\]\nIf the trend is non-linear, we can transform the time variable so we can still apply linear models. For example, if we think the trend is quadratic with time, we can pass \\(t\\) and \\(t^2\\) as independent variables to a linear model: \\[\ntarget = a t^2 + b t + c\n\\]\nWe may want to split the trend and residual components of the data and model them separately. If the residuals are stations (more on this later) then there are more models that would be applicable.\nA moving average term can be useful to eyeball changes in trend.\n\nplot_ma_df = df[['BTCUSD']].copy()\nplot_ma_df['100_day_MA'] = plot_ma_df['BTCUSD'].rolling(100).mean()\n\nplot_ma_df.plot()"
  },
  {
    "objectID": "posts/ml/timeseries/timeseries.html#seasonality",
    "href": "posts/ml/timeseries/timeseries.html#seasonality",
    "title": "Time Series Analysis",
    "section": "",
    "text": "Seasonality is when there are regular, periodic changes in the mean of a time series. They often happen at “human-interpretable” intervals, e.g. daily, weekly, monthly, etc.\nA seasonal plot can help identify such seasonality. If we suspect day-of-week seasonality, we can plot the day of week vs target value to see if there is a common behaviour.\nThis time series doesn’t actually exhibit any strong seasonality, but let’s see how we’d check.\n\nplot_seasonal_df = df[['BTCUSD']].copy()\nplot_seasonal_df['day_of_week'] = plot_seasonal_df.index.dayofweek\n\nplot_seasonal_df.tail(50).set_index('day_of_week').plot()\n\n                                                \n\n\nSeasonal indicators are binary features that represent the seasonality level of interest.\nFor example, if we believed there was weekly seasonality, we could one-hot encode each day of the week as a feature.\n\nplot_seasonal_df['Monday'] = (plot_seasonal_df['day_of_week'] == 0) * 1.\nplot_seasonal_df['Tuesday'] = (plot_seasonal_df['day_of_week'] == 1) * 1.\nplot_seasonal_df['Wednesday'] = (plot_seasonal_df['day_of_week'] == 2) * 1.\nplot_seasonal_df['Thursday'] = (plot_seasonal_df['day_of_week'] == 3) * 1.\nplot_seasonal_df['Friday'] = (plot_seasonal_df['day_of_week'] == 4) * 1.\nplot_seasonal_df['Saturday'] = (plot_seasonal_df['day_of_week'] == 5) * 1.\n\nplot_seasonal_df\n\n\n\n\n\n\n\n\nBTCUSD\nday_of_week\nMonday\nTuesday\nWednesday\nThursday\nFriday\nSaturday\n\n\ntimestamp\n\n\n\n\n\n\n\n\n\n\n\n\n2018-01-01\n13657.200195\n0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2018-01-02\n14982.099609\n1\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n2018-01-03\n15201.000000\n2\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n2018-01-04\n15599.200195\n3\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n2018-01-05\n17429.500000\n4\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-07-05\n30514.166016\n2\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n2023-07-06\n29909.337891\n3\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n2023-07-07\n30342.265625\n4\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n2023-07-08\n30292.541016\n5\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n2023-07-09\n30280.958984\n6\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n2016 rows × 8 columns\n\n\n\nWe can decompose a timeseries into trend + seasonal + residual components.\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndecomp = seasonal_decompose(df['BTCUSD'], model='additive', filt=None, period=None, two_sided=False, extrapolate_trend=0)\nfig = decomp.plot()\n\n\n\n\n\n\n\n\nFourier analysis can be useful in determining frequencies of seasonality. More on this later.\nIn brief, we can plot the periodogram to determine the strength of different frequencies.\n\n# From https://www.kaggle.com/code/ryanholbrook/seasonality\ndef plot_periodogram(ts, detrend='linear', ax=None):\n    from scipy.signal import periodogram\n    fs = pd.Timedelta(\"365D\") / pd.Timedelta(\"1D\")\n    freqencies, spectrum = periodogram(\n        ts,\n        fs=fs,\n        detrend=detrend,\n        window=\"boxcar\",\n        scaling='spectrum',\n    )\n    if ax is None:\n        _, ax = plt.subplots()\n    ax.step(freqencies, spectrum, color=\"purple\")\n    ax.set_xscale(\"log\")\n    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104])\n    ax.set_xticklabels(\n        [\n            \"Annual (1)\",\n            \"Semiannual (2)\",\n            \"Quarterly (4)\",\n            \"Bimonthly (6)\",\n            \"Monthly (12)\",\n            \"Biweekly (26)\",\n            \"Weekly (52)\",\n            \"Semiweekly (104)\",\n        ],\n        rotation=30,\n    )\n    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n    ax.set_ylabel(\"Variance\")\n    ax.set_title(\"Periodogram\")\n    return ax\n\n\nplot_periodogram(df['BTCUSD'])"
  },
  {
    "objectID": "posts/ml/timeseries/timeseries.html#stationarity",
    "href": "posts/ml/timeseries/timeseries.html#stationarity",
    "title": "Time Series Analysis",
    "section": "",
    "text": "A stationary time series is one whose properties do not depend on the time at which the series is observed\n\nFrom Forecasting: Principles and Practice\nIn other words, the mean and variance do not change over time.\nIn the context of financial time series, it is often the case that price is non-stationary, but returns (the lag-1 difference) is stationary.\nThe Augmented Dickey-Fuller test provides a test statistic to quantify stationarity. \nUsing the BTCUSD time series as an example, the price series is definitely not stationary. We can see this from a plot, but the ADF test corroborates this with a p-value of 0.58.\n\nfrom statsmodels.tsa.stattools import adfuller\n\nadf_test = adfuller(df['BTCUSD'])\nadf_results = {\n    \"ADF Statistic\": adf_test[0],\n    \"p-value\": adf_test[1],\n    \"Critical Values\": adf_test[4],\n}\n\nadf_results\n\n{'ADF Statistic': -1.4058608330365159,\n 'p-value': 0.5794631585252685,\n 'Critical Values': {'1%': -3.4336386745240652,\n  '5%': -2.8629927557359443,\n  '10%': -2.5675433856598793}}\n\n\nNext we take the differences. This is stationary, with a tiny p-value.\n\nbtc_rets = df['BTCUSD'].diff().dropna()\n\nadf_test = adfuller(btc_rets)\nadf_results = {\n    \"ADF Statistic\": adf_test[0],\n    \"p-value\": adf_test[1],\n    \"Critical Values\": adf_test[4],\n}\n\nadf_results\n\n{'ADF Statistic': -7.742323245600769,\n 'p-value': 1.0538877703747789e-11,\n 'Critical Values': {'1%': -3.433643643742798,\n  '5%': -2.862994949652858,\n  '10%': -2.5675445538118042}}"
  },
  {
    "objectID": "posts/ml/graph_ml/part_1_2/graph_algorithms.html",
    "href": "posts/ml/graph_ml/part_1_2/graph_algorithms.html",
    "title": "Graph ML: Graph Algorithms",
    "section": "",
    "text": "The following are standard graph algorithms."
  },
  {
    "objectID": "posts/ml/graph_ml/part_1_2/graph_algorithms.html#graph-search",
    "href": "posts/ml/graph_ml/part_1_2/graph_algorithms.html#graph-search",
    "title": "Graph ML: Graph Algorithms",
    "section": "1. Graph Search",
    "text": "1. Graph Search\nWe are searching for a particular vertex in the graph.\nFor DFS and BFS, we need to iterate over each of the \\(V\\) nodes and each of its edges \\(E\\). We visit the edge from both sides, i.e. from each of the nodes attached to it, so we perform \\(V + 2E\\) steps.\nThe complexity is therefore \\(O(V+ E)\\).\n\n1.1. Depth-First Search (DFS)\nStart at the root node and go as deep as possible before backtracking.\nDFS is a recursive algorithm where we keep track of the nodes visited so far.\nIt is similar to the binary tree traversal algorithm, which shouldn’t be surprising since a tree is a type of graph and we want to traverse it to find our target node.\nSteps:\n\nInitialise: Start at any (random) node.\nVisit the node: Add the current node to the visited_nodes hash table.\nRecursively visit neighbors: Iterate through the current node’s neighbors. If the neighbor has already been visited, ignore it. Otherwise, recursively call DFS on the unvisited neighbor, i.e. go back to Step 2.\n\nDFS can be used to find cycles, because a node will be visited twice during traversal if and only if a cycle exists.\n\n\n1.2. Breadth-First Search (BFS)\nStart at the root node and explore all neighbors before going deeper.\nBFS does not rely on recursion, instead using a queue to track the execution order.\nSteps:\n\nInitialise: Start at any (random) node.\nVisit the node: Add the current node to the visited_nodes hash table and to the node_queue.\nIterate through the queue: Run a while-loop to iterate while the queue is populated.\nProcess the queue: Pop the first node from node_queue. Iterate over its neighbors. If the neighbor has been visited already, ignore it. Otherwise, visit the node - add it to the visited_nodes hash table and to the node_queue. Repeat until the queue is empty.\n\n\n\n1.3. A* Algorithm\n\n\n1.4. Bidirectional Search"
  },
  {
    "objectID": "posts/ml/graph_ml/part_1_2/graph_algorithms.html#shortest-path-algorithms",
    "href": "posts/ml/graph_ml/part_1_2/graph_algorithms.html#shortest-path-algorithms",
    "title": "Graph ML: Graph Algorithms",
    "section": "2. Shortest Path Algorithms",
    "text": "2. Shortest Path Algorithms\nWe are trying to find the shortest path between two nodes.\n\n2.1. Dijkstra’s Algorithm\nDijkstra’s algorithm maintains\n\nA hash map shortest_distances containing shortest known distances from the starting node to each node visited.\nA hash map shortest_path_previous_node tracking the previous node visited on the shortest knonw path to a node.\n\nA nice bonus is that it ends up finding the shortest path from the source node to all other nodes, not just the target node we care about.\nSteps:\n\nInitialise: Start at the source node and make it the current_node.\nCheck the immediate neighbours: If the distance to a neighbor from the source node is shorter than the current known shortest_distances, then update shortest_distances and shortest_path_previous_node.\nVisit the new node closest to source: Visit whichever unvisited node has the shortest known distance. Make it the current_node and repeat Step 2."
  },
  {
    "objectID": "posts/ml/graph_ml/part_1_2/graph_algorithms.html#minimum-spanning-tree",
    "href": "posts/ml/graph_ml/part_1_2/graph_algorithms.html#minimum-spanning-tree",
    "title": "Graph ML: Graph Algorithms",
    "section": "3. Minimum Spanning Tree",
    "text": "3. Minimum Spanning Tree\n\n3.1. Kruskal’s Algorithm\n\n\n3.2. Prim’s Algorithm"
  },
  {
    "objectID": "posts/ml/graph_ml/part_1_2/graph_algorithms.html#further-topics-in-graph-theory",
    "href": "posts/ml/graph_ml/part_1_2/graph_algorithms.html#further-topics-in-graph-theory",
    "title": "Graph ML: Graph Algorithms",
    "section": "4. Further Topics in Graph Theory",
    "text": "4. Further Topics in Graph Theory\n\nTarjan’s strongly connected components algorithm\nTopological sort\nFloyd-Warshall algorithms\nBellman-Ford algorithm\nGraph coloring\nMin-cut max-flow"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson3/chapter3.html",
    "href": "posts/ml/generative_deep_learning/lesson3/chapter3.html",
    "title": "Generative AI: VAEs",
    "section": "",
    "text": "Notes on VAEs.\n\n\n\n\n\n\nStory Time\n\n\n\nImagine an infinite wardrobe organised by “type” of clothing.\nShoes would be close together, but formal shoes might be closer to the suits and trainers closer to the sports gear. Shirts and t-shirts would be close together. Coats might be nearby; the shirt-&gt;coat vector applied to t-shirts might lead you to “invent” gilets.\nThis encapsulates the idea of using a lower dimensional (2D in this case) latent space to encode the representation of more complex objects.\nWe could sample from some of the empty spaces to invent new hybrids of clothing. This generative step is decoding the latent space.\n\n\n\n\nThe idea of autoencoders (read: self-encoders) is that they learn to simplify the input then reconstruct it; the input and target output are the same.\n\nThe encoder learns to compress high-dimensional input data into a lower dimensional representation called the embedding.\nThe decoder takes an embedding and recreates a higher-dimensional image. This should be an accurate reconstruction of the input.\n\nThis can be used as a generative model because we can the sample and decode new points from the latent space to generate novel outputs. The goal of training an autoencoder is to learn a meaningful embedding \\(z\\).\n\n\n\n\n\nflowchart LR\n\n  A(Encoder) --&gt; B(z)\n  B(z) --&gt; c(Decoder)\n\n\n\n\n\n\nThis also makes autoencoders useful as denoising models, because the embedding should retain the salient information but “lose” the noise.\n\n\n\nWe will implement an autoencoder to learn lower-dimensional embeddings for the fashion MNIST data set.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nimport tensorflow as tf\nfrom tensorflow.keras import (\n    layers,\n    models,\n    datasets,\n    callbacks,\n    losses,\n    optimizers,\n    metrics,\n)\n\n\n# Parameters\nIMAGE_SIZE = 32\nCHANNELS = 1\nBATCH_SIZE = 100\nBUFFER_SIZE = 1000\nVALIDATION_SPLIT = 0.2\nEMBEDDING_DIM = 2\nEPOCHS = 3\n\n\n\n\nScale the pixel values and reshape the images.\n\n\nCode\n(x_train, y_train), (x_test, y_test) = datasets.fashion_mnist.load_data()\n\ndef preprocess(images):\n    images = images.astype(\"float32\") / 255.0\n    images = np.pad(images, ((0, 0), (2, 2), (2, 2)), constant_values=0.0)\n    images = np.expand_dims(images, -1)\n    return images\n\nx_train = preprocess(x_train)\nx_test = preprocess(x_test)\n\n\nWe can see an example from our training set:\n\n\nCode\nplt.imshow(x_train[0])\n\n\n\n\n\n\n\n\n\n\n\n\nThe encoder compresses the dimensionality on the input to a smaller embedding dimension.\n\n\nCode\n# Input\nencoder_input = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS),name=\"encoder_input\")\n\n# Conv layers\nx = layers.Conv2D(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(encoder_input)\nx = layers.Conv2D(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2D(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\npre_flatten_shape = tf.keras.backend.int_shape(x)[1:]  # Used by the decoder later\n\n# Output\nx = layers.Flatten()(x)\nencoder_output = layers.Dense(EMBEDDING_DIM, name=\"encoder_output\")(x)\n\n# Model\nencoder = models.Model(encoder_input, encoder_output)\nencoder.summary()\n\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n encoder_input (InputLayer)  [(None, 32, 32, 1)]       0         \n                                                                 \n conv2d (Conv2D)             (None, 16, 16, 32)        320       \n                                                                 \n conv2d_1 (Conv2D)           (None, 8, 8, 64)          18496     \n                                                                 \n conv2d_2 (Conv2D)           (None, 4, 4, 128)         73856     \n                                                                 \n flatten (Flatten)           (None, 2048)              0         \n                                                                 \n encoder_output (Dense)      (None, 2)                 4098      \n                                                                 \n=================================================================\nTotal params: 96770 (378.01 KB)\nTrainable params: 96770 (378.01 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\nThe decoder reconstructs the original image from the embedding.\n\n\nIn a standard convolutional layer, if we have stride=2 it will half the image size.\nIn a convolutional transpose layer, we are increasing the image size. The stride parameter determines the amount of zero padding to add between each pixel. A kernel is then applied to this “internally padded” image to expand the image size.\n\n\nCode\n# Input\ndecoder_input = layers.Input(shape=(EMBEDDING_DIM,),name=\"decoder_input\")\n\n# Reshape the input using the pre-flattening shape from the encoder\nx = layers.Dense(np.prod(pre_flatten_shape))(decoder_input)\nx = layers.Reshape(pre_flatten_shape)(x)\n\n# Scale up the image back to its original size. These are the reverse of the conv layers applied in the encoder.\nx = layers.Conv2DTranspose(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2DTranspose(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n\n# Output\ndecoder_output = layers.Conv2D(\n    CHANNELS,\n    (3, 3),\n    strides=1,\n    activation='sigmoid',\n    padding=\"same\",\n    name=\"decoder_output\",\n)(x)\n\n# Model\ndecoder = models.Model(decoder_input, decoder_output)\ndecoder.summary()\n\n\nModel: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n decoder_input (InputLayer)  [(None, 2)]               0         \n                                                                 \n dense (Dense)               (None, 2048)              6144      \n                                                                 \n reshape (Reshape)           (None, 4, 4, 128)         0         \n                                                                 \n conv2d_transpose (Conv2DTr  (None, 8, 8, 128)         147584    \n anspose)                                                        \n                                                                 \n conv2d_transpose_1 (Conv2D  (None, 16, 16, 64)        73792     \n Transpose)                                                      \n                                                                 \n conv2d_transpose_2 (Conv2D  (None, 32, 32, 32)        18464     \n Transpose)                                                      \n                                                                 \n decoder_output (Conv2D)     (None, 32, 32, 1)         289       \n                                                                 \n=================================================================\nTotal params: 246273 (962.00 KB)\nTrainable params: 246273 (962.00 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\n\nCombine the encoder and decoder into a single model.\n\n\nCode\nautoencoder = models.Model(encoder_input, decoder(encoder_output))\nautoencoder.summary()\n\n\nModel: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n encoder_input (InputLayer)  [(None, 32, 32, 1)]       0         \n                                                                 \n conv2d (Conv2D)             (None, 16, 16, 32)        320       \n                                                                 \n conv2d_1 (Conv2D)           (None, 8, 8, 64)          18496     \n                                                                 \n conv2d_2 (Conv2D)           (None, 4, 4, 128)         73856     \n                                                                 \n flatten (Flatten)           (None, 2048)              0         \n                                                                 \n encoder_output (Dense)      (None, 2)                 4098      \n                                                                 \n model_1 (Functional)        (None, 32, 32, 1)         246273    \n                                                                 \n=================================================================\nTotal params: 343043 (1.31 MB)\nTrainable params: 343043 (1.31 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\nThe autoencoder is trained with the source images as both input and target output.\nThe loss function is usually chosen as either RMSE or binary cross-entropy between pixels of original image vs reconstruction.\n\n\nCode\nautoencoder.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\nautoencoder.fit(\n    x_train,\n    x_train,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    validation_data=(x_test, x_test)\n)\n\n\nEpoch 1/3\n600/600 [==============================] - 35s 58ms/step - loss: 0.2910 - val_loss: 0.2610\nEpoch 2/3\n600/600 [==============================] - 36s 60ms/step - loss: 0.2569 - val_loss: 0.2561\nEpoch 3/3\n600/600 [==============================] - 34s 57ms/step - loss: 0.2536 - val_loss: 0.2540\n\n\n&lt;keras.src.callbacks.History at 0x156e4a310&gt;\n\n\n\n\n\n\nWe can use our trained autoencoder to:\n\nReconstruct images\nAnalyse embeddings\nGenerate new images\n\n\n\nReconstruct a sample of test images using the autoencoder.\nThe reconstruction isn’t perfect; some information is lost when reducing down to just 2 dimensions. But it does a surprisingly good job of compressing 32x32 pixel values into just 2 embedding values.\n\n\nCode\nNUM_IMAGES_TO_RECONSTRUCT = 5000\nexample_images = x_test[:NUM_IMAGES_TO_RECONSTRUCT]\nexample_labels = y_test[:NUM_IMAGES_TO_RECONSTRUCT]\n\npredictions = autoencoder.predict(example_images)\n\n\n  7/157 [&gt;.............................] - ETA: 1s 157/157 [==============================] - 1s 8ms/step\n\n\nOriginal images:\n\n\nCode\ndef plot_sample_images(images, n=10, size=(20, 3), cmap=\"gray_r\"):\n    plt.figure(figsize=size)\n    for i in range(n):\n        _ = plt.subplot(1, n, i + 1)\n        plt.imshow(images[i].astype(\"float32\"), cmap=cmap)\n        plt.axis(\"off\")\n    \n    plt.show()\n\nplot_sample_images(example_images)\n\n\n\n\n\n\n\n\n\nReconstructed images:\n\n\nCode\nplot_sample_images(predictions)\n\n\n\n\n\n\n\n\n\n\n\n\nEach of the images above has been encoded as a 2-dimensional embedding.\nWe can look at these embeddings to gain some insight into how the autoencoder works.\nThe embedding vectors for our sample images above:\n\n\nCode\n# Encode the example images\nembeddings = encoder.predict(example_images)\nprint(embeddings[:10])\n\n\n102/157 [==================&gt;...........] - ETA: 0s157/157 [==============================] - 0s 2ms/step\n[[ 2.2441912  -2.711683  ]\n [ 6.1558456   6.0202003 ]\n [-3.787192    7.3368516 ]\n [-2.5938551   4.2098355 ]\n [ 3.8645594   2.7229536 ]\n [-2.0130231   6.0485506 ]\n [ 1.2749226   2.1347647 ]\n [ 2.8239484   2.898773  ]\n [-0.48542604 -1.0869933 ]\n [ 0.30643728 -2.6099105 ]]\n\n\nWe can plot the 2D latent space, colouring each point by its label. This shows how similar items are clustered together in latent space.\nThis is impressive! Remember, we never showed the model the labels when training, so it has learned to cluster images that look alike.\n\n\nCode\n# Colour the embeddings by their label\nexample_labels = y_test[:NUM_IMAGES_TO_RECONSTRUCT]\n\n# Plot the latent space\nfigsize = 8\nplt.figure(figsize=(figsize, figsize))\nplt.scatter(\n    embeddings[:, 0],\n    embeddings[:, 1],\n    cmap=\"rainbow\",\n    c=example_labels,\n    alpha=0.6,\n    s=3,\n)\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nWe can sample from the latent space and decode these sampled points to generate new images.\nFirst we sample some random points in the latent space:\n\n\nCode\n# Get the range of existing embedding values so we can sample sensible points within the latent space.\nembedding_min = np.min(embeddings, axis=0)\nembedding_max = np.max(embeddings, axis=0)\n\n# Sample some points\ngrid_width = 6\ngrid_height = 3\nsample = np.random.uniform(\n    embedding_min, embedding_max, size=(grid_width * grid_height, EMBEDDING_DIM)\n)\nprint(sample)\n\n\n[[ 1.47862929  9.28394749]\n [-3.19389344 -3.04713146]\n [-0.57161452 -0.35644389]\n [10.97632621 -2.12482484]\n [ 4.05160668  9.04420005]\n [ 9.50105167  5.71270956]\n [ 3.24765456  4.95969011]\n [-3.68217634  4.52120851]\n [-1.7067196   5.87696959]\n [ 5.99883565 -2.11597183]\n [ 1.84553131  6.04266323]\n [ 0.15552252  1.98655625]\n [ 3.55479856  2.35587959]\n [-0.32278762  6.07537408]\n [ 8.98977414 -1.15893539]\n [ 2.1476981   4.97819188]\n [-2.0896675   3.9166368 ]\n [ 6.49229371 -4.75611412]]\n\n\nWe can then decode these sampled points.\n\n\nCode\n# Decode the sampled points\nreconstructions = decoder.predict(sample)\n\n\n1/1 [==============================] - 0s 59ms/step\n\n\n\n\nCode\nfigsize = 8\nplt.figure(figsize=(figsize, figsize))\n\n# Plot the latent space and overlay the positions of the sampled points\nplt.scatter(embeddings[:, 0], embeddings[:, 1], c=\"black\", alpha=0.5, s=2)\nplt.scatter(sample[:, 0], sample[:, 1], c=\"#00B0F0\", alpha=1, s=40)\nplt.show()\n\n# Plot a grid of the reconstructed images which decode those sampled points\nfig = plt.figure(figsize=(figsize, grid_height * 2))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i in range(grid_width * grid_height):\n    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n    ax.axis(\"off\")\n    ax.text(\n        0.5,\n        -0.35,\n        str(np.round(sample[i, :], 1)),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow let’s see what happens when we regularly sample the latent space.\n\n\nCode\n# Colour the embeddings by their label (clothing type - see table)\nfigsize = 12\ngrid_size = 15\nplt.figure(figsize=(figsize, figsize))\nplt.scatter(\n    embeddings[:, 0],\n    embeddings[:, 1],\n    cmap=\"rainbow\",\n    c=example_labels,\n    alpha=0.8,\n    s=300,\n)\nplt.colorbar()\n\nx = np.linspace(min(embeddings[:, 0]), max(embeddings[:, 0]), grid_size)\ny = np.linspace(max(embeddings[:, 1]), min(embeddings[:, 1]), grid_size)\nxv, yv = np.meshgrid(x, y)\nxv = xv.flatten()\nyv = yv.flatten()\ngrid = np.array(list(zip(xv, yv)))\n\nreconstructions = decoder.predict(grid)\n# plt.scatter(grid[:, 0], grid[:, 1], c=\"black\", alpha=1, s=10)\nplt.show()\n\nfig = plt.figure(figsize=(figsize, figsize))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\nfor i in range(grid_size**2):\n    ax = fig.add_subplot(grid_size, grid_size, i + 1)\n    ax.axis(\"off\")\n    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")\n\n\n8/8 [==============================] - 0s 9ms/step\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe latent space exploration above yields some interesting insights into “regular” autoencoders that motivate the use of variational autoencoders to address these shortcomings.\n\nDifferent categories occupy varying amounts of area in latent space.\nThe latent space distribution is not symmetrical or bounded.\nThere are gaps in the latent space.\n\nThis makes it difficult for us to sample from this latent space effectively. We could sample a “gap” and get a nonsensical image. If a category (say, trousers) occupies a larger area in latent space, we are more likely to generate images of trousers than of categories which occupy a small area (say, shoes).\n\n\n\n\n\n\n\n\n\n\nStory Time\n\n\n\nIf we revisit our wardrobe, rather than assigning each item to a specific location, let’s assign it to a general region of the wardrobe.\nAnd let’s also insist that this region should be as close to the centre of the wardrobe as possible, otherwise we are penalised. This should yield a more uniform latent space.\nThis is the idea behind variational autoencoders (VAE).\n\n\n\n\nIn a standard autoencoder, each image is mapped directly to one point in the latent space.\nIn a variational autoencoder, each image is mapped to a multivariate Normal distribution around a point in the latent space. Variational autoencoders assume their is no correlation between latent space dimensions.\nSo we will typically use isotropic Normal distributions, meaning the covariance matrix is diagonal so the distribution is independent in each dimension. The encoder only needs to map each input to a mean vector and a variance vector; it does not need to worry about covariances.\nIn practice we choose to map to log variances because this can be any value in the range \\((-\\infty, \\infty)\\) which gives a smoother value to learn rather than variances whihc are positive.\nIn summary, the encoder maps \\(image \\rightarrow (z_{mean}, z_{log\\_var})\\)\nWe can then sample a point \\(z\\) from this distribution using:\n\\[\nz = z_{mean} + z_{sigma} * epsilon\n\\]\nwhere: \\[\nz_{sigma} = e^{z_{log\\_var} * 0.5}\n\\] \\[\nepsilon \\sim \\mathcal{N}(0, I)\n\\]\n\n\n\nThis is identical to the standard autoencoder.\n\n\n\nPutting these together, we get the overall architecture:\n\n\n\n\n\nflowchart LR\n\n\n  A[Encoder] --&gt; B1(z_mean)\n  A[Encoder] --&gt; B2(z_log_var)\n\n  B1(z_mean) --&gt; C[sample]\n  B2(z_log_var) --&gt; C[sample]\n\n  C[sample] --&gt; D(z)\n  D(z) --&gt; E[Decoder]\n\n\n\n\n\n\nWhy does this change to the encoder help?\nIn the standard autoencoder, there is no requirement for the latent space to be continuous. So we could sample a point, say, \\((1, 2)\\) and decode it to a well-formed image. But there is no guarantee that a point next to it \\((1.1, 2.1)\\) would look similar or even be intelligible.\nThe “variational” part of the VAE addresses this problem. We now sample from an area around z_mean, so the decoder must ensure that all points in that region produce similar images to keep the reconstruction loss small.\n\n\n\nRather than sample directly from a Normal distribution parameterised by z_mean and z_log_var, we can sample epsilon from a standard Normal distribution and manually adjust the sample to correct its mean and variance.\nThis means gradients can backpropagate freely through the layer. The randomness in the layer is all encapsulated in epsilon, so the partial derivative of the layer output w.r.t. the layer input is deterministic, making backpropagation possible.\n\n\n\nThe loss function of the standard autoencoder was the reconstruction loss between original image and its decoded version.\nFor VAEs, we add an additional term which encourages points to have small mean and variance by penalising z_mean and z_log_var variables that differ significantly from 0.\nThis is the Kullback-Leibler (KL) divergence. It measures how much one probability distribution differs from another. We use it to measure how much our Normal distribution, with parameters z_mean and z_log_var, differs from a standard Normal distribution.\nFor this special case of KL divergence between our Normal distribution and a standard Normal, the closed form solution is: \\[\nD_{KL}[\\mathcal{N}(\\mu, \\sigma) || \\mathcal{N}(0, 1)] = -\\frac{1}{2} \\sum (1 + \\log(\\sigma ^2) - \\mu ^2 - \\sigma ^ 2)\n\\]\nSo using our variables, we can describe this in code as:\nkl_loss = -0.5 * sum(1 + z_log_var - z_mean ** 2 - exp(z_log_var))\nThis loss is minimised when z_mean=0 and z_log_var=0, i.e. it encourages our distrubution towards a stand Normal distribution, thus using the space around the origin symmetrically and efficently with few gaps.\nThe original paper simply summed the reconstruction_loss and the kl_loss. A variant of this includes a hyperparameter \\(\\beta\\) to vary the weight of the KL divergence term. This is called a “\\(\\beta-VAE\\)”:\nvae_loss = reconstruction_error + beta * kl_loss\n\n\n\n\n\n\nWe need a sampling layer which allows us to sample \\(z\\) from the distribution defined by \\(z_{mean}\\) and \\(z_{log\\_var}\\).\n\n\nCode\nclass Sampling(layers.Layer):\n    def call(self, z_mean, z_log_var):\n        batch = tf.shape(z_mean)[0]\n        dim = tf.shape(z_mean)[1]\n        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n\n\n\n\n\nThe encoder incorporates the Sampling layer as the final step. This is what is passed to the decoder.\n\n\nCode\n# Encoder\nencoder_input = layers.Input(\n    shape=(IMAGE_SIZE, IMAGE_SIZE, 1), name=\"encoder_input\"\n)\nx = layers.Conv2D(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(encoder_input)\nx = layers.Conv2D(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2D(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nshape_before_flattening = tf.keras.backend.int_shape(x)[1:]  # the decoder will need this!\n\nx = layers.Flatten()(x)\nz_mean = layers.Dense(EMBEDDING_DIM, name=\"z_mean\")(x)\nz_log_var = layers.Dense(EMBEDDING_DIM, name=\"z_log_var\")(x)\nz = Sampling()(z_mean, z_log_var)\n\nencoder = models.Model(encoder_input, [z_mean, z_log_var, z], name=\"encoder\")\nencoder.summary()\n\n\nModel: \"encoder\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n encoder_input (InputLayer)  [(None, 32, 32, 1)]          0         []                            \n                                                                                                  \n conv2d_3 (Conv2D)           (None, 16, 16, 32)           320       ['encoder_input[0][0]']       \n                                                                                                  \n conv2d_4 (Conv2D)           (None, 8, 8, 64)             18496     ['conv2d_3[0][0]']            \n                                                                                                  \n conv2d_5 (Conv2D)           (None, 4, 4, 128)            73856     ['conv2d_4[0][0]']            \n                                                                                                  \n flatten_1 (Flatten)         (None, 2048)                 0         ['conv2d_5[0][0]']            \n                                                                                                  \n z_mean (Dense)              (None, 2)                    4098      ['flatten_1[0][0]']           \n                                                                                                  \n z_log_var (Dense)           (None, 2)                    4098      ['flatten_1[0][0]']           \n                                                                                                  \n sampling (Sampling)         (None, 2)                    0         ['z_mean[0][0]',              \n                                                                     'z_log_var[0][0]']           \n                                                                                                  \n==================================================================================================\nTotal params: 100868 (394.02 KB)\nTrainable params: 100868 (394.02 KB)\nNon-trainable params: 0 (0.00 Byte)\n__________________________________________________________________________________________________\n\n\n\n\n\nThe decoder is the same as a standard autoencoder.\n\n\nCode\n# Decoder\ndecoder_input = layers.Input(shape=(EMBEDDING_DIM,), name=\"decoder_input\")\nx = layers.Dense(np.prod(shape_before_flattening))(decoder_input)\nx = layers.Reshape(shape_before_flattening)(x)\nx = layers.Conv2DTranspose(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2DTranspose(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n\ndecoder_output = layers.Conv2D(1, (3, 3), strides=1, activation=\"sigmoid\", padding=\"same\", name=\"decoder_output\")(x)\n\ndecoder = models.Model(decoder_input, decoder_output)\ndecoder.summary()\n\n\nModel: \"model_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n decoder_input (InputLayer)  [(None, 2)]               0         \n                                                                 \n dense_1 (Dense)             (None, 2048)              6144      \n                                                                 \n reshape_1 (Reshape)         (None, 4, 4, 128)         0         \n                                                                 \n conv2d_transpose_3 (Conv2D  (None, 8, 8, 128)         147584    \n Transpose)                                                      \n                                                                 \n conv2d_transpose_4 (Conv2D  (None, 16, 16, 64)        73792     \n Transpose)                                                      \n                                                                 \n conv2d_transpose_5 (Conv2D  (None, 32, 32, 32)        18464     \n Transpose)                                                      \n                                                                 \n decoder_output (Conv2D)     (None, 32, 32, 1)         289       \n                                                                 \n=================================================================\nTotal params: 246273 (962.00 KB)\nTrainable params: 246273 (962.00 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\nPutting the encoder and decoder together.\n\n\nCode\nEPOCHS = 5\nBETA = 500\n\n\n\n\nCode\nclass VAE(models.Model):\n    def __init__(self, encoder, decoder, **kwargs):\n        super(VAE, self).__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n        self.total_loss_tracker = metrics.Mean(name=\"total_loss\")\n        self.reconstruction_loss_tracker = metrics.Mean(name=\"reconstruction_loss\")\n        self.kl_loss_tracker = metrics.Mean(name=\"kl_loss\")\n\n    @property\n    def metrics(self):\n        return [\n            self.total_loss_tracker,\n            self.reconstruction_loss_tracker,\n            self.kl_loss_tracker,\n        ]\n\n    def call(self, inputs):\n        \"\"\"Call the model on a particular input.\"\"\"\n        z_mean, z_log_var, z = encoder(inputs)\n        reconstruction = decoder(z)\n        return z_mean, z_log_var, reconstruction\n\n    def train_step(self, data):\n        \"\"\"Step run during training.\"\"\"\n        with tf.GradientTape() as tape:\n            z_mean, z_log_var, reconstruction = self(data)\n            reconstruction_loss = tf.reduce_mean(\n                BETA * losses.binary_crossentropy(data, reconstruction, axis=(1, 2, 3))\n            )\n            kl_loss = tf.reduce_mean(\n                tf.reduce_sum(-0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)), axis=1)\n            )\n            total_loss = reconstruction_loss + kl_loss\n\n        grads = tape.gradient(total_loss, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n\n        self.total_loss_tracker.update_state(total_loss)\n        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n        self.kl_loss_tracker.update_state(kl_loss)\n\n        return {m.name: m.result() for m in self.metrics}\n\n    def test_step(self, data):\n        \"\"\"Step run during validation.\"\"\"\n        if isinstance(data, tuple):\n            data = data[0]\n\n        z_mean, z_log_var, reconstruction = self(data)\n        reconstruction_loss = tf.reduce_mean(\n            BETA * losses.binary_crossentropy(data, reconstruction, axis=(1, 2, 3))\n        )\n        kl_loss = tf.reduce_mean(\n            tf.reduce_sum(\n                -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)),\n                axis=1,\n            )\n        )\n        total_loss = reconstruction_loss + kl_loss\n\n        return {\n            \"loss\": total_loss,\n            \"reconstruction_loss\": reconstruction_loss,\n            \"kl_loss\": kl_loss,\n        }\n\n\nInstantiate the VAE model and compile it.\n\n\nCode\nvae = VAE(encoder, decoder)\n\n# optimizer = optimizers.Adam(learning_rate=0.0005)\noptimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0005)\nvae.compile(optimizer=optimizer)\n\n\n\n\n\nTrain the VAE as before.\n\n\nCode\nvae.fit(\n    x_train,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    validation_data=(x_test, x_test),\n)\n\n\nEpoch 1/5\n600/600 [==============================] - 37s 61ms/step - total_loss: 160.4693 - reconstruction_loss: 155.9913 - kl_loss: 4.4779 - val_loss: 141.2442 - val_reconstruction_loss: 136.1877 - val_kl_loss: 5.0565\nEpoch 2/5\n600/600 [==============================] - 34s 57ms/step - total_loss: 135.9397 - reconstruction_loss: 130.9409 - kl_loss: 4.9988 - val_loss: 138.5623 - val_reconstruction_loss: 133.5856 - val_kl_loss: 4.9767\nEpoch 3/5\n600/600 [==============================] - 34s 56ms/step - total_loss: 134.3719 - reconstruction_loss: 129.3381 - kl_loss: 5.0338 - val_loss: 137.1351 - val_reconstruction_loss: 132.1540 - val_kl_loss: 4.9811\nEpoch 4/5\n600/600 [==============================] - 34s 56ms/step - total_loss: 133.4455 - reconstruction_loss: 128.3819 - kl_loss: 5.0637 - val_loss: 136.5461 - val_reconstruction_loss: 131.4780 - val_kl_loss: 5.0681\nEpoch 5/5\n600/600 [==============================] - 34s 57ms/step - total_loss: 132.7808 - reconstruction_loss: 127.6688 - kl_loss: 5.1120 - val_loss: 135.8917 - val_reconstruction_loss: 130.7375 - val_kl_loss: 5.1542\n\n\n&lt;keras.src.callbacks.History at 0x2c59e9f10&gt;\n\n\n\n\n\n\n\n\nAs before, we can eyeball the reconstructions from our model.\n\n\nCode\n# Select a subset of the test set\nn_to_predict = 5000\nexample_images = x_test[:n_to_predict]\nexample_labels = y_test[:n_to_predict]\n\n# Create autoencoder predictions and display\nz_mean, z_log_var, reconstructions = vae.predict(example_images)\nprint(\"Example real clothing items\")\nplot_sample_images(example_images)\nprint(\"Reconstructions\")\nplot_sample_images(reconstructions)\n\n\n 42/157 [=======&gt;......................] - ETA: 1s157/157 [==============================] - 1s 9ms/step\nExample real clothing items\nReconstructions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can sample and decode points in the latent space to observe how the model generates images. We note that:\n\nThe latent space has more even coverage and does not stray to far from a standard Normal distribution. If this is not the case, we can vary the \\(\\beta\\) value used to give more weight to the KL loss term.\nWe do not see as many poorly formed images as we did when sampling a “gap” in a standard autoencoder.\n\n\n\nCode\n# Encode the example images\nz_mean, z_var, z = encoder.predict(example_images)\n\n# Sample some points in the latent space, from the standard normal distribution\ngrid_width, grid_height = (6, 3)\nz_sample = np.random.normal(size=(grid_width * grid_height, 2))\n# Decode the sampled points\nreconstructions = decoder.predict(z_sample)\n# Convert original embeddings and sampled embeddings to p-values\np = norm.cdf(z)\np_sample = norm.cdf(z_sample)\n# Draw a plot of...\nfigsize = 8\nplt.figure(figsize=(figsize, figsize))\n\n# ... the original embeddings ...\nplt.scatter(z[:, 0], z[:, 1], c=\"black\", alpha=0.5, s=2)\n\n# ... and the newly generated points in the latent space\nplt.scatter(z_sample[:, 0], z_sample[:, 1], c=\"#00B0F0\", alpha=1, s=40)\nplt.show()\n\n# Add underneath a grid of the decoded images\nfig = plt.figure(figsize=(figsize, grid_height * 2))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i in range(grid_width * grid_height):\n    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n    ax.axis(\"off\")\n    ax.text(\n        0.5,\n        -0.35,\n        str(np.round(z_sample[i, :], 1)),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")\n\n\n1/1 [==============================] - 0s 14ms/step\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe plots below show the latent space coloured by clothing type. The left plot shows this in terms of z-values and the right in terms of p-values.\nThe latent space is more continuous with fewer gaps, and different categories take similar amounts of space.\n\n\nCode\n# Colour the embeddings by their label (clothing type - see table)\nfigsize = 8\nfig = plt.figure(figsize=(figsize * 2, figsize))\nax = fig.add_subplot(1, 2, 1)\nplot_1 = ax.scatter(\n    z[:, 0], z[:, 1], cmap=\"rainbow\", c=example_labels, alpha=0.8, s=3\n)\nplt.colorbar(plot_1)\nax = fig.add_subplot(1, 2, 2)\nplot_2 = ax.scatter(\n    p[:, 0], p[:, 1], cmap=\"rainbow\", c=example_labels, alpha=0.8, s=3\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nNext we see what happens when we sample from the latent space in a regular grid.\n\n\nCode\n# Colour the embeddings by their label (clothing type - see table)\nfigsize = 12\ngrid_size = 15\nplt.figure(figsize=(figsize, figsize))\nplt.scatter(\n    p[:, 0], p[:, 1], cmap=\"rainbow\", c=example_labels, alpha=0.8, s=300\n)\nplt.colorbar()\n\nx = norm.ppf(np.linspace(0, 1, grid_size))\ny = norm.ppf(np.linspace(1, 0, grid_size))\nxv, yv = np.meshgrid(x, y)\nxv = xv.flatten()\nyv = yv.flatten()\ngrid = np.array(list(zip(xv, yv)))\n\nreconstructions = decoder.predict(grid)\n# plt.scatter(grid[:, 0], grid[:, 1], c=\"black\", alpha=1, s=10)\nplt.show()\n\nfig = plt.figure(figsize=(figsize, figsize))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\nfor i in range(grid_size**2):\n    ax = fig.add_subplot(grid_size, grid_size, i + 1)\n    ax.axis(\"off\")\n    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")\n\n\n8/8 [==============================] - 0s 6ms/step\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 3 of Generative Deep Learning by David Foster."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson3/chapter3.html#autoencoders",
    "href": "posts/ml/generative_deep_learning/lesson3/chapter3.html#autoencoders",
    "title": "Generative AI: VAEs",
    "section": "",
    "text": "The idea of autoencoders (read: self-encoders) is that they learn to simplify the input then reconstruct it; the input and target output are the same.\n\nThe encoder learns to compress high-dimensional input data into a lower dimensional representation called the embedding.\nThe decoder takes an embedding and recreates a higher-dimensional image. This should be an accurate reconstruction of the input.\n\nThis can be used as a generative model because we can the sample and decode new points from the latent space to generate novel outputs. The goal of training an autoencoder is to learn a meaningful embedding \\(z\\).\n\n\n\n\n\nflowchart LR\n\n  A(Encoder) --&gt; B(z)\n  B(z) --&gt; c(Decoder)\n\n\n\n\n\n\nThis also makes autoencoders useful as denoising models, because the embedding should retain the salient information but “lose” the noise."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson3/chapter3.html#building-an-autoencoder",
    "href": "posts/ml/generative_deep_learning/lesson3/chapter3.html#building-an-autoencoder",
    "title": "Generative AI: VAEs",
    "section": "",
    "text": "We will implement an autoencoder to learn lower-dimensional embeddings for the fashion MNIST data set.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nimport tensorflow as tf\nfrom tensorflow.keras import (\n    layers,\n    models,\n    datasets,\n    callbacks,\n    losses,\n    optimizers,\n    metrics,\n)\n\n\n# Parameters\nIMAGE_SIZE = 32\nCHANNELS = 1\nBATCH_SIZE = 100\nBUFFER_SIZE = 1000\nVALIDATION_SPLIT = 0.2\nEMBEDDING_DIM = 2\nEPOCHS = 3\n\n\n\n\nScale the pixel values and reshape the images.\n\n\nCode\n(x_train, y_train), (x_test, y_test) = datasets.fashion_mnist.load_data()\n\ndef preprocess(images):\n    images = images.astype(\"float32\") / 255.0\n    images = np.pad(images, ((0, 0), (2, 2), (2, 2)), constant_values=0.0)\n    images = np.expand_dims(images, -1)\n    return images\n\nx_train = preprocess(x_train)\nx_test = preprocess(x_test)\n\n\nWe can see an example from our training set:\n\n\nCode\nplt.imshow(x_train[0])\n\n\n\n\n\n\n\n\n\n\n\n\nThe encoder compresses the dimensionality on the input to a smaller embedding dimension.\n\n\nCode\n# Input\nencoder_input = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS),name=\"encoder_input\")\n\n# Conv layers\nx = layers.Conv2D(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(encoder_input)\nx = layers.Conv2D(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2D(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\npre_flatten_shape = tf.keras.backend.int_shape(x)[1:]  # Used by the decoder later\n\n# Output\nx = layers.Flatten()(x)\nencoder_output = layers.Dense(EMBEDDING_DIM, name=\"encoder_output\")(x)\n\n# Model\nencoder = models.Model(encoder_input, encoder_output)\nencoder.summary()\n\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n encoder_input (InputLayer)  [(None, 32, 32, 1)]       0         \n                                                                 \n conv2d (Conv2D)             (None, 16, 16, 32)        320       \n                                                                 \n conv2d_1 (Conv2D)           (None, 8, 8, 64)          18496     \n                                                                 \n conv2d_2 (Conv2D)           (None, 4, 4, 128)         73856     \n                                                                 \n flatten (Flatten)           (None, 2048)              0         \n                                                                 \n encoder_output (Dense)      (None, 2)                 4098      \n                                                                 \n=================================================================\nTotal params: 96770 (378.01 KB)\nTrainable params: 96770 (378.01 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\nThe decoder reconstructs the original image from the embedding.\n\n\nIn a standard convolutional layer, if we have stride=2 it will half the image size.\nIn a convolutional transpose layer, we are increasing the image size. The stride parameter determines the amount of zero padding to add between each pixel. A kernel is then applied to this “internally padded” image to expand the image size.\n\n\nCode\n# Input\ndecoder_input = layers.Input(shape=(EMBEDDING_DIM,),name=\"decoder_input\")\n\n# Reshape the input using the pre-flattening shape from the encoder\nx = layers.Dense(np.prod(pre_flatten_shape))(decoder_input)\nx = layers.Reshape(pre_flatten_shape)(x)\n\n# Scale up the image back to its original size. These are the reverse of the conv layers applied in the encoder.\nx = layers.Conv2DTranspose(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2DTranspose(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n\n# Output\ndecoder_output = layers.Conv2D(\n    CHANNELS,\n    (3, 3),\n    strides=1,\n    activation='sigmoid',\n    padding=\"same\",\n    name=\"decoder_output\",\n)(x)\n\n# Model\ndecoder = models.Model(decoder_input, decoder_output)\ndecoder.summary()\n\n\nModel: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n decoder_input (InputLayer)  [(None, 2)]               0         \n                                                                 \n dense (Dense)               (None, 2048)              6144      \n                                                                 \n reshape (Reshape)           (None, 4, 4, 128)         0         \n                                                                 \n conv2d_transpose (Conv2DTr  (None, 8, 8, 128)         147584    \n anspose)                                                        \n                                                                 \n conv2d_transpose_1 (Conv2D  (None, 16, 16, 64)        73792     \n Transpose)                                                      \n                                                                 \n conv2d_transpose_2 (Conv2D  (None, 32, 32, 32)        18464     \n Transpose)                                                      \n                                                                 \n decoder_output (Conv2D)     (None, 32, 32, 1)         289       \n                                                                 \n=================================================================\nTotal params: 246273 (962.00 KB)\nTrainable params: 246273 (962.00 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\n\nCombine the encoder and decoder into a single model.\n\n\nCode\nautoencoder = models.Model(encoder_input, decoder(encoder_output))\nautoencoder.summary()\n\n\nModel: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n encoder_input (InputLayer)  [(None, 32, 32, 1)]       0         \n                                                                 \n conv2d (Conv2D)             (None, 16, 16, 32)        320       \n                                                                 \n conv2d_1 (Conv2D)           (None, 8, 8, 64)          18496     \n                                                                 \n conv2d_2 (Conv2D)           (None, 4, 4, 128)         73856     \n                                                                 \n flatten (Flatten)           (None, 2048)              0         \n                                                                 \n encoder_output (Dense)      (None, 2)                 4098      \n                                                                 \n model_1 (Functional)        (None, 32, 32, 1)         246273    \n                                                                 \n=================================================================\nTotal params: 343043 (1.31 MB)\nTrainable params: 343043 (1.31 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\nThe autoencoder is trained with the source images as both input and target output.\nThe loss function is usually chosen as either RMSE or binary cross-entropy between pixels of original image vs reconstruction.\n\n\nCode\nautoencoder.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\nautoencoder.fit(\n    x_train,\n    x_train,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    validation_data=(x_test, x_test)\n)\n\n\nEpoch 1/3\n600/600 [==============================] - 35s 58ms/step - loss: 0.2910 - val_loss: 0.2610\nEpoch 2/3\n600/600 [==============================] - 36s 60ms/step - loss: 0.2569 - val_loss: 0.2561\nEpoch 3/3\n600/600 [==============================] - 34s 57ms/step - loss: 0.2536 - val_loss: 0.2540\n\n\n&lt;keras.src.callbacks.History at 0x156e4a310&gt;"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson3/chapter3.html#analysing-the-autoencoder",
    "href": "posts/ml/generative_deep_learning/lesson3/chapter3.html#analysing-the-autoencoder",
    "title": "Generative AI: VAEs",
    "section": "",
    "text": "We can use our trained autoencoder to:\n\nReconstruct images\nAnalyse embeddings\nGenerate new images\n\n\n\nReconstruct a sample of test images using the autoencoder.\nThe reconstruction isn’t perfect; some information is lost when reducing down to just 2 dimensions. But it does a surprisingly good job of compressing 32x32 pixel values into just 2 embedding values.\n\n\nCode\nNUM_IMAGES_TO_RECONSTRUCT = 5000\nexample_images = x_test[:NUM_IMAGES_TO_RECONSTRUCT]\nexample_labels = y_test[:NUM_IMAGES_TO_RECONSTRUCT]\n\npredictions = autoencoder.predict(example_images)\n\n\n  7/157 [&gt;.............................] - ETA: 1s 157/157 [==============================] - 1s 8ms/step\n\n\nOriginal images:\n\n\nCode\ndef plot_sample_images(images, n=10, size=(20, 3), cmap=\"gray_r\"):\n    plt.figure(figsize=size)\n    for i in range(n):\n        _ = plt.subplot(1, n, i + 1)\n        plt.imshow(images[i].astype(\"float32\"), cmap=cmap)\n        plt.axis(\"off\")\n    \n    plt.show()\n\nplot_sample_images(example_images)\n\n\n\n\n\n\n\n\n\nReconstructed images:\n\n\nCode\nplot_sample_images(predictions)\n\n\n\n\n\n\n\n\n\n\n\n\nEach of the images above has been encoded as a 2-dimensional embedding.\nWe can look at these embeddings to gain some insight into how the autoencoder works.\nThe embedding vectors for our sample images above:\n\n\nCode\n# Encode the example images\nembeddings = encoder.predict(example_images)\nprint(embeddings[:10])\n\n\n102/157 [==================&gt;...........] - ETA: 0s157/157 [==============================] - 0s 2ms/step\n[[ 2.2441912  -2.711683  ]\n [ 6.1558456   6.0202003 ]\n [-3.787192    7.3368516 ]\n [-2.5938551   4.2098355 ]\n [ 3.8645594   2.7229536 ]\n [-2.0130231   6.0485506 ]\n [ 1.2749226   2.1347647 ]\n [ 2.8239484   2.898773  ]\n [-0.48542604 -1.0869933 ]\n [ 0.30643728 -2.6099105 ]]\n\n\nWe can plot the 2D latent space, colouring each point by its label. This shows how similar items are clustered together in latent space.\nThis is impressive! Remember, we never showed the model the labels when training, so it has learned to cluster images that look alike.\n\n\nCode\n# Colour the embeddings by their label\nexample_labels = y_test[:NUM_IMAGES_TO_RECONSTRUCT]\n\n# Plot the latent space\nfigsize = 8\nplt.figure(figsize=(figsize, figsize))\nplt.scatter(\n    embeddings[:, 0],\n    embeddings[:, 1],\n    cmap=\"rainbow\",\n    c=example_labels,\n    alpha=0.6,\n    s=3,\n)\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nWe can sample from the latent space and decode these sampled points to generate new images.\nFirst we sample some random points in the latent space:\n\n\nCode\n# Get the range of existing embedding values so we can sample sensible points within the latent space.\nembedding_min = np.min(embeddings, axis=0)\nembedding_max = np.max(embeddings, axis=0)\n\n# Sample some points\ngrid_width = 6\ngrid_height = 3\nsample = np.random.uniform(\n    embedding_min, embedding_max, size=(grid_width * grid_height, EMBEDDING_DIM)\n)\nprint(sample)\n\n\n[[ 1.47862929  9.28394749]\n [-3.19389344 -3.04713146]\n [-0.57161452 -0.35644389]\n [10.97632621 -2.12482484]\n [ 4.05160668  9.04420005]\n [ 9.50105167  5.71270956]\n [ 3.24765456  4.95969011]\n [-3.68217634  4.52120851]\n [-1.7067196   5.87696959]\n [ 5.99883565 -2.11597183]\n [ 1.84553131  6.04266323]\n [ 0.15552252  1.98655625]\n [ 3.55479856  2.35587959]\n [-0.32278762  6.07537408]\n [ 8.98977414 -1.15893539]\n [ 2.1476981   4.97819188]\n [-2.0896675   3.9166368 ]\n [ 6.49229371 -4.75611412]]\n\n\nWe can then decode these sampled points.\n\n\nCode\n# Decode the sampled points\nreconstructions = decoder.predict(sample)\n\n\n1/1 [==============================] - 0s 59ms/step\n\n\n\n\nCode\nfigsize = 8\nplt.figure(figsize=(figsize, figsize))\n\n# Plot the latent space and overlay the positions of the sampled points\nplt.scatter(embeddings[:, 0], embeddings[:, 1], c=\"black\", alpha=0.5, s=2)\nplt.scatter(sample[:, 0], sample[:, 1], c=\"#00B0F0\", alpha=1, s=40)\nplt.show()\n\n# Plot a grid of the reconstructed images which decode those sampled points\nfig = plt.figure(figsize=(figsize, grid_height * 2))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i in range(grid_width * grid_height):\n    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n    ax.axis(\"off\")\n    ax.text(\n        0.5,\n        -0.35,\n        str(np.round(sample[i, :], 1)),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow let’s see what happens when we regularly sample the latent space.\n\n\nCode\n# Colour the embeddings by their label (clothing type - see table)\nfigsize = 12\ngrid_size = 15\nplt.figure(figsize=(figsize, figsize))\nplt.scatter(\n    embeddings[:, 0],\n    embeddings[:, 1],\n    cmap=\"rainbow\",\n    c=example_labels,\n    alpha=0.8,\n    s=300,\n)\nplt.colorbar()\n\nx = np.linspace(min(embeddings[:, 0]), max(embeddings[:, 0]), grid_size)\ny = np.linspace(max(embeddings[:, 1]), min(embeddings[:, 1]), grid_size)\nxv, yv = np.meshgrid(x, y)\nxv = xv.flatten()\nyv = yv.flatten()\ngrid = np.array(list(zip(xv, yv)))\n\nreconstructions = decoder.predict(grid)\n# plt.scatter(grid[:, 0], grid[:, 1], c=\"black\", alpha=1, s=10)\nplt.show()\n\nfig = plt.figure(figsize=(figsize, figsize))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\nfor i in range(grid_size**2):\n    ax = fig.add_subplot(grid_size, grid_size, i + 1)\n    ax.axis(\"off\")\n    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")\n\n\n8/8 [==============================] - 0s 9ms/step\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe latent space exploration above yields some interesting insights into “regular” autoencoders that motivate the use of variational autoencoders to address these shortcomings.\n\nDifferent categories occupy varying amounts of area in latent space.\nThe latent space distribution is not symmetrical or bounded.\nThere are gaps in the latent space.\n\nThis makes it difficult for us to sample from this latent space effectively. We could sample a “gap” and get a nonsensical image. If a category (say, trousers) occupies a larger area in latent space, we are more likely to generate images of trousers than of categories which occupy a small area (say, shoes)."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson3/chapter3.html#variational-autoencoders-1",
    "href": "posts/ml/generative_deep_learning/lesson3/chapter3.html#variational-autoencoders-1",
    "title": "Generative AI: VAEs",
    "section": "",
    "text": "Story Time\n\n\n\nIf we revisit our wardrobe, rather than assigning each item to a specific location, let’s assign it to a general region of the wardrobe.\nAnd let’s also insist that this region should be as close to the centre of the wardrobe as possible, otherwise we are penalised. This should yield a more uniform latent space.\nThis is the idea behind variational autoencoders (VAE).\n\n\n\n\nIn a standard autoencoder, each image is mapped directly to one point in the latent space.\nIn a variational autoencoder, each image is mapped to a multivariate Normal distribution around a point in the latent space. Variational autoencoders assume their is no correlation between latent space dimensions.\nSo we will typically use isotropic Normal distributions, meaning the covariance matrix is diagonal so the distribution is independent in each dimension. The encoder only needs to map each input to a mean vector and a variance vector; it does not need to worry about covariances.\nIn practice we choose to map to log variances because this can be any value in the range \\((-\\infty, \\infty)\\) which gives a smoother value to learn rather than variances whihc are positive.\nIn summary, the encoder maps \\(image \\rightarrow (z_{mean}, z_{log\\_var})\\)\nWe can then sample a point \\(z\\) from this distribution using:\n\\[\nz = z_{mean} + z_{sigma} * epsilon\n\\]\nwhere: \\[\nz_{sigma} = e^{z_{log\\_var} * 0.5}\n\\] \\[\nepsilon \\sim \\mathcal{N}(0, I)\n\\]\n\n\n\nThis is identical to the standard autoencoder.\n\n\n\nPutting these together, we get the overall architecture:\n\n\n\n\n\nflowchart LR\n\n\n  A[Encoder] --&gt; B1(z_mean)\n  A[Encoder] --&gt; B2(z_log_var)\n\n  B1(z_mean) --&gt; C[sample]\n  B2(z_log_var) --&gt; C[sample]\n\n  C[sample] --&gt; D(z)\n  D(z) --&gt; E[Decoder]\n\n\n\n\n\n\nWhy does this change to the encoder help?\nIn the standard autoencoder, there is no requirement for the latent space to be continuous. So we could sample a point, say, \\((1, 2)\\) and decode it to a well-formed image. But there is no guarantee that a point next to it \\((1.1, 2.1)\\) would look similar or even be intelligible.\nThe “variational” part of the VAE addresses this problem. We now sample from an area around z_mean, so the decoder must ensure that all points in that region produce similar images to keep the reconstruction loss small.\n\n\n\nRather than sample directly from a Normal distribution parameterised by z_mean and z_log_var, we can sample epsilon from a standard Normal distribution and manually adjust the sample to correct its mean and variance.\nThis means gradients can backpropagate freely through the layer. The randomness in the layer is all encapsulated in epsilon, so the partial derivative of the layer output w.r.t. the layer input is deterministic, making backpropagation possible.\n\n\n\nThe loss function of the standard autoencoder was the reconstruction loss between original image and its decoded version.\nFor VAEs, we add an additional term which encourages points to have small mean and variance by penalising z_mean and z_log_var variables that differ significantly from 0.\nThis is the Kullback-Leibler (KL) divergence. It measures how much one probability distribution differs from another. We use it to measure how much our Normal distribution, with parameters z_mean and z_log_var, differs from a standard Normal distribution.\nFor this special case of KL divergence between our Normal distribution and a standard Normal, the closed form solution is: \\[\nD_{KL}[\\mathcal{N}(\\mu, \\sigma) || \\mathcal{N}(0, 1)] = -\\frac{1}{2} \\sum (1 + \\log(\\sigma ^2) - \\mu ^2 - \\sigma ^ 2)\n\\]\nSo using our variables, we can describe this in code as:\nkl_loss = -0.5 * sum(1 + z_log_var - z_mean ** 2 - exp(z_log_var))\nThis loss is minimised when z_mean=0 and z_log_var=0, i.e. it encourages our distrubution towards a stand Normal distribution, thus using the space around the origin symmetrically and efficently with few gaps.\nThe original paper simply summed the reconstruction_loss and the kl_loss. A variant of this includes a hyperparameter \\(\\beta\\) to vary the weight of the KL divergence term. This is called a “\\(\\beta-VAE\\)”:\nvae_loss = reconstruction_error + beta * kl_loss"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson3/chapter3.html#building-a-variational-autoencoder-vae",
    "href": "posts/ml/generative_deep_learning/lesson3/chapter3.html#building-a-variational-autoencoder-vae",
    "title": "Generative AI: VAEs",
    "section": "",
    "text": "We need a sampling layer which allows us to sample \\(z\\) from the distribution defined by \\(z_{mean}\\) and \\(z_{log\\_var}\\).\n\n\nCode\nclass Sampling(layers.Layer):\n    def call(self, z_mean, z_log_var):\n        batch = tf.shape(z_mean)[0]\n        dim = tf.shape(z_mean)[1]\n        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n\n\n\n\n\nThe encoder incorporates the Sampling layer as the final step. This is what is passed to the decoder.\n\n\nCode\n# Encoder\nencoder_input = layers.Input(\n    shape=(IMAGE_SIZE, IMAGE_SIZE, 1), name=\"encoder_input\"\n)\nx = layers.Conv2D(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(encoder_input)\nx = layers.Conv2D(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2D(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nshape_before_flattening = tf.keras.backend.int_shape(x)[1:]  # the decoder will need this!\n\nx = layers.Flatten()(x)\nz_mean = layers.Dense(EMBEDDING_DIM, name=\"z_mean\")(x)\nz_log_var = layers.Dense(EMBEDDING_DIM, name=\"z_log_var\")(x)\nz = Sampling()(z_mean, z_log_var)\n\nencoder = models.Model(encoder_input, [z_mean, z_log_var, z], name=\"encoder\")\nencoder.summary()\n\n\nModel: \"encoder\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n encoder_input (InputLayer)  [(None, 32, 32, 1)]          0         []                            \n                                                                                                  \n conv2d_3 (Conv2D)           (None, 16, 16, 32)           320       ['encoder_input[0][0]']       \n                                                                                                  \n conv2d_4 (Conv2D)           (None, 8, 8, 64)             18496     ['conv2d_3[0][0]']            \n                                                                                                  \n conv2d_5 (Conv2D)           (None, 4, 4, 128)            73856     ['conv2d_4[0][0]']            \n                                                                                                  \n flatten_1 (Flatten)         (None, 2048)                 0         ['conv2d_5[0][0]']            \n                                                                                                  \n z_mean (Dense)              (None, 2)                    4098      ['flatten_1[0][0]']           \n                                                                                                  \n z_log_var (Dense)           (None, 2)                    4098      ['flatten_1[0][0]']           \n                                                                                                  \n sampling (Sampling)         (None, 2)                    0         ['z_mean[0][0]',              \n                                                                     'z_log_var[0][0]']           \n                                                                                                  \n==================================================================================================\nTotal params: 100868 (394.02 KB)\nTrainable params: 100868 (394.02 KB)\nNon-trainable params: 0 (0.00 Byte)\n__________________________________________________________________________________________________\n\n\n\n\n\nThe decoder is the same as a standard autoencoder.\n\n\nCode\n# Decoder\ndecoder_input = layers.Input(shape=(EMBEDDING_DIM,), name=\"decoder_input\")\nx = layers.Dense(np.prod(shape_before_flattening))(decoder_input)\nx = layers.Reshape(shape_before_flattening)(x)\nx = layers.Conv2DTranspose(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2DTranspose(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n\ndecoder_output = layers.Conv2D(1, (3, 3), strides=1, activation=\"sigmoid\", padding=\"same\", name=\"decoder_output\")(x)\n\ndecoder = models.Model(decoder_input, decoder_output)\ndecoder.summary()\n\n\nModel: \"model_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n decoder_input (InputLayer)  [(None, 2)]               0         \n                                                                 \n dense_1 (Dense)             (None, 2048)              6144      \n                                                                 \n reshape_1 (Reshape)         (None, 4, 4, 128)         0         \n                                                                 \n conv2d_transpose_3 (Conv2D  (None, 8, 8, 128)         147584    \n Transpose)                                                      \n                                                                 \n conv2d_transpose_4 (Conv2D  (None, 16, 16, 64)        73792     \n Transpose)                                                      \n                                                                 \n conv2d_transpose_5 (Conv2D  (None, 32, 32, 32)        18464     \n Transpose)                                                      \n                                                                 \n decoder_output (Conv2D)     (None, 32, 32, 1)         289       \n                                                                 \n=================================================================\nTotal params: 246273 (962.00 KB)\nTrainable params: 246273 (962.00 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\nPutting the encoder and decoder together.\n\n\nCode\nEPOCHS = 5\nBETA = 500\n\n\n\n\nCode\nclass VAE(models.Model):\n    def __init__(self, encoder, decoder, **kwargs):\n        super(VAE, self).__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n        self.total_loss_tracker = metrics.Mean(name=\"total_loss\")\n        self.reconstruction_loss_tracker = metrics.Mean(name=\"reconstruction_loss\")\n        self.kl_loss_tracker = metrics.Mean(name=\"kl_loss\")\n\n    @property\n    def metrics(self):\n        return [\n            self.total_loss_tracker,\n            self.reconstruction_loss_tracker,\n            self.kl_loss_tracker,\n        ]\n\n    def call(self, inputs):\n        \"\"\"Call the model on a particular input.\"\"\"\n        z_mean, z_log_var, z = encoder(inputs)\n        reconstruction = decoder(z)\n        return z_mean, z_log_var, reconstruction\n\n    def train_step(self, data):\n        \"\"\"Step run during training.\"\"\"\n        with tf.GradientTape() as tape:\n            z_mean, z_log_var, reconstruction = self(data)\n            reconstruction_loss = tf.reduce_mean(\n                BETA * losses.binary_crossentropy(data, reconstruction, axis=(1, 2, 3))\n            )\n            kl_loss = tf.reduce_mean(\n                tf.reduce_sum(-0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)), axis=1)\n            )\n            total_loss = reconstruction_loss + kl_loss\n\n        grads = tape.gradient(total_loss, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n\n        self.total_loss_tracker.update_state(total_loss)\n        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n        self.kl_loss_tracker.update_state(kl_loss)\n\n        return {m.name: m.result() for m in self.metrics}\n\n    def test_step(self, data):\n        \"\"\"Step run during validation.\"\"\"\n        if isinstance(data, tuple):\n            data = data[0]\n\n        z_mean, z_log_var, reconstruction = self(data)\n        reconstruction_loss = tf.reduce_mean(\n            BETA * losses.binary_crossentropy(data, reconstruction, axis=(1, 2, 3))\n        )\n        kl_loss = tf.reduce_mean(\n            tf.reduce_sum(\n                -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)),\n                axis=1,\n            )\n        )\n        total_loss = reconstruction_loss + kl_loss\n\n        return {\n            \"loss\": total_loss,\n            \"reconstruction_loss\": reconstruction_loss,\n            \"kl_loss\": kl_loss,\n        }\n\n\nInstantiate the VAE model and compile it.\n\n\nCode\nvae = VAE(encoder, decoder)\n\n# optimizer = optimizers.Adam(learning_rate=0.0005)\noptimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0005)\nvae.compile(optimizer=optimizer)\n\n\n\n\n\nTrain the VAE as before.\n\n\nCode\nvae.fit(\n    x_train,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    validation_data=(x_test, x_test),\n)\n\n\nEpoch 1/5\n600/600 [==============================] - 37s 61ms/step - total_loss: 160.4693 - reconstruction_loss: 155.9913 - kl_loss: 4.4779 - val_loss: 141.2442 - val_reconstruction_loss: 136.1877 - val_kl_loss: 5.0565\nEpoch 2/5\n600/600 [==============================] - 34s 57ms/step - total_loss: 135.9397 - reconstruction_loss: 130.9409 - kl_loss: 4.9988 - val_loss: 138.5623 - val_reconstruction_loss: 133.5856 - val_kl_loss: 4.9767\nEpoch 3/5\n600/600 [==============================] - 34s 56ms/step - total_loss: 134.3719 - reconstruction_loss: 129.3381 - kl_loss: 5.0338 - val_loss: 137.1351 - val_reconstruction_loss: 132.1540 - val_kl_loss: 4.9811\nEpoch 4/5\n600/600 [==============================] - 34s 56ms/step - total_loss: 133.4455 - reconstruction_loss: 128.3819 - kl_loss: 5.0637 - val_loss: 136.5461 - val_reconstruction_loss: 131.4780 - val_kl_loss: 5.0681\nEpoch 5/5\n600/600 [==============================] - 34s 57ms/step - total_loss: 132.7808 - reconstruction_loss: 127.6688 - kl_loss: 5.1120 - val_loss: 135.8917 - val_reconstruction_loss: 130.7375 - val_kl_loss: 5.1542\n\n\n&lt;keras.src.callbacks.History at 0x2c59e9f10&gt;"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson3/chapter3.html#analysing-the-vae",
    "href": "posts/ml/generative_deep_learning/lesson3/chapter3.html#analysing-the-vae",
    "title": "Generative AI: VAEs",
    "section": "",
    "text": "As before, we can eyeball the reconstructions from our model.\n\n\nCode\n# Select a subset of the test set\nn_to_predict = 5000\nexample_images = x_test[:n_to_predict]\nexample_labels = y_test[:n_to_predict]\n\n# Create autoencoder predictions and display\nz_mean, z_log_var, reconstructions = vae.predict(example_images)\nprint(\"Example real clothing items\")\nplot_sample_images(example_images)\nprint(\"Reconstructions\")\nplot_sample_images(reconstructions)\n\n\n 42/157 [=======&gt;......................] - ETA: 1s157/157 [==============================] - 1s 9ms/step\nExample real clothing items\nReconstructions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can sample and decode points in the latent space to observe how the model generates images. We note that:\n\nThe latent space has more even coverage and does not stray to far from a standard Normal distribution. If this is not the case, we can vary the \\(\\beta\\) value used to give more weight to the KL loss term.\nWe do not see as many poorly formed images as we did when sampling a “gap” in a standard autoencoder.\n\n\n\nCode\n# Encode the example images\nz_mean, z_var, z = encoder.predict(example_images)\n\n# Sample some points in the latent space, from the standard normal distribution\ngrid_width, grid_height = (6, 3)\nz_sample = np.random.normal(size=(grid_width * grid_height, 2))\n# Decode the sampled points\nreconstructions = decoder.predict(z_sample)\n# Convert original embeddings and sampled embeddings to p-values\np = norm.cdf(z)\np_sample = norm.cdf(z_sample)\n# Draw a plot of...\nfigsize = 8\nplt.figure(figsize=(figsize, figsize))\n\n# ... the original embeddings ...\nplt.scatter(z[:, 0], z[:, 1], c=\"black\", alpha=0.5, s=2)\n\n# ... and the newly generated points in the latent space\nplt.scatter(z_sample[:, 0], z_sample[:, 1], c=\"#00B0F0\", alpha=1, s=40)\nplt.show()\n\n# Add underneath a grid of the decoded images\nfig = plt.figure(figsize=(figsize, grid_height * 2))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i in range(grid_width * grid_height):\n    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n    ax.axis(\"off\")\n    ax.text(\n        0.5,\n        -0.35,\n        str(np.round(z_sample[i, :], 1)),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")\n\n\n1/1 [==============================] - 0s 14ms/step\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe plots below show the latent space coloured by clothing type. The left plot shows this in terms of z-values and the right in terms of p-values.\nThe latent space is more continuous with fewer gaps, and different categories take similar amounts of space.\n\n\nCode\n# Colour the embeddings by their label (clothing type - see table)\nfigsize = 8\nfig = plt.figure(figsize=(figsize * 2, figsize))\nax = fig.add_subplot(1, 2, 1)\nplot_1 = ax.scatter(\n    z[:, 0], z[:, 1], cmap=\"rainbow\", c=example_labels, alpha=0.8, s=3\n)\nplt.colorbar(plot_1)\nax = fig.add_subplot(1, 2, 2)\nplot_2 = ax.scatter(\n    p[:, 0], p[:, 1], cmap=\"rainbow\", c=example_labels, alpha=0.8, s=3\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nNext we see what happens when we sample from the latent space in a regular grid.\n\n\nCode\n# Colour the embeddings by their label (clothing type - see table)\nfigsize = 12\ngrid_size = 15\nplt.figure(figsize=(figsize, figsize))\nplt.scatter(\n    p[:, 0], p[:, 1], cmap=\"rainbow\", c=example_labels, alpha=0.8, s=300\n)\nplt.colorbar()\n\nx = norm.ppf(np.linspace(0, 1, grid_size))\ny = norm.ppf(np.linspace(1, 0, grid_size))\nxv, yv = np.meshgrid(x, y)\nxv = xv.flatten()\nyv = yv.flatten()\ngrid = np.array(list(zip(xv, yv)))\n\nreconstructions = decoder.predict(grid)\n# plt.scatter(grid[:, 0], grid[:, 1], c=\"black\", alpha=1, s=10)\nplt.show()\n\nfig = plt.figure(figsize=(figsize, figsize))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\nfor i in range(grid_size**2):\n    ax = fig.add_subplot(grid_size, grid_size, i + 1)\n    ax.axis(\"off\")\n    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")\n\n\n8/8 [==============================] - 0s 6ms/step"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson3/chapter3.html#references",
    "href": "posts/ml/generative_deep_learning/lesson3/chapter3.html#references",
    "title": "Generative AI: VAEs",
    "section": "",
    "text": "Chapter 3 of Generative Deep Learning by David Foster."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson1/lesson.html",
    "href": "posts/ml/generative_deep_learning/lesson1/lesson.html",
    "title": "Generative AI: Intro",
    "section": "",
    "text": "The 30000ft view of generative AI.\n\n\n\nGenerative modeling is a branch of machine learning that involves training a model to produce new data that is similar ot a given dataset.\n\nThis means we require some training data which we wish to imitate. The model should be probabilistic rather than deterministic, so that we can sample different variations of the output.\nA helpful way to define generative modeling is by contrasting it with its counterpart discriminative modeling:\n\nDiscriminative modeling estimates \\(p(y|x)\\) - the probability of a label \\(y\\) given some observation \\(x\\)\nGenerative modeling estimates \\(p(x)\\) - the probability of seeing an observation \\(x\\). By sampling from this distribution we can generate new observations.\n\nWe can also create a conditional generative model to estimate \\(p(x|y)\\), the probability of seeing an observation \\(x\\) with label \\(y\\). For ample, if we train a model to generate a given type of fruit.\n\n\n\n\n\nThe aim:\n\nWe have training data of observations \\(X\\)\nWe assume the training data has been generated by an unknown distribution \\(p_{data}\\)\nWe want to build a generative model \\(p_{model}\\) that mimics \\(p_{data}\\)\n\nIf we succeed, we can sample from \\(p_{model}\\) to generate synthetic observations\n\n\nDesirable properties of \\(p_{model}\\) are:\n\nAccuracy - If \\(p_{model}\\) is a high value it should look like a real observation, and likewise low values should look fake.\nGeneration - It should be easy to sample from it.\nRepresentation - It should be possible to understand how high-level features are represented by the model.\n\n\n\n\nWe want to be able to describe things in terms of high-level features. For example, when describing appearance, we might talk about hair colour, length, eye colour, etc. Not RGB values pixel by pixel…\nThis is the idea behind representation learning. We describe each training data observation using some lower-dimensional latent space. Then we learn a maaping function from the latent space to the original domain.\nEncoder-decoder techniques try to transform a high-dimensional nonlinear manifold into a simpler latent space.\n\n\n\n\n\nThe complete set of all values that an observation \\(x\\) can take.\n\n\n\nA function \\(p(x)\\) that maps a point \\(x\\) in the sample space to a number between 0 and 1. The integral over the sample space must equal 1 for it to be a well-define probability distribtution.\nThere is one true data distribution \\(p_{data}\\) but infinitely many approximations \\(p_{model}\\) that we can find.\n\n\n\nWe can structure our approach to finding \\(p_{model}\\) by restricting ourselves to a family of density functions \\(p_{\\theta}(x)\\) which can be described with a finite set of parameters \\(\\theta\\).\nFor example, restricting our search to a linear model \\(y = w*x + b\\) where we need to find the parameters \\(w\\) and \\(b\\).\n\n\n\nThe likelihood, \\(L(\\theta|x)\\), of a parameter set \\(\\theta\\) is a function which measures the plausability of the parameters having observed some data point \\(x\\). It is the probability of seeing the data \\(x\\) if the true data-generating distribution was the model parameterized by \\(\\theta\\).\nIt is defined as the parametric model density: \\[\nL(\\theta|x) = p_{\\theta}(x)\n\\]\nIf we have a dataset of independent observations \\(X\\) then the probabilities multiply: \\[\nL(\\theta|X) = \\prod_{x \\in X} p_{\\theta}(x)\n\\]\nThe product of a lot of decimals can get unwieldy, so we often take the log-likelihood \\(l\\) to turn the product into a sum: \\[\nl(\\theta|X) = \\sum_{x \\in X} \\log{p_{\\theta}(x) }\n\\]\nThe focus of parametric modeling is therefore to find the optimal parameter set \\(\\hat{\\theta}\\), which leads to…\n\n\n\nA technique to estimate \\(\\hat{\\theta}\\), the set of parameters that is most likely to explain the observed data \\(X\\): \\[\n\\hat{\\theta} = \\arg\\max_x l(\\theta|X)\n\\]\nNeural networks often work with a loss function, so this is equivalent to minimising the negative log-likelihood: \\[\n\\hat{\\theta} = \\arg\\min_\\theta -l(\\theta|X) = \\arg\\min_\\theta -\\log{p_{\\theta}(X)}\n\\]\n\n\n\nGenerative modeling is a form of MLE where the parameters are the neural network weights. We want to find the weights that maximise the likelihood of observing the training data.\nBut for high-dimensional problems \\(p_{\\theta}(X)\\) is intractable, it cannot be calculated directly.\nThere are different approaches to maing the problem tractable. These are summarised in the following section and explored in more detail in subsequent chapters.\n\n\n\n\nAll types of generative model are seeking to solve the same problem, but they take different approaches to modeling the intractable density function \\(p_{\\theta}(x)\\).\nThere are three general approaches:\n\nExplicitly model the density function but constrain the model in some way.\nExplicitly model a tractable approximation of the density function.\nImplicitly model the density function, using a stochastic process that generates the data directly.\n\n\n\n\n\n\nflowchart LR\n\n\n  A(Generative models) --&gt; B1(Explicit density)\n  A(Generative models) --&gt; B2(Implicit density)\n\n  B1 --&gt; C1(Approximate density)\n  B1 --&gt; C2(Tractable density)\n\n  C1 --&gt; D1(VAE)\n  C1 --&gt; D2(Energy-based model)\n  C1 --&gt; D3(Diffusion model)\n\n  C2 --&gt; D4(Autoregressive model)\n  C2 --&gt; D5(Normalizing flow model)\n\n  B2 --&gt; D6(GAN)\n\n\n\n\n\n\n\nImplicit density models don’t even try to estimate the probability density, instead focusing on producing a stochastic data generation process directly.\nTractable models place constraints on the model architecture (i.e. the parameters \\(\\theta\\)).\n\n\n\n\n\nChapter 1 of Generative Deep Learning by David Foster."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson1/lesson.html#what-is-generative-modeling",
    "href": "posts/ml/generative_deep_learning/lesson1/lesson.html#what-is-generative-modeling",
    "title": "Generative AI: Intro",
    "section": "",
    "text": "Generative modeling is a branch of machine learning that involves training a model to produce new data that is similar ot a given dataset.\n\nThis means we require some training data which we wish to imitate. The model should be probabilistic rather than deterministic, so that we can sample different variations of the output.\nA helpful way to define generative modeling is by contrasting it with its counterpart discriminative modeling:\n\nDiscriminative modeling estimates \\(p(y|x)\\) - the probability of a label \\(y\\) given some observation \\(x\\)\nGenerative modeling estimates \\(p(x)\\) - the probability of seeing an observation \\(x\\). By sampling from this distribution we can generate new observations.\n\nWe can also create a conditional generative model to estimate \\(p(x|y)\\), the probability of seeing an observation \\(x\\) with label \\(y\\). For ample, if we train a model to generate a given type of fruit."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson1/lesson.html#the-generative-modeling-framework",
    "href": "posts/ml/generative_deep_learning/lesson1/lesson.html#the-generative-modeling-framework",
    "title": "Generative AI: Intro",
    "section": "",
    "text": "The aim:\n\nWe have training data of observations \\(X\\)\nWe assume the training data has been generated by an unknown distribution \\(p_{data}\\)\nWe want to build a generative model \\(p_{model}\\) that mimics \\(p_{data}\\)\n\nIf we succeed, we can sample from \\(p_{model}\\) to generate synthetic observations\n\n\nDesirable properties of \\(p_{model}\\) are:\n\nAccuracy - If \\(p_{model}\\) is a high value it should look like a real observation, and likewise low values should look fake.\nGeneration - It should be easy to sample from it.\nRepresentation - It should be possible to understand how high-level features are represented by the model."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson1/lesson.html#representation-learning",
    "href": "posts/ml/generative_deep_learning/lesson1/lesson.html#representation-learning",
    "title": "Generative AI: Intro",
    "section": "",
    "text": "We want to be able to describe things in terms of high-level features. For example, when describing appearance, we might talk about hair colour, length, eye colour, etc. Not RGB values pixel by pixel…\nThis is the idea behind representation learning. We describe each training data observation using some lower-dimensional latent space. Then we learn a maaping function from the latent space to the original domain.\nEncoder-decoder techniques try to transform a high-dimensional nonlinear manifold into a simpler latent space."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson1/lesson.html#core-probability-theory",
    "href": "posts/ml/generative_deep_learning/lesson1/lesson.html#core-probability-theory",
    "title": "Generative AI: Intro",
    "section": "",
    "text": "The complete set of all values that an observation \\(x\\) can take.\n\n\n\nA function \\(p(x)\\) that maps a point \\(x\\) in the sample space to a number between 0 and 1. The integral over the sample space must equal 1 for it to be a well-define probability distribtution.\nThere is one true data distribution \\(p_{data}\\) but infinitely many approximations \\(p_{model}\\) that we can find.\n\n\n\nWe can structure our approach to finding \\(p_{model}\\) by restricting ourselves to a family of density functions \\(p_{\\theta}(x)\\) which can be described with a finite set of parameters \\(\\theta\\).\nFor example, restricting our search to a linear model \\(y = w*x + b\\) where we need to find the parameters \\(w\\) and \\(b\\).\n\n\n\nThe likelihood, \\(L(\\theta|x)\\), of a parameter set \\(\\theta\\) is a function which measures the plausability of the parameters having observed some data point \\(x\\). It is the probability of seeing the data \\(x\\) if the true data-generating distribution was the model parameterized by \\(\\theta\\).\nIt is defined as the parametric model density: \\[\nL(\\theta|x) = p_{\\theta}(x)\n\\]\nIf we have a dataset of independent observations \\(X\\) then the probabilities multiply: \\[\nL(\\theta|X) = \\prod_{x \\in X} p_{\\theta}(x)\n\\]\nThe product of a lot of decimals can get unwieldy, so we often take the log-likelihood \\(l\\) to turn the product into a sum: \\[\nl(\\theta|X) = \\sum_{x \\in X} \\log{p_{\\theta}(x) }\n\\]\nThe focus of parametric modeling is therefore to find the optimal parameter set \\(\\hat{\\theta}\\), which leads to…\n\n\n\nA technique to estimate \\(\\hat{\\theta}\\), the set of parameters that is most likely to explain the observed data \\(X\\): \\[\n\\hat{\\theta} = \\arg\\max_x l(\\theta|X)\n\\]\nNeural networks often work with a loss function, so this is equivalent to minimising the negative log-likelihood: \\[\n\\hat{\\theta} = \\arg\\min_\\theta -l(\\theta|X) = \\arg\\min_\\theta -\\log{p_{\\theta}(X)}\n\\]\n\n\n\nGenerative modeling is a form of MLE where the parameters are the neural network weights. We want to find the weights that maximise the likelihood of observing the training data.\nBut for high-dimensional problems \\(p_{\\theta}(X)\\) is intractable, it cannot be calculated directly.\nThere are different approaches to maing the problem tractable. These are summarised in the following section and explored in more detail in subsequent chapters."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson1/lesson.html#generative-model-taxonomy",
    "href": "posts/ml/generative_deep_learning/lesson1/lesson.html#generative-model-taxonomy",
    "title": "Generative AI: Intro",
    "section": "",
    "text": "All types of generative model are seeking to solve the same problem, but they take different approaches to modeling the intractable density function \\(p_{\\theta}(x)\\).\nThere are three general approaches:\n\nExplicitly model the density function but constrain the model in some way.\nExplicitly model a tractable approximation of the density function.\nImplicitly model the density function, using a stochastic process that generates the data directly.\n\n\n\n\n\n\nflowchart LR\n\n\n  A(Generative models) --&gt; B1(Explicit density)\n  A(Generative models) --&gt; B2(Implicit density)\n\n  B1 --&gt; C1(Approximate density)\n  B1 --&gt; C2(Tractable density)\n\n  C1 --&gt; D1(VAE)\n  C1 --&gt; D2(Energy-based model)\n  C1 --&gt; D3(Diffusion model)\n\n  C2 --&gt; D4(Autoregressive model)\n  C2 --&gt; D5(Normalizing flow model)\n\n  B2 --&gt; D6(GAN)\n\n\n\n\n\n\n\nImplicit density models don’t even try to estimate the probability density, instead focusing on producing a stochastic data generation process directly.\nTractable models place constraints on the model architecture (i.e. the parameters \\(\\theta\\))."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson1/lesson.html#references",
    "href": "posts/ml/generative_deep_learning/lesson1/lesson.html#references",
    "title": "Generative AI: Intro",
    "section": "",
    "text": "Chapter 1 of Generative Deep Learning by David Foster."
  },
  {
    "objectID": "posts/ml/fastai/lesson4/lesson.html",
    "href": "posts/ml/fastai/lesson4/lesson.html",
    "title": "FastAI Lesson 4: Natural Language Processing",
    "section": "",
    "text": "These are notes from lesson 4 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\nKaggle NLP pattern similarity notebook: see notebook\n\n\n\n\nNLP applications: categorising documents, translation, text generation.\nWe are using the Huggingface transformers library for this lesson. It is now incorporated into the fastai library.\nULMFit is an algorithm which uses fine-tuning, in this example to train a positve/negative sentiment classifier in 3 steps: 1. Train an RNN on wikipedia to predict the next word. No labels required. 2. Fine-tune this for IMDb reviews to predict the next word of a movie review. Still no labels required. 3. Fine-tune this to classify the sentiment.\nTransformers have overtaken ULMFit as the state-of-the-art.\nLooking “inside” a CNN, the first layer contains elementary detectors like edge detectors, blob detectors, gradient detectors etc. These get combined in non-linear ways to make increasingly complex detectors. Layer 2 might combine vertical and horizontal edge detectors into a corner detector. By the later layers, it is detecting rich features like lizard eyes, dog faces etc. See Zeiler and Fegus.\nFor the fine-tuning process, the earlier layers are unlikely to need changing because they are more general. So we only need to fine-tune (i.e. re-train) the later layers.\n\n\n\nAs an example of NLP in practice, we walk through this notebook for U.S. Patent Phrase to Phrase Matching.\nReshape the input to fit a standard NLP task\n\nWe want to learn the similarity between two fields and are provided with similarity scores.\nWe concat the fields of interest (with identifiers in between). The identifiers themselves are not important, they just need to be consistent.\nThe NLP model is then a supervised regression task to predict the score given the concatendated string.\n\ndf['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor\n\n\nSplit the text into tokens (words). Tokens are, broadly speaking, words.\nThere are some caveats to that, as some languages like Chinese don’t fit nicely into that model. We don’t want the vocabulary to be too big. Alternative approaches use character tokenization rather than word tokenization.\nIn practice, we tokenize into subwords.\n\n\n\nMap each unique token to a number. One-hot encoding.\nThe choice of tokenization and numericalization depends on the model you use. Whoever trained the model chose a convention for tokenizing. We need to be consistent with that if we want the model to work correctly.\n\n\n\nThe Huggingface model hub contains thousands of pretrained models.\nFor NLP tasks, it is useful to choose a model that was trained on a similar corpus, so you can search the model hub. In this case, we search for “patent”.\nSome models are general purpose, e.g. deberta-v3 used in the lesson.\nULMFit handles large documents better as it can split up the document. Transformer approaches require loading the whole document into GPU memory, so struggle for larger documents.\n\n\n\nIf a model is too simple (i.e. not flexible enough) then it cannot fit the data and be biased, leading to underfitting.\nIf the model fits the data points too closely, it is overfitting.\nA good validation set, and monitoring validation error rather than training error as a metric, is key to avoiding overfitting. See this article for an in-depth discussion on the importance of choosing a good validation set.\nOften people will default to using a random train/test split. This is what scikit-learn uses. This is a BAD idea very often.\nFor time-series data, it’s easier to infer gaps than it is to predict a block in the future. The latter is the more common task but a random split simulates the former, giving unrealistically good performance.\nFor image data, there may be people, boats, etc that are in the training set but not the test set. By failing to have new people in the validation set, the model can learn things about specific people/boats that it can’t rely on in practice.\n\n\n\nMetrics are things that are human-understandable.\nLoss functions should be smooth and differentiable to aid in training.\nThese can sometimes be the same thing, but not in general. For example, accuracy is a good metric in image classification. We could tweak the weights in such a way that it improves the model slightly, but not so much that it now correctly classifies a previously incorrect image. This means the metric function is bumpy, therefore a bad loss function.\nThis article goes into more detail on the choice of metrics.\nAI can be particularly dangerous at confirming systematic biases, because it is so good at optimising metrics, so it will conform to any biases present in the training data. Making decisions based on the model then reinforces those biases.\n\nGoodhart’s law applies: If a metric becomes a target it’s no longer a good metric\n\n\n\n\nThe best way to understand a metric is not to look at the mathematical formula, but to plot some data for which the metric is high, medium and low, then see what that tells you.\nAfter that, look at the equation to see if your intuition matches the logic.\n\n\n\nFast AI has a function to find a good starting point. Otherwise, pick a small value and keep doubling it until it falls apart.\n\n\n\n\n\nCourse lesson page\nHuggingFace Transformers\nVisualizing and Understanding Convolutional Networks\nHuggingFace models\nValidation sets\nModel metrics"
  },
  {
    "objectID": "posts/ml/fastai/lesson4/lesson.html#nlp-background-and-transformers",
    "href": "posts/ml/fastai/lesson4/lesson.html#nlp-background-and-transformers",
    "title": "FastAI Lesson 4: Natural Language Processing",
    "section": "",
    "text": "NLP applications: categorising documents, translation, text generation.\nWe are using the Huggingface transformers library for this lesson. It is now incorporated into the fastai library.\nULMFit is an algorithm which uses fine-tuning, in this example to train a positve/negative sentiment classifier in 3 steps: 1. Train an RNN on wikipedia to predict the next word. No labels required. 2. Fine-tune this for IMDb reviews to predict the next word of a movie review. Still no labels required. 3. Fine-tune this to classify the sentiment.\nTransformers have overtaken ULMFit as the state-of-the-art.\nLooking “inside” a CNN, the first layer contains elementary detectors like edge detectors, blob detectors, gradient detectors etc. These get combined in non-linear ways to make increasingly complex detectors. Layer 2 might combine vertical and horizontal edge detectors into a corner detector. By the later layers, it is detecting rich features like lizard eyes, dog faces etc. See Zeiler and Fegus.\nFor the fine-tuning process, the earlier layers are unlikely to need changing because they are more general. So we only need to fine-tune (i.e. re-train) the later layers."
  },
  {
    "objectID": "posts/ml/fastai/lesson4/lesson.html#kaggle-competition-walkthrough",
    "href": "posts/ml/fastai/lesson4/lesson.html#kaggle-competition-walkthrough",
    "title": "FastAI Lesson 4: Natural Language Processing",
    "section": "",
    "text": "As an example of NLP in practice, we walk through this notebook for U.S. Patent Phrase to Phrase Matching.\nReshape the input to fit a standard NLP task\n\nWe want to learn the similarity between two fields and are provided with similarity scores.\nWe concat the fields of interest (with identifiers in between). The identifiers themselves are not important, they just need to be consistent.\nThe NLP model is then a supervised regression task to predict the score given the concatendated string.\n\ndf['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor\n\n\nSplit the text into tokens (words). Tokens are, broadly speaking, words.\nThere are some caveats to that, as some languages like Chinese don’t fit nicely into that model. We don’t want the vocabulary to be too big. Alternative approaches use character tokenization rather than word tokenization.\nIn practice, we tokenize into subwords.\n\n\n\nMap each unique token to a number. One-hot encoding.\nThe choice of tokenization and numericalization depends on the model you use. Whoever trained the model chose a convention for tokenizing. We need to be consistent with that if we want the model to work correctly.\n\n\n\nThe Huggingface model hub contains thousands of pretrained models.\nFor NLP tasks, it is useful to choose a model that was trained on a similar corpus, so you can search the model hub. In this case, we search for “patent”.\nSome models are general purpose, e.g. deberta-v3 used in the lesson.\nULMFit handles large documents better as it can split up the document. Transformer approaches require loading the whole document into GPU memory, so struggle for larger documents.\n\n\n\nIf a model is too simple (i.e. not flexible enough) then it cannot fit the data and be biased, leading to underfitting.\nIf the model fits the data points too closely, it is overfitting.\nA good validation set, and monitoring validation error rather than training error as a metric, is key to avoiding overfitting. See this article for an in-depth discussion on the importance of choosing a good validation set.\nOften people will default to using a random train/test split. This is what scikit-learn uses. This is a BAD idea very often.\nFor time-series data, it’s easier to infer gaps than it is to predict a block in the future. The latter is the more common task but a random split simulates the former, giving unrealistically good performance.\nFor image data, there may be people, boats, etc that are in the training set but not the test set. By failing to have new people in the validation set, the model can learn things about specific people/boats that it can’t rely on in practice.\n\n\n\nMetrics are things that are human-understandable.\nLoss functions should be smooth and differentiable to aid in training.\nThese can sometimes be the same thing, but not in general. For example, accuracy is a good metric in image classification. We could tweak the weights in such a way that it improves the model slightly, but not so much that it now correctly classifies a previously incorrect image. This means the metric function is bumpy, therefore a bad loss function.\nThis article goes into more detail on the choice of metrics.\nAI can be particularly dangerous at confirming systematic biases, because it is so good at optimising metrics, so it will conform to any biases present in the training data. Making decisions based on the model then reinforces those biases.\n\nGoodhart’s law applies: If a metric becomes a target it’s no longer a good metric\n\n\n\n\nThe best way to understand a metric is not to look at the mathematical formula, but to plot some data for which the metric is high, medium and low, then see what that tells you.\nAfter that, look at the equation to see if your intuition matches the logic.\n\n\n\nFast AI has a function to find a good starting point. Otherwise, pick a small value and keep doubling it until it falls apart."
  },
  {
    "objectID": "posts/ml/fastai/lesson4/lesson.html#references",
    "href": "posts/ml/fastai/lesson4/lesson.html#references",
    "title": "FastAI Lesson 4: Natural Language Processing",
    "section": "",
    "text": "Course lesson page\nHuggingFace Transformers\nVisualizing and Understanding Convolutional Networks\nHuggingFace models\nValidation sets\nModel metrics"
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html",
    "href": "posts/ml/fastai/lesson2/lesson.html",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "These are notes from lesson 2 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\n\nDeploy a model to Huggingface Spaces: see car classifier model\nDeploy a model to a Github Pages website: see car classifier website\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nA website for quizzes based on the book: AI quizzes\n\n\n\n\nIt can be useful to train a model on the data BEFORE you clean it\n\nCounterintuitive!\nThe confusion matrix output of the learner gives you a good intuition about which classifications are hard\nplot_top_losses shows which examples were hardest for the model to classify. This can find (1) when the model is correct but not confident, and (2) when the model was confident but incorrect\nImageClassifierCleaner shows the examples in the training and validation set ordered by loss, so we can choose to keep, reclassify or remove them\n\nFor image resizing, random resize crop can often be more effective.\n\nSquishing can result in weird, unrealistic images.\nPadding or mirroring can add false information that the model will erroneously learn.\nRandom crops give different sections of the image which acts as a form of data augmentation.\naug_transforms can be use for more sophisticated data augmentation like warping and recoloring images.\n\n\n\n\nOnce you are happy with the model you’ve trained, you can pickle the learner object and save it.\nlearn.export('model.pkl')\nThen you can add the saved model to the hugging face space. To use the model to make predictions, any external functions you used to create the model will need to be instantiated too.\nlearn = load_learner('model.pkl')\nlearn.predict(image)\n\n\n\nHugging face spaces hosts models with a choice of pre-canned interfaces to quickly deploy a model to the public. Gradio is used in the example in the lecture. Streamlit is an alternative to Gradio that is more flexible.\nGradio requires a dict of classes as keys and probabilities (as floats not tensors) as the values. To go from the Gradio prototype to a production app, you can view the Gradio API from the huggingface space which will show you the API. The API exposes an endpoint which you can then hit from your own frontend app.\nGithub pages is a free and simple way to host a public website. See this fastai repo as an example of a minimal example html website which issues GET requests to the Gradio API\n\n\n\nTo convert a notebook to a python script, you can add #|export to the top of any cells to include in the script, then use:\nfrom nbdev.export import notebook2script\nnotebook2script('name_of_output_file.py')\nUse #|default_exp app in the first cell of the notebook to set the default name of the exported python file.\n\n\n\nHow do we choose the number of epochs to train for? Whenever it is “good enough” for your use case.\nIf you need to train for longer, you may need to use data augmentation to prevent overfitting. Keep an eye on the validation error to check overfitting.\n\n\n\n\nCourse lesson page\nHuggingFace Spaces"
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html#data-pre-processing",
    "href": "posts/ml/fastai/lesson2/lesson.html#data-pre-processing",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "It can be useful to train a model on the data BEFORE you clean it\n\nCounterintuitive!\nThe confusion matrix output of the learner gives you a good intuition about which classifications are hard\nplot_top_losses shows which examples were hardest for the model to classify. This can find (1) when the model is correct but not confident, and (2) when the model was confident but incorrect\nImageClassifierCleaner shows the examples in the training and validation set ordered by loss, so we can choose to keep, reclassify or remove them\n\nFor image resizing, random resize crop can often be more effective.\n\nSquishing can result in weird, unrealistic images.\nPadding or mirroring can add false information that the model will erroneously learn.\nRandom crops give different sections of the image which acts as a form of data augmentation.\naug_transforms can be use for more sophisticated data augmentation like warping and recoloring images."
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html#saving-a-model",
    "href": "posts/ml/fastai/lesson2/lesson.html#saving-a-model",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "Once you are happy with the model you’ve trained, you can pickle the learner object and save it.\nlearn.export('model.pkl')\nThen you can add the saved model to the hugging face space. To use the model to make predictions, any external functions you used to create the model will need to be instantiated too.\nlearn = load_learner('model.pkl')\nlearn.predict(image)"
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html#hosting-a-model",
    "href": "posts/ml/fastai/lesson2/lesson.html#hosting-a-model",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "Hugging face spaces hosts models with a choice of pre-canned interfaces to quickly deploy a model to the public. Gradio is used in the example in the lecture. Streamlit is an alternative to Gradio that is more flexible.\nGradio requires a dict of classes as keys and probabilities (as floats not tensors) as the values. To go from the Gradio prototype to a production app, you can view the Gradio API from the huggingface space which will show you the API. The API exposes an endpoint which you can then hit from your own frontend app.\nGithub pages is a free and simple way to host a public website. See this fastai repo as an example of a minimal example html website which issues GET requests to the Gradio API"
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html#using-nbdev",
    "href": "posts/ml/fastai/lesson2/lesson.html#using-nbdev",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "To convert a notebook to a python script, you can add #|export to the top of any cells to include in the script, then use:\nfrom nbdev.export import notebook2script\nnotebook2script('name_of_output_file.py')\nUse #|default_exp app in the first cell of the notebook to set the default name of the exported python file."
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html#training-the-model-and-when-to-stop",
    "href": "posts/ml/fastai/lesson2/lesson.html#training-the-model-and-when-to-stop",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "How do we choose the number of epochs to train for? Whenever it is “good enough” for your use case.\nIf you need to train for longer, you may need to use data augmentation to prevent overfitting. Keep an eye on the validation error to check overfitting."
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html#references",
    "href": "posts/ml/fastai/lesson2/lesson.html#references",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "Course lesson page\nHuggingFace Spaces"
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html",
    "href": "posts/ml/fastai/lesson7/lesson.html",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "These are notes from lesson 7 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\n\nCreate a collaborative filtering model in a spreadsheet\n\n\n\n\n\nWe have users ratings of movies.\nSay we had “embeddings” of a set of categories for each. So for a given movie, we have a vector of [action, sci-fi, romance] and for a given user we have their preference for [action, sci-fi, romance]. Then we could do the dot product between user embedding and movie embedding to get the probability that the user likes that movie. That is, the predicted user rating.\nSo the problem boils down to:\n\nWhat are the embeddings? i.e. the salient factors ([action, sci-fi, romance] in the example above)\nHow do we get them?\n\nThe answer to both questions is: we just let the model learn them.\nLet’s just pick a randomised embedding for each movie and each user. Then we have a loss function which is the MAE between predicted user rating for a movie and actual rating. Now we can use SGD to optimise those embeddings to find the best values.\n\n\n\nTo gain an intuition behind the calculations behind a collaborative filter, we can work through a (smaller) example in excel. This allows us to see the logic and dig into the calculations before we create them “for real” in Python.\n\n\n\n\n\n\nTip\n\n\n\nThis can be found in this spreadsheet.\n\n\nWe first look at an example where the results are in a cross-table and we can take the dot product of user embeddings and movie embeddings.\nThen we reshape the problem slightly by placing all of the embeddings in a matrix and doing a lookup. This is essentially what pytorch does, although it uses matrix multiplication by one-hot encoded vectors rather than array lookups for computational efficiency.\nWe then add a bias term to account for some users who love all movies, or hate all movies. And also movies that are universally beloved.\n\n\n\nThe broad idea behind collaborative filtering is:\n\nIf we could quantify the most salient “latent factors” about a movie, and…\nQuantify how much a user cares about that factor, then…\nIf we multiplied the two (dot product) it would give a measure of their rating.\n\nBut what are those latent factors? We let the model learn it. 1. We initialise randomised latent factors (called embeddings) 2. We use that to predict the user’s rating for each move. Initially, those randomised weights will give terrible predictions. 3. Our loss function is the MSE of the ground truth actual predictions and the prediction rating. 4. We can optimise the embedding values to minimise this loss function.\n\n\nWe use data on user ratings of movies sourced from MovieLens. The ml-latest-small data set is downloaded and saved in the DATA_DIR folder.\n\nfrom pathlib import Path\n\nfrom fastai.collab import CollabDataLoaders, Module, Embedding, collab_learner\nfrom fastai.tabular.all import one_hot, sigmoid_range, MSELossFlat, Learner, get_emb_sz\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd \nimport torch\n\n\nDATA_DIR = Path(\"/Users/gurpreetjohl/workspace/python/ml-practice/ml-practice/datasets/ml-latest-small\")\n\nLoad the ratings data which we will use for this task:\n\nratings = pd.read_csv(DATA_DIR / 'ratings.csv')\nratings\n\n\n\n\n\n\n\n\nuserId\nmovieId\nrating\ntimestamp\n\n\n\n\n0\n1\n1\n4.0\n964982703\n\n\n1\n1\n3\n4.0\n964981247\n\n\n2\n1\n6\n4.0\n964982224\n\n\n3\n1\n47\n5.0\n964983815\n\n\n4\n1\n50\n5.0\n964982931\n\n\n...\n...\n...\n...\n...\n\n\n100831\n610\n166534\n4.0\n1493848402\n\n\n100832\n610\n168248\n5.0\n1493850091\n\n\n100833\n610\n168250\n5.0\n1494273047\n\n\n100834\n610\n168252\n5.0\n1493846352\n\n\n100835\n610\n170875\n3.0\n1493846415\n\n\n\n\n100836 rows × 4 columns\n\n\n\nThe users and movies are encoded as integers.\nFor reference, we can load the movies data to see what each movieId corresponds to:\n\nmovies = pd.read_csv(DATA_DIR / 'movies.csv')\nmovies\n\n\n\n\n\n\n\n\nmovieId\ntitle\ngenres\n\n\n\n\n0\n1\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n1\n2\nJumanji (1995)\nAdventure|Children|Fantasy\n\n\n2\n3\nGrumpier Old Men (1995)\nComedy|Romance\n\n\n3\n4\nWaiting to Exhale (1995)\nComedy|Drama|Romance\n\n\n4\n5\nFather of the Bride Part II (1995)\nComedy\n\n\n...\n...\n...\n...\n\n\n9737\n193581\nBlack Butler: Book of the Atlantic (2017)\nAction|Animation|Comedy|Fantasy\n\n\n9738\n193583\nNo Game No Life: Zero (2017)\nAnimation|Comedy|Fantasy\n\n\n9739\n193585\nFlint (2017)\nDrama\n\n\n9740\n193587\nBungo Stray Dogs: Dead Apple (2018)\nAction|Animation\n\n\n9741\n193609\nAndrew Dice Clay: Dice Rules (1991)\nComedy\n\n\n\n\n9742 rows × 3 columns\n\n\n\nWe’ll merge the two for easier human readability.\n\nratings = ratings.merge(movies)\nratings\n\n\n\n\n\n\n\n\nuserId\nmovieId\nrating\ntimestamp\ntitle\ngenres\n\n\n\n\n0\n1\n1\n4.0\n964982703\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n1\n1\n3\n4.0\n964981247\nGrumpier Old Men (1995)\nComedy|Romance\n\n\n2\n1\n6\n4.0\n964982224\nHeat (1995)\nAction|Crime|Thriller\n\n\n3\n1\n47\n5.0\n964983815\nSeven (a.k.a. Se7en) (1995)\nMystery|Thriller\n\n\n4\n1\n50\n5.0\n964982931\nUsual Suspects, The (1995)\nCrime|Mystery|Thriller\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n100831\n610\n166534\n4.0\n1493848402\nSplit (2017)\nDrama|Horror|Thriller\n\n\n100832\n610\n168248\n5.0\n1493850091\nJohn Wick: Chapter Two (2017)\nAction|Crime|Thriller\n\n\n100833\n610\n168250\n5.0\n1494273047\nGet Out (2017)\nHorror\n\n\n100834\n610\n168252\n5.0\n1493846352\nLogan (2017)\nAction|Sci-Fi\n\n\n100835\n610\n170875\n3.0\n1493846415\nThe Fate of the Furious (2017)\nAction|Crime|Drama|Thriller\n\n\n\n\n100836 rows × 6 columns\n\n\n\n\n\n\n\ndls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\ndls.show_batch()\n\n\n\n\n\nuserId\ntitle\nrating\n\n\n\n\n0\n4\nMighty Aphrodite (1995)\n3.0\n\n\n1\n573\nDark Knight, The (2008)\n5.0\n\n\n2\n116\nAmadeus (1984)\n3.0\n\n\n3\n380\nAddams Family, The (1991)\n5.0\n\n\n4\n353\nBrothers McMullen, The (1995)\n4.0\n\n\n5\n37\nFugitive, The (1993)\n4.0\n\n\n6\n356\nUnbreakable (2000)\n4.0\n\n\n7\n489\nAlien³ (a.k.a. Alien 3) (1992)\n3.5\n\n\n8\n174\nNell (1994)\n5.0\n\n\n9\n287\nPanic Room (2002)\n2.5\n\n\n\n\n\nInitialise randomised 5-dimensional embeddings.\nHow should we choose the number of latent factors? (5 in the example above). Jeremy wrote down some ballpark values for models of different sizes in excel, then fit a function to it to get a heuristic measure. This is the default used by fast AI.\n\nn_users  = len(dls.classes['userId'])\nn_movies = len(dls.classes['title'])\nn_factors = 5\n\nuser_factors = torch.randn(n_users, n_factors)\nmovie_factors = torch.randn(n_movies, n_factors)\n\nDoing a matrix multiply by a one hot encoded vector is the same as doing a lookup in an array, just in a more computationally efficient way. Recall the softmax example.\nAn embedding is essentially just “look up in an array”.\n\n\n\n\nPutting a sigmoid_range on the final layer to squish ratings to fit 0 to 5 means “the model doesn’t have to work as hard” to get movies in the right range. In practice we use 5.5 as the sigmoid scale value as a sigmoid can never hit 1, but we want ratings to be able to hit 5.\n\nfrom fastai.collab import CollabDataLoaders, Module, Embedding\nfrom fastai.tabular.all import one_hot, sigmoid_range, MSELossFlat, Learner\n\n\nclass DotProduct(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.y_range = y_range\n\n    def forward(self, x):\n        users = self.user_factors(x[:, 0])\n        movies = self.movie_factors(x[:, 1])\n        # Apply a sigmoid to the raw_output\n        raw_output = (users * movies).sum(dim=1)\n        return sigmoid_range(raw_output, *self.y_range)\n\nWe can now fit a model\n\nembedding_dim = 50\nnum_epochs = 5\nmax_learning_rate = 5e-3\n\nmodel = DotProduct(n_users, n_movies, embedding_dim)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(num_epochs, max_learning_rate)\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\nAdding a user bias term and a movie bias term to the prediction call helps account for the fact that some users always rate high (4 or 5) but other users always rate low. And similarly for movies if everyone always rates it a 5 or a 1.\n\nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.user_bias = Embedding(n_users, 1)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.movie_bias = Embedding(n_movies, 1)\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        raw_output = (users * movies).sum(dim=1, keepdim=True)\n        raw_output += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n        return sigmoid_range(raw_output, *self.y_range)\n\n\nmodel = DotProductBias(n_users, n_movies, embedding_dim)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(num_epochs, max_learning_rate)\n\n\n\n\n\n\n    \n      \n      0.00% [0/5 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/1260 00:00&lt;?]\n    \n    \n\n\n\n\n\nThe validation loss in the previous model decreases then icnreases, which is a clear indication of overfitting.\nWe want to avoid overfitting, but data augmentation isn’t possible here. One approach is to use weight decay AKA L2 regularisation. We add sum of weights squared to the loss function.\nHow does this prevent overfitting? The larger the coefficients, the sharper the canyons the model is able to produce, which allows it to fit individual data points. By penalising larger weights, it will only produce sharp changes if this causes the model to fit many points well, so it should generalise better.\nWe essentially want to modify our loss function with an additional term dependent on the magnitude of the weights:\nloss_with_weight_decay = loss + weight_decay * (parameters**2).sum()\nIn practice, these values would be large and numerically unstable. We only actually care about the gradient of the loss, so we can add the gradient of the additional term to the existing gradient.\nparameters.grad += weight_decay * 2 * parameters\nBut weight_decay is just a constant that we choose, so we can fold the 2* term into it.\n\nweight_decay = 0.1\n\nmodel = DotProductBias(n_users, n_movies, embedding_dim)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(num_epochs, max_learning_rate, wd=weight_decay)\n\n\n\n\n\n\n    \n      \n      0.00% [0/5 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n\n\n\n    \n      \n      0.16% [2/1260 00:00&lt;00:20 1.4480]\n    \n    \n\n\n\n\n\nIn the previous section, we used that pytorch (technically the fastai version) Embedding module.\nLet’s briefly take a look at this and create our own Embedding module from scratch.\n\n\nThe way pytorch knows if a tensor is a parameter (and therefore can calculate gradients on it) is if it inherits from nn.Parameter. Then a Module’s .parameters() method will list this tensor.\nAs an example of this behaviour, let’s create a module with some parameters but WITHOUT declaring these as Parameters:\n\nclass MyModule(Module):\n    def __init__(self): \n        self.a = torch.ones(3)\n\nmm = MyModule()\nlist(mm.parameters())\n\n[]\n\n\nWe declared a tensor a in MyModule but we don’t see it! Which means it wouldn’t be trained by Pytorch.\nInstead, let’s declare is as a Parameter:\n\nclass MyModuleWithParams(Module):\n    def __init__(self): \n        self.a = torch.nn.Parameter(torch.ones(3))\n\nmm_params = MyModuleWithParams()\nlist(mm_params.parameters())\n\n[Parameter containing:\n tensor([1., 1., 1.], requires_grad=True)]\n\n\nPytorch’s builtin modules all use Parameter for any trainable parameters, so we haven’t needed to explicitly declare this.\nAs an example, if we use Pytorch’s Linear layer, it will automatically appear as a parameter:\n\nclass MyModuleWithLinear(Module):\n    def __init__(self): \n        self.a = torch.nn.Linear(1, 3, bias=False)\n\nmm_linear = MyModuleWithLinear()\nlist(mm_linear.parameters())\n\n[Parameter containing:\n tensor([[-0.6689],\n         [-0.0181],\n         [ 0.8172]], requires_grad=True)]\n\n\n\ntype(mm_linear.a.weight)\n\ntorch.nn.parameter.Parameter\n\n\n\n\n\nAn Embedding object essentially instantiates a tensor of random weights of the given dimensions and declares this as a Parameter. Pytorch can then modify the weights when training.\n\ndef create_params(tensor_dims):\n    \"\"\"Create a tensor of the required size and fill it with random values.\"\"\"\n    embedding_tensor = torch.zeros(*tensor_dims).normal_(0, 0.01)\n    return torch.nn.Parameter(embedding_tensor)\n\nNow we can replace the import Embedding module with our custom implementation create_params in the DotProductBias module:\n\nclass DotProductBiasCustomEmbedding(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = create_params([n_users, n_factors])\n        self.user_bias = create_params([n_users])\n        self.movie_factors = create_params([n_movies, n_factors])\n        self.movie_bias = create_params([n_movies])\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors[x[:,0]]\n        movies = self.movie_factors[x[:,1]]\n        raw_output = (users * movies).sum(dim=1)\n        raw_output += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]\n        return sigmoid_range(raw_output, *self.y_range)\n\n\nmodel = DotProductBiasCustomEmbedding(n_users, n_movies, embedding_dim)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(num_epochs, max_learning_rate, wd=weight_decay)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.795456\n0.787701\n00:07\n\n\n1\n0.693971\n0.728376\n00:06\n\n\n2\n0.546227\n0.711909\n00:07\n\n\n3\n0.402994\n0.707349\n00:06\n\n\n4\n0.282765\n0.708693\n00:06\n\n\n\n\n\n\n\n\n\nWe can interrogate the model to learn more about these embeddings it has learned.\n\n\nWe can visualise the biases of our collaborative filter model to see:\n\nMovie biases: Which movies are bad even compared to other similar movies of that type? Lawnmower man 2 is crap even compared to similar action movies. But people love titanic even if they don’t normally like romance dramas.\nUser biases: Which users love any and all movies? Users who give a high rating to all movies.\n\nAccording to our biases, these movies are crap even for those who like that style of movie:\n\nmovie_bias = learn.model.movie_bias.squeeze()\nidxs = movie_bias.argsort()[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Karate Kid, Part III, The (1989)',\n 'Catwoman (2004)',\n 'Stuart Saves His Family (1995)',\n 'Speed 2: Cruise Control (1997)',\n 'Dungeons & Dragons (2000)']\n\n\nWhereas these are highly rated, even when users don’t normally like that type of movie:\n\nidxs = movie_bias.argsort()[-5:]\n[dls.classes['title'][i] for i in idxs]\n\n['Star Wars: Episode IV - A New Hope (1977)',\n 'Dark Knight, The (2008)',\n 'Green Mile, The (1999)',\n 'Forrest Gump (1994)',\n 'Shawshank Redemption, The (1994)']\n\n\n\n\n\nWe can visualise the weights to see what human-interpretable features the model is learning.\nWe can condense our embedding to 2 axes with PCA. we get a critically-acclaimed -&gt; popular x-axis and a action-dialog y-axis.\n\n\n\n\n\n\nTip\n\n\n\nFor more details on PCA and related methods, see computational linear algebra fastai course\n\n\n\ng = ratings.groupby('title')['rating'].count()\ntop_movies = g.sort_values(ascending=False).index.values[:1000]\ntop_idxs = torch.tensor([learn.dls.classes['title'].o2i[m] for m in top_movies])\nmovie_w = learn.model.movie_factors[top_idxs].cpu().detach()\nmovie_pca = movie_w.pca(3)\nfac0,fac1,fac2 = movie_pca.t()\nidxs = list(range(50))\nX = fac0[idxs]\nY = fac2[idxs]\nplt.figure(figsize=(12,12))\nplt.scatter(X, Y)\nfor i, x, y in zip(top_movies[idxs], X, Y):\n    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWe can also use the “embedding distance” (distance in the latent space) to see when two movies are similar. We use cosine similarity to determine this distance, which is similar in principle to Euclidean distance but normalised.\nIn the example below, we start with the movie Forrest Gump and find the closest movie to it in our embedding:\n\nmovie_idx = dls.classes['title'].o2i['Forrest Gump (1994)']\n\nmovie_factors = learn.model.movie_factors\ndistances = torch.nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[movie_idx][None])\nclosest_distance_idx = distances.argsort(descending=True)[1]\ndls.classes['title'][closest_distance_idx]\n\n'Beautiful Mind, A (2001)'\n\n\n\n\n\n\n\nWe can repeat the same exercise using the collaborative filter from the fastai library to see how it compares to our from-scratch implementation.\n\nlearn_fast = collab_learner(dls, n_factors=embedding_dim, y_range=(0, 5.5))\nlearn_fast.fit_one_cycle(num_epochs, max_learning_rate, wd=weight_decay)\n\n\n\n\n\n\n    \n      \n      0.00% [0/5 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n\n\n\n    \n      \n      0.40% [5/1260 00:00&lt;00:20 1.5528]\n    \n    \n\n\nWe can repeat any of the analysis of our from-scratch model. For example, the movies with the highest bias:\n\nmovie_bias = learn_fast.model.i_bias.weight.squeeze()\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Shawshank Redemption, The (1994)',\n 'Forrest Gump (1994)',\n 'Green Mile, The (1999)',\n 'Star Wars: Episode IV - A New Hope (1977)',\n 'Dark Knight, The (2008)']\n\n\n\n\n\nHow do you start off a collaborative filtering model? For example, when you first start out, you have no data on users or items.\nOr even for established companies, what happens when we have a new user or a new item, so the entire row or column is null?\nThere is no hard and fast solution, they all boil down to “use common sense”.\n\nA tempting option is to fill NaNs with the median latent vectors. But this might result in an odd combination that doesn’t exist in practice, i.e. the latent space isn’t continuous so this could be where a gap in the latent space lies. For example, a medium action, medium sci-fi film with medium romance and medium comedy that is medium popular and medium critically acclaimed.\nAnother option is to pick a user/item that is representative of the average taste.\nCreate a tabular model using answers to a new user survey. Ask the user some questions when they sign up, then create a model where the user’s embedding vector is the dependent variable and their answers, along with any other relevant signup metadata, are the independent variables.\n\nIt is important to be careful of a small number of extremely enthusiastic users dominating the recommendations. For example, people who watch anime watch a LOT of it, and rate a lot of it highly. So this could end up getting recommended to users outside of this niche.\nThis can create positive feedback loops that change the behaviour of your product in unexpected ways.\n\n\n\nThe matrix completion approach used previously is known as Probabilistic Matrix Factorization (PMF). An alternative approach is to use deep learning.\nIn practice the two approaches are often stacked in an ensemble.\nThis section explores the deep learning collaborative filtering approach from scratch, then recreates it using fastai’s library.\n\n\nWe are concatenating the embedding matrices together, rather than taking the dot product, so that we can pass it through a dense ANN.\nThese matrices can be different sizes, and the size of embedding to use for each depends on the number of classes in the data. Fastai has a heuristic method for this which we use here:\n\n(user_num_classes, user_num_embeddings), (item_num_classes, item_num_embeddings) = get_emb_sz(dls)\n\nWe can then use this in a simple neural network with one hidden layer:\n\nclass CollabNN(Module):\n    def __init__(self, user_embedding_size, item_embedding_size, y_range=(0, 5.5), n_activations=100):\n        self.user_factors = Embedding(*user_embedding_size)\n        self.item_factors = Embedding(*item_embedding_size)\n        self.layers = torch.nn.Sequential(\n            torch.nn.Linear(user_embedding_size[1] + item_embedding_size[1], n_activations),\n            torch.nn.ReLU(),\n            torch.nn.Linear(n_activations, 1))\n        self.y_range = y_range\n        \n    def forward(self, x):\n        embs = self.user_factors(x[:, 0]), self.item_factors(x[:, 1])\n        x = self.layers(torch.cat(embs, dim=1))\n        return sigmoid_range(x, *self.y_range)\n    \ncollab_nn_model = CollabNN(user_embedding_size=(user_num_classes, user_num_embeddings),\n                           item_embedding_size=(item_num_classes, item_num_embeddings))\n\nNow train this model on the data:\n\nlearn_nn = Learner(dls, collab_nn_model, loss_func=MSELossFlat())\nlearn_nn.fit_one_cycle(num_epochs, max_learning_rate, wd=weight_decay)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.799004\n0.792579\n00:10\n\n\n1\n0.747623\n0.755708\n00:10\n\n\n2\n0.706981\n0.723887\n00:10\n\n\n3\n0.650337\n0.719642\n00:10\n\n\n4\n0.569418\n0.734302\n00:10\n\n\n\n\n\n\n\n\nWe can repeat the same exercise using fastai’s implementation.\nThis is almost identical to the PMF approach, simply with an additional argument use_nn=True.\n\nlearn_nn_fast = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100, 50])\nlearn_nn_fast.fit_one_cycle(num_epochs, max_learning_rate, wd=weight_decay)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.838047\n0.801519\n00:14\n\n\n1\n0.761085\n0.744033\n00:12\n\n\n2\n0.709788\n0.734091\n00:13\n\n\n3\n0.653415\n0.728950\n00:13\n\n\n4\n0.545074\n0.743957\n00:12\n\n\n\n\n\n\n\n\n\nThe recommender problem is one where we have some users and their ratings of some items. We want to know which unseen items a user may like.\nWe implemented two approaches to collaborative filtering:\n\nProbabilistic Matrix Factorization (PMF)\nA neural network\n\nFor each approach, we build a model from scratch in Pytorch, then compared that with fastai’s implementation. For the PMF approach, we even gained some intuition by creating a spreadsheet implementation first!\nIn practice, both approaches can be stacked for improved recommendations.\nThe idea we explored here of user some (initially random) embeddings to represent an entity and then letting our model learn them is a powerful one and it is not limited to collaborative learning. NLP uses embeddings to represent each unique token (i.e. each word with word-level tokenisation). It can then understand relationships between similar words, much like we were able to use embedding distances to identify similar movies.\n\n\n\n\nCourse lesson page\nCollaborative filtering notebook\nComputational linear algebra fastai course"
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html#the-intuition-behind-collaborative-filtering",
    "href": "posts/ml/fastai/lesson7/lesson.html#the-intuition-behind-collaborative-filtering",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "We have users ratings of movies.\nSay we had “embeddings” of a set of categories for each. So for a given movie, we have a vector of [action, sci-fi, romance] and for a given user we have their preference for [action, sci-fi, romance]. Then we could do the dot product between user embedding and movie embedding to get the probability that the user likes that movie. That is, the predicted user rating.\nSo the problem boils down to:\n\nWhat are the embeddings? i.e. the salient factors ([action, sci-fi, romance] in the example above)\nHow do we get them?\n\nThe answer to both questions is: we just let the model learn them.\nLet’s just pick a randomised embedding for each movie and each user. Then we have a loss function which is the MAE between predicted user rating for a movie and actual rating. Now we can use SGD to optimise those embeddings to find the best values."
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html#a-deep-learning-spreadsheet",
    "href": "posts/ml/fastai/lesson7/lesson.html#a-deep-learning-spreadsheet",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "To gain an intuition behind the calculations behind a collaborative filter, we can work through a (smaller) example in excel. This allows us to see the logic and dig into the calculations before we create them “for real” in Python.\n\n\n\n\n\n\nTip\n\n\n\nThis can be found in this spreadsheet.\n\n\nWe first look at an example where the results are in a cross-table and we can take the dot product of user embeddings and movie embeddings.\nThen we reshape the problem slightly by placing all of the embeddings in a matrix and doing a lookup. This is essentially what pytorch does, although it uses matrix multiplication by one-hot encoded vectors rather than array lookups for computational efficiency.\nWe then add a bias term to account for some users who love all movies, or hate all movies. And also movies that are universally beloved."
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html#input-data-and-factors",
    "href": "posts/ml/fastai/lesson7/lesson.html#input-data-and-factors",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "The broad idea behind collaborative filtering is:\n\nIf we could quantify the most salient “latent factors” about a movie, and…\nQuantify how much a user cares about that factor, then…\nIf we multiplied the two (dot product) it would give a measure of their rating.\n\nBut what are those latent factors? We let the model learn it. 1. We initialise randomised latent factors (called embeddings) 2. We use that to predict the user’s rating for each move. Initially, those randomised weights will give terrible predictions. 3. Our loss function is the MSE of the ground truth actual predictions and the prediction rating. 4. We can optimise the embedding values to minimise this loss function.\n\n\nWe use data on user ratings of movies sourced from MovieLens. The ml-latest-small data set is downloaded and saved in the DATA_DIR folder.\n\nfrom pathlib import Path\n\nfrom fastai.collab import CollabDataLoaders, Module, Embedding, collab_learner\nfrom fastai.tabular.all import one_hot, sigmoid_range, MSELossFlat, Learner, get_emb_sz\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd \nimport torch\n\n\nDATA_DIR = Path(\"/Users/gurpreetjohl/workspace/python/ml-practice/ml-practice/datasets/ml-latest-small\")\n\nLoad the ratings data which we will use for this task:\n\nratings = pd.read_csv(DATA_DIR / 'ratings.csv')\nratings\n\n\n\n\n\n\n\n\nuserId\nmovieId\nrating\ntimestamp\n\n\n\n\n0\n1\n1\n4.0\n964982703\n\n\n1\n1\n3\n4.0\n964981247\n\n\n2\n1\n6\n4.0\n964982224\n\n\n3\n1\n47\n5.0\n964983815\n\n\n4\n1\n50\n5.0\n964982931\n\n\n...\n...\n...\n...\n...\n\n\n100831\n610\n166534\n4.0\n1493848402\n\n\n100832\n610\n168248\n5.0\n1493850091\n\n\n100833\n610\n168250\n5.0\n1494273047\n\n\n100834\n610\n168252\n5.0\n1493846352\n\n\n100835\n610\n170875\n3.0\n1493846415\n\n\n\n\n100836 rows × 4 columns\n\n\n\nThe users and movies are encoded as integers.\nFor reference, we can load the movies data to see what each movieId corresponds to:\n\nmovies = pd.read_csv(DATA_DIR / 'movies.csv')\nmovies\n\n\n\n\n\n\n\n\nmovieId\ntitle\ngenres\n\n\n\n\n0\n1\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n1\n2\nJumanji (1995)\nAdventure|Children|Fantasy\n\n\n2\n3\nGrumpier Old Men (1995)\nComedy|Romance\n\n\n3\n4\nWaiting to Exhale (1995)\nComedy|Drama|Romance\n\n\n4\n5\nFather of the Bride Part II (1995)\nComedy\n\n\n...\n...\n...\n...\n\n\n9737\n193581\nBlack Butler: Book of the Atlantic (2017)\nAction|Animation|Comedy|Fantasy\n\n\n9738\n193583\nNo Game No Life: Zero (2017)\nAnimation|Comedy|Fantasy\n\n\n9739\n193585\nFlint (2017)\nDrama\n\n\n9740\n193587\nBungo Stray Dogs: Dead Apple (2018)\nAction|Animation\n\n\n9741\n193609\nAndrew Dice Clay: Dice Rules (1991)\nComedy\n\n\n\n\n9742 rows × 3 columns\n\n\n\nWe’ll merge the two for easier human readability.\n\nratings = ratings.merge(movies)\nratings\n\n\n\n\n\n\n\n\nuserId\nmovieId\nrating\ntimestamp\ntitle\ngenres\n\n\n\n\n0\n1\n1\n4.0\n964982703\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n1\n1\n3\n4.0\n964981247\nGrumpier Old Men (1995)\nComedy|Romance\n\n\n2\n1\n6\n4.0\n964982224\nHeat (1995)\nAction|Crime|Thriller\n\n\n3\n1\n47\n5.0\n964983815\nSeven (a.k.a. Se7en) (1995)\nMystery|Thriller\n\n\n4\n1\n50\n5.0\n964982931\nUsual Suspects, The (1995)\nCrime|Mystery|Thriller\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n100831\n610\n166534\n4.0\n1493848402\nSplit (2017)\nDrama|Horror|Thriller\n\n\n100832\n610\n168248\n5.0\n1493850091\nJohn Wick: Chapter Two (2017)\nAction|Crime|Thriller\n\n\n100833\n610\n168250\n5.0\n1494273047\nGet Out (2017)\nHorror\n\n\n100834\n610\n168252\n5.0\n1493846352\nLogan (2017)\nAction|Sci-Fi\n\n\n100835\n610\n170875\n3.0\n1493846415\nThe Fate of the Furious (2017)\nAction|Crime|Drama|Thriller\n\n\n\n\n100836 rows × 6 columns\n\n\n\n\n\n\n\ndls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\ndls.show_batch()\n\n\n\n\n\nuserId\ntitle\nrating\n\n\n\n\n0\n4\nMighty Aphrodite (1995)\n3.0\n\n\n1\n573\nDark Knight, The (2008)\n5.0\n\n\n2\n116\nAmadeus (1984)\n3.0\n\n\n3\n380\nAddams Family, The (1991)\n5.0\n\n\n4\n353\nBrothers McMullen, The (1995)\n4.0\n\n\n5\n37\nFugitive, The (1993)\n4.0\n\n\n6\n356\nUnbreakable (2000)\n4.0\n\n\n7\n489\nAlien³ (a.k.a. Alien 3) (1992)\n3.5\n\n\n8\n174\nNell (1994)\n5.0\n\n\n9\n287\nPanic Room (2002)\n2.5\n\n\n\n\n\nInitialise randomised 5-dimensional embeddings.\nHow should we choose the number of latent factors? (5 in the example above). Jeremy wrote down some ballpark values for models of different sizes in excel, then fit a function to it to get a heuristic measure. This is the default used by fast AI.\n\nn_users  = len(dls.classes['userId'])\nn_movies = len(dls.classes['title'])\nn_factors = 5\n\nuser_factors = torch.randn(n_users, n_factors)\nmovie_factors = torch.randn(n_movies, n_factors)\n\nDoing a matrix multiply by a one hot encoded vector is the same as doing a lookup in an array, just in a more computationally efficient way. Recall the softmax example.\nAn embedding is essentially just “look up in an array”."
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html#building-a-collaborative-filter-from-scratch",
    "href": "posts/ml/fastai/lesson7/lesson.html#building-a-collaborative-filter-from-scratch",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "Putting a sigmoid_range on the final layer to squish ratings to fit 0 to 5 means “the model doesn’t have to work as hard” to get movies in the right range. In practice we use 5.5 as the sigmoid scale value as a sigmoid can never hit 1, but we want ratings to be able to hit 5.\n\nfrom fastai.collab import CollabDataLoaders, Module, Embedding\nfrom fastai.tabular.all import one_hot, sigmoid_range, MSELossFlat, Learner\n\n\nclass DotProduct(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.y_range = y_range\n\n    def forward(self, x):\n        users = self.user_factors(x[:, 0])\n        movies = self.movie_factors(x[:, 1])\n        # Apply a sigmoid to the raw_output\n        raw_output = (users * movies).sum(dim=1)\n        return sigmoid_range(raw_output, *self.y_range)\n\nWe can now fit a model\n\nembedding_dim = 50\nnum_epochs = 5\nmax_learning_rate = 5e-3\n\nmodel = DotProduct(n_users, n_movies, embedding_dim)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(num_epochs, max_learning_rate)\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\nAdding a user bias term and a movie bias term to the prediction call helps account for the fact that some users always rate high (4 or 5) but other users always rate low. And similarly for movies if everyone always rates it a 5 or a 1.\n\nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.user_bias = Embedding(n_users, 1)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.movie_bias = Embedding(n_movies, 1)\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        raw_output = (users * movies).sum(dim=1, keepdim=True)\n        raw_output += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n        return sigmoid_range(raw_output, *self.y_range)\n\n\nmodel = DotProductBias(n_users, n_movies, embedding_dim)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(num_epochs, max_learning_rate)\n\n\n\n\n\n\n    \n      \n      0.00% [0/5 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/1260 00:00&lt;?]\n    \n    \n\n\n\n\n\nThe validation loss in the previous model decreases then icnreases, which is a clear indication of overfitting.\nWe want to avoid overfitting, but data augmentation isn’t possible here. One approach is to use weight decay AKA L2 regularisation. We add sum of weights squared to the loss function.\nHow does this prevent overfitting? The larger the coefficients, the sharper the canyons the model is able to produce, which allows it to fit individual data points. By penalising larger weights, it will only produce sharp changes if this causes the model to fit many points well, so it should generalise better.\nWe essentially want to modify our loss function with an additional term dependent on the magnitude of the weights:\nloss_with_weight_decay = loss + weight_decay * (parameters**2).sum()\nIn practice, these values would be large and numerically unstable. We only actually care about the gradient of the loss, so we can add the gradient of the additional term to the existing gradient.\nparameters.grad += weight_decay * 2 * parameters\nBut weight_decay is just a constant that we choose, so we can fold the 2* term into it.\n\nweight_decay = 0.1\n\nmodel = DotProductBias(n_users, n_movies, embedding_dim)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(num_epochs, max_learning_rate, wd=weight_decay)\n\n\n\n\n\n\n    \n      \n      0.00% [0/5 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n\n\n\n    \n      \n      0.16% [2/1260 00:00&lt;00:20 1.4480]\n    \n    \n\n\n\n\n\nIn the previous section, we used that pytorch (technically the fastai version) Embedding module.\nLet’s briefly take a look at this and create our own Embedding module from scratch.\n\n\nThe way pytorch knows if a tensor is a parameter (and therefore can calculate gradients on it) is if it inherits from nn.Parameter. Then a Module’s .parameters() method will list this tensor.\nAs an example of this behaviour, let’s create a module with some parameters but WITHOUT declaring these as Parameters:\n\nclass MyModule(Module):\n    def __init__(self): \n        self.a = torch.ones(3)\n\nmm = MyModule()\nlist(mm.parameters())\n\n[]\n\n\nWe declared a tensor a in MyModule but we don’t see it! Which means it wouldn’t be trained by Pytorch.\nInstead, let’s declare is as a Parameter:\n\nclass MyModuleWithParams(Module):\n    def __init__(self): \n        self.a = torch.nn.Parameter(torch.ones(3))\n\nmm_params = MyModuleWithParams()\nlist(mm_params.parameters())\n\n[Parameter containing:\n tensor([1., 1., 1.], requires_grad=True)]\n\n\nPytorch’s builtin modules all use Parameter for any trainable parameters, so we haven’t needed to explicitly declare this.\nAs an example, if we use Pytorch’s Linear layer, it will automatically appear as a parameter:\n\nclass MyModuleWithLinear(Module):\n    def __init__(self): \n        self.a = torch.nn.Linear(1, 3, bias=False)\n\nmm_linear = MyModuleWithLinear()\nlist(mm_linear.parameters())\n\n[Parameter containing:\n tensor([[-0.6689],\n         [-0.0181],\n         [ 0.8172]], requires_grad=True)]\n\n\n\ntype(mm_linear.a.weight)\n\ntorch.nn.parameter.Parameter\n\n\n\n\n\nAn Embedding object essentially instantiates a tensor of random weights of the given dimensions and declares this as a Parameter. Pytorch can then modify the weights when training.\n\ndef create_params(tensor_dims):\n    \"\"\"Create a tensor of the required size and fill it with random values.\"\"\"\n    embedding_tensor = torch.zeros(*tensor_dims).normal_(0, 0.01)\n    return torch.nn.Parameter(embedding_tensor)\n\nNow we can replace the import Embedding module with our custom implementation create_params in the DotProductBias module:\n\nclass DotProductBiasCustomEmbedding(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = create_params([n_users, n_factors])\n        self.user_bias = create_params([n_users])\n        self.movie_factors = create_params([n_movies, n_factors])\n        self.movie_bias = create_params([n_movies])\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors[x[:,0]]\n        movies = self.movie_factors[x[:,1]]\n        raw_output = (users * movies).sum(dim=1)\n        raw_output += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]\n        return sigmoid_range(raw_output, *self.y_range)\n\n\nmodel = DotProductBiasCustomEmbedding(n_users, n_movies, embedding_dim)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(num_epochs, max_learning_rate, wd=weight_decay)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.795456\n0.787701\n00:07\n\n\n1\n0.693971\n0.728376\n00:06\n\n\n2\n0.546227\n0.711909\n00:07\n\n\n3\n0.402994\n0.707349\n00:06\n\n\n4\n0.282765\n0.708693\n00:06\n\n\n\n\n\n\n\n\n\nWe can interrogate the model to learn more about these embeddings it has learned.\n\n\nWe can visualise the biases of our collaborative filter model to see:\n\nMovie biases: Which movies are bad even compared to other similar movies of that type? Lawnmower man 2 is crap even compared to similar action movies. But people love titanic even if they don’t normally like romance dramas.\nUser biases: Which users love any and all movies? Users who give a high rating to all movies.\n\nAccording to our biases, these movies are crap even for those who like that style of movie:\n\nmovie_bias = learn.model.movie_bias.squeeze()\nidxs = movie_bias.argsort()[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Karate Kid, Part III, The (1989)',\n 'Catwoman (2004)',\n 'Stuart Saves His Family (1995)',\n 'Speed 2: Cruise Control (1997)',\n 'Dungeons & Dragons (2000)']\n\n\nWhereas these are highly rated, even when users don’t normally like that type of movie:\n\nidxs = movie_bias.argsort()[-5:]\n[dls.classes['title'][i] for i in idxs]\n\n['Star Wars: Episode IV - A New Hope (1977)',\n 'Dark Knight, The (2008)',\n 'Green Mile, The (1999)',\n 'Forrest Gump (1994)',\n 'Shawshank Redemption, The (1994)']\n\n\n\n\n\nWe can visualise the weights to see what human-interpretable features the model is learning.\nWe can condense our embedding to 2 axes with PCA. we get a critically-acclaimed -&gt; popular x-axis and a action-dialog y-axis.\n\n\n\n\n\n\nTip\n\n\n\nFor more details on PCA and related methods, see computational linear algebra fastai course\n\n\n\ng = ratings.groupby('title')['rating'].count()\ntop_movies = g.sort_values(ascending=False).index.values[:1000]\ntop_idxs = torch.tensor([learn.dls.classes['title'].o2i[m] for m in top_movies])\nmovie_w = learn.model.movie_factors[top_idxs].cpu().detach()\nmovie_pca = movie_w.pca(3)\nfac0,fac1,fac2 = movie_pca.t()\nidxs = list(range(50))\nX = fac0[idxs]\nY = fac2[idxs]\nplt.figure(figsize=(12,12))\nplt.scatter(X, Y)\nfor i, x, y in zip(top_movies[idxs], X, Y):\n    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWe can also use the “embedding distance” (distance in the latent space) to see when two movies are similar. We use cosine similarity to determine this distance, which is similar in principle to Euclidean distance but normalised.\nIn the example below, we start with the movie Forrest Gump and find the closest movie to it in our embedding:\n\nmovie_idx = dls.classes['title'].o2i['Forrest Gump (1994)']\n\nmovie_factors = learn.model.movie_factors\ndistances = torch.nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[movie_idx][None])\nclosest_distance_idx = distances.argsort(descending=True)[1]\ndls.classes['title'][closest_distance_idx]\n\n'Beautiful Mind, A (2001)'"
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html#building-a-collaborative-filter-with-fastais-library",
    "href": "posts/ml/fastai/lesson7/lesson.html#building-a-collaborative-filter-with-fastais-library",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "We can repeat the same exercise using the collaborative filter from the fastai library to see how it compares to our from-scratch implementation.\n\nlearn_fast = collab_learner(dls, n_factors=embedding_dim, y_range=(0, 5.5))\nlearn_fast.fit_one_cycle(num_epochs, max_learning_rate, wd=weight_decay)\n\n\n\n\n\n\n    \n      \n      0.00% [0/5 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n\n\n\n    \n      \n      0.40% [5/1260 00:00&lt;00:20 1.5528]\n    \n    \n\n\nWe can repeat any of the analysis of our from-scratch model. For example, the movies with the highest bias:\n\nmovie_bias = learn_fast.model.i_bias.weight.squeeze()\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Shawshank Redemption, The (1994)',\n 'Forrest Gump (1994)',\n 'Green Mile, The (1999)',\n 'Star Wars: Episode IV - A New Hope (1977)',\n 'Dark Knight, The (2008)']"
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html#bootstrapping-a-collaborative-filtering-model",
    "href": "posts/ml/fastai/lesson7/lesson.html#bootstrapping-a-collaborative-filtering-model",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "How do you start off a collaborative filtering model? For example, when you first start out, you have no data on users or items.\nOr even for established companies, what happens when we have a new user or a new item, so the entire row or column is null?\nThere is no hard and fast solution, they all boil down to “use common sense”.\n\nA tempting option is to fill NaNs with the median latent vectors. But this might result in an odd combination that doesn’t exist in practice, i.e. the latent space isn’t continuous so this could be where a gap in the latent space lies. For example, a medium action, medium sci-fi film with medium romance and medium comedy that is medium popular and medium critically acclaimed.\nAnother option is to pick a user/item that is representative of the average taste.\nCreate a tabular model using answers to a new user survey. Ask the user some questions when they sign up, then create a model where the user’s embedding vector is the dependent variable and their answers, along with any other relevant signup metadata, are the independent variables.\n\nIt is important to be careful of a small number of extremely enthusiastic users dominating the recommendations. For example, people who watch anime watch a LOT of it, and rate a lot of it highly. So this could end up getting recommended to users outside of this niche.\nThis can create positive feedback loops that change the behaviour of your product in unexpected ways."
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html#deep-learning-for-collaborative-filtering",
    "href": "posts/ml/fastai/lesson7/lesson.html#deep-learning-for-collaborative-filtering",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "The matrix completion approach used previously is known as Probabilistic Matrix Factorization (PMF). An alternative approach is to use deep learning.\nIn practice the two approaches are often stacked in an ensemble.\nThis section explores the deep learning collaborative filtering approach from scratch, then recreates it using fastai’s library.\n\n\nWe are concatenating the embedding matrices together, rather than taking the dot product, so that we can pass it through a dense ANN.\nThese matrices can be different sizes, and the size of embedding to use for each depends on the number of classes in the data. Fastai has a heuristic method for this which we use here:\n\n(user_num_classes, user_num_embeddings), (item_num_classes, item_num_embeddings) = get_emb_sz(dls)\n\nWe can then use this in a simple neural network with one hidden layer:\n\nclass CollabNN(Module):\n    def __init__(self, user_embedding_size, item_embedding_size, y_range=(0, 5.5), n_activations=100):\n        self.user_factors = Embedding(*user_embedding_size)\n        self.item_factors = Embedding(*item_embedding_size)\n        self.layers = torch.nn.Sequential(\n            torch.nn.Linear(user_embedding_size[1] + item_embedding_size[1], n_activations),\n            torch.nn.ReLU(),\n            torch.nn.Linear(n_activations, 1))\n        self.y_range = y_range\n        \n    def forward(self, x):\n        embs = self.user_factors(x[:, 0]), self.item_factors(x[:, 1])\n        x = self.layers(torch.cat(embs, dim=1))\n        return sigmoid_range(x, *self.y_range)\n    \ncollab_nn_model = CollabNN(user_embedding_size=(user_num_classes, user_num_embeddings),\n                           item_embedding_size=(item_num_classes, item_num_embeddings))\n\nNow train this model on the data:\n\nlearn_nn = Learner(dls, collab_nn_model, loss_func=MSELossFlat())\nlearn_nn.fit_one_cycle(num_epochs, max_learning_rate, wd=weight_decay)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.799004\n0.792579\n00:10\n\n\n1\n0.747623\n0.755708\n00:10\n\n\n2\n0.706981\n0.723887\n00:10\n\n\n3\n0.650337\n0.719642\n00:10\n\n\n4\n0.569418\n0.734302\n00:10\n\n\n\n\n\n\n\n\nWe can repeat the same exercise using fastai’s implementation.\nThis is almost identical to the PMF approach, simply with an additional argument use_nn=True.\n\nlearn_nn_fast = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100, 50])\nlearn_nn_fast.fit_one_cycle(num_epochs, max_learning_rate, wd=weight_decay)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.838047\n0.801519\n00:14\n\n\n1\n0.761085\n0.744033\n00:12\n\n\n2\n0.709788\n0.734091\n00:13\n\n\n3\n0.653415\n0.728950\n00:13\n\n\n4\n0.545074\n0.743957\n00:12"
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html#summary",
    "href": "posts/ml/fastai/lesson7/lesson.html#summary",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "The recommender problem is one where we have some users and their ratings of some items. We want to know which unseen items a user may like.\nWe implemented two approaches to collaborative filtering:\n\nProbabilistic Matrix Factorization (PMF)\nA neural network\n\nFor each approach, we build a model from scratch in Pytorch, then compared that with fastai’s implementation. For the PMF approach, we even gained some intuition by creating a spreadsheet implementation first!\nIn practice, both approaches can be stacked for improved recommendations.\nThe idea we explored here of user some (initially random) embeddings to represent an entity and then letting our model learn them is a powerful one and it is not limited to collaborative learning. NLP uses embeddings to represent each unique token (i.e. each word with word-level tokenisation). It can then understand relationships between similar words, much like we were able to use embedding distances to identify similar movies."
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html#references",
    "href": "posts/ml/fastai/lesson7/lesson.html#references",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "Course lesson page\nCollaborative filtering notebook\nComputational linear algebra fastai course"
  },
  {
    "objectID": "posts/ml/fastai/lesson1/lesson.html",
    "href": "posts/ml/fastai/lesson1/lesson.html",
    "title": "FastAI Lesson 1: Image Classification Models",
    "section": "",
    "text": "These are notes from lesson 1 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\nTrain an image classifier: see car classification notebook\n\n\n\n\nThere is a companion course focusing on AI ethics here\nThoughts on education: play the whole game first to get an idea of the big picture. Don’t become afflicted with “elementitis” getting too bogged down in details too early.\n\nColoured cups - green, amber, red - for each student to indicate how well they are following\nMeta Learning by Radek Osmulski - a book based on the author’s experience of learning about deep learning (via the fastai course)\nMathematician’s Lament by Paul Lockhart - a book about the state of mathematics education\nMaking Learning Whole by David Perkins - a book about apporaches to holistic learning\n\nRISE is a jupyter notebook extension to turn notebooks into slides. Jeremy uses notebooks for: source code, book, blogging, CI/CD.\n\n\n\nBefore deep learning, the approach to machine learning was to enlist many domain experts to handcraft features and feed this into a constrained linear model (e.g. ridge regression). This is time-consuming, expensive and requires many domain experts.\nNeural networks learn these features. Looking inside a CNN, for example, shows that these learned features match interpretable features that an expert might handcraft. An illustration of the features learned is given in this paper by Zeiler and Fergus.\nFor image classifiers, you don’t need particularly large images as inputs. GPUs are so quick now that if you use large images, most of the time is spent on opening the file rather than computations. So often we resize images down to 400x400 pixels or smaller.\nFor most use cases, there are pre-trained models and sensible default values that we can use. In practice, most of the time is spent on the input layer and output layer. For most models the middle layers are identical.\n\n\n\nData blocks structure the input to learners. An overview of the DataBlock class:\n\nblocks determines the input and output type as a tuple. For multi-target classification this tuple can be arbitrary length.\nget_items is a function that returns a list of all the inputs.\nsplitter defines how to split the training/validation set.\nget_y is a function that returns the label of a given input image.\nitem_tfms defines what transforms to apply to the inputs before training, e.g. resize.\ndataloaders is a method that parallelises loading the data.\n\nA learner combines the model (e.g. resnet or something from timm library) and the data to run that model on (the dataloaders from the DataBlock).\nThe fine_tune method starts with a pretrained model weights rather than randomised weights, and only needs to learn the differences between your data and the original model.\nOther image problems that can utilise deep learning:\n\nImage classification\nImage segmentation\n\nOther problem types use the same process, just with different DataBlock blocks types and the rest is the same. For example, tabular data, collaborative filtering.\n\n\n\n\nTraditional computer programs are essentially:\n\n\n\n\n\n\nFigure 1: Traditional computer programs\n\n\n\nDeep learning models are:\n\n\n\n\n\n\nFigure 2: Deep learning model\n\n\n\n\n\n\n\nCourse lesson page\nVisualizing and Understanding Convolutional Networks\nTIMM library\nStatistical Modeling: The Two Cultures\n\n\n\n\nFigure 1: Traditional computer programs\nFigure 2: Deep learning model"
  },
  {
    "objectID": "posts/ml/fastai/lesson1/lesson.html#notes-on-learning",
    "href": "posts/ml/fastai/lesson1/lesson.html#notes-on-learning",
    "title": "FastAI Lesson 1: Image Classification Models",
    "section": "",
    "text": "There is a companion course focusing on AI ethics here\nThoughts on education: play the whole game first to get an idea of the big picture. Don’t become afflicted with “elementitis” getting too bogged down in details too early.\n\nColoured cups - green, amber, red - for each student to indicate how well they are following\nMeta Learning by Radek Osmulski - a book based on the author’s experience of learning about deep learning (via the fastai course)\nMathematician’s Lament by Paul Lockhart - a book about the state of mathematics education\nMaking Learning Whole by David Perkins - a book about apporaches to holistic learning\n\nRISE is a jupyter notebook extension to turn notebooks into slides. Jeremy uses notebooks for: source code, book, blogging, CI/CD."
  },
  {
    "objectID": "posts/ml/fastai/lesson1/lesson.html#background-on-deep-learning-and-image-classification",
    "href": "posts/ml/fastai/lesson1/lesson.html#background-on-deep-learning-and-image-classification",
    "title": "FastAI Lesson 1: Image Classification Models",
    "section": "",
    "text": "Before deep learning, the approach to machine learning was to enlist many domain experts to handcraft features and feed this into a constrained linear model (e.g. ridge regression). This is time-consuming, expensive and requires many domain experts.\nNeural networks learn these features. Looking inside a CNN, for example, shows that these learned features match interpretable features that an expert might handcraft. An illustration of the features learned is given in this paper by Zeiler and Fergus.\nFor image classifiers, you don’t need particularly large images as inputs. GPUs are so quick now that if you use large images, most of the time is spent on opening the file rather than computations. So often we resize images down to 400x400 pixels or smaller.\nFor most use cases, there are pre-trained models and sensible default values that we can use. In practice, most of the time is spent on the input layer and output layer. For most models the middle layers are identical."
  },
  {
    "objectID": "posts/ml/fastai/lesson1/lesson.html#overview-of-the-fastai-learner",
    "href": "posts/ml/fastai/lesson1/lesson.html#overview-of-the-fastai-learner",
    "title": "FastAI Lesson 1: Image Classification Models",
    "section": "",
    "text": "Data blocks structure the input to learners. An overview of the DataBlock class:\n\nblocks determines the input and output type as a tuple. For multi-target classification this tuple can be arbitrary length.\nget_items is a function that returns a list of all the inputs.\nsplitter defines how to split the training/validation set.\nget_y is a function that returns the label of a given input image.\nitem_tfms defines what transforms to apply to the inputs before training, e.g. resize.\ndataloaders is a method that parallelises loading the data.\n\nA learner combines the model (e.g. resnet or something from timm library) and the data to run that model on (the dataloaders from the DataBlock).\nThe fine_tune method starts with a pretrained model weights rather than randomised weights, and only needs to learn the differences between your data and the original model.\nOther image problems that can utilise deep learning:\n\nImage classification\nImage segmentation\n\nOther problem types use the same process, just with different DataBlock blocks types and the rest is the same. For example, tabular data, collaborative filtering."
  },
  {
    "objectID": "posts/ml/fastai/lesson1/lesson.html#deep-learning-vs-traditional-computer-programs",
    "href": "posts/ml/fastai/lesson1/lesson.html#deep-learning-vs-traditional-computer-programs",
    "title": "FastAI Lesson 1: Image Classification Models",
    "section": "",
    "text": "Traditional computer programs are essentially:\n\n\n\n\n\n\nFigure 1: Traditional computer programs\n\n\n\nDeep learning models are:\n\n\n\n\n\n\nFigure 2: Deep learning model"
  },
  {
    "objectID": "posts/ml/fastai/lesson1/lesson.html#references",
    "href": "posts/ml/fastai/lesson1/lesson.html#references",
    "title": "FastAI Lesson 1: Image Classification Models",
    "section": "",
    "text": "Course lesson page\nVisualizing and Understanding Convolutional Networks\nTIMM library\nStatistical Modeling: The Two Cultures\n\n\n\n\nFigure 1: Traditional computer programs\nFigure 2: Deep learning model"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter6/lesson.html",
    "href": "posts/ml/hands_on_llm/chapter6/lesson.html",
    "title": "Hands-On LLMs: Prompt Engineering",
    "section": "",
    "text": "Prompt engineering is an umbrella term for techniques that can improve the quality of a response from an LLM.\nIt extends beyond this as a tool to evaluate the model’s outputs and implement guardrails on the response."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter6/lesson.html#special-tokens",
    "href": "posts/ml/hands_on_llm/chapter6/lesson.html#special-tokens",
    "title": "Hands-On LLMs: Prompt Engineering",
    "section": "1.1. Special Tokens",
    "text": "1.1. Special Tokens\nWe can explore the tokens used by a model in a transformers pipeline by inspecting the chat template.\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False)\nThis gives a prompt in the form:\n&lt;s&gt;&lt;|user|&gt; Create a funny joke about chickens.&lt;|end|&gt; &lt;|assistant|&gt; \nThere are special tokens to indicate:\n\n&lt;s&gt; - the start of the prompt\n&lt;|user|&gt; - when a user (i.e. you) begins their message\n&lt;|end|&gt; - when that message ends\n&lt;|assistant|&gt; - when the assistant (the LLM) begins their message.\n\nThis gets passed to the model to complete the sequence, one token and a time until it generates an &lt;|end|&gt; token.\nThese special tokens help the model keep track of the context."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter6/lesson.html#temperature",
    "href": "posts/ml/hands_on_llm/chapter6/lesson.html#temperature",
    "title": "Hands-On LLMs: Prompt Engineering",
    "section": "1.2. Temperature",
    "text": "1.2. Temperature\nLLMs are fundamentally trained neural networks, so their output should be (and is) deterministic.\nBut we don’t want to always get the same response to a given prompt. That would be boring. We want a bit of pizazz.\nWhen the model is predicting the next token, what it actually does is create a probability distribution over all possible tokens in the vocabulary. It then samples from this distribution to choose the next word.\nTemperature defines how likely it is to choose less probable tokens.\nA temperature=0 will always choose the most likely token; this is greedy sampling.\nHigher temperatures lead to more creative outputs."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter6/lesson.html#constraining-the-sample-space-top-p-and-top-k",
    "href": "posts/ml/hands_on_llm/chapter6/lesson.html#constraining-the-sample-space-top-p-and-top-k",
    "title": "Hands-On LLMs: Prompt Engineering",
    "section": "1.3. Constraining the sample space: Top p and Top k",
    "text": "1.3. Constraining the sample space: Top p and Top k\nA related concept in sampling is to restrict the sample space, so rather than having the possibility (however small) of selecting any token in the vocab list, we constrain the possibilities.\nThe top_p parameter considers tokens from most to least probable until the cumulative probability reaches the given value. So top_p=1 considers all tokens, and a lower value filters out less probable tokens. This is known as nucleus sampling.\nThe top_k parameter restrict the sample space to the k most probable tokens."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter6/lesson.html#example-use-cases",
    "href": "posts/ml/hands_on_llm/chapter6/lesson.html#example-use-cases",
    "title": "Hands-On LLMs: Prompt Engineering",
    "section": "1.4. Example use cases",
    "text": "1.4. Example use cases\n\n\n\n\n\n\n\n\n\nUse case\ntemperature\ntop_p\nDescription\n\n\n\n\nBrainstorming\nHigh\nHigh\nCreative and unexpected responses\n\n\nEmail generation\nLow\nLow\nPredictable and focused responses that aren’t too out there\n\n\nCreative writing\nHigh\nLow\nCreative but still coherent\n\n\nTranslation\nLow\nHigh\nDeterministic output but with linguistic variety"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter6/lesson.html#the-ingredients-of-a-prompt",
    "href": "posts/ml/hands_on_llm/chapter6/lesson.html#the-ingredients-of-a-prompt",
    "title": "Hands-On LLMs: Prompt Engineering",
    "section": "2.1. The ingredients of a prompt",
    "text": "2.1. The ingredients of a prompt\nThe most basic prompt is simply an input, without even an instruction. The LLM will simply complete the sequence. E.g.\n\nThe sky is\n\n\nblue\n\nWe can extend this to an instruction prompt where we now have two components:\n\nInstruction - Classify the text into negative or positive\nData - “This is a great movie!”\n\nThe model may have seen similar instructions in its training data, or at least seen similar enough example to allow it to generalise.\nThis is called instruction-based prompting."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter6/lesson.html#instruction-based-tasks",
    "href": "posts/ml/hands_on_llm/chapter6/lesson.html#instruction-based-tasks",
    "title": "Hands-On LLMs: Prompt Engineering",
    "section": "2.2. Instruction-based tasks",
    "text": "2.2. Instruction-based tasks\nIt’s helpful to understand common tasks LLMs are used to perform, as they may have been trained on such examples, so if you use the phrasing they are familiar with, they are more likely to give you the desired output.\n\nClassification - Classify the text into positive, neutral or negative.\nSearch - Find the ___ in the following text\nSummarization - Summarise the following text\nCode generation - Generate python code for…\nNamed entity recognition - An entity is… Extract the named entities from the following text\n\nThe common features of these prompts for these different tasks are:\n\nSpecificity - accurately describe what you want to achieve\nHallucination - LLMs are confident, not correct. We can ask the LLM to generate an answer if it knows the answer, otherwise respond with I don’t know\nOrder - The instruction should come either at the beginning or end. LLMs tend to focus at either extreme, known as the primacy effect and the recency effect respectively."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter6/lesson.html#zero-shot",
    "href": "posts/ml/hands_on_llm/chapter6/lesson.html#zero-shot",
    "title": "Hands-On LLMs: Prompt Engineering",
    "section": "4.1. Zero-shot",
    "text": "4.1. Zero-shot\nZero-shot prompts provide no examples but outline the shape of the desired response. E.g.\n\nClassify the text into positive, neutral or negative Text: The food was great! Sentiment:"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter6/lesson.html#one-shot-and-few-shot",
    "href": "posts/ml/hands_on_llm/chapter6/lesson.html#one-shot-and-few-shot",
    "title": "Hands-On LLMs: Prompt Engineering",
    "section": "4.2. One-shot and Few-shot",
    "text": "4.2. One-shot and Few-shot\nFew-shot prompts provide two or more examples, one-shot prompts provide one example.\nWe can use the user and assistant roles in the prompt to distinguish the user prompt from the exemplar assistant output.\none_shot_prompt = [  \n    {\"role\": \"user\",\n    \"content\": \"A 'Gigamuru' is a type of Japanese musical instrument. An example of a sentence that uses the word Gigamuru is:\"  },\n    {\"role\": \"assistant\",\n     \"content\": \"I have a Gigamuru that my uncle gave me as a gift. I love to play it at home.\"  },\n    {\"role\": \"user\",\n     \"content\": \"To 'screeg' something is to swing a sword at it. An example of a sentence that uses the word screeg is:\"}\n]"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter6/lesson.html#chain-of-thought",
    "href": "posts/ml/hands_on_llm/chapter6/lesson.html#chain-of-thought",
    "title": "Hands-On LLMs: Prompt Engineering",
    "section": "6.1. Chain of thought",
    "text": "6.1. Chain of thought\nWe can coax this behaviour which mimics reasoning out of the LLM to improve the output.\nThis is analogous to the System 1 and System 2 thinking of Kahneman and Tversky; by default the LLM will give a knee-jerk System 1 response, but if we ask it to reason it will give an more considered System 2 response.\nWe can encourage this with a one-shot chain-of-thought prompt demonstrating reasoning. E.g.\ncot_prompt = [\n    {\"role\": \"user\",\n     \"content\": \"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\"},\n    {\"role\": \"assistant\", \n     \"content\": \"Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\"},\n     {\"role\": \"user\",\n      \"content\": \"The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\"}\n]\nThis guides the model towards providing an explanation as well as the answer. “Show your working!”\nThere is a zero-shot chain-of-thought approach that doesn’t require us to give an example of reasoning. A common and effective method is use this phrase to prime reasoning:\n\nLet’s think set-by-step\n\n(AKA the Bobby Valentino approach: slow down!)"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter6/lesson.html#self-consistency",
    "href": "posts/ml/hands_on_llm/chapter6/lesson.html#self-consistency",
    "title": "Hands-On LLMs: Prompt Engineering",
    "section": "6.2. Self-consistency",
    "text": "6.2. Self-consistency\nWe can use the same prompt multiple times and get different responses (with differing quality) due to the sampling nature of LLMs.\nWe can sample multiple responses and ask the LLM to give the majority vote as the response.\nThis does make it slower and more expensive though, since we’re prompting n times for each prompt."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter6/lesson.html#tree-of-thought",
    "href": "posts/ml/hands_on_llm/chapter6/lesson.html#tree-of-thought",
    "title": "Hands-On LLMs: Prompt Engineering",
    "section": "6.3. Tree of thought",
    "text": "6.3. Tree of thought\nThis is a combination of the previous ideas.\nTree of thought = Chain of thought + Self consistency \nWe break the problem down into multiple steps. At each step, we ask the model to explore different solutions, then vote for the best solution(s) and move on to the next step. The thoughts are rates, with the most promising kept and the least promising pruned.\nThe disadvantage is that it requires even more calls to the model, slowing things down.\nA zero-shot tree-of-thought approach is to ask the model to emulate a “discussion between multiple experts”. E.g.\nzeroshot_tot_prompt = [\n    {\"role\": \"user\",\n     \"content\": \"Imagine three different experts are answering this question. All experts will write down 1 step of their thinking, then share it with the group. Then all experts will go on to the next step, etc. If any expert realizes they're wrong at any point then they leave. The question is 'The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?' Make sure to discuss the results.\"}\n]\nAn extention of Three-of-thought is Graph-of-thought, where each prompt is treated like a node in a graph. Rather than a tree following a linear train of thought, the prompts can be reused in different orders and combinations in a graph."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter5/lesson.html#the-goal",
    "href": "posts/ml/hands_on_llm/chapter5/lesson.html#the-goal",
    "title": "Hands-On LLMs: Text Clustering and Topic Modeling",
    "section": "1.1 The goal",
    "text": "1.1 The goal\nText clustering is an unsupervised technique that aims to group similar texts based on their content, meaning and relationships.\n\n\n\nClusters in embedding space\n\n\nThe clusters can the be used for applications such as outlier detection, speeding up labelling and finding mislabelled data.\nIt also has applications in topic modelling, where we assign a label or keywords to a cluster describing its constituents.\nWe apply this to an example data set of Arxiv articles. Let’s load the Arxiv data we’ll use for this:\n\nfrom datasets import load_dataset\n\n\ndataset = load_dataset(\"maartengr/arxiv_nlp\")[\"train\"]\nabstracts = dataset[\"Abstracts\"]\ntitles = dataset[\"Titles\"]"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter5/lesson.html#text-clustering-pipeline",
    "href": "posts/ml/hands_on_llm/chapter5/lesson.html#text-clustering-pipeline",
    "title": "Hands-On LLMs: Text Clustering and Topic Modeling",
    "section": "1.2. Text clustering pipeline",
    "text": "1.2. Text clustering pipeline\nThere are many approaches to text clustering, including GNNs and centroid-based clustering. A common approach is:\n\nConvert input documents to embeddings, using an embedding model\nReduce the dimensionality of those embeddings, using a dimensionality reduction model\nGroup similar documents, using a cluster model\n\n\n1.2.1. The embedding model\nWe convert our text to embedding vectors using an embedding model. We should choose one that was trained to optimise semantic similarity (which most are). We can use the MTEB leaderboard to help select a good model.\nWe can load a pre-trained model and create our embeddings.\n\nfrom sentence_transformers import SentenceTransformer \n\nembedding_model = SentenceTransformer(\"thenlper/gte-small\") \nembeddings = embedding_model.encode(abstracts, show_progress_bar=True)\n\n\n\n\n\n\n\n\n\n\n/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.2.2. The dimensionality reduction model\nHigh-dimensional data can suffer from the curse of dimensionality, making it difficult to find meaningful clusters.\nWe can use a dimensionality reduction model to compress (not remove) dimensions which makes the downstream clustering easier.\nThis is, by it’s nature, a lossy transformation. But we hope that enough of the information is retained to be useful.\nStandard dimensionality reduction techniques include Principal Component Analysis (PCA) and Uniform Manifold Approximation (UMAP). We’ll use UMAP as it tends to handle nonlinear relationships better.\nThe following code reduces our embeddings from 384 -&gt; 5 dimensions. We set min_dist=0 as this allows embedded points to be arbitrarily close together, which results in tighter clusters, and metric=‘cosine’ generally performs better than Euclidean methods for high-dimensional data.\n\nfrom umap import UMAP\n\numap_model = UMAP(n_components=5, min_dist=0.0, metric='cosine', random_state=42)\nreduced_embeddings = umap_model.fit_transform(embeddings)\n\n/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n  warn(\nOMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n\n\n\n\n1.2.3. The cluster model\nCentroid-based algorithms like K-Nearest Neighbours (KNN) are popular in other settings but require us to specify the number of clusters ahead of time (which we don’t know) and forces all data points to be part of a cluster (there can’t be unassigned points). This makes them less useful for our use case.\nDensity-based algorithms calculate the number of clusters freely and do not force all points into a cluster. We’ll use HDBSCAN for our case.\n\n\n\nCentroid vs density approaches\n\n\nWe can cluster the data with the following code. We can vary min_cluster_size to change the number of clusters produced.\n\nfrom hdbscan import HDBSCAN \n\nhdbscan_model = HDBSCAN(min_cluster_size=50, metric=\"euclidean\", cluster_selection_method=\"eom\").fit(reduced_embeddings)\nclusters = hdbscan_model.labels_\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\nWe can inspect the clusters and plot the data in the reduced dimension space. Although helpful, it’s worth remembering that this is just an approximation of the real embeddings; some information is lost.\nFirst, let’s observe a selection of 3 documents from the first cluster to see if they seem similar:\n\nimport numpy as np \n\n# Print first three documents in cluster 0 \ncluster = 0 \nfor index in np.where(clusters==cluster)[0][:3]:\n    print(abstracts[index][:300] + \"... \\n\")\n\n  This works aims to design a statistical machine translation from English text\nto American Sign Language (ASL). The system is based on Moses tool with some\nmodifications and the results are synthesized through a 3D avatar for\ninterpretation. First, we translate the input text to gloss, a written fo... \n\n  Researches on signed languages still strongly dissociate lin- guistic issues\nrelated on phonological and phonetic aspects, and gesture studies for\nrecognition and synthesis purposes. This paper focuses on the imbrication of\nmotion and meaning for the analysis, synthesis and evaluation of sign lang... \n\n  Modern computational linguistic software cannot produce important aspects of\nsign language translation. Using some researches we deduce that the majority of\nautomatic sign language translation systems ignore many aspects when they\ngenerate animation; therefore the interpretation lost the truth inf... \n\n\n\nThey do! Now we can plot the data in the reduced embedding space:\n\nimport pandas as pd \n\n# Reduce 384-dimensional embeddings to two dimensions for easier visualization \nreduced_embeddings = UMAP(n_components=2, min_dist=0.0, metric=\"cosine\", random_state=42 ).fit_transform(embeddings) \n\ndf = pd.DataFrame(reduced_embeddings, columns=[\"x\", \"y\"]) \ndf[\"title\"] = titles \ndf[\"cluster\"] = [str(c) for c in clusters] \n\n# Select outliers and non-outliers (clusters) \nto_plot = df.loc[df.cluster != \"-1\", :] \noutliers = df.loc[df.cluster == \"-1\", :]\n\n/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n  warn(\n\n\n\nimport matplotlib.pyplot as plt \n\n# Plot outliers and non-outliers separately \nplt.scatter(outliers.x, outliers.y, alpha=0.05, s=2, c=\"grey\") \nplt.scatter(to_plot.x, to_plot.y, c=to_plot.cluster.astype(int), alpha=0.6, s=2, cmap=\"tab20b\" ) \nplt.axis(\"off\")\n\n(-7.562705826759339,\n 10.960084271430969,\n -3.4470335602760316,\n 18.276195919513704)"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter5/lesson.html#bertopic",
    "href": "posts/ml/hands_on_llm/chapter5/lesson.html#bertopic",
    "title": "Hands-On LLMs: Text Clustering and Topic Modeling",
    "section": "2.1. BERTopic",
    "text": "2.1. BERTopic\nThe first step is to perform text clustering using the same 3 steps outlined in the previous section.\nWe then use a bag-of-words approach per cluster (instead of per document as would usually be the case) to model a distribution over words per class. This is the CountVectorizer step.\nWe similarly use a class-specific variant of term-frequency inverse document-frequency called c-TF-IDF, which puts more weight on the meaningful words of that cluster.\nWe now have a generic text clustering pipeline:\n\n\n\n\n\nflowchart LR\n\n  A(Embeddings) --&gt; B(Dimensionality Reduction) --&gt; C(Clustering)\n\n\n\n\n\n\nAnd a topic modeling pipeline:\n\n\n\n\n\nflowchart LR\n\n  D(Cluster Bag-of-Words) --&gt; E(Keyword Selection)\n\n\n\n\n\n\nPutting this all together with our choice of components:\n\n\n\n\n\nflowchart LR\n\n  A(SBERT) --&gt; B(UMAP) --&gt; C(HDBSCAN) --&gt; D(CountVectorizer) --&gt; E(c-TF-IDF)\n\n\n\n\n\n\nThe idea behind BERTopic is that these components are modular, so each can be swapped out like lego blocks. For example, if you prefer K-means clusters or PCA over UMAP, just swap it.\n\n\n\nBERTopic modular components\n\n\nThis modularity also means the same base model can be used and adapted for different tasks and use cases by adding/removing components downstream of the base model.\n\n2.1.1. Create a BERTopic model\nWe can run this end-to-end pipeline using the previously defined models in BERTopic:\n\nfrom bertopic import BERTopic \n\n# Train our BERTopic model with our previously defined component models \ntopic_model = (BERTopic(embedding_model=embedding_model, \n                        umap_model=umap_model,\n                        hdbscan_model=hdbscan_model,\n                        verbose=True)\n               .fit(abstracts, embeddings))\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n2025-01-15 11:48:19,327 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n2025-01-15 11:48:47,848 - BERTopic - Dimensionality - Completed ✓\n2025-01-15 11:48:47,851 - BERTopic - Cluster - Start clustering the reduced embeddings\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n2025-01-15 11:48:49,745 - BERTopic - Cluster - Completed ✓\n2025-01-15 11:48:49,757 - BERTopic - Representation - Extracting topics from clusters using representation models.\n2025-01-15 11:48:51,583 - BERTopic - Representation - Completed ✓\n\n\n\n\n2.1.2. Explore the topics\nWe can then explore the topics found by the model.\nNote that the topic labelled -1 is a bucket for outliers that do not fit in any other cluster.\n\ntopic_model.get_topic_info()\n\n\n\n\n\n\n\n\nTopic\nCount\nName\nRepresentation\nRepresentative_Docs\n\n\n\n\n0\n-1\n13779\n-1_of_the_and_to\n[of, the, and, to, in, we, language, that, for...\n[ Language models have emerged as a central c...\n\n\n1\n0\n2224\n0_speech_asr_recognition_end\n[speech, asr, recognition, end, acoustic, spea...\n[ The amount of labeled data to train models ...\n\n\n2\n1\n2104\n1_question_qa_questions_answer\n[question, qa, questions, answer, answering, a...\n[ Multi-hop question answering (QA) requires ...\n\n\n3\n2\n1428\n2_medical_clinical_biomedical_patient\n[medical, clinical, biomedical, patient, healt...\n[ Clinical texts, such as admission notes, di...\n\n\n4\n3\n986\n3_translation_nmt_machine_neural\n[translation, nmt, machine, neural, bleu, engl...\n[ In this paper, we introduce a hybrid search...\n\n\n...\n...\n...\n...\n...\n...\n\n\n146\n145\n54\n145_gans_gan_adversarial_generation\n[gans, gan, adversarial, generation, generativ...\n[ Text generation is of particular interest i...\n\n\n147\n146\n54\n146_emoji_emojis_emoticons_sentiment\n[emoji, emojis, emoticons, sentiment, twitter,...\n[ The frequent use of Emojis on social media ...\n\n\n148\n147\n51\n147_prompt_prompts_optimization_prompting\n[prompt, prompts, optimization, prompting, llm...\n[ Prompt optimization aims to find the best p...\n\n\n149\n148\n51\n148_coherence_discourse_paragraph_text\n[coherence, discourse, paragraph, text, cohesi...\n[ While there has been significant progress t...\n\n\n150\n149\n51\n149_long_context_window_length\n[long, context, window, length, llms, memory, ...\n[ We present a series of long-context LLMs th...\n\n\n\n\n151 rows × 5 columns\n\n\n\nWe can explore a particular topic by its topic number:\n\ntopic_model.get_topic(2)\n\n[('medical', 0.0220337946328463),\n ('clinical', 0.02092442350104087),\n ('biomedical', 0.014552038344966458),\n ('patient', 0.010048801098837407),\n ('health', 0.008769124731484461),\n ('notes', 0.008421182820081155),\n ('patients', 0.0067969193810322485),\n ('healthcare', 0.0067470745955792765),\n ('and', 0.006483211946307094),\n ('drug', 0.006111735386306484)]\n\n\nWe can also search for clusters which match a given search term:\n\ntopic_model.find_topics(\"rocket science\")\n\n([131, 46, 28, -1, 9], [0.8482957, 0.8474297, 0.8343923, 0.8332416, 0.8269581])\n\n\nTopic number 131 allegedly matches the term, so we can look closer at this topic:\n\ntopic_model.get_topic(131)\n\n[('materials', 0.050279482237225254),\n ('science', 0.02243336305054669),\n ('chemistry', 0.0215702079363354),\n ('chemical', 0.019510674137408444),\n ('scientific', 0.019096261213199146),\n ('material', 0.01734997997000861),\n ('synthesis', 0.013922383987668636),\n ('literature', 0.011377588070962407),\n ('reaction', 0.010392948527677913),\n ('extraction', 0.009880316014163601)]\n\n\n\n\n2.1.3. Visualise the topics\nAs we did in the “manual” example, we can visualise the text clusters. The library provides a handy convenience method for interactive plotting.\n\nfig = topic_model.visualize_documents( titles, reduced_embeddings=reduced_embeddings, width=1200, hide_annotations=True ) \nfig.update_layout(font=dict(size=16))\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\n\nCluster embedding space\n\n\nWe can also plot keywords per topic:\n\ntopic_model.visualize_barchart()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\n\nTopic word scores\n\n\nWe can plot the similarity between topics as a heatmap:\n\ntopic_model.visualize_heatmap(n_clusters=30)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\n\nTopic similarity heatmap\n\n\nWe can also see the hierarchies within topics:\n\ntopic_model.visualize_hierarchy()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\n\nTopic hierarchies"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter5/lesson.html#re-ranking",
    "href": "posts/ml/hands_on_llm/chapter5/lesson.html#re-ranking",
    "title": "Hands-On LLMs: Text Clustering and Topic Modeling",
    "section": "2.2. Re-ranking",
    "text": "2.2. Re-ranking\nThe pipeline so far relied on the bag-of-words (BoW) approach to identify key words. This is fast, but does not take the semantic structure of a sentence into account.\nWe could swap the bag-of-words “lego block” for something more sophisticated. Another approach is to instead keep it as is but add an extra “re-ranker” block at the end to fine-tune the ordering of the keywords. This can be a slower algorithm, but it only processes the list of words identified by BoW for each topic (tens or hundreds), not the entire dcoument corpus (millions or more).\nThese re-rankers are referred to as representation models.\nSo our overall pipeline is extended to:\n\n\n\n\n\nflowchart LR\n\n  A(Embeddings) --&gt; B(Dimensionality Reduction) --&gt; C(Clustering)  --&gt; D(Tokenization) --&gt; E(Topic Modeling) --&gt; F(Reranker)\n\n\n\n\n\n\n\n\n\nRe-ranker block\n\n\n\n2.2.1. KeyBERTInspired\nThe idea behind this apporach is to use the similarity between embedding and words vectors to give a score, then order key words by their match score.\n\nAverage document embedding: Calculate embeddings for each document, then average\nCalculate embeddings for each keyword\nCalculate cosine similarity between each keyword and the average document embedding\nOrder by the most similar\n\n\n\n\nKeyBERTInspired\n\n\nAs well as reranking, KeyBERTInspired is effective at removing stopwords.\nWe will save the representations from the previousl model so we can compare them to the re-ranked versions:\n\n# Save original representations \nfrom copy import deepcopy \n\noriginal_topics = deepcopy(topic_model.topic_representations_)\n\nThe following convenience function helps to compare topics between the original and reranked versions.\n\ndef topic_differences(model, original_topics, num_topics=5):\n    \"\"\"Show the differences in topic representations between two models\"\"\"\n    topic_words = []\n    for topic in range(num_topics): \n        # Extract top 4 words per topic per model \n        og_words = \" | \".join(list(zip(*original_topics[topic])) [0][:4]) \n        reranked_words = \" | \".join(list(zip(*model.get_topic(topic))) [0][:4]) \n        topic_words.append((topic, og_words, reranked_words,))\n    \n    return pd.DataFrame(columns=[\"Topic\", \"Original\", \"Reranked\"], data=topic_words)\n\n\nfrom bertopic.representation import KeyBERTInspired \n\n# Update our topic representations using KeyBERTInspired \nrepresentation_model = KeyBERTInspired() \ntopic_model.update_topics(abstracts, representation_model=representation_model)\n\nWe can observe the topic differences:\n\ntopic_differences(topic_model, original_topics, num_topics=10)\n\n\n\n\n\n\n\n\nTopic\nOriginal\nReranked\n\n\n\n\n0\n0\nspeech | asr | recognition | end\ntranscription | phonetic | speech | language\n\n\n1\n1\nquestion | qa | questions | answer\nanswering | comprehension | retrieval | questions\n\n\n2\n2\nmedical | clinical | biomedical | patient\nnlp | clinical | text | language\n\n\n3\n3\ntranslation | nmt | machine | neural\ntranslation | translate | translations | multi...\n\n\n4\n4\nsummarization | summaries | summary | abstractive\nsummarization | summarizers | summaries | abst...\n\n\n5\n5\nhate | offensive | speech | detection\nhate | hateful | language | cyberbullying\n\n\n6\n6\ngender | bias | biases | debiasing\ngendered | gender | bias | biases\n\n\n7\n7\nrelation | extraction | relations | re\nrelation | relations | relational | extracting\n\n\n8\n8\nner | entity | named | recognition\nentity | entities | labeled | name\n\n\n9\n9\nagents | agent | game | games\nreasoning | ai | game | agents\n\n\n\n\n\n\n\nThere is still some redundancy in the re-ranked topics. For example, translation | translate | translations and relation | relations | relational. The next approach seeks to address this…\n\n\n2.2.2. Maximal Marginal Relevance\nMaximal Marginal Relevance (MMR) can be used to diversify the topic keywords, so that we do not end up with translation | translate | translations. It attempts to find keywords that are diverse from one another but still related to the topic.\nThe approach is to start with a set of candidate keywords and iteratively add the next best keyword that is “diverse enough” according to a user-defined diversity parameter.\n\nfrom bertopic.representation import MaximalMarginalRelevance \n\n# Update our topic representations to MaximalMarginalRelevance \nrepresentation_model = MaximalMarginalRelevance(diversity=0.2) \ntopic_model.update_topics(abstracts, representation_model=representation_model)\n\nAgain we can plot the topic differences and see the effect of MMR reranking:\n\ntopic_differences(topic_model, original_topics, num_topics=10)\n\n\n\n\n\n\n\n\nTopic\nOriginal\nReranked\n\n\n\n\n0\n0\nspeech | asr | recognition | end\nspeech | asr | audio | error\n\n\n1\n1\nquestion | qa | questions | answer\nquestions | retrieval | comprehension | passage\n\n\n2\n2\nmedical | clinical | biomedical | patient\nmedical | clinical | biomedical | patient\n\n\n3\n3\ntranslation | nmt | machine | neural\ntranslation | nmt | neural | bleu\n\n\n4\n4\nsummarization | summaries | summary | abstractive\nsummarization | summaries | abstractive | docu...\n\n\n5\n5\nhate | offensive | speech | detection\nhate | offensive | toxic | abusive\n\n\n6\n6\ngender | bias | biases | debiasing\ngender | bias | biases | debiasing\n\n\n7\n7\nrelation | extraction | relations | re\nrelation | extraction | relations | entities\n\n\n8\n8\nner | entity | named | recognition\nner | recognition | entities | data\n\n\n9\n9\nagents | agent | game | games\nagents | games | planning | environments\n\n\n\n\n\n\n\nThis does seem to give more diverse keywords."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter5/lesson.html#text-generation",
    "href": "posts/ml/hands_on_llm/chapter5/lesson.html#text-generation",
    "title": "Hands-On LLMs: Text Clustering and Topic Modeling",
    "section": "2.3. Text Generation",
    "text": "2.3. Text Generation\nOne more “lego block” we can add to our modular set up is a text generation module that will generate a label for the topic.\nWe can prompt the text generation LLM as follows to produce a label:\n\nI have a topic that contains the following documents: [DOCUMENTS]\nThe topic is described by the following keywords: [KEYWORDS]\nGive a short label of the topic.\n\nNote we only pass a small subset of the highest matching documents, not all of them.\nNow our pipeline extends further to:\n\n\n\n\n\nflowchart LR\n\n  A(Embeddings) --&gt; B(Dimensionality Reduction) --&gt; C(Clustering)  --&gt; D(Tokenization) --&gt; E(Topic Modeling) --&gt; F(Reranker) --&gt; G(Label Generation)\n\n\n\n\n\n\nWe only need to call the LLM for each topic, not each document.\n\nfrom transformers import pipeline \nfrom bertopic.representation import TextGeneration \n\nprompt = \"\"\"\n    I have a topic that contains the following documents: [DOCUMENTS] \n    \n    The topic is described by the following keywords: '[KEYWORDS]'. \n    \n    Based on the documents and keywords, what is this topic about?\n\"\"\" \n\n# Update our topic representations using Flan-T5 \ngenerator = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\") \nrepresentation_model = TextGeneration(generator, prompt=prompt, doc_length=50, tokenizer=\"whitespace\") \ntopic_model.update_topics(abstracts, representation_model=representation_model)\n\n100%|██████████| 151/151 [00:19&lt;00:00,  7.74it/s]\n\n\nNow we can see the LLM-generated keywords for each topic:\n\ntopic_differences(topic_model, original_topics, num_topics=10)\n\n\n\n\n\n\n\n\nTopic\nOriginal\nReranked\n\n\n\n\n0\n0\nspeech | asr | recognition | end\nSpeech-to-text translation | | |\n\n\n1\n1\nquestion | qa | questions | answer\nQuestion answering | | |\n\n\n2\n2\nmedical | clinical | biomedical | patient\nScience/Tech | | |\n\n\n3\n3\ntranslation | nmt | machine | neural\nLearning neural machine translation | | |\n\n\n4\n4\nsummarization | summaries | summary | abstractive\nAbstractive summarization | | |\n\n\n5\n5\nhate | offensive | speech | detection\nScience/Tech | | |\n\n\n6\n6\ngender | bias | biases | debiasing\nScience/Tech | | |\n\n\n7\n7\nrelation | extraction | relations | re\nrelation extraction | | |\n\n\n8\n8\nner | entity | named | recognition\nScience/Tech | | |\n\n\n9\n9\nagents | agent | game | games\nScience/Tech | | |\n\n\n\n\n\n\n\nSome are pretty good, e.g. “Speech-to-text translation”. Some are a bit too broad, e.g. a lot of “Science/Tech”.\nThis was with a pretty tiny LLM. A more capable model would have fared better. But we get the general idea.\nAs a cherry on top, we can get a nice plot of our clusters with the topic labels overlaid:\n\nfig = topic_model.visualize_document_datamap(\n    titles, topics=list(range(20)), reduced_embeddings=reduced_embeddings, width=1200, label_font_size=11, label_wrap_width=20, use_medoids=True\n)"
  },
  {
    "objectID": "posts/ml/tensorflow/tensorflow.html",
    "href": "posts/ml/tensorflow/tensorflow.html",
    "title": "Tensorflow Notes",
    "section": "",
    "text": "These are notes taken primarily from the Complete Tensorflow and Keras Udemy course\n\n\n\nData sourcing - train_test_split\nExploratory data analysis - plot distributions, correlations\nData cleaning - drop redundant columns, handle missing data (drop or impute)\nFeature engineering - one hot encoding categories, scaling/normalising numerical values, combining columns, extracting info from columns (e.g. zip code from address)\nModel selection and training - model and hyperparameters\nModel tuning and evaluation metrics - classification: classification_report, confusion matrix, accuracy, recall, F1. Regression: error (RMSE, MAE, etc)\nPredictions\n\n\n\n\n\nSupervised learning\n\nLinear regression\nSVM\nNaive Bayes\nKNN\nRandom forests\n\nUnsupervised learning\n\nDimensionality reduction\nClustering\n\nReinforcement Learning\n\nSupervised learning tasks can be broadly broken into:\n\nRegression\nClassification\n\nSingle class\nMulti class\n\n\n\n\n\n\n\nGeneral idea of a neural network that can then be extended to specific cases, e.g. CNNs and RNNs.\n\nPerceptron: a weighted average of inputs passed through some activation gate function to arrive at an output decision\nNetwork of perceptrons\nActivation function: a non-linear transfer function f(wx+b)\nCost function and gradient descent: convex optimisation of a loss function using sub-gradient descent. The optimizer can be set in the compile method of the model.\nBackpropagation: use chain rule to determine partial gradients of each weight and bias. This means we only need a single forward pass followed by a single backward pass. Contrast this to if we perturbed each weight or bias to determine each partial gradient: in that case, for each epoch we would need to run a forward pass per weight/bias in the network, which is potentially millions!\nDropout: a technique to avoid overfitting by randomly dropping neurons in each epoch.\n\nGeneral structure:\n\nInput layer\nHidden layer(s)\nOutput layer\n\nInput and output layers are determined by the problem:\n\nInput size: number of features in the data\nOutput size number of targets to predict, i.e. one for single class classification or single target regression, or multiple for multiclass (one per class)\nOutput layer activation determined by problem. For single class classification activation='sigmoid', for multiclass classification activation='softmax'\nLoss function determined by problem. For single class classification loss='binary_crossentropy', for multiclass classification loss='categorical_crossentropy'\n\nHidden layers are less well-defined. Some heuristics here.\nVanishing gradients can be an issue for lower layers in particular, where the partial gradients of individual layers can be very small, so when multiplied together in chain rule the gradient is vanishingly small. Exploding gradients are a similar issue but where gradients get increasingly large when multiplied together. Some techniques to rectify vanishing/exploding gradients:\n\nUse different activation functions with larger gradients close to 0 and 1, e.g. leaky ReLU or ELU (exponential linear unit)\nBatch normalisation: scale each gradient by the mean and standard deviation of the batch\nDifferent weight initialisation methods, e.g. Xavier initialisation\n\nExample model outline\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\n\nmodel = Sequential()\n\n# Input layer\nmodel.add(Dense(78, activation='relu'))  # Number of input features\nmodel.add(Dropout(0.2))  # Optional dropout layer after each layer. Omitted for next layers for clarity\n\n# Hidden layers\nmodel.add(Dense(39, activation='relu'))\nmodel.add(Dense(19, activation='relu'))\n\n# Output layer\nmodel.add(Dense(1, activation='sigmoid'))  # Number of target classes (single class in this case)\n\n# Problem setup\nmodel.compile(loss='binary_crossentropy', optimizer='adam')  # Type of problem (single class classification in this case)\n\n# Training\nmodel.fit(\n    x=X_train,\n    y=y_train,\n    batch_size=256,\n    epochs=30,\n    validation_data=(X_test, y_test)\n)\n\n# Prediction\nmodel.predict(new_input)  # The new input needs to be shaped and scaled the sane as the training data\n\n\n\nThese are used for image classification problems where convolutional filters are useful for extracting features from input arrays.\n\nImage kernels/filters\n\nGrayscale 2D arrays\nRGB 3D tensors\n\nConvolutional layers\nPooling layers\n\nGeneral structure:\n\nInput layer\nConvolutional layer\nPooling layer\n(Optionally more pairs of convolutional and pooling layers)\nFlattening layer\nDense hidden layer(s)\nOutput layer\n\nExample model outline\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nmodel = Sequential()\n\n# Convolutional layer followed by pooling. Deeper networks may have multiple pairs of these.\nmodel.add(Conv2D(filters=32, kernel_size=(4,4),input_shape=(28, 28, 1), activation='relu',))  # Input shape determined by input data size\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\n# Flatten _images from 2D to 1D before final layer\nmodel.add(Flatten())\n\n# Dense hidden layer\nmodel.add(Dense(128, activation='relu'))\n\n# Output layer\nmodel.add(Dense(10, activation='softmax'))  # Size and activation determined by problem; multiclass classification in this case\n\n# Problem setup\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')  # Loss determined by problem, multiclass classification in this case\n\n# Training (with optional early stopping)\nearly_stop = EarlyStopping(monitor='val_loss',patience=2)\nmodel.fit(x_train,y_cat_train,epochs=10,validation_data=(x_test,y_cat_test),callbacks=[early_stop])\n\n# Prediction\nmodel.predict(new_input)  # The new input needs to be shaped and scaled the sane as the training data\n\n\n\nThese are used for modelling sequences with variable lengths of inputs and outputs.\nRecurrent neurons take as their input the current data AND the previous epoch’s output. We could try to pass EVERY previous epoch’s output as an input, but would run into issues of vanushing gradients.\nLSTMs take this idea a step further by incorporating the previous epochs input AND some longer lookback of epoch where some old outputs are “forgotten”.\nA basic RNN:\n\n            --------------------------------\n            |                              |\n H_t-1 ---&gt; |                              |---&gt; H_t\n            |                              |\n X_t   ---&gt; |                              |\n            |                              |\n            --------------------------------\n\nwhere:\nH_t is the neuron's output at current epoch t\nH_t-1 is the neuron's output from the previous epoch t-1\nX_t is the input data at current epoch t\n\nH_t = tanh(W[H_t-1, X_t] + b)\nAn LSTM (Long Short Term Memory) unit:\n\n            --------------------------------\n            |                              |\n H_t-1 ---&gt; |                              |---&gt; H_t\n            |                              |\n C_t-1 ---&gt; |                              |---&gt; C_t\n            |                              |\n X_t   ---&gt; |                              |\n            |                              |\n            --------------------------------\n            \nThe `forget gate` determines which part of the old short-term memory and current input is \"forgotten\" by the new long-term memory.\nThese are a set of weights (between 0 and 1) that get applied to the old long-term memory to downweight it.\nF_t = sigmoid(W_F[H_t-1, X_t] + b_F)\n\nThe `input gate` i_t similarly gates the input and old short-term memory.\nThis will later (in the update gate) get combined  with a candidate value for the new long-term memory `C_candidate_t`\nI_t = sigmoid(W_I[H_t-1, X_t] + b_I)\nC_candidate_t = tanh(W_C_cand[H_t-1, X_t] + b_C_cand) \n\nThe `update gate` for the new long-term memory `C_t` is then calculated as a sum of forgotten old memory and input-weighted candidate memory:\nC_t = F_t*C_t-1 + I_t*C_candidate_t \n\nThe `output gate` O_t is a combination of the old short-term memory and latest input data.\nThis is then combined with the latest long-term memory to produce the output of the recurrent neuron, which is also the updated short-term memory H_t:\nO_t = sigmoid(W_O[H_t-1, X_t] + b_O)\nH_t = O_t * tanh(C_t) \n\nwhere:\nH_t is short-term memory at epoch t\nC_t is long-term memory at epoch t\nX_t is the input data at current epoch t\n\nsigmoid is a sigmoid  activation function\nF_t is an intermediate forget gate weight\nI_t is an intermediate input gate weight\nO_t is an intermediate output gate weight\nThere are several variants of RNNs. RNNs with peepholes\nThis leaks long-term memory into the forget, input and output gates. Note that the forget gate and input gate each get the OLD long-term memory, whereas the output gate gets the NEW long-term memory.\nF_t = sigmoid(W_F[C_t-1, H_t-1, X_t] + b_F)\nI_t = sigmoid(W_I[C_t-1, H_t-1, X_t] + b_I)\nO_t = sigmoid(W_O[C_t, H_t-1, X_t] + b_O)\nGated Recurrent Unit (GRU)\nThis combines the forget and input gates into a single gate. It also has some other changes. This is simpler than a typical LSTM model as it has fewer parameters. This makes it more computationally efficient, and in practice they can have similar performance.\nz_t = sigmoid(W_z[H_t-1, X_t])\nr_t = sigmoid(W_r[H_t-1, X_t])\nH_candidate_t = tanh(W_H_candidate[r_t*h_t-1, x_t])\nH_t = (1 - z_t) * H_t-1 + z_t * H_candidate_t\nThe sequences modelled with RNNs can be:\n\nOne-to-many\nMany-to-many\nMany-to-one\n\nConsiderations for choosing the input sequence length:\n\nIt should be long enough to capture any patterns or seasonality in the data\nThe validation set and test set each need to have a size at least (input_length + output_length), so the longer the input length, the more data is required to be set aside.\n\nThe validation loss for RNNs/LSTMs can be volatile, so allow a higher patience when using early stopping.\nExample model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,LSTM\n\n# Generators for helper functions to chunk up the input series into an array of pairs of (input_batch, target_output)\ntrain_generator = TimeseriesGenerator(scaled_train, scaled_train, length=batch_length, batch_size=1)\nvalidation_generator = TimeseriesGenerator(scaled_test, scaled_test, length=batch_length, batch_size=1)\n\n# Create an LSTM model\nmodel = Sequential()\nmodel.add(LSTM(100, activation='relu', input_shape=(batch_length, n_features)))\nmodel.add(Dense(1))  # Output layer\nmodel.compile(optimizer='adam', loss='mse')  # Problem setup for regression\n\n# Fit the model\nearly_stop = EarlyStopping(monitor='val_loss', patience=10)  # RNNs can be volatile so if use a higher patience with an early stopping callback\nmodel.fit(train_generator, epochs=20, validation_data=validation_generator, callbacks=[early_stop])\n\n# Evaluate test data\ntest_predictions = []\nfinal_batch = scaled_train[-batch_length:, :]\ncurrent_batch = final_batch.reshape((1, batch_length, n_features))\n\nfor test_val in scaled_test:\n    pred_val = model.predict(current_batch)  # These will later need to use the scaler.inverse_transform of the scaler originally used on the training data\n    test_predictions.append(pred_val[0])\n    current_batch = np.append(current_batch[:,1:,:], pred_val.reshape(1,1,1), axis=1)\n\n\n\nCharacter-based generative NLP: given an input string, e.g. [‘h’, ‘e’, ‘l’, ‘l’], predict the sequence shifted by one character, e.g. [‘e’, ‘l’, ‘l’, ‘o’]\nThe embedding, GRU and dense layers work on the sequence in the following way: \nSteps:\n\nRead in text data\nText processing and vectorisation - one-hot encoding\nCreate batches\nCreate the model\nTrain the model\nGenerate new text\n\nStep 1: Read in text data\n\nA corpus of &gt;1 million characters is a good size\nThe more distinct the style of your corpus, the more obvious it will be if your model has worked.\n\nGiven some structured input_text string, we can get the one-hot encoded vocab list of unique characters.\nvocab_list = sorted(set(input_text))\nStep 2: Text processing\nVectorise the text - create a mapping between character and integer. This is the encoding dictionary used to vectorise the corpus.\n# Mappings back and forth between characters and their encoded integers\nchar_to_ind = {char: ind for ind, char in enumerate(vocab_list)}\nind_to_char = {ind: char for ind, char in enumerate(vocab_list)}\n\nencoded_text = np.array([char_to_ind[char] for char in input_text])\nStep 3: Create batches\nThe sequence length should be long enough to capture structure, e.g. for poetry with rhyming couplets it should be the number of characters in 3 lines to capture the rhyme and non-rhyme. Too long a sequence makes the model take longer to train and captures too much historical noise.\nCreate a shuffled dataset where:\n\ninput sequence is the first n characters\noutput sequence is n characters lagged by one\n\nWe want to shuffle these sequence pairs into a random order so the model doesn’t overfit to any section of the text, but can instead generate characters given any seed text.\nseq_len = 120\ntotal_num_seq = len(input_text) // (seq_len + 1)\n\n# Create Training Sequences\nchar_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\nsequences = char_dataset.batch(seq_len + 1, drop_remainder=True)  # drop_remainder drops the last incomplete sequence\n\n# Create tuples of input and output sequences\ndef create_seq_targets(seq):\n    input_seq = seq[:-1]\n    output_seq = seq[-1:]\n    return input_seq, output_seq\ndataset = sequences.map(create_seq_targets)\n\n# Generate training batches\nbatch_size = 128\nbuffer_size = 10000  # Shuffle this amount of batches rather than shuffling the entire dataset in memory\ndataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\nStep 4: Create the model\nSet up the loss function and layers.\nThis model uses 3 layers:\n\nEmbedding\n\nEmbed positive integers, e.g. “a”&lt;-&gt;1, as dense vectors of fixed size. This embedding size is a hyperparameter for the user to decide.\nNumber of layers should be smaller but a similar scale to the vocab size; typically use the power of 2 that is just smaller than the vocab size.\n\nGRU\n\nThis is the main thing to vary in the model: number of hidden layers and number of neurons in each, types of neurons (RNN, LSTM, GRU), etc. Typically use lots of layers here.\n\nDense\n\nOne neuron per character (one-hot encoded), so this produces a probability per character. The user can vary the “temperature”, choosing less probable characters more/less often.\n\n\nLoss function:\n\nSparse categorical cross-entropy\nUse sparse categorical cross-entropy because the classes are mutually exclusive. Use regular categorical cross-entropy when one smaple can have multiple classes, or the labels are soft probabilities.\nSee link in NLP appendix for discussion.\nUse logits=True because vocab input is one hot encoded.\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, GRU\nfrom tensorflow.keras.losses import sparse_categorical_crossentropy\n\n# Hyperparameters\nvocab_size = len(vocab_list)\nembed_dim = 64\nrnn_neurons = 1026\n\n# Create model \nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embed_dim, batch_input_shape=[batch_size, None]))\nmodel.add(GRU(rnn_neurons, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'))\nmodel.add(Dense(vocab_size))\nsparse_cat_loss = lambda y_true,y_pred: sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)  # We want a custom loss function with logits=True\nmodel.compile(optimizer='adam', loss=sparse_cat_loss) \nStep 5: Train the model\nepochs = 30\nmodel.fit(dataset,epochs=epochs)\nStep 6: Generate text\nAllow the model to accept a batch size of 1 to allow us to give an input seed text from which to generate text from.\nWe format the seed text so it can be fed into the network, then loop through generating one character at a time and feeding that back into the model.\nmodel.build(tf.TensorShape([1, None]))\n\ndef generate_text(model, input_seed_text, gen_size=100, temperature=1.0):\n    \"\"\"\n    model: tensorflow.model\n    input_seed_text: str\n    gen_size: int\n    temperature: float\n    \"\"\"\n    generated_text_result = []\n    vectorised_seed = tf.expand_dims([char_to_ind[s] for s in input_seed_text], 0)\n    model.reset_states()\n    \n    for k in range(gen_size):\n        raw_predictions = model(vectorised_seed)\n        predictions = tf.squeeze(raw_predictions, 0) / temperature\n        predicted_index = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n        generated_text_result.append(ind_to_char[predicted_index])\n        \n        # Set a new vectorised seed to generate the next character in the loop\n        vectorised_seed = tf.expand_dims([predicted_index], 0)\n        \n    return (input_seed_text + ''.join(generated_text_result))\n\n\n\nThis is an unsupervised learning problem, therefore evaluation metrics are different. Sometimes called semi-supervised as labels may be used in training the model, but not available when it’s used to predict new values.\nApplications:\n\nDimensionality reduction\nNoise removal\n\nDesigned to reproduce its input in the output layer. Number of input neurons is the same as the number of output layers. The encoder reduces dimensionality to the hidden layer. The hidden layer should contain the most relevant information required to reconstruct the input. The decoder reconstructs a high dimensional output from a low dimensional input.\n\nEncoder: Input layer -&gt; Hidden layer\nDecoder: Hidden layer -&gt; Output\n\nStacked autoencoders include multiple hidden layers.\nAutoencoder for dimensionality reduction\nLet’s try to reduce an input dataset from 3 dimensions to 2. We want to train an autoencoder to go from 3 -&gt; 2 -&gt; 3.\nWhen we scale data, we fit and transform on the entire dataset rather than a train/test split, because there is no ground truth for the “correct” dimensionality reduction.\nUsing SGD in Autoencoders allows the learning rate to be varied as a hyperparameter to affect how well the hidden layer learns. Larger values will train quicker but potentially perform worse.\nTo retrieve the lower dimensionality representation, use just the encoder half of the trained autoencoder to predict the input.\n# Scale data\nscaler = MinMaxScaler()\nscaled_data = scaler.fit_transform(input_features)\n\n# Create model\nencoder = Sequential()\nencoder.add(Dense(units=2, acivation='relu', input_shape=[3]))\n\ndecoder = Sequential()\ndecoder.add(Dense(units=3, acivation='relu', input_shape=[2]))\n\nautoencoder = Sequential([encoder, decoder])\nautoencoder.compile(loss='mse', optimizer=SGD(lr=1.5))\n\n# Train the full autoencoder model\nautoencoder.fit(scaled_data, scaled_data, epochs=5)\n\n# Reduce dimensionality by using just the encoder part of the trained model\nencoded_2dim = encoder.predict(scaled_data)\nAutoencoder for noise removal\nStarting with clean images, we want to train an autoencoder to learn to remove noise. We create a noisy dataset by adding noise to our clean images. We then try to reconstruct that input, and compare to the original clean images as the ground truth.\nStarting with 28x28 images (e.g. MNIST dataset), this gives 784 input features when flattened. Use multiple hidden layers as there are a large number of input features that would be difficult to learn in one layer. The numbers of layers and number of neurons in each are hyperparameters to tune; decreasing in powers of 2 is a good rule of thumb.\nAdd a Gaussian noise layer to train the model to remove noise. Without the Gaussian layer, the model would be used for dimensionality reduction rather than noise removal.\nThe final layer uses a sigmoid activation because it is a binary classification problem - does the output match the input?\nencoder = Sequential()\nencoder.add(Flatten(input_shape=[28, 28]))\nencoder.add(Gaussian(0.2))  # Optional layer if training for noise removal rather than dimensionality reduction\nencoder.add(Dense(400, activation='relu'))\nencoder.add(Dense(200, activation='relu'))\nencoder.add(Dense(100, activation='relu'))\nencoder.add(Dense(50, activation='relu'))\nencoder.add(Dense(25, activation='relu'))\n\ndecoder = Sequential()\ndecoder.add(Dense(50, input_shape=[25], activation='relu'))\ndecoder.add(Dense(100, activation='relu'))\ndecoder.add(Dense(200, activation='relu'))\ndecoder.add(Dense(400, activation='relu'))\ndecoder.add(Dense(28*28, activation='sigmoid'))\n\nautoencoder = Sequential([encoder, decoder])\nautoencoder.compile(loss='binary_crossentropy', optimizer=SGD(lr=1.5), metrics=['accuracy'])\nautoencoder.fit(X_train, X_train, epochs=5)\n\n\n\nUse 2 networks competing against each other to generate data - counterfeiter (generator) vs detective (discriminator).\n\nGenerator: recieves random noise\nDiscriminator: receives data set containing real data and fake generated data, and attempts to calssify real vs fake (binary classificaiton).\n\nTwo training phases, each only training one of the networks:\n\nTrain discriminator - real images (labeled 1) and generated images (labeled 0) are fed to the discriminator network.\nTrain generator - only feed generated images to discriminator, ALL labeled 1.\n\nThe generator never sees real images, it creates images based only off of the gradients flowing back from the discriminator.\nDifficulties with GANs:\n\nTraining resources - GPUs generally required\nMode collapse - generator learns an image that fools the discriminator, then only produces this image. Deep convolutional GANs and mini-batch discrimination are two solution to this problem.\nInstability - True performance is hard to measure; just because the discriminator was fooled, it doesn’t mean that the generated image was actually realistic.\n\nCreating the model\nWe create the generator and discriminator as separate models, then join them into a GAN object. This is analogous to how we joined an encoder and decoder into an autoencoder.\nThe discriminator is trained on a binary classification problem, real or fake, so uses a binary cross entropy loss function. Backpropagation only alters the weights of the discriminator in the first phase, not the generator.\ndiscriminator = Sequential()\n\n# Flatten the image\ndiscriminator.add(Flatten(input_shape=[28,28]))\n\n# Some hidden layers\ndiscriminator.add(Dense(150,activation='relu'))\ndiscriminator.add(Dense(100,activation='relu'))\n\n# Binary classification - real vs fake\ndiscriminator.add(Dense(1,activation=\"sigmoid\"))\ndiscriminator.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\nThe generator is trained in the second phase.\nWe set a codings_size, which is the size of the latent representation from which the generator will create a full image. For example, if we want to create a 28x28 image (784 pixels), we may want to generate this from a 100 element input. This is analogous to the decoder part of an autoencoder.\nThe codings_size should be smaller than the total number of features but large enough so that there is something to learn from.\nThe generator is NOT compiled at this step, it is compiled in the combined GAN later so that it is only trained at that step.\ncodings_size = 100\ngenerator = Sequential()\n\n# Input shape of first layer is the codings_size\ngenerator.add(Dense(150, activation=\"relu\", input_shape=[codings_size]))\n\n# Some hidden layers\ngenerator.add(Dense(200,activation='relu'))\n\n# Output is the size of the image we want to create\ngenerator.add(Dense(784, activation=\"sigmoid\"))\ngenerator.add(Reshape([28,28]))\nThe GAN combines the generator and discriminator.\nThe discriminator is only trained in the first phase, not the second phase.\nGAN = Sequential([generator, discriminator])\ndiscriminator.trainable = False\nGAN.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\nTraining the model\nWe first create batches of images from our input data, similarly to previous models.\nbatch_size = 32  # Size of training input batches\nbuffer_size = 1000  # Don't load the entire dataset into memory\nepochs = 1\n\nraw_dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(buffer_size=buffer_size)\nbatched_dataset = raw_dataset.batch(batch_size, drop_remainder=True).prefetch(1)\nThe training loop for each epoch trains the discriminator to distinguish real vs fake images (binary classification). Then it trains the generator to generate fake images using ONLY the gradients learned in the discriminator training step; the generator NEVER sees any real images.\ngenerator, discriminator = GAN.layers\n\nfor epoch in range(epochs):\n    for index, X_batch in enumerate(batched_dataset):\n        # Phase 1: Train the discriminator\n        noise = tf.random.normal(shape=[batch_size, codings_size])\n        generated_images = generator(noise)\n        real_images = tf.dtypes.cast(X_batch,tf.float32)\n        X_train_discriminator = tf.concat([generated_images, real_images], axis=0)\n        y_train_discriminator = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)  # Targets set to zero for fake _images and 1 for real _images\n        discriminator.trainable = True\n        discriminator.train_on_batch(X_train_discriminator, y_train_discriminator)\n\n        # Phase 2: Train the generator\n        noise = tf.random.normal(shape=[batch_size, codings_size])\n        y_train_generator = tf.constant([[1.]] * batch_size)  # We want discriminator to believe that fake _images are real\n        discriminator.trainable = False\n        GAN.train_on_batch(noise, y_train_generator)    \nDeep convolutional GANs\nThese are GANs which use convolutional layers inside the discriminator and generator models.\nThe models are almost identical to the models above, just with additional Conv2D, Conv2DTranspose and BatchNormalization layers.\nChanges compared to regular GANs:\n\nAdditional convolutional layers in model.\nReshape the training set to match the images.\nRescale the training data to be between -1 and 1 so that tanh activation function works.\nActivation of discriminator output layer is tanh rather than sigmoid.\n\n\n\n\n\n\n\n\nComplete Tensorflow and Keras Udemy course\nIntuition behind neural networks\nThe deep learning bible (with lectures)\nHeuristics for choosing hidden layers\nAlternatives to the deprecated model predict for different classification problems\nMIT course\n\n\n\n\n\nImage kernels explained\nChoosing CNN layers\n\n\n\n\n\nOverview of RNNs\nWikipedia page contains the equations of LSTMs and peepholes\nLSTMs vs GRUs\nWorked example of LSTM\nRNN cheatsheet\n\n\n\n\n\nThe unreasonable effectiveness of RNNs - essay on RNNs applied to NLP\nSparse vs dense categorical crossentropy loss function\n\n\n\n\n\nbuffer_size argument in tensorflow prefetch and shuffle\nMode collapse"
  },
  {
    "objectID": "posts/ml/tensorflow/tensorflow.html#end-end-process",
    "href": "posts/ml/tensorflow/tensorflow.html#end-end-process",
    "title": "Tensorflow Notes",
    "section": "",
    "text": "Data sourcing - train_test_split\nExploratory data analysis - plot distributions, correlations\nData cleaning - drop redundant columns, handle missing data (drop or impute)\nFeature engineering - one hot encoding categories, scaling/normalising numerical values, combining columns, extracting info from columns (e.g. zip code from address)\nModel selection and training - model and hyperparameters\nModel tuning and evaluation metrics - classification: classification_report, confusion matrix, accuracy, recall, F1. Regression: error (RMSE, MAE, etc)\nPredictions"
  },
  {
    "objectID": "posts/ml/tensorflow/tensorflow.html#machine-learning-categories",
    "href": "posts/ml/tensorflow/tensorflow.html#machine-learning-categories",
    "title": "Tensorflow Notes",
    "section": "",
    "text": "Supervised learning\n\nLinear regression\nSVM\nNaive Bayes\nKNN\nRandom forests\n\nUnsupervised learning\n\nDimensionality reduction\nClustering\n\nReinforcement Learning\n\nSupervised learning tasks can be broadly broken into:\n\nRegression\nClassification\n\nSingle class\nMulti class"
  },
  {
    "objectID": "posts/ml/tensorflow/tensorflow.html#deep-learning-models",
    "href": "posts/ml/tensorflow/tensorflow.html#deep-learning-models",
    "title": "Tensorflow Notes",
    "section": "",
    "text": "General idea of a neural network that can then be extended to specific cases, e.g. CNNs and RNNs.\n\nPerceptron: a weighted average of inputs passed through some activation gate function to arrive at an output decision\nNetwork of perceptrons\nActivation function: a non-linear transfer function f(wx+b)\nCost function and gradient descent: convex optimisation of a loss function using sub-gradient descent. The optimizer can be set in the compile method of the model.\nBackpropagation: use chain rule to determine partial gradients of each weight and bias. This means we only need a single forward pass followed by a single backward pass. Contrast this to if we perturbed each weight or bias to determine each partial gradient: in that case, for each epoch we would need to run a forward pass per weight/bias in the network, which is potentially millions!\nDropout: a technique to avoid overfitting by randomly dropping neurons in each epoch.\n\nGeneral structure:\n\nInput layer\nHidden layer(s)\nOutput layer\n\nInput and output layers are determined by the problem:\n\nInput size: number of features in the data\nOutput size number of targets to predict, i.e. one for single class classification or single target regression, or multiple for multiclass (one per class)\nOutput layer activation determined by problem. For single class classification activation='sigmoid', for multiclass classification activation='softmax'\nLoss function determined by problem. For single class classification loss='binary_crossentropy', for multiclass classification loss='categorical_crossentropy'\n\nHidden layers are less well-defined. Some heuristics here.\nVanishing gradients can be an issue for lower layers in particular, where the partial gradients of individual layers can be very small, so when multiplied together in chain rule the gradient is vanishingly small. Exploding gradients are a similar issue but where gradients get increasingly large when multiplied together. Some techniques to rectify vanishing/exploding gradients:\n\nUse different activation functions with larger gradients close to 0 and 1, e.g. leaky ReLU or ELU (exponential linear unit)\nBatch normalisation: scale each gradient by the mean and standard deviation of the batch\nDifferent weight initialisation methods, e.g. Xavier initialisation\n\nExample model outline\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\n\nmodel = Sequential()\n\n# Input layer\nmodel.add(Dense(78, activation='relu'))  # Number of input features\nmodel.add(Dropout(0.2))  # Optional dropout layer after each layer. Omitted for next layers for clarity\n\n# Hidden layers\nmodel.add(Dense(39, activation='relu'))\nmodel.add(Dense(19, activation='relu'))\n\n# Output layer\nmodel.add(Dense(1, activation='sigmoid'))  # Number of target classes (single class in this case)\n\n# Problem setup\nmodel.compile(loss='binary_crossentropy', optimizer='adam')  # Type of problem (single class classification in this case)\n\n# Training\nmodel.fit(\n    x=X_train,\n    y=y_train,\n    batch_size=256,\n    epochs=30,\n    validation_data=(X_test, y_test)\n)\n\n# Prediction\nmodel.predict(new_input)  # The new input needs to be shaped and scaled the sane as the training data\n\n\n\nThese are used for image classification problems where convolutional filters are useful for extracting features from input arrays.\n\nImage kernels/filters\n\nGrayscale 2D arrays\nRGB 3D tensors\n\nConvolutional layers\nPooling layers\n\nGeneral structure:\n\nInput layer\nConvolutional layer\nPooling layer\n(Optionally more pairs of convolutional and pooling layers)\nFlattening layer\nDense hidden layer(s)\nOutput layer\n\nExample model outline\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nmodel = Sequential()\n\n# Convolutional layer followed by pooling. Deeper networks may have multiple pairs of these.\nmodel.add(Conv2D(filters=32, kernel_size=(4,4),input_shape=(28, 28, 1), activation='relu',))  # Input shape determined by input data size\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\n# Flatten _images from 2D to 1D before final layer\nmodel.add(Flatten())\n\n# Dense hidden layer\nmodel.add(Dense(128, activation='relu'))\n\n# Output layer\nmodel.add(Dense(10, activation='softmax'))  # Size and activation determined by problem; multiclass classification in this case\n\n# Problem setup\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')  # Loss determined by problem, multiclass classification in this case\n\n# Training (with optional early stopping)\nearly_stop = EarlyStopping(monitor='val_loss',patience=2)\nmodel.fit(x_train,y_cat_train,epochs=10,validation_data=(x_test,y_cat_test),callbacks=[early_stop])\n\n# Prediction\nmodel.predict(new_input)  # The new input needs to be shaped and scaled the sane as the training data\n\n\n\nThese are used for modelling sequences with variable lengths of inputs and outputs.\nRecurrent neurons take as their input the current data AND the previous epoch’s output. We could try to pass EVERY previous epoch’s output as an input, but would run into issues of vanushing gradients.\nLSTMs take this idea a step further by incorporating the previous epochs input AND some longer lookback of epoch where some old outputs are “forgotten”.\nA basic RNN:\n\n            --------------------------------\n            |                              |\n H_t-1 ---&gt; |                              |---&gt; H_t\n            |                              |\n X_t   ---&gt; |                              |\n            |                              |\n            --------------------------------\n\nwhere:\nH_t is the neuron's output at current epoch t\nH_t-1 is the neuron's output from the previous epoch t-1\nX_t is the input data at current epoch t\n\nH_t = tanh(W[H_t-1, X_t] + b)\nAn LSTM (Long Short Term Memory) unit:\n\n            --------------------------------\n            |                              |\n H_t-1 ---&gt; |                              |---&gt; H_t\n            |                              |\n C_t-1 ---&gt; |                              |---&gt; C_t\n            |                              |\n X_t   ---&gt; |                              |\n            |                              |\n            --------------------------------\n            \nThe `forget gate` determines which part of the old short-term memory and current input is \"forgotten\" by the new long-term memory.\nThese are a set of weights (between 0 and 1) that get applied to the old long-term memory to downweight it.\nF_t = sigmoid(W_F[H_t-1, X_t] + b_F)\n\nThe `input gate` i_t similarly gates the input and old short-term memory.\nThis will later (in the update gate) get combined  with a candidate value for the new long-term memory `C_candidate_t`\nI_t = sigmoid(W_I[H_t-1, X_t] + b_I)\nC_candidate_t = tanh(W_C_cand[H_t-1, X_t] + b_C_cand) \n\nThe `update gate` for the new long-term memory `C_t` is then calculated as a sum of forgotten old memory and input-weighted candidate memory:\nC_t = F_t*C_t-1 + I_t*C_candidate_t \n\nThe `output gate` O_t is a combination of the old short-term memory and latest input data.\nThis is then combined with the latest long-term memory to produce the output of the recurrent neuron, which is also the updated short-term memory H_t:\nO_t = sigmoid(W_O[H_t-1, X_t] + b_O)\nH_t = O_t * tanh(C_t) \n\nwhere:\nH_t is short-term memory at epoch t\nC_t is long-term memory at epoch t\nX_t is the input data at current epoch t\n\nsigmoid is a sigmoid  activation function\nF_t is an intermediate forget gate weight\nI_t is an intermediate input gate weight\nO_t is an intermediate output gate weight\nThere are several variants of RNNs. RNNs with peepholes\nThis leaks long-term memory into the forget, input and output gates. Note that the forget gate and input gate each get the OLD long-term memory, whereas the output gate gets the NEW long-term memory.\nF_t = sigmoid(W_F[C_t-1, H_t-1, X_t] + b_F)\nI_t = sigmoid(W_I[C_t-1, H_t-1, X_t] + b_I)\nO_t = sigmoid(W_O[C_t, H_t-1, X_t] + b_O)\nGated Recurrent Unit (GRU)\nThis combines the forget and input gates into a single gate. It also has some other changes. This is simpler than a typical LSTM model as it has fewer parameters. This makes it more computationally efficient, and in practice they can have similar performance.\nz_t = sigmoid(W_z[H_t-1, X_t])\nr_t = sigmoid(W_r[H_t-1, X_t])\nH_candidate_t = tanh(W_H_candidate[r_t*h_t-1, x_t])\nH_t = (1 - z_t) * H_t-1 + z_t * H_candidate_t\nThe sequences modelled with RNNs can be:\n\nOne-to-many\nMany-to-many\nMany-to-one\n\nConsiderations for choosing the input sequence length:\n\nIt should be long enough to capture any patterns or seasonality in the data\nThe validation set and test set each need to have a size at least (input_length + output_length), so the longer the input length, the more data is required to be set aside.\n\nThe validation loss for RNNs/LSTMs can be volatile, so allow a higher patience when using early stopping.\nExample model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,LSTM\n\n# Generators for helper functions to chunk up the input series into an array of pairs of (input_batch, target_output)\ntrain_generator = TimeseriesGenerator(scaled_train, scaled_train, length=batch_length, batch_size=1)\nvalidation_generator = TimeseriesGenerator(scaled_test, scaled_test, length=batch_length, batch_size=1)\n\n# Create an LSTM model\nmodel = Sequential()\nmodel.add(LSTM(100, activation='relu', input_shape=(batch_length, n_features)))\nmodel.add(Dense(1))  # Output layer\nmodel.compile(optimizer='adam', loss='mse')  # Problem setup for regression\n\n# Fit the model\nearly_stop = EarlyStopping(monitor='val_loss', patience=10)  # RNNs can be volatile so if use a higher patience with an early stopping callback\nmodel.fit(train_generator, epochs=20, validation_data=validation_generator, callbacks=[early_stop])\n\n# Evaluate test data\ntest_predictions = []\nfinal_batch = scaled_train[-batch_length:, :]\ncurrent_batch = final_batch.reshape((1, batch_length, n_features))\n\nfor test_val in scaled_test:\n    pred_val = model.predict(current_batch)  # These will later need to use the scaler.inverse_transform of the scaler originally used on the training data\n    test_predictions.append(pred_val[0])\n    current_batch = np.append(current_batch[:,1:,:], pred_val.reshape(1,1,1), axis=1)\n\n\n\nCharacter-based generative NLP: given an input string, e.g. [‘h’, ‘e’, ‘l’, ‘l’], predict the sequence shifted by one character, e.g. [‘e’, ‘l’, ‘l’, ‘o’]\nThe embedding, GRU and dense layers work on the sequence in the following way: \nSteps:\n\nRead in text data\nText processing and vectorisation - one-hot encoding\nCreate batches\nCreate the model\nTrain the model\nGenerate new text\n\nStep 1: Read in text data\n\nA corpus of &gt;1 million characters is a good size\nThe more distinct the style of your corpus, the more obvious it will be if your model has worked.\n\nGiven some structured input_text string, we can get the one-hot encoded vocab list of unique characters.\nvocab_list = sorted(set(input_text))\nStep 2: Text processing\nVectorise the text - create a mapping between character and integer. This is the encoding dictionary used to vectorise the corpus.\n# Mappings back and forth between characters and their encoded integers\nchar_to_ind = {char: ind for ind, char in enumerate(vocab_list)}\nind_to_char = {ind: char for ind, char in enumerate(vocab_list)}\n\nencoded_text = np.array([char_to_ind[char] for char in input_text])\nStep 3: Create batches\nThe sequence length should be long enough to capture structure, e.g. for poetry with rhyming couplets it should be the number of characters in 3 lines to capture the rhyme and non-rhyme. Too long a sequence makes the model take longer to train and captures too much historical noise.\nCreate a shuffled dataset where:\n\ninput sequence is the first n characters\noutput sequence is n characters lagged by one\n\nWe want to shuffle these sequence pairs into a random order so the model doesn’t overfit to any section of the text, but can instead generate characters given any seed text.\nseq_len = 120\ntotal_num_seq = len(input_text) // (seq_len + 1)\n\n# Create Training Sequences\nchar_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\nsequences = char_dataset.batch(seq_len + 1, drop_remainder=True)  # drop_remainder drops the last incomplete sequence\n\n# Create tuples of input and output sequences\ndef create_seq_targets(seq):\n    input_seq = seq[:-1]\n    output_seq = seq[-1:]\n    return input_seq, output_seq\ndataset = sequences.map(create_seq_targets)\n\n# Generate training batches\nbatch_size = 128\nbuffer_size = 10000  # Shuffle this amount of batches rather than shuffling the entire dataset in memory\ndataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\nStep 4: Create the model\nSet up the loss function and layers.\nThis model uses 3 layers:\n\nEmbedding\n\nEmbed positive integers, e.g. “a”&lt;-&gt;1, as dense vectors of fixed size. This embedding size is a hyperparameter for the user to decide.\nNumber of layers should be smaller but a similar scale to the vocab size; typically use the power of 2 that is just smaller than the vocab size.\n\nGRU\n\nThis is the main thing to vary in the model: number of hidden layers and number of neurons in each, types of neurons (RNN, LSTM, GRU), etc. Typically use lots of layers here.\n\nDense\n\nOne neuron per character (one-hot encoded), so this produces a probability per character. The user can vary the “temperature”, choosing less probable characters more/less often.\n\n\nLoss function:\n\nSparse categorical cross-entropy\nUse sparse categorical cross-entropy because the classes are mutually exclusive. Use regular categorical cross-entropy when one smaple can have multiple classes, or the labels are soft probabilities.\nSee link in NLP appendix for discussion.\nUse logits=True because vocab input is one hot encoded.\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, GRU\nfrom tensorflow.keras.losses import sparse_categorical_crossentropy\n\n# Hyperparameters\nvocab_size = len(vocab_list)\nembed_dim = 64\nrnn_neurons = 1026\n\n# Create model \nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embed_dim, batch_input_shape=[batch_size, None]))\nmodel.add(GRU(rnn_neurons, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'))\nmodel.add(Dense(vocab_size))\nsparse_cat_loss = lambda y_true,y_pred: sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)  # We want a custom loss function with logits=True\nmodel.compile(optimizer='adam', loss=sparse_cat_loss) \nStep 5: Train the model\nepochs = 30\nmodel.fit(dataset,epochs=epochs)\nStep 6: Generate text\nAllow the model to accept a batch size of 1 to allow us to give an input seed text from which to generate text from.\nWe format the seed text so it can be fed into the network, then loop through generating one character at a time and feeding that back into the model.\nmodel.build(tf.TensorShape([1, None]))\n\ndef generate_text(model, input_seed_text, gen_size=100, temperature=1.0):\n    \"\"\"\n    model: tensorflow.model\n    input_seed_text: str\n    gen_size: int\n    temperature: float\n    \"\"\"\n    generated_text_result = []\n    vectorised_seed = tf.expand_dims([char_to_ind[s] for s in input_seed_text], 0)\n    model.reset_states()\n    \n    for k in range(gen_size):\n        raw_predictions = model(vectorised_seed)\n        predictions = tf.squeeze(raw_predictions, 0) / temperature\n        predicted_index = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n        generated_text_result.append(ind_to_char[predicted_index])\n        \n        # Set a new vectorised seed to generate the next character in the loop\n        vectorised_seed = tf.expand_dims([predicted_index], 0)\n        \n    return (input_seed_text + ''.join(generated_text_result))\n\n\n\nThis is an unsupervised learning problem, therefore evaluation metrics are different. Sometimes called semi-supervised as labels may be used in training the model, but not available when it’s used to predict new values.\nApplications:\n\nDimensionality reduction\nNoise removal\n\nDesigned to reproduce its input in the output layer. Number of input neurons is the same as the number of output layers. The encoder reduces dimensionality to the hidden layer. The hidden layer should contain the most relevant information required to reconstruct the input. The decoder reconstructs a high dimensional output from a low dimensional input.\n\nEncoder: Input layer -&gt; Hidden layer\nDecoder: Hidden layer -&gt; Output\n\nStacked autoencoders include multiple hidden layers.\nAutoencoder for dimensionality reduction\nLet’s try to reduce an input dataset from 3 dimensions to 2. We want to train an autoencoder to go from 3 -&gt; 2 -&gt; 3.\nWhen we scale data, we fit and transform on the entire dataset rather than a train/test split, because there is no ground truth for the “correct” dimensionality reduction.\nUsing SGD in Autoencoders allows the learning rate to be varied as a hyperparameter to affect how well the hidden layer learns. Larger values will train quicker but potentially perform worse.\nTo retrieve the lower dimensionality representation, use just the encoder half of the trained autoencoder to predict the input.\n# Scale data\nscaler = MinMaxScaler()\nscaled_data = scaler.fit_transform(input_features)\n\n# Create model\nencoder = Sequential()\nencoder.add(Dense(units=2, acivation='relu', input_shape=[3]))\n\ndecoder = Sequential()\ndecoder.add(Dense(units=3, acivation='relu', input_shape=[2]))\n\nautoencoder = Sequential([encoder, decoder])\nautoencoder.compile(loss='mse', optimizer=SGD(lr=1.5))\n\n# Train the full autoencoder model\nautoencoder.fit(scaled_data, scaled_data, epochs=5)\n\n# Reduce dimensionality by using just the encoder part of the trained model\nencoded_2dim = encoder.predict(scaled_data)\nAutoencoder for noise removal\nStarting with clean images, we want to train an autoencoder to learn to remove noise. We create a noisy dataset by adding noise to our clean images. We then try to reconstruct that input, and compare to the original clean images as the ground truth.\nStarting with 28x28 images (e.g. MNIST dataset), this gives 784 input features when flattened. Use multiple hidden layers as there are a large number of input features that would be difficult to learn in one layer. The numbers of layers and number of neurons in each are hyperparameters to tune; decreasing in powers of 2 is a good rule of thumb.\nAdd a Gaussian noise layer to train the model to remove noise. Without the Gaussian layer, the model would be used for dimensionality reduction rather than noise removal.\nThe final layer uses a sigmoid activation because it is a binary classification problem - does the output match the input?\nencoder = Sequential()\nencoder.add(Flatten(input_shape=[28, 28]))\nencoder.add(Gaussian(0.2))  # Optional layer if training for noise removal rather than dimensionality reduction\nencoder.add(Dense(400, activation='relu'))\nencoder.add(Dense(200, activation='relu'))\nencoder.add(Dense(100, activation='relu'))\nencoder.add(Dense(50, activation='relu'))\nencoder.add(Dense(25, activation='relu'))\n\ndecoder = Sequential()\ndecoder.add(Dense(50, input_shape=[25], activation='relu'))\ndecoder.add(Dense(100, activation='relu'))\ndecoder.add(Dense(200, activation='relu'))\ndecoder.add(Dense(400, activation='relu'))\ndecoder.add(Dense(28*28, activation='sigmoid'))\n\nautoencoder = Sequential([encoder, decoder])\nautoencoder.compile(loss='binary_crossentropy', optimizer=SGD(lr=1.5), metrics=['accuracy'])\nautoencoder.fit(X_train, X_train, epochs=5)\n\n\n\nUse 2 networks competing against each other to generate data - counterfeiter (generator) vs detective (discriminator).\n\nGenerator: recieves random noise\nDiscriminator: receives data set containing real data and fake generated data, and attempts to calssify real vs fake (binary classificaiton).\n\nTwo training phases, each only training one of the networks:\n\nTrain discriminator - real images (labeled 1) and generated images (labeled 0) are fed to the discriminator network.\nTrain generator - only feed generated images to discriminator, ALL labeled 1.\n\nThe generator never sees real images, it creates images based only off of the gradients flowing back from the discriminator.\nDifficulties with GANs:\n\nTraining resources - GPUs generally required\nMode collapse - generator learns an image that fools the discriminator, then only produces this image. Deep convolutional GANs and mini-batch discrimination are two solution to this problem.\nInstability - True performance is hard to measure; just because the discriminator was fooled, it doesn’t mean that the generated image was actually realistic.\n\nCreating the model\nWe create the generator and discriminator as separate models, then join them into a GAN object. This is analogous to how we joined an encoder and decoder into an autoencoder.\nThe discriminator is trained on a binary classification problem, real or fake, so uses a binary cross entropy loss function. Backpropagation only alters the weights of the discriminator in the first phase, not the generator.\ndiscriminator = Sequential()\n\n# Flatten the image\ndiscriminator.add(Flatten(input_shape=[28,28]))\n\n# Some hidden layers\ndiscriminator.add(Dense(150,activation='relu'))\ndiscriminator.add(Dense(100,activation='relu'))\n\n# Binary classification - real vs fake\ndiscriminator.add(Dense(1,activation=\"sigmoid\"))\ndiscriminator.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\nThe generator is trained in the second phase.\nWe set a codings_size, which is the size of the latent representation from which the generator will create a full image. For example, if we want to create a 28x28 image (784 pixels), we may want to generate this from a 100 element input. This is analogous to the decoder part of an autoencoder.\nThe codings_size should be smaller than the total number of features but large enough so that there is something to learn from.\nThe generator is NOT compiled at this step, it is compiled in the combined GAN later so that it is only trained at that step.\ncodings_size = 100\ngenerator = Sequential()\n\n# Input shape of first layer is the codings_size\ngenerator.add(Dense(150, activation=\"relu\", input_shape=[codings_size]))\n\n# Some hidden layers\ngenerator.add(Dense(200,activation='relu'))\n\n# Output is the size of the image we want to create\ngenerator.add(Dense(784, activation=\"sigmoid\"))\ngenerator.add(Reshape([28,28]))\nThe GAN combines the generator and discriminator.\nThe discriminator is only trained in the first phase, not the second phase.\nGAN = Sequential([generator, discriminator])\ndiscriminator.trainable = False\nGAN.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\nTraining the model\nWe first create batches of images from our input data, similarly to previous models.\nbatch_size = 32  # Size of training input batches\nbuffer_size = 1000  # Don't load the entire dataset into memory\nepochs = 1\n\nraw_dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(buffer_size=buffer_size)\nbatched_dataset = raw_dataset.batch(batch_size, drop_remainder=True).prefetch(1)\nThe training loop for each epoch trains the discriminator to distinguish real vs fake images (binary classification). Then it trains the generator to generate fake images using ONLY the gradients learned in the discriminator training step; the generator NEVER sees any real images.\ngenerator, discriminator = GAN.layers\n\nfor epoch in range(epochs):\n    for index, X_batch in enumerate(batched_dataset):\n        # Phase 1: Train the discriminator\n        noise = tf.random.normal(shape=[batch_size, codings_size])\n        generated_images = generator(noise)\n        real_images = tf.dtypes.cast(X_batch,tf.float32)\n        X_train_discriminator = tf.concat([generated_images, real_images], axis=0)\n        y_train_discriminator = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)  # Targets set to zero for fake _images and 1 for real _images\n        discriminator.trainable = True\n        discriminator.train_on_batch(X_train_discriminator, y_train_discriminator)\n\n        # Phase 2: Train the generator\n        noise = tf.random.normal(shape=[batch_size, codings_size])\n        y_train_generator = tf.constant([[1.]] * batch_size)  # We want discriminator to believe that fake _images are real\n        discriminator.trainable = False\n        GAN.train_on_batch(noise, y_train_generator)    \nDeep convolutional GANs\nThese are GANs which use convolutional layers inside the discriminator and generator models.\nThe models are almost identical to the models above, just with additional Conv2D, Conv2DTranspose and BatchNormalization layers.\nChanges compared to regular GANs:\n\nAdditional convolutional layers in model.\nReshape the training set to match the images.\nRescale the training data to be between -1 and 1 so that tanh activation function works.\nActivation of discriminator output layer is tanh rather than sigmoid."
  },
  {
    "objectID": "posts/ml/tensorflow/tensorflow.html#a.-appendix",
    "href": "posts/ml/tensorflow/tensorflow.html#a.-appendix",
    "title": "Tensorflow Notes",
    "section": "",
    "text": "Complete Tensorflow and Keras Udemy course\nIntuition behind neural networks\nThe deep learning bible (with lectures)\nHeuristics for choosing hidden layers\nAlternatives to the deprecated model predict for different classification problems\nMIT course\n\n\n\n\n\nImage kernels explained\nChoosing CNN layers\n\n\n\n\n\nOverview of RNNs\nWikipedia page contains the equations of LSTMs and peepholes\nLSTMs vs GRUs\nWorked example of LSTM\nRNN cheatsheet\n\n\n\n\n\nThe unreasonable effectiveness of RNNs - essay on RNNs applied to NLP\nSparse vs dense categorical crossentropy loss function\n\n\n\n\n\nbuffer_size argument in tensorflow prefetch and shuffle\nMode collapse"
  },
  {
    "objectID": "posts/ml/kalman_filter/kalman_filter.html",
    "href": "posts/ml/kalman_filter/kalman_filter.html",
    "title": "Kalman Filter Notes",
    "section": "",
    "text": "Notes from https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python\n\n\n\n\nWith measurements from 2 noisy sensors, we can combine them to get a much more accurate measurement. Never throw data away.\nIf we have a prediction model for how the world evolves (e.g. predicted daily weight gain) we can combine this with noisy measurements in the same way. Treat the prediction as a noisy sensor and combine as before, so that our estimate is some way between the prediction and the estimate. How close it is to one or the other depends on the relative accuracy of each, and this is a hyperparameter we can set.\nprediction =  x_est + dx*dt \n\nwhere:\n`x_est` is an initial constant that gets updated\n`dx` is a predicted daily weight gain rate\n`dt` is the time step\n\n\n\nWe can create a single parameter filter, where the parameter g corresponds to how strongly we trust the prediction over the measurement\nresidual = prediction - measurement\nestimate = measurement + g * residual\n\ng=0 =&gt; estimate=measurement\ng=1 =&gt; estimate=prediction\nfor other values of g, the estimate will be somewhere between the measurement and prediction\n\n\n\n\n\n\n\nFigure 1: g-h filter diagram\n\n\n\n\n\n\nWe can go one step further and use our noisy estimates of weight to refine our the daily weight gain w used in our prediction\ndx_t+1 = dx_t + h * residual / dt\nThis gives us the g-h filter - a generic filter which allows us to set a parameter g for the weight measurement confidence and h for the weight change measurement confidence.\nWe can then update the estimates for weight and weight change on every time step (or every measurement if time steps are irregular).\nThis insight forms the basis for Kalman filters, where we will set g and h dynamically on each time step.\n\n\n\nKey takeaways:\n\nMultiple data points are more accurate than one data point, so throw nothing away no matter how inaccurate it is.\nAlways choose a number part way between two data points to create a more accurate estimate.\nPredict the next measurement and rate of change based on the current estimate and how much we think it will change.\nThe new estimate is then chosen as part way between the prediction and next measurement scaled by how accurate each is.\nThe filter is only as good as the mathematical model used to express the system.\nFilters are designed, not selected ad hoc. No choice of g and h applies to all scenarios.\n\nTerminology:\n\nSystem - the object we want to estimate\nState, x - the current configuration of the system. Hidden.\nMeasurement, z - measured value of the state from a noisy sensor. Observable.\nState estimate - our filter’s estimate of the state.\nProcess model - the model we use to predict the next state based on the current state.\nSystem propagation - the predict step\nMeasurement update - the update step\nEpoch - one iteration of system propagation and measurement update.\n\n\n\n\n\n\n\nBayesian statistics treats probability as a belief about a single event. Frequentist statistics describes past events based on their frequency. It has no bearing on future events.\nIf I flip a coin 100 times and get 50 heads and 50 tails, frequentist statistics states the probability of heads was 50% for those cases. On the next coin flip, frequentist statistics has nothing to say about the probability. The state is simply unknown. Bayesian statistics incorporates these past events as a prior belief, so that we can say the next coin flip has a 50% chance of landing heads. “Belief” is a measure of the strength of our knowledge.\nWhen talking about the probability of something, we are implicitly saying “the probability that this event is true given past events”. This is a Bayesian approach. In practice, we may incorporate frequentist techniques too, as in the example above when the 100 previous coin tosses were used to inform our prior.\n\nPrior is the probability distribution before including the measurement’s information. This corresponds to the prediction in the Kalman filter.\nPosterior is a probability distribution after incorporating the measurement’s information. This corresponds to the estimated state in the Kalman filter.\nLikelihood is the joint probability of the observed data - how likely is each position given the measurement. This is not a probability distribution as it does not sum to 1.\n\nThe filter will use Bayes theorem:\nposterior = likelihood * prior / normalization\nIn even simpler filter terms:\nupdated knowledge = || likelihood of new knowledge * prior knowledge ||\nIf we have a prior distribution of positions and system model of the subsequent movement, we can convolve the two to calculate the posterior.\n\n\n\nThe discrete Bayes filter is a form of g-h filter. It is useful for multimodal, discrete problems.\nThe equations are:\nPredict step:\nx_bar = x * f_x(.)\nwhere:\n- x is the current state\n- f_x(.) is the state propagation function for x, i.e. the system model.\n\n\nUpdate step:\nx = ||L . x_bar||\nwhere:\n- L is the likely function\nIn pseudocode this is:\nInitialisation:\n1. Initialise our belief in the state.\n\nPredict:\n1. Predict state for the next time step using the system model.\n2. Adjust belief to account for uncertainty in prediction.\n\nUpdate:\n1. Get a measurement and belief about its accuracy (noise estimate).\n2. Compute likelihood of measurement matching each state.\n3. Update posterior state belief based on likelihood.\nAlgorithms of this form a called “predictor correctors”; we make a prediction then correct it. The predict step will always degrade our knowledge due to the uncertainty in the second step of the predict stage. But adding another measurement, even if noisy, improves our knowledge again. So we can converge on the most likely result.\n\n\n\nThe algorithm is trivial to implement, debug and understand.\nLimitations:\n\nScaling - Tracking i state variables results in O(n^i) runtime complexity.\nDiscrete - Most real-world examples are continuous. We can increase the granularity to get a discrete approximation of continuous measurements, but this increases the scale again.\nMultimodal - Sometimes you require a single output value.\nNeeds a state change measurement.\n\nThe Kalman filter is based on the same idea that we can use Bayesian reasoning to combine measurements and system models. The fundamental insight in this chapter is that we multiply (convolve) probabilities when we measure and shift probabilities when we update, which leads to a converging solution.\n\n\n\n\nGaussian distributions address some of the limitations of the discrete Bayes filter, as they are continuous and unimodal.\n\n\n\nRandom variables - the possible values and associated probabilities of a particular event. Denoted with capital letter, e.g. X\nSample space - the range of values a random variable can take. This is not unique, e.g. for a dice roll could be {1,2,3,4,5,6}, or {even,odd} or {d}\nProbability distribution - the probability for the random variable to take any value in the sample space. Probability distribution denoted with lower case p, and probability of a single event denoted with upper case P, e.g. P(X=1) = p(1)\nTotal probability - Probabilities are non-negative and sum/integrate to 1.\nSummary statistics - mean, median, mode\nExpected value is the probability-weighted mean of values. This equals the mean if the probability distribution is uniform. E[X] = \\sum{p_i x_i} mean = \\sum{x_i}/n\nVariance - var(X) = E[(x-mu)^2] = \\sum{(x_i - mu)^2}/n The square in the variance (rather han absolute value) is somewhat arbitrary, and reflects that the sign of the deviation shouldn’t matter and larger deviations are “worse” than smaller deviations. Precision is sometimes used which is the inverse of the variance.\n\n\n\n\nContinuous, unimodal probability distributions.\nf(x, mu, sigma) = (1 / sigma sqrt(2*pi)) * e^(-(x-mu)^2/2*sigma^2)\n\nwhich, removing constants, is proportional to e^-x^2\nThe probability of a single point is infinitesimally small. We can think of this in Bayesian terms or frequentist terms. As a Bayesian, if the thermometer reads exactly 22°C, then our belief is described by the Gaussian curve; our belief that the actual (system) temperature is near 22°C is very high, and our belief that the actual temperature is, say, near 18 is very low. As a frequentist we would say that if we took 1 billion temperature measurements of a system at exactly 22°C, then a histogram of the measurements would look like a Gaussian curve.\nGaussians are nice to work with because the sum of independent Gaussian random variables is another Gaussian random variable. The product of Gaussian distributions will be a Gaussian function, i.e. it is Gaussian but may not sum to 1 so will need to be scaled. Sum of two Gaussians:\n\nmu = mu_1 + mu_2\nvar = var_1 + var_2\n\nProduct of two Gaussians:\n\nmu = (var_1mu_2 + var_2mu_1) / (var_1 + var_2)\nvar = (var_1*var_2) / (var_1 + var_2)\n\nThey also mean we can summarise/approximate large datasets with only two numbers: mean and variance.\nBayes theorem applies to probability distribution functions (p) just like it does to individual events (P). From the general form of Bayes theorem:\np(A|B) = p(B|A)p(A)/p(B)\nRecasting this for the sensor problem, where A=x_i (the position) and B=z (sensor reading):\np(x_i|z) = p(z|x_i)p(x_i)/p(z)\n\np(z|x_i) is the likelihood - the probability that we get the sensor measurement z at each position x_i\np(x_i) is the prior - our belief before incorporating the measurement\np(x_i) is the evidence - in practice this is often an intractable integral which we can sidestep by treating this as a normalisation factor, as we know the posterior pdf must sum to 1.\np(x_i|z) is the posterior - this is typically a hard question to answer, but using Bayes we can substitute it with the easier question answered by the likelihood and prior. Rather than answering “what is the probability of cancer given this test result” (hard problem) we instead answer “what is the probability of this test result given someone has cancer” (easier problem)\n\n\n\n\nA limitation of using Gaussians to model physical systems is that it has infinitely long tails, but in practice many physical quantities have hard bounds, e.g. weight cannot be negative. Under a Gaussian assumption, model people’s weight as a Gaussian would suggest there is a miniscule chance that someone weights 1000000kg, which is obviously wrong.\nThis can impact the performance of Kalman filters as they rely on assumptions of Gaussian noise, which aren’t strictly true.\n\n\n\n\nA filter to track one state variable, position. A Kalman filter is essentially a Bayesian filter that uses Gaussians, so combines the previous two chapters.\n\n\n\n\nKalman filter repo\nArtificial Intelligence for Robotics course\nThink Stats ebook\n\n\n\n\nFigure 1: g-h filter diagram"
  },
  {
    "objectID": "posts/ml/kalman_filter/kalman_filter.html#g-h-filters",
    "href": "posts/ml/kalman_filter/kalman_filter.html#g-h-filters",
    "title": "Kalman Filter Notes",
    "section": "",
    "text": "With measurements from 2 noisy sensors, we can combine them to get a much more accurate measurement. Never throw data away.\nIf we have a prediction model for how the world evolves (e.g. predicted daily weight gain) we can combine this with noisy measurements in the same way. Treat the prediction as a noisy sensor and combine as before, so that our estimate is some way between the prediction and the estimate. How close it is to one or the other depends on the relative accuracy of each, and this is a hyperparameter we can set.\nprediction =  x_est + dx*dt \n\nwhere:\n`x_est` is an initial constant that gets updated\n`dx` is a predicted daily weight gain rate\n`dt` is the time step\n\n\n\nWe can create a single parameter filter, where the parameter g corresponds to how strongly we trust the prediction over the measurement\nresidual = prediction - measurement\nestimate = measurement + g * residual\n\ng=0 =&gt; estimate=measurement\ng=1 =&gt; estimate=prediction\nfor other values of g, the estimate will be somewhere between the measurement and prediction\n\n\n\n\n\n\n\nFigure 1: g-h filter diagram\n\n\n\n\n\n\nWe can go one step further and use our noisy estimates of weight to refine our the daily weight gain w used in our prediction\ndx_t+1 = dx_t + h * residual / dt\nThis gives us the g-h filter - a generic filter which allows us to set a parameter g for the weight measurement confidence and h for the weight change measurement confidence.\nWe can then update the estimates for weight and weight change on every time step (or every measurement if time steps are irregular).\nThis insight forms the basis for Kalman filters, where we will set g and h dynamically on each time step.\n\n\n\nKey takeaways:\n\nMultiple data points are more accurate than one data point, so throw nothing away no matter how inaccurate it is.\nAlways choose a number part way between two data points to create a more accurate estimate.\nPredict the next measurement and rate of change based on the current estimate and how much we think it will change.\nThe new estimate is then chosen as part way between the prediction and next measurement scaled by how accurate each is.\nThe filter is only as good as the mathematical model used to express the system.\nFilters are designed, not selected ad hoc. No choice of g and h applies to all scenarios.\n\nTerminology:\n\nSystem - the object we want to estimate\nState, x - the current configuration of the system. Hidden.\nMeasurement, z - measured value of the state from a noisy sensor. Observable.\nState estimate - our filter’s estimate of the state.\nProcess model - the model we use to predict the next state based on the current state.\nSystem propagation - the predict step\nMeasurement update - the update step\nEpoch - one iteration of system propagation and measurement update."
  },
  {
    "objectID": "posts/ml/kalman_filter/kalman_filter.html#discrete-bayes-filter",
    "href": "posts/ml/kalman_filter/kalman_filter.html#discrete-bayes-filter",
    "title": "Kalman Filter Notes",
    "section": "",
    "text": "Bayesian statistics treats probability as a belief about a single event. Frequentist statistics describes past events based on their frequency. It has no bearing on future events.\nIf I flip a coin 100 times and get 50 heads and 50 tails, frequentist statistics states the probability of heads was 50% for those cases. On the next coin flip, frequentist statistics has nothing to say about the probability. The state is simply unknown. Bayesian statistics incorporates these past events as a prior belief, so that we can say the next coin flip has a 50% chance of landing heads. “Belief” is a measure of the strength of our knowledge.\nWhen talking about the probability of something, we are implicitly saying “the probability that this event is true given past events”. This is a Bayesian approach. In practice, we may incorporate frequentist techniques too, as in the example above when the 100 previous coin tosses were used to inform our prior.\n\nPrior is the probability distribution before including the measurement’s information. This corresponds to the prediction in the Kalman filter.\nPosterior is a probability distribution after incorporating the measurement’s information. This corresponds to the estimated state in the Kalman filter.\nLikelihood is the joint probability of the observed data - how likely is each position given the measurement. This is not a probability distribution as it does not sum to 1.\n\nThe filter will use Bayes theorem:\nposterior = likelihood * prior / normalization\nIn even simpler filter terms:\nupdated knowledge = || likelihood of new knowledge * prior knowledge ||\nIf we have a prior distribution of positions and system model of the subsequent movement, we can convolve the two to calculate the posterior.\n\n\n\nThe discrete Bayes filter is a form of g-h filter. It is useful for multimodal, discrete problems.\nThe equations are:\nPredict step:\nx_bar = x * f_x(.)\nwhere:\n- x is the current state\n- f_x(.) is the state propagation function for x, i.e. the system model.\n\n\nUpdate step:\nx = ||L . x_bar||\nwhere:\n- L is the likely function\nIn pseudocode this is:\nInitialisation:\n1. Initialise our belief in the state.\n\nPredict:\n1. Predict state for the next time step using the system model.\n2. Adjust belief to account for uncertainty in prediction.\n\nUpdate:\n1. Get a measurement and belief about its accuracy (noise estimate).\n2. Compute likelihood of measurement matching each state.\n3. Update posterior state belief based on likelihood.\nAlgorithms of this form a called “predictor correctors”; we make a prediction then correct it. The predict step will always degrade our knowledge due to the uncertainty in the second step of the predict stage. But adding another measurement, even if noisy, improves our knowledge again. So we can converge on the most likely result.\n\n\n\nThe algorithm is trivial to implement, debug and understand.\nLimitations:\n\nScaling - Tracking i state variables results in O(n^i) runtime complexity.\nDiscrete - Most real-world examples are continuous. We can increase the granularity to get a discrete approximation of continuous measurements, but this increases the scale again.\nMultimodal - Sometimes you require a single output value.\nNeeds a state change measurement.\n\nThe Kalman filter is based on the same idea that we can use Bayesian reasoning to combine measurements and system models. The fundamental insight in this chapter is that we multiply (convolve) probabilities when we measure and shift probabilities when we update, which leads to a converging solution."
  },
  {
    "objectID": "posts/ml/kalman_filter/kalman_filter.html#probability-and-gaussians",
    "href": "posts/ml/kalman_filter/kalman_filter.html#probability-and-gaussians",
    "title": "Kalman Filter Notes",
    "section": "",
    "text": "Gaussian distributions address some of the limitations of the discrete Bayes filter, as they are continuous and unimodal.\n\n\n\nRandom variables - the possible values and associated probabilities of a particular event. Denoted with capital letter, e.g. X\nSample space - the range of values a random variable can take. This is not unique, e.g. for a dice roll could be {1,2,3,4,5,6}, or {even,odd} or {d}\nProbability distribution - the probability for the random variable to take any value in the sample space. Probability distribution denoted with lower case p, and probability of a single event denoted with upper case P, e.g. P(X=1) = p(1)\nTotal probability - Probabilities are non-negative and sum/integrate to 1.\nSummary statistics - mean, median, mode\nExpected value is the probability-weighted mean of values. This equals the mean if the probability distribution is uniform. E[X] = \\sum{p_i x_i} mean = \\sum{x_i}/n\nVariance - var(X) = E[(x-mu)^2] = \\sum{(x_i - mu)^2}/n The square in the variance (rather han absolute value) is somewhat arbitrary, and reflects that the sign of the deviation shouldn’t matter and larger deviations are “worse” than smaller deviations. Precision is sometimes used which is the inverse of the variance.\n\n\n\n\nContinuous, unimodal probability distributions.\nf(x, mu, sigma) = (1 / sigma sqrt(2*pi)) * e^(-(x-mu)^2/2*sigma^2)\n\nwhich, removing constants, is proportional to e^-x^2\nThe probability of a single point is infinitesimally small. We can think of this in Bayesian terms or frequentist terms. As a Bayesian, if the thermometer reads exactly 22°C, then our belief is described by the Gaussian curve; our belief that the actual (system) temperature is near 22°C is very high, and our belief that the actual temperature is, say, near 18 is very low. As a frequentist we would say that if we took 1 billion temperature measurements of a system at exactly 22°C, then a histogram of the measurements would look like a Gaussian curve.\nGaussians are nice to work with because the sum of independent Gaussian random variables is another Gaussian random variable. The product of Gaussian distributions will be a Gaussian function, i.e. it is Gaussian but may not sum to 1 so will need to be scaled. Sum of two Gaussians:\n\nmu = mu_1 + mu_2\nvar = var_1 + var_2\n\nProduct of two Gaussians:\n\nmu = (var_1mu_2 + var_2mu_1) / (var_1 + var_2)\nvar = (var_1*var_2) / (var_1 + var_2)\n\nThey also mean we can summarise/approximate large datasets with only two numbers: mean and variance.\nBayes theorem applies to probability distribution functions (p) just like it does to individual events (P). From the general form of Bayes theorem:\np(A|B) = p(B|A)p(A)/p(B)\nRecasting this for the sensor problem, where A=x_i (the position) and B=z (sensor reading):\np(x_i|z) = p(z|x_i)p(x_i)/p(z)\n\np(z|x_i) is the likelihood - the probability that we get the sensor measurement z at each position x_i\np(x_i) is the prior - our belief before incorporating the measurement\np(x_i) is the evidence - in practice this is often an intractable integral which we can sidestep by treating this as a normalisation factor, as we know the posterior pdf must sum to 1.\np(x_i|z) is the posterior - this is typically a hard question to answer, but using Bayes we can substitute it with the easier question answered by the likelihood and prior. Rather than answering “what is the probability of cancer given this test result” (hard problem) we instead answer “what is the probability of this test result given someone has cancer” (easier problem)\n\n\n\n\nA limitation of using Gaussians to model physical systems is that it has infinitely long tails, but in practice many physical quantities have hard bounds, e.g. weight cannot be negative. Under a Gaussian assumption, model people’s weight as a Gaussian would suggest there is a miniscule chance that someone weights 1000000kg, which is obviously wrong.\nThis can impact the performance of Kalman filters as they rely on assumptions of Gaussian noise, which aren’t strictly true."
  },
  {
    "objectID": "posts/ml/kalman_filter/kalman_filter.html#one-dimensional-kalman-filter",
    "href": "posts/ml/kalman_filter/kalman_filter.html#one-dimensional-kalman-filter",
    "title": "Kalman Filter Notes",
    "section": "",
    "text": "A filter to track one state variable, position. A Kalman filter is essentially a Bayesian filter that uses Gaussians, so combines the previous two chapters."
  },
  {
    "objectID": "posts/ml/kalman_filter/kalman_filter.html#references",
    "href": "posts/ml/kalman_filter/kalman_filter.html#references",
    "title": "Kalman Filter Notes",
    "section": "",
    "text": "Kalman filter repo\nArtificial Intelligence for Robotics course\nThink Stats ebook\n\n\n\n\nFigure 1: g-h filter diagram"
  },
  {
    "objectID": "posts/maths/basic_statistics/lesson.html",
    "href": "posts/maths/basic_statistics/lesson.html",
    "title": "Basic Statistics",
    "section": "",
    "text": "A distribution is just “the list of observed values”.\nIt is tempting to think of a distribution as a function or a plot, but these are ultimately representations of the distribution itself.\nBasic concepts to be aware of regarding distributions:\n\nContinuous vs discrete values\nMean, median, mode.\nVariance (and standard deviation). Analog of MSE and RMSE.\nSkewness"
  },
  {
    "objectID": "posts/maths/basic_statistics/lesson.html#background",
    "href": "posts/maths/basic_statistics/lesson.html#background",
    "title": "Basic Statistics",
    "section": "2.1. Background",
    "text": "2.1. Background\nWe can sample from a distribution, which is to say we observe a subset of the possible observations.\nThe central limit theorem states three properties observed when repeatedly \\(N\\) items from any distribution.\n\nThe samples are normally distributed\nThe estimated sample mean converges to the true population mean\nThe sample standard deviation of the estimated mean converges to $\n\nA tangible example of this might be if we wanted to estimate the words per page in a book.\n\nGroup A takes the intelligent decision to count every word on every page.\nGroup B splits the work. They each count 20 pages and report the mean. But they don’t coordinate, so they each pick 20 random, possibly overlapping pages.\n\nGroup B’s mean will converge to the true Group A mean if they do this enough times.\nIn practice, we don’t actually sample a set number of observations thousands of times and plot the distribution. But the CLT result is useful because it tells us what we would have observed if we had done so. We can then use this for hypothesis testing. We have a sample value. We can say whether it is above or below the sample mean (which is a decent estimate of the true mean) and by how much (in terms of std devs)."
  },
  {
    "objectID": "posts/maths/basic_statistics/lesson.html#z-score",
    "href": "posts/maths/basic_statistics/lesson.html#z-score",
    "title": "Basic Statistics",
    "section": "2.2. Z-Score",
    "text": "2.2. Z-Score\nThis leads on to the concept of Z-scores. This is essentially normalising values on any scale. So the number is telling us “how many standard deviations am I away from the mean?”\nThese numbers are tabulated, so we know what the probability is of obtaining a Z-score at least as extreme as this."
  },
  {
    "objectID": "posts/maths/basic_statistics/lesson.html#two-universe",
    "href": "posts/maths/basic_statistics/lesson.html#two-universe",
    "title": "Basic Statistics",
    "section": "3.1. Two Universe",
    "text": "3.1. Two Universe\nWe can think of hypothesis tests as proposing two “universes”:\n\\[\n\\begin{aligned}\nH_0 &: \\text{This is a fair coin} \\\\\nH_1 &: \\text{This is not a fair coin}\n\\end{aligned}\n\\]\nWe conduct some experiments and calculate the probability of this outcome in universe 0. If this is sufficiently unlikely, we reject the null hypothesis, ie we don’t believe we live in universe 0 therefore we must live in universe 1."
  },
  {
    "objectID": "posts/maths/basic_statistics/lesson.html#p-values",
    "href": "posts/maths/basic_statistics/lesson.html#p-values",
    "title": "Basic Statistics",
    "section": "3.2. P-values",
    "text": "3.2. P-values\nAs an example, if we flip the coin 6 times and get 6 heads in a row, the probability if this were a fair coin is 1%. In other words, the p-value is 0.01.\nThis makes us feel “uneasy” so we start to question whether we actually live in universe 0. If we lived in universe 1 and the coin was always heads, then P(6 heads) would be 100% so we wouldn’t get an uneasy feeling.\nThe confidence level is the critical p-value at which we reject the null hypothesis. In other words, at what point we feel “uneasy” enough that we change our world view."
  },
  {
    "objectID": "posts/maths/basic_statistics/lesson.html#hypothesis-testing-teps",
    "href": "posts/maths/basic_statistics/lesson.html#hypothesis-testing-teps",
    "title": "Basic Statistics",
    "section": "3.3. Hypothesis testing teps",
    "text": "3.3. Hypothesis testing teps\nProcedure for testing if the sample mean is statistically significantly different from a population mean. This is a Z-test because we are testing based on the Z-score.\n\nState the null hypothesis\nState the alternative hypothesis\nCalculate the sample standard deviation: \\(\\sigma_{sample} = \\frac{\\sigma_{pop}}{\\sqrt{N}}\\)\nCalc Z-score\nLook up p-value of our Z-score\nCompare p-value to confidence level. Accept or reject the null hypothesis."
  },
  {
    "objectID": "posts/maths/basic_statistics/lesson.html#rejection-region",
    "href": "posts/maths/basic_statistics/lesson.html#rejection-region",
    "title": "Basic Statistics",
    "section": "3.4. Rejection Region",
    "text": "3.4. Rejection Region\nAn alternative approach to choosing whether to accept/reject the null hypothesis is by calculating the critical Z-value for the rejection.\nWe proceed through the first 4 steps as before. But rather than converting our Z-score to a p-value and then comparing to our critical p-value (the confidence level), we go the other way. We convert the confidence level p-value to a critical Z-score. Then we can say whether our observed Z-score was more extreme than the critical Z-score in order to accept/reject the null hypothesis.\n\nConvert confidence level to critical Z-score\nCompare Z vs Z_critical. Accept or reject the null hypothesis.\n\nWe can shade in the “rejection region” of the normal distribution and see if our sample value lies in it."
  },
  {
    "objectID": "posts/maths/basic_statistics/lesson.html#hypothesis-testing-assumptions",
    "href": "posts/maths/basic_statistics/lesson.html#hypothesis-testing-assumptions",
    "title": "Basic Statistics",
    "section": "3.5. Hypothesis Testing Assumptions",
    "text": "3.5. Hypothesis Testing Assumptions\nZ-test assumptions:\n\nSample is selected at random.\nObservations are independent.\nThe population’s standard deviation is known, OR the sample contains at least 30 samples. With an unknown population standard deviation, use a T-test, but above 30 samples the distribution basically converges to Normal anyway."
  },
  {
    "objectID": "posts/maths/basic_statistics/lesson.html#proportion-testing",
    "href": "posts/maths/basic_statistics/lesson.html#proportion-testing",
    "title": "Basic Statistics",
    "section": "3.6. Proportion Testing",
    "text": "3.6. Proportion Testing\nThis is a very similar problem where we want to test whether the proportion within a population has changed.\nFor example, a 2016 study says that 58% of households have tablets (i.e. iPads). We survey 100 random households and find that 73 do. We want to know if only 58% do.\n(Confusingly, p refers to population here, not probability)\nH0: 58% or fewer households have tablets H1: More than 58% have tablets.\nFirst we have to check that our sample size is big enough:\n\\[\n\\begin{aligned}\nnp &\\ge 10 \\\\\nnq &\\ge 10 \\\\\n\\text{where } q &= 1 - p\n\\end{aligned}\n\\]\nWe then conduct a hypothesis test as before, with: \\[\n\\begin{aligned}\n\\mu_{\\text{population}} &= p_{\\text{population}} \\, (0.58) \\\\\n\\sigma_{\\text{population}} &= \\sqrt{p q}\n\\end{aligned}\n\\]\nThe proportions are Normally-distributed as before so we can conduct a Z-test."
  },
  {
    "objectID": "posts/maths/basic_statistics/lesson.html#failing-to-reject-the-null-hypothesis",
    "href": "posts/maths/basic_statistics/lesson.html#failing-to-reject-the-null-hypothesis",
    "title": "Basic Statistics",
    "section": "3.7. Failing to Reject the Null Hypothesis",
    "text": "3.7. Failing to Reject the Null Hypothesis\nThe two options when hypothesis testing are “reject the null hypothesis” or “fail to reject the null hypothesis”.\nWe don’t accept any specific hypothesis. If we have a critical Z-score, it doesn’t necessarily mean H1 is true, there could be some other hypothesis H2 that also explains the result. So we don’t “accept H1”.\nEqually we don’t “accept H0”. If we don’t have the critical Z-score, it may just be that we need more samples, the data wasn’t randomly sampled, etc."
  },
  {
    "objectID": "posts/maths/basic_statistics/lesson.html#the-t-distribution",
    "href": "posts/maths/basic_statistics/lesson.html#the-t-distribution",
    "title": "Basic Statistics",
    "section": "4.1. The t-distribution",
    "text": "4.1. The t-distribution\nThe t-distribution describes a sample rather than the distribution of the population.\nThe t-distribution has heavier tails than the Normal distribution. The fewer “degrees of freedom” the heavier the tails. In other words, the fewer observations we have, the more possible it is that the true values could be further away that what we’ve observed.\nAs the DOF (symbol \\(\\nu\\)) tends to infinity, the distribution converges towards a Normal distribution. At about 30, it’s almost identical to a Normal.\nUse a t-test when:\n\nThe population standard deviation is unknown AND\nSample size is small (\\(n \\lt 30\\))\n\n\\[\nt = \\frac{x - \\mu}{s / \\sqrt{n}}\n\\]\n\\(\\nu\\) is basically always \\(N-1\\) (where N is number of samples)"
  },
  {
    "objectID": "posts/maths/basic_statistics/lesson.html#t-tests",
    "href": "posts/maths/basic_statistics/lesson.html#t-tests",
    "title": "Basic Statistics",
    "section": "4.2. t-tests",
    "text": "4.2. t-tests\nThe t-test hypothesis testing process is almost identical to the z-test. The only difference is we calculate a t-statistic (rather than a z-score). The rest is the same: we formulate our hypotheses, calculate the score and compare to our critical value."
  },
  {
    "objectID": "posts/maths/basic_statistics/lesson.html#tailed-and-2-tailed-tests",
    "href": "posts/maths/basic_statistics/lesson.html#tailed-and-2-tailed-tests",
    "title": "Basic Statistics",
    "section": "4.3. 1-tailed and 2-tailed tests",
    "text": "4.3. 1-tailed and 2-tailed tests\nFor 2-tailed tests, you split the confidence interval on either side, e.g. a 5% confidence level means the rejection region is the top 2.5% and bottom 2.5% of the distribution.\nSo a 2-tailed test is more strict, i.e. more difficult to reject the null hypothesis.\nYou can think of this as a 1-tailed test incorporates prior knowledge that the variable can only have increased (or decreased). But for a 2-tailed test, we have no prior knowledge so it could be either side."
  },
  {
    "objectID": "posts/maths/basic_statistics/lesson.html#misuse-of-p-values",
    "href": "posts/maths/basic_statistics/lesson.html#misuse-of-p-values",
    "title": "Basic Statistics",
    "section": "4.4. Misuse of p-values",
    "text": "4.4. Misuse of p-values\nThe p-value only tells us the probability of our binary accept/reject decision being incorrect.\nIt doesn’t tell us about the magnitude or uncertainty of the effect we are claiming.\nIt is best practice to report the effect sizes and confidence intervals as well as the p-values, to give a fuller picture.\nFor example, a shampoo increases hair volume at a 5% confidence level. How much does it increase hair volume?"
  },
  {
    "objectID": "posts/maths/basic_statistics/lesson.html#confidence-intervals",
    "href": "posts/maths/basic_statistics/lesson.html#confidence-intervals",
    "title": "Basic Statistics",
    "section": "4.5. Confidence Intervals",
    "text": "4.5. Confidence Intervals\nThis gives us the range within which our true population parameter lies.\nIt is the sample mean ± the margin of error. The margin of error is the standard error \\(\\frac{\\sigma}{\\sqrt{n}}\\) multiplied by the Z-score.\nThe Z-score corresponds to the confidence level chosen, e.g. 1.96 for a 95% confidence level."
  },
  {
    "objectID": "react-series.html",
    "href": "react-series.html",
    "title": "Series: React",
    "section": "",
    "text": "React: TypeScript Essentials\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 7: I don’t got no type, bad code is the only thing that I like\n\n\n\n\n\nMar 21, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: Testing\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 6: Testing my patience\n\n\n\n\n\nMar 20, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: Deployment\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 5: Deploying React Apps\n\n\n\n\n\nMar 18, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: Debugging\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 4: A Bug’s Life\n\n\n\n\n\nMar 17, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: Styling\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 3: Styling it Out\n\n\n\n\n\nMar 16, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: JavaScript Essentials\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 2: WTF is JSX\n\n\n\n\n\nMar 14, 2024\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: A Gentle Introduction\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 1: Getting Started with React\n\n\n\n\n\nMar 12, 2024\n\n\n12 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "This site contains my personal software and AI projects.\nFor projects that I’ve worked on, see my projects section. For notes on various topics that I’ve made, see my notes section.\nIf you’re interested in any of this stuff, my socials are at the bottom of this page - get in touch!"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "About",
    "section": "Bio",
    "text": "Bio\nI’m a data scientist with experience at big hedge funds and plucky start-ups. I spent a few years at Man Group, both as a discretionary long-short equity analyst in GLG and on the systematic trading side as a quant in AHL, where I focused on equities and futures.\nI currently work at a fintech startup called BMLL Technologies, where I lead projects doing interesting things with massive amounts of financial order book data."
  },
  {
    "objectID": "fastai-series.html",
    "href": "fastai-series.html",
    "title": "Series: FastAI course",
    "section": "",
    "text": "FastAI Lesson 8: Convolutions\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 8\n\n\n\n\n\nMar 5, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 7: Collaborative Filtering\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 7\n\n\n\n\n\nMar 1, 2024\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 6.5: Road to the Top\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 6.5\n\n\n\n\n\nFeb 13, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 6: Random Forests\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 6\n\n\n\n\n\nOct 9, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 5: Natural Language Processing\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 5\n\n\n\n\n\nSep 23, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 4: Natural Language Processing\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 4\n\n\n\n\n\nSep 7, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 3: How Does a Neural Network Learn?\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 3\n\n\n\n\n\nSep 1, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 2: Deployment\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 2\n\n\n\n\n\nAug 30, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 1: Image Classification Models\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 1\n\n\n\n\n\nAug 23, 2023\n\n\n3 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "gen-deep-learning-series.html",
    "href": "gen-deep-learning-series.html",
    "title": "Series: Generative AI",
    "section": "",
    "text": "Generative AI: GANs\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\nGAN\n\n\n\nPart 4: Generative Adversarial Networks\n\n\n\n\n\nApr 10, 2024\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI: VAEs\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\nVAE\n\n\n\nPart 3: Variational Autoencoders\n\n\n\n\n\nMar 6, 2024\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI: Deep Learning Foundations\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\n\nPart 2: The Building Blocks for Generative AI\n\n\n\n\n\nFeb 26, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI: Intro\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\n\nPart 1: Introduction to Generative AI\n\n\n\n\n\nFeb 19, 2024\n\n\n5 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#sqs",
    "href": "posts/software/aws/aws_saa_notes.html#sqs",
    "title": "AWS Solutions Architect",
    "section": "14.1. SQS",
    "text": "14.1. SQS\n\n14.1.1. Overview\nA producer sends messages to a queue. A consumer polls the queue for messages. The queue essentially acts as a buffer between producer and consumer.\n\nStandard queue. Unlimited throughput and number of messages in queue. Messages are short-lived; they can stay in the queue for 14 days maximum, and this is set to 4 days by default. The latency is low, &lt;10ms on publish and receive. Messages can be 256 KB maximum. It uses “at least once delivery” so it is possible to have multiple messages in the queue, and messages may be out of order (best effort ordering), so the application should be able to handle this.\nProducing messages. The application code sends a message to the queue using the SendMessage API in the AWS SDK. The message is persisted in SQS until a consumer deletes it, or the retention period is reached.\nConsuming messages. The application code may be running on premises or in AWS. The consumer polls SQS for messages (up to 10 at a time). Once the consumer processes the message, it deletes the message from the queue using the DeleteMessage API.\nEncryption. Inflight encryption using HTTPS API. At rest encryption using KMS keys. You can use client-side encryption if the client wants to handle encryption/decryption itself.\nIAM policies regulate access to the SQS API (SendMessage and DeleteMessage). SQS Access Policies can be used for cross-account access to SQS queues or allowing access from other services like SNS or S3; analogous to S3 bucket policies.\n\n\n\nMessage Visibility Timeout\nAfter a message is polled by a consumer, it becomes invisible to other consumers. This ensures that multiple consumers do not try to process the same message.\nBy default, the visibility timeout is 30 seconds, so consumers have 30 seconds to process the message before it “rejoins” the queue.\nIf a consumer is processing a message but knows that it needs more time, it can call the ChangeMessageVisibility API to get more time. This is helpful to ensure a message isn’t processed twice.\nThe value of the timeout should be high enough to avoid duplicate processing from multiple consumers, but low enough that if a consumer crashes then the message is made available on the queue again in reasonable time.\n\n\nLong Polling\nWhen a consumer requests messages from the queue, it can optionally wait for messages to arrive if there are none in the queue. This is “long polling”.\nThis reduces the number of API calls made to SQS and improves the latency of the application.\nThe long polling wait time can be 1-20 seconds.\nLong polling can be enabled either at the queue level or the API level using WaitTimeSeconds.\n\n\nFIFO Queues\nFirst In First Out ordering of messages. FIFO queues guarantee the order of messages at the expense of limiting throughput.\nMessages are processed in order by the consumer. Ordering is done by Message Group ID which is a mandatory parameter.\nFIFO queues also support “exactly-once” send capability. You add a unique Deduplication ID to each message.\nThe queue name when you create it must end in .fifo\n\n\nSQS with ASG\nTo increase throughput, we can scale the number of consumers horizontally. A common pattern is to have EC2 instances as the consumers which are inside an Auto Scaling Group. There is a CloudWatch metric monitoring the queue length, and the ASG scales the number of instances based on that CloudWatch alarm.\nAnother common pattern is to use SQS as a buffer between EC2 instances and the database to ensure no data is dropped. If the EC2 instances are writing directly to the database, they may hit the write limit and lose data. SQS is added as an intermediate step. EC2 publishes to the SQS queue which is infinitely scalable to ensure no data is dropped. Then a different EC2 instance in a different ASG acts as the consumer to pick up messages and write them to the database in a durable way. This pattern only works if the client does not need write confirmation."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#sns",
    "href": "posts/software/aws/aws_saa_notes.html#sns",
    "title": "AWS Solutions Architect",
    "section": "14.2. SNS",
    "text": "14.2. SNS\n.\nSNS allows you to send one message to many receivers using the pub/sub pattern. A publisher publishes a message on an SNS topic and various subscribers can read the message and act accordingly.\nAn “event producer” sends a message to one SNS topic. Many “event receivers” can listen for topic notifications. By default, subscribers see all messages but you can filter this.\nYou can have up to 12.5 million subscriptions per topic. An account can have up to 100k topics.\nSubscribers can be: SQS, Lambda, Kinesis Data Firehose, emails, SMS and push notifications, and HTTP(S) endpoints.\nSNS can receive data from many AWS services.\nTo publish from SNS, there are two options:\n\nTopic publish using the SDK\nDirect publish using the mobile apps SDK\n\nSecurity for SNS is similar to SQS:\n\nin flight encryption using HTTPS\nAt rest encryption using KMS keys\nClient side encryption if the client wants to handle encryption/decryption themselves\n\nIAM policies regulate access to the SNS. SNS Access Policies can be used for cross-account access to SNS topics or allowing access from other services like S3; analogous to S3 bucket policies and SQS Access Policies.\n\nFan Out Pattern\nThis is a common SQS + SNS pattern. We may want to publish a message to multiple SQS queues. We can decouple this using the fan out pattern, so the application code doesn’t need to be changed for every added/removed queue.\nWe push once to SNS and let all of the SQS queues subscribe to that SNS topic.\nMake sure the SQS queue access policy allows for SNS to write. There is cross-region delivery, meaning an SNS topic can be read by multiple SQS queues in different regions.\nAnother application is S3 + SNS + SQS. S3 has a limitation that you can only have one S3 Event rule per event type, prefix combination. If you want to send the same S3 event to multiple queues, publish it to SNS and let that fan out to the different SQS queues.\nSNS can write to S3 (or another destination supported by KDF) by going via Kinesis Data Firehose.\nLike SQS, we can have an SNS FIFO topic to ensure ordering. Again, we order by message group ID and pass a Deduplication ID. This is helpful if fanning out to SQS FIFO queues.\nMessage filtering. This is an optional JSON policy applied to a subscriber to only filter on some messages. Useful if we want one queue to handle orders, one for cancellations etc."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#kinesis-data-streams",
    "href": "posts/software/aws/aws_saa_notes.html#kinesis-data-streams",
    "title": "AWS Solutions Architect",
    "section": "14.3. Kinesis Data Streams",
    "text": "14.3. Kinesis Data Streams\nKDS used to Collect and store real-time streaming data.\nA producer is application code that you write, or a Kinesis Agent if connecting to an AWS service, which writes to a Kinesis Data Stream.\nA consumer is an application that can read from the data stream. Example consumers may be: your application, Lambda functions, Amazon Data Firehose, managed service for Apache Flink.\nData is retained on the data stream for up to 365 days which allows consumers to “replay” data. Data cannot be deleted from Kinesis, you have to wait for it to expire.\nData cannot be up to 1 MB; a typical use case is lots of “small” realtime data. Ordering is guaranteed for data with the same partition ID.\nEncryption - KMS at-rest and HTTPS in-flight.\nWe can write optimised producers and consumers using Kinesis Producer Library and Kinesis Client Library respectively.\nThere are two capacity modes:\n\nProvisioned mode. You define the number of shards. Each shard allows for 1 MB/s in and 2 MB/s out. You can manually increase or decrease the number of shards and you pay per shard per hour.\nOn-demand mode. No manual intervention required. You start with the default capacity (4 shards) which scales automatically based on observed throughput peak during the last 30 days. You are billed per stream per hour and for each GB in and out."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#amazon-data-firehose",
    "href": "posts/software/aws/aws_saa_notes.html#amazon-data-firehose",
    "title": "AWS Solutions Architect",
    "section": "14.4. Amazon Data Firehose",
    "text": "14.4. Amazon Data Firehose\n\n14.4.1. Overview\nProducers send records (up to 1 MB of data) to Firehose. They can optionally be transformed by a lambda function. Data is accumulated in a buffer and written as batches to a destination; therefore it is “near real time” since there is a lag between flushes of the buffer. You can optionally write all data or just failed data to an S3 backup bucket.\nThe buffer can be set to flush based on a storage limit (GB accumulated) or a time limit; it will be flushed when the first of these limits is hit.\nIt is a fully managed service, serverless with auto scaling. Supported file types: CSV, JSON, Parquet, Avro, raw text, binary data. It used to be called “Kinesis Data Firehose” but was renamed because it is more generally applicable beyond just Kinesis.\nProducers can be applications, clients, SDK, Kinesis Agents, Kinesis Data Streams, CloudWatch logs and events, AWS IoT.\nDestinations can be AWS destinations: S3, Redshift, OpenSearch. Or third party destinations: Datadog, Splunk, New Relic, MongoDB. Or you can write to custom destinations via an HTTP endpoint."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#amazon-mq",
    "href": "posts/software/aws/aws_saa_notes.html#amazon-mq",
    "title": "AWS Solutions Architect",
    "section": "14.6. Amazon MQ",
    "text": "14.6. Amazon MQ\nAmazon MQ is a managed message broker service for RabbitMQ and ActiveMQ. It has both queue feature and topic feature, so can be made to be roughly equivalent to SQS and SNS respectively.\nIf you are already using RabbitMQ or ActiveMQ on premises, it may be easier to migrate to Amazon’s managed service.\nSQS and SNS are “cloud-native” services, proprietary from AWS. Amazon MQ doesn’t scale as well as the cloud-native services.\nIt is essentially a halfway house for cases where you can’t / don’t want to migrate your whole application to use SQS/SNS but want some cloud features.\nFor high availability, you can have MQ Brokers in two different AZs, one as active and one as failover. Both write to the same Amazon EFS storage volume so that no data is lost in the event of a failover."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#simple-queuing-service-sqs",
    "href": "posts/software/aws/aws_saa_notes.html#simple-queuing-service-sqs",
    "title": "AWS Solutions Architect",
    "section": "14.1. Simple Queuing Service (SQS)",
    "text": "14.1. Simple Queuing Service (SQS)\n\n14.1.1. Overview\nA producer sends messages to a queue. A consumer polls the queue for messages. The queue essentially acts as a buffer between producer and consumer.\n\nStandard queue. Unlimited throughput and number of messages in queue. Messages are short-lived; they can stay in the queue for 14 days maximum, and this is set to 4 days by default. The latency is low, &lt;10ms on publish and receive. Messages can be 256 KB maximum. It uses “at least once delivery” so it is possible to have multiple messages in the queue, and messages may be out of order (best effort ordering), so the application should be able to handle this.\nProducing messages. The application code sends a message to the queue using the SendMessage API in the AWS SDK. The message is persisted in SQS until a consumer deletes it, or the retention period is reached.\nConsuming messages. The application code may be running on premises or in AWS. The consumer polls SQS for messages (up to 10 at a time). Once the consumer processes the message, it deletes the message from the queue using the DeleteMessage API.\nEncryption. Inflight encryption using HTTPS API. At rest encryption using KMS keys. You can use client-side encryption if the client wants to handle encryption/decryption itself.\nIAM policies regulate access to the SQS API (SendMessage and DeleteMessage). SQS Access Policies can be used for cross-account access to SQS queues or allowing access from other services like SNS or S3; analogous to S3 bucket policies.\n\n\n\n14.1.2. Message Visibility Timeout\nAfter a message is polled by a consumer, it becomes invisible to other consumers. This ensures that multiple consumers do not try to process the same message.\nBy default, the visibility timeout is 30 seconds, so consumers have 30 seconds to process the message before it “rejoins” the queue.\nIf a consumer is processing a message but knows that it needs more time, it can call the ChangeMessageVisibility API to get more time. This is helpful to ensure a message isn’t processed twice.\nThe value of the timeout should be high enough to avoid duplicate processing from multiple consumers, but low enough that if a consumer crashes then the message is made available on the queue again in reasonable time.\n\n\n14.1.3. Long Polling\nWhen a consumer requests messages from the queue, it can optionally wait for messages to arrive if there are none in the queue. This is “long polling”.\nThis reduces the number of API calls made to SQS and improves the latency of the application.\nThe long polling wait time can be 1-20 seconds.\nLong polling can be enabled either at the queue level or the API level using WaitTimeSeconds.\n\n\n14.1.4. FIFO Queues\nFirst In First Out ordering of messages. FIFO queues guarantee the order of messages at the expense of limiting throughput.\nMessages are processed in order by the consumer. Ordering is done by Message Group ID which is a mandatory parameter.\nFIFO queues also support “exactly-once” send capability. You add a unique Deduplication ID to each message.\nThe queue name when you create it must end in .fifo\n\n\n14.1.5. SQS with ASG\nTo increase throughput, we can scale the number of consumers horizontally.\nA common pattern is to have EC2 instances as the consumers which are inside an Auto Scaling Group. There is a CloudWatch metric monitoring the queue length, and the ASG scales the number of instances based on that CloudWatch alarm.\nAnother common pattern is to use SQS as a buffer between EC2 instances and the database to ensure no data is dropped. If the EC2 instances are writing directly to the database, they may hit the write limit and lose data. SQS is added as an intermediate step. EC2 publishes to the SQS queue which is infinitely scalable to ensure no data is dropped. Then a different EC2 instance in a different ASG acts as the consumer to pick up messages and write them to the database in a durable way. This pattern only works if the client does not need write confirmation."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#simple-notification-service-sns",
    "href": "posts/software/aws/aws_saa_notes.html#simple-notification-service-sns",
    "title": "AWS Solutions Architect",
    "section": "14.2. Simple Notification Service (SNS)",
    "text": "14.2. Simple Notification Service (SNS)\n\n14.2.1. Overview\nSNS allows you to send one message to many receivers using the pub/sub pattern. A publisher publishes a message on an SNS topic and various subscribers can read the message and act accordingly.\nAn “event producer” sends a message to one SNS topic. Many “event receivers” can listen for topic notifications. By default, subscribers see all messages but you can filter this.\nYou can have up to 12.5 million subscriptions per topic. An account can have up to 100k topics.\nSubscribers can be: SQS, Lambda, Kinesis Data Firehose, emails, SMS and push notifications, and HTTP(S) endpoints.\nSNS can receive data from many AWS services.\nTo publish from SNS, there are two options:\n\nTopic publish using the SDK\nDirect publish using the mobile apps SDK\n\nSecurity for SNS is similar to SQS:\n\nin flight encryption using HTTPS\nAt rest encryption using KMS keys\nClient side encryption if the client wants to handle encryption/decryption themselves\n\nIAM policies regulate access to the SNS. SNS Access Policies can be used for cross-account access to SNS topics or allowing access from other services like S3; analogous to S3 bucket policies and SQS Access Policies.\n\n\n14.2.2. Fan Out Pattern\nThis is a common SQS + SNS pattern. We may want to publish a message to multiple SQS queues. We can decouple this using the fan out pattern, so the application code doesn’t need to be changed for every added/removed queue.\nWe push once to SNS and let all of the SQS queues subscribe to that SNS topic.\nMake sure the SQS queue access policy allows for SNS to write. There is cross-region delivery, meaning an SNS topic can be read by multiple SQS queues in different regions.\nAnother application is S3 + SNS + SQS. S3 has a limitation that you can only have one S3 Event rule per event type, prefix combination. If you want to send the same S3 event to multiple queues, publish it to SNS and let that fan out to the different SQS queues.\nSNS can write to S3 (or another destination supported by KDF) by going via Kinesis Data Firehose.\nLike SQS, we can have an SNS FIFO topic to ensure ordering. Again, we order by message group ID and pass a Deduplication ID. This is helpful if fanning out to SQS FIFO queues.\n\n\n14.2.3. Message Filtering\nThis is an optional JSON policy applied to a subscriber to only filter on some messages. Useful if we want one queue to handle orders, one for cancellations etc."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#comparison-of-messaging-services",
    "href": "posts/software/aws/aws_saa_notes.html#comparison-of-messaging-services",
    "title": "AWS Solutions Architect",
    "section": "14.5. Comparison of Messaging Services",
    "text": "14.5. Comparison of Messaging Services\n\n14.4.2. Kinesis Data Streams vs Amazon Data Firehose\nFirehose does not store any data or allow for replay. There is a lag so it is not fully real time, unlike Kinesis data streams.\nFirehose automatically scales, whereas Kinesis data streams allow a self-managed (provisioned mode) or fully managed (on demand mode) option.\nSQS vs SNS vs Kinesis.\nSQS - consumers pull data and delete it from the queue. You can have as many consumers as you want. No need to provision throughput.\nSNS - producers push data. Data is not persisted, so data can be lost if not delivered. No need to provision throughput.\nKinesis - the standard approach is to pull data, but this can be adapted to push data using the fan-out pattern. Data is persisted so can be replayed. There are two modes to self-manage or auto-scale."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#comparison-of-messaging-and-integrationservices",
    "href": "posts/software/aws/aws_saa_notes.html#comparison-of-messaging-and-integrationservices",
    "title": "AWS Solutions Architect",
    "section": "14.5. Comparison of Messaging and IntegrationServices",
    "text": "14.5. Comparison of Messaging and IntegrationServices\n\n14.4.2. Kinesis Data Streams vs Amazon Data Firehose\nFirehose does not store any data or allow for replay. There is a lag so it is not fully real time, unlike Kinesis data streams.\nFirehose automatically scales, whereas Kinesis data streams allow a self-managed (provisioned mode) or fully managed (on demand mode) option.\nSQS vs SNS vs Kinesis.\nSQS - consumers pull data and delete it from the queue. You can have as many consumers as you want. No need to provision throughput.\nSNS - producers push data. Data is not persisted, so data can be lost if not delivered. No need to provision throughput.\nKinesis - the standard approach is to pull data, but this can be adapted to push data using the fan-out pattern. Data is persisted so can be replayed. There are two modes to self-manage or auto-scale."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#comparison-of-messaging-and-integration-services",
    "href": "posts/software/aws/aws_saa_notes.html#comparison-of-messaging-and-integration-services",
    "title": "AWS Solutions Architect",
    "section": "14.5. Comparison of Messaging and Integration Services",
    "text": "14.5. Comparison of Messaging and Integration Services\n\n14.5.1. Kinesis Data Streams vs Amazon Data Firehose\nFirehose does not store any data or allow for replay. There is a lag so it is not fully real time, unlike Kinesis data streams.\nFirehose automatically scales, whereas Kinesis data streams allow a self-managed (provisioned mode) or fully managed (on demand mode) option.\n\n\n14.5.2. SQS vs SNS vs Kinesis\n\nSQS - consumers pull data and delete it from the queue. You can have as many consumers as you want. No need to provision throughput.\n\n0 SNS - producers push data. Data is not persisted, so data can be lost if not delivered. No need to provision throughput.\n\nKinesis - the standard approach is to pull data, but this can be adapted to push data using the fan-out pattern. Data is persisted so can be replayed. There are two modes to self-manage or auto-scale."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#docker",
    "href": "posts/software/aws/aws_saa_notes.html#docker",
    "title": "AWS Solutions Architect",
    "section": "15.1. Docker",
    "text": "15.1. Docker\nUse Docker to containerise applications. Common use cases are for microservices or to “lift and shift” and app from on-premises to cloud.\nDocker images are stored in a container repository. Docker Hub is a common public repository, AWS ECR is private (although there is “public gallery” if you want to make images in ECR public).\nDocker vs virtual machines: \nAWS container services: ECR, ECS, EKS, Fargate."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#ecs",
    "href": "posts/software/aws/aws_saa_notes.html#ecs",
    "title": "AWS Solutions Architect",
    "section": "15.2. ECS",
    "text": "15.2. ECS\nElastic Container Service. This is Amazon’s managed container service. You launch an ECS Task on an ECS Cluster.\nThere are two launch types: EC2 and Fargate.\n\n15.2.1. EC2 Launch Type\nAn ECS cluster is essentially an cluster of EC2 instances each running an “ECS Agent”, which is essentially Docker and logic to register them as elements of the ECS cluster so AWS knows to start/stop/update containers within them.\nWith an EC2 launch type, you need to manage the infrastructure yourself, ie define the instance size, number, etc.\n\n\n15.2.2. Fargate Launch Type\nServerless service. You don’t provision the infrastructure so no need to manage EC2 instances.\nYou just create task definitions and AWS runs ECS Tasks for you based on the CPU/RAM needed. To scale, just increase the number of tasks.\nThere are two categories of IAM roles needed:\n\nEC2 Instance Profile (only for EC2 launch type). Used by the ECS Agent to make API called to the ECS service, pull images from ECR, send container logs to CloudWatch, get credentials from Secrets Manager of SSM Parameter Store. The IAM profile needs access to all of these services.\nECS Task Roles. Allows each task to have a specific role. Eg task A might only need access to S3, task B might only need access to RDS.\n\nWe can run an ALB in front of the ECS Cluster.\nFor data persistence, we need a volume. EFS file systems can be mounted onto ECS tasks, and this works with both EC2 and Fargate launch types. EFS is serverless. EFS is a network drive, so tasks running in any AZ will share the same data. S3 cannot be mounted as a file system for ECS tasks.\n\n\n15.2.3. ECS Service Auto Scaling\nAutomatically increase/decrease the number of ECS tasks. ECS Auto Scaling uses AWS Application Auto Scaling to scale based on: CPU utilisation, RAM utilisation or ALB request count per target.\nThere are three types of scaling:\n\nTarget tracking - based on a target value for a specific CloudWatch metric\nStep scaling - based on a CloudWatch alarm\nScheduled scaling - based on a date/time\n\nEC2 Service Auto Scaling is scaling the service (at the task level). It is not the same as EC2 Auto Scaling which is at the instance level. Fargate auto scaling is easier to set up because it is serverless.\nECS Cluster Capacity Provider is the preferred approach to scaling. It is a capacity provider paired with an auto scaling group.\nYou can use Auto Scaling Group Scaling, but this is the older discouraged method."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#ecs-service-auto-scaling",
    "href": "posts/software/aws/aws_saa_notes.html#ecs-service-auto-scaling",
    "title": "AWS Solutions Architect",
    "section": "ECS Service Auto Scaling",
    "text": "ECS Service Auto Scaling\nAutomatically increase/decrease the number of ECS tasks. ECS Auto Scaling uses AWS Application Auto Scaling to scale based on: CPU utilisation, RAM utilisation or ALB request count per target.\nThere are three types of scaling:\n\ntarget tracking - based on a target value for a specific CloudWatch metric\nStep scaling - based on a CloudWatch alarm\nScheduled scaling - based on a date/time\n\nEC2 Service Auto Scaling is scaling the service (at the task level). It is not the same as EC2 Auto Scaling which is at the instance level. Fargate auto scaling is easier to set up because it is serverless.\nEcs Cluster Capacity Provider is the preferred approach to scaling. It is a capacity provider paired with an auto scaling group.\nYou can use Auto Scaling Group Scaling, but this is the older discouraged method."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#common-ecs-architectures",
    "href": "posts/software/aws/aws_saa_notes.html#common-ecs-architectures",
    "title": "AWS Solutions Architect",
    "section": "15.2.4. Common ECS Architectures",
    "text": "15.2.4. Common ECS Architectures\nAmazon EventBridge can have a rule set up to run an ECS task in response to a trigger. For example, when a user uploads a file to a specific S3 bucket, EventBridge will start an ECS task inside a Fargate container to process the data and write it to RDS.\nA similar approach is to use EventBridge to do the same but on a schedule, e.g. every hour do some batch processing.\nAnother scenario is processing messages in an SQS queue. ECS tasks poll the queue and auto scale depending on the number of items in the queue.\nAnother scenario is having EventBridge monitor the ECS task and trigger an event if the task fails or is stopped. It sends the event to SNS which emails the Ops team."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#ecr",
    "href": "posts/software/aws/aws_saa_notes.html#ecr",
    "title": "AWS Solutions Architect",
    "section": "15.3. ECR",
    "text": "15.3. ECR\nElastic Container Registry. Store and manage docker images on AWS. It is integrated with ECS and backed by S3 under the hood. Access is controlled by IAM.\nImages can be public or private. Public images are stored in Amazon ECR Public Gallery."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#eks",
    "href": "posts/software/aws/aws_saa_notes.html#eks",
    "title": "AWS Solutions Architect",
    "section": "15.4. EKS",
    "text": "15.4. EKS\nElastic Kubernetes Service. Managed Kubernetes clusters on AWS. Kubernetes is cloud-agnostic, so can be helpful when migrating between cloud providers.\nLike ECS, it supports EC2 and Fargate launch types.\nNode types:\n\nManaged node groups. AWS creates and manages nodes (EC2 instances) for you. These can be spot or on demand instances.\nSelf-managed nodes. You create the nodes and register them to an EKS cluster.\nAWS Fargate. Serverless, no maintenance required.\n\nYou can attach data volumes by specifying a StorageClass manifest on your EKS cluster and using a Container Storage Interface (CSI) driver. EKS supports EBS, EFS and FSx; Fargate can only use EFS."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#aws-app-runner",
    "href": "posts/software/aws/aws_saa_notes.html#aws-app-runner",
    "title": "AWS Solutions Architect",
    "section": "15.5. AWS App Runner",
    "text": "15.5. AWS App Runner\nManaged service to make it easy to deploy web apps and APIs. You pass it your source code or container images and configure some settings like number of vCPUs, RAM, auto scaling, health checks etc. AWS creates all of the services under the hood, you don’t need any infrastructure experience. It is the easiest option to get something running without much cloud knowledge."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#aws-app2container-a2c",
    "href": "posts/software/aws/aws_saa_notes.html#aws-app2container-a2c",
    "title": "AWS Solutions Architect",
    "section": "15.6. AWS App2Container (A2C)",
    "text": "15.6. AWS App2Container (A2C)\nA2C is a CLI tool for migrating Java and .NET containers into Docker containers. Lift and shift from on-premises to cloud.\nIt generates CloudFormation templates and registers the Docker containers to ECR. You can then deploy to ECS, EKS or AppRunner.\n\n\n\nAWS App2Container Example"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#aws-lambda",
    "href": "posts/software/aws/aws_saa_notes.html#aws-lambda",
    "title": "AWS Solutions Architect",
    "section": "16.1. AWS Lambda",
    "text": "16.1. AWS Lambda\n\n16.1.1. Overview\nLambdas are virtual functions; there are no servers to manage. Execution time must be &lt;15 mins. They run on-demand; you are only billed when your function is running. Scaling is automated.\nLambdas are the serverless counterparts to EC2 instances. With EC2 instances you need to intervene to scale up and down.\nThere is integration with CloudWatch for monitoring. You can have up to 10GB of RAM per function; increasing the RAM also improves the CPU and networking.\nLambda pricing is pay per request and compute time.\nSupported languages: JavaScript, Python, Java, C#, Ruby. Other languages are supported via a custom runtime API.\nYou can have Lambda Container Images which must implement the Lambda Runtime API. Generally, ECS or Fargate are preferred for running arbitrary Docker images.\nA common pattern is to use Lambda to create a “serverless CRON job”. CloudWatch Events triggers an event on a schedule, say every hour. This triggers a Lambda function to run a certain task.\n\n\n16.1.2. Lambda Limits\nThese limits are per region.\nExecution:\n\nMemory allocation - 128MB-10 GB in 1MB increments\nMax execution time is 15 minutes\nEnvironment variables can take up to 4 KB\nDisk capacity in /tmp - 512 MB to 10GB\nConcurrency executions: 1000\n\nDeployment:\n\nDeployment size (compressed zip) 50 MB, uncompressed 250 MB\nCan use /tmp directory to load other files at startup\nEnvironment variables 4KB\n\n\n\n16.1.3. Lambda Concurrency and Throttling\nConcurrency limit: up to 1000 concurrent executions across all Lambda functions in our account.\nWe can set a “reserved concurrency” at the function-level to limit calls to individual Lambda functions. It is good practice to do this so that one application in your account scaling does not cause it to use all of the available Lambda functions in your account and cause unrelated Lambda functions for other applications to be throttled.\nEach invocation over the concurrency limit triggers a “throttle”. For synchronous invocations, this returns a 429 ThrottleError. For asynchronous invocations, it will retry automatically for up to 6 hours with exponential backoff and then go to a dead letter queue.\n\n\n16.1.4. Cold Starts\nWhen a new instance is starting, it needs to initialise by running all of the code and dependencies. This can take a long time, causing the first request to a new instance to have higher latency than the rest.\nProvisioned concurrency is allocated before the function is invoked to avoid cold starts.\n\n\n16.1.5. Lambda SnapStart\nWhen a regular Lambda function is invoked the lifecycle it goes through is: initialise, invoke, shutdown.\nWhen SnapStart is enabled, the function is pre-initialised so it can skip straight to the invoke stage.\nWhen you publish a new version: lambda initialises your function, takes a snapshot of memory and disk state, then that snapshot is cached for low-latency access.\n\n\n16.1.6. Customisation at the Edge\nSome applications may require some form of logic at the edge location, e.g. to customise the CDN content.\nAn edge function is code that you write and attach to CloudFront distributions. It runs close to the user to minimise latency.\nCloudFront provides two types:\n\nCloudFront Functions\nLambda@Edge\n\nBoth are serverless and deployed globally. You only pay for what you use.\n\nCloudFront Functions\nThese are JavaScript functions that modify the response sent from CloudFront to the user. They can change the viewer request (what the user sends to CloudFront before it reaches the origin server) or the viewer response (what the origin server sends back before it reaches the user).\n\n\n\nCloudFront Function\n\n\n\n\nLambda@Edge\nThis can modify viewer request/ response or origin request/response. You write the function in one region (us-east-1) then CloudFront replicates it across all regions. The function can be written in node.js or Python.\nIt is more expensive with a higher max execution time, so you can run more logic.\n\n\n\nLambda@Edge Function\n\n\n\n\nComparison\n\n\n\nComparison of CloudFront Functions vs Lambda@Edge\n\n\nUse cases of CloudFront Functions:\n\nCache key normalization\nHeader manipulation\nURL rewrites or redirects\nRequest authentication & authorization\n\nUse cases of Lambda@Edge Functions:\n\nLonger execution time (several ms)\nAdjustable CPU or memory\nYour code depends on a 3rd-party libraries (e.g., AWS SDK to access other AWS services)\nNetwork access to use external services for processing\nFile system access or access to the body of HTTP requests\n\n\n\n\n16.1.7. VPC\nBy default, Lambda launches in its own AWS-owned VPC, so can only access public resources and not the resources in your VPC.\nWe can launch Lambda in a VPC if we specify the VPC ID, subnets and security groups. Lambda will create an ENI in your subnets.\n\nLambda with RDS Proxy\nA common use case for launching lambda in your VPC is to connect to your RDS database. But we don’t want to connect Lambdas directly to RDS, as this can result in lots of open connections and high load.\nInstead the Lambda functions connect to an RDS proxy which pools and shares database connections.\n\n\n\nLambda with RDS Proxy\n\n\nThis requires running Lambda in a VPC because the RDS Proxy is never publicly accessible.\n\n\nInvoking a Lambda Function from RDS\nYou can invoke a Lambda function from within your database instance to process data events from within the database.\nYou need to allow outbound traffic to the Lambda function from within the DB instance. This is done in the database, not the AWS console.\nThis should not be confused with RDS Event Notifications. These tell you information about the database instance, not the data itself. So you can see when the database was created, stopped, started etc. But you cannot see anything about the data itself is actually processing."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#dynamodb",
    "href": "posts/software/aws/aws_saa_notes.html#dynamodb",
    "title": "AWS Solutions Architect",
    "section": "16.2. DynamoDB",
    "text": "16.2. DynamoDB\n\n16.2.1. Overview\nFully managed NoSQL database with replication across multiple AZs. It is a good choice if the schema needs to change frequently.\nSecurity is all handled via IAM.\nDynamoDB auto-scales and has fast (single digit milliseconds) performance.\nThere are two classes: standard and infrequent access.\nDynamoDB is made of tables. Each table must have a primary key, and can have an infinite number of items (rows). Each item has attributes (columns). These can be added to over time without having to alter a table schema.\nMaximum item size is 400KB. Supported data types are ScalarTypes, DocumentTypes (list, map), SetTypes.\nCapacity modes:\n\nProvisioned mode. You specify the number of reads/writes per second required and pay per Read Capacity Unit (RCU) and Write Capacity Unit (WCU). You can optionally add autoscaling.\nOn-demand mode. Reads/writes scale automatically, no capacity planning required. You pay for what you use but it is more expensive. Good for unpredictable, spiky workloads.\n\n\n\n16.2.2. DynamoDB Accelerator (DAX)\nFully managed in-memory cache for DynamoDB. Microsecond latency for cached content. No change to application logic required. Default TTL of 5 minutes.\nDAX is for caching individual objects or table scan results. Amazon Elasticache is for caching aggregation results.\n\n\n16.2.3. Stream Processing\nOrdered stream of item-level updates (create/update/delete). This is useful for realtime analytics or reacting to changes like sending welcome emails to new users.\nDynamoDB Streams have 24 hour retention with limited number of consumers. Kinesis Data Streams have 1 year retentions, more consumers and more integration with other AWS services.\n\n\n16.2.4. Global Tables\nA table that is replicated in multiple regions. It is a two-way replication; changes made in either table are reflected in the other. It is an active-active replication, means applications can read and write to tables in any region.\nDynamoDB Streams must be enabled as this is used under the hood.\n\n\n16.2.5. TTL\nAutomatically delete items in the table after a certain expiry timestamp.\nUse cases are enforcing a retention policy and reducing storage after that, or web session handling.\n\n\n16.2.6. Backups for Disaster Recovery\nContinuous backups for point-in-time recovery. This can be optionally enabled for the last 35 days. The recovery process creates a new table.\nOn demand backups are retained until explicitly deleted.\n\n\n16.2.7. Integration with S3\nYou can export to S3 and import from it.\nPoint-in-time recovery must be enabled for export to S3."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#api-gateway",
    "href": "posts/software/aws/aws_saa_notes.html#api-gateway",
    "title": "AWS Solutions Architect",
    "section": "16.3. API Gateway",
    "text": "16.3. API Gateway\n\n16.3.1. Overview\nWe could connect our client directly to our Lambda function / EC2 instances, but it would need appropriate IAM permissions. Alternatively, we can have an ALB.\nAPI Gateway is another alternative that acts as a proxy like ALB, but we also get some convenient features for managing our API. It is a serverless service.\nHandles API versioning and multiple environments (dev, staging, prod), authentication, creates API keys. Support for websocket protocol, swagger and open API interfaces, caching. We can transform and validate requests and responses.\nAPI gateway can integrate with:\n\nInvoke Lambda functions. This is a common way to expose a REST API.\nHTTP. Expose any HTTP endpoints in the backend.\nAWS service. Expose any AWS API through the API Gateway.\n\nEndpoint types:\n\nEdge-optimised. For global clients. This is the default. Requests are routed through CloudFront, although API Gateway still lives in one region.\nRegional. For clients in the same region.\nPrivate. Can only be accessed from your VPC using an interface VPC endpoint (ENI). Use a resource policy to define access.\n\n\n\n16.3.2. Security\nUser auth can be via:\n\nIAM roles. For internal applications.\nCognito. For exposing to external users.\nCustom auth that you define.\n\nYou can use ACM (AWS Certificate Manager) to define a custom HTTPS domain name."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#step-functions",
    "href": "posts/software/aws/aws_saa_notes.html#step-functions",
    "title": "AWS Solutions Architect",
    "section": "16.4. Step Functions",
    "text": "16.4. Step Functions\nBuild a serverless visual workflow to orchestrate Lambda functions using step functions.\nCan integrate with AWS services by defining a flowchart. You can optionally include a human approval step."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#cognito",
    "href": "posts/software/aws/aws_saa_notes.html#cognito",
    "title": "AWS Solutions Architect",
    "section": "16.5. Cognito",
    "text": "16.5. Cognito\nCognito is a service to give users an identity to interact with our web or mobile application. Typical use cases are when dealing with external users (where we don’t want to set up IAM permissions), where there are hundreds of users or more, or mobile users.\nThere are two types:\n\nCognito User Pools. Sign in functionality for app users that integrates with API Gateway and ALB. Serverless database of users.\nCognito Identity Pools. Also called Federated Identities. Provide AWS credentials so they can access AWS resources directly."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#section",
    "href": "posts/software/aws/aws_saa_notes.html#section",
    "title": "AWS Solutions Architect",
    "section": "17.1.",
    "text": "17.1."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#rest-api",
    "href": "posts/software/aws/aws_saa_notes.html#rest-api",
    "title": "AWS Solutions Architect",
    "section": "17.1. REST API",
    "text": "17.1. REST API\n\n\n\nREST API Serverless Example"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#rest-api-1",
    "href": "posts/software/aws/aws_saa_notes.html#rest-api-1",
    "title": "AWS Solutions Architect",
    "section": "17.1. REST API",
    "text": "17.1. REST API\n\n\n\nREST API Serverless Example"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#giving-access-to-an-s3-bucket",
    "href": "posts/software/aws/aws_saa_notes.html#giving-access-to-an-s3-bucket",
    "title": "AWS Solutions Architect",
    "section": "17.1. Giving Access to an S3 Bucket",
    "text": "17.1. Giving Access to an S3 Bucket\n\n\n\nREST API Serverless Example"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#givingmobile-users-access-to-an-s3-bucket",
    "href": "posts/software/aws/aws_saa_notes.html#givingmobile-users-access-to-an-s3-bucket",
    "title": "AWS Solutions Architect",
    "section": "17.1. GivingMobile Users Access to an S3 Bucket",
    "text": "17.1. GivingMobile Users Access to an S3 Bucket\n\n\n\nREST API Serverless Example"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#giving-mobile-users-access-to-an-s3-bucket",
    "href": "posts/software/aws/aws_saa_notes.html#giving-mobile-users-access-to-an-s3-bucket",
    "title": "AWS Solutions Architect",
    "section": "17.2. Giving Mobile Users Access to an S3 Bucket",
    "text": "17.2. Giving Mobile Users Access to an S3 Bucket\n\n\n\nS3 Serverless Example"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#giving-mobile-users-access-to-an-s3-bucket-1",
    "href": "posts/software/aws/aws_saa_notes.html#giving-mobile-users-access-to-an-s3-bucket-1",
    "title": "AWS Solutions Architect",
    "section": "17.3. Giving Mobile Users Access to an S3 Bucket",
    "text": "17.3. Giving Mobile Users Access to an S3 Bucket\n\n\n\nHigh Throughput Serverless Example"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#high-throughput-example",
    "href": "posts/software/aws/aws_saa_notes.html#high-throughput-example",
    "title": "AWS Solutions Architect",
    "section": "17.3. High Throughput Example",
    "text": "17.3. High Throughput Example\n\n\n\nHigh Throughput Serverless Example\n\n\nWe can add a caching layer to improve read times.\n\n\n\nServerless Example with Cache"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#database-overview",
    "href": "posts/software/aws/aws_saa_notes.html#database-overview",
    "title": "AWS Solutions Architect",
    "section": "18.1. Database Overview",
    "text": "18.1. Database Overview\nDatabase types:\n\nRDBMS (SQL OLTP)\nNoSQL\nObject store\nData warehouse (SQL OLAP)\nSearch database\nGraph database\nLedger\nTime series"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#rds-2",
    "href": "posts/software/aws/aws_saa_notes.html#rds-2",
    "title": "AWS Solutions Architect",
    "section": "18.2. RDS",
    "text": "18.2. RDS\nManaged RDBMS. Discussed in previous section."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#aurora-1",
    "href": "posts/software/aws/aws_saa_notes.html#aurora-1",
    "title": "AWS Solutions Architect",
    "section": "18.3. Aurora",
    "text": "18.3. Aurora\nSeparation of storage vs compute. Aurora has a PostgreSQL and MySQL compatible API.\nData is stored in 6 replicas across 3 AZ.\nThere is a serverless option."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#elasticache-1",
    "href": "posts/software/aws/aws_saa_notes.html#elasticache-1",
    "title": "AWS Solutions Architect",
    "section": "18.4. ElastiCache",
    "text": "18.4. ElastiCache\nManaged Redis or Memcached cache; in-memory data store.\nRequires changes to your application code to use the cache."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#dynamodb-1",
    "href": "posts/software/aws/aws_saa_notes.html#dynamodb-1",
    "title": "AWS Solutions Architect",
    "section": "18.5. DynamoDB",
    "text": "18.5. DynamoDB\nManaged serverless NoSQL database.\nThere are two capacity modes: provisioned or on-demand.\nYou can optionally add a DAX cluster for read caching."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#s3-1",
    "href": "posts/software/aws/aws_saa_notes.html#s3-1",
    "title": "AWS Solutions Architect",
    "section": "18.6. S3",
    "text": "18.6. S3\nKey/value store for objects.\nBest for large objects, not many small objects. Max object size is 5 TB."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#documentdb",
    "href": "posts/software/aws/aws_saa_notes.html#documentdb",
    "title": "AWS Solutions Architect",
    "section": "18.7. DocumentDB",
    "text": "18.7. DocumentDB\nDocumentDB is like a managed MongoDB (NoSQL). Analogous to how Aurora is managed SQL."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#neptune",
    "href": "posts/software/aws/aws_saa_notes.html#neptune",
    "title": "AWS Solutions Architect",
    "section": "18.8. Neptune",
    "text": "18.8. Neptune\nManaged graph database.\nHighly available with up to 15 read replicas across 3 AZs.\nNeptune Streams is a real-time ordered sequence of every change to your graph DB. The streams data is available via a REST API."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#amazon-keyspaces",
    "href": "posts/software/aws/aws_saa_notes.html#amazon-keyspaces",
    "title": "AWS Solutions Architect",
    "section": "18.9. Amazon Keyspaces",
    "text": "18.9. Amazon Keyspaces\nThis is a managed Cassandra (NoSQL) database service.\nQueries are done with Cassandra Query Language (CQL).\nThere are two capacity modes: on-demand and provisioned."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#amazon-timestream",
    "href": "posts/software/aws/aws_saa_notes.html#amazon-timestream",
    "title": "AWS Solutions Architect",
    "section": "18.10. Amazon Timestream",
    "text": "18.10. Amazon Timestream\nManaged time series database.\nRecent data is stored in memory and older data is stored in cost optimised storage.\nCompatible with SQL and has additional time series analytics functions."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#athena",
    "href": "posts/software/aws/aws_saa_notes.html#athena",
    "title": "AWS Solutions Architect",
    "section": "19.1. Athena",
    "text": "19.1. Athena\n\n19.1.1. Overview\nAthena is a serverless query service to analyse data stored in S3 using SQL.\nIt supports CSV, JSON, ORC, Avro and Parquet file formats.\nThink of it like Snowflake external tables. The data is in files in S3 but you can query it with SQL.\nYou need to specify an S3 bucket where query results are saved to.\n\n\n19.1.2. Athena Performance Improvements\n\nUse columnar data. This means Athena has fewer columns to scan. Parquet and ORC are recommended. AWS Glue can be used for ETL jobs to convert files to parquet from other formats.\nCompress data. Smaller retrieval.\nPartition datasets in S3 to allow querying on virtual columns. Name the directories column_name=value, e.g. s3://example_bucket/year=2025/month=1/date=2/data.parquet\n\n\n\n19.1.3. Federated Query\nThis allows you to run SQL queries across data stored in different places - Elasticache, DynamoDB, Aurora, on-premises, etc.\nIt does this using Data Source Connectors that run as a Lambda function.\n\n\n\nAthena Federated Query Example"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#redshift",
    "href": "posts/software/aws/aws_saa_notes.html#redshift",
    "title": "AWS Solutions Architect",
    "section": "19.2. Redshift",
    "text": "19.2. Redshift\n\n19.2.1. Overview\nRedshift is based on PostgreSQL but used for OLAP not OLTP (analytics not transactions). It uses columnar storage.\nThere are two modes: provisioned cluster or serverless cluster. Provisioned mode allows you to select instance types in advance and reserve instances for cost savings.\nCompared to Athena, Redshift is faster to join, query, aggregate. The downside is you need a cluster whereas Athena is completely serverless.\nA Redshift cluster has a leader node for query planning and results aggregation, and compute nodes which perform queries and send results to the leader node.\n\n\n19.2.2. Disaster Recovery\nRedshift has multi0AZ for some clusters. Otherwise snapshots are required for DR.\nSnapshots are point-in-time backups stored in S3. They are incremental, so only diffs are saved. They can be automated, either every 8 hours or every 5 GB, with a set retention periods. Manual snapshots are retained indefinitely.\nSnapshots can be configured to save to another region for DR.\n\n\n19.2.2. Loading Data into Redshift\nLarge inserts are much better.\n\n\n19.2.3. Redshift Spectrum\nThis can be used to query data into S3 without loading it, using an existing Redshift cluster that is already running.\nThe query is sent to Redshift referencing an S3 external table, then the leader node routes the query to compute nodes, which route to Redshift Spectrum nodes, which query S3.\n\n\n\nRedshift Spectrum"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#amazon-opensearch",
    "href": "posts/software/aws/aws_saa_notes.html#amazon-opensearch",
    "title": "AWS Solutions Architect",
    "section": "19.3. Amazon OpenSearch",
    "text": "19.3. Amazon OpenSearch\nOpenSearch is a successor to ElasticSearch. You can search any field, even partial matches. Contrast this to DynamoDB, where queries must be by primary key or indexes.\nOpenSearch is commonly used to complement other databases; for example, DynamoDB provides the retrieval capability, then writes to OpenSearch via a Lambda function so that OpenSearch can be used for the search capability.\nOpenSearch uses its own query language, but can be made compatible with SQL using a plugin.\nTwo modes: managed cluster or serverless cluster."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#emr",
    "href": "posts/software/aws/aws_saa_notes.html#emr",
    "title": "AWS Solutions Architect",
    "section": "19.4. EMR",
    "text": "19.4. EMR\nElastic MapReduce (EMR) creates Hadoop clusters to analyse and process big data. The cluster can be made of hundreds of EC2 instances. The cluster can autoscale and use spot or ondemand instances.\nEMR comes bundled with Spark, HBase, Presto, Flink, etc so requires less provisioning and configuration.\nNode types:\n\nMaster node: manage the cluster, coordinate, health checks. Long running.\nCore node: run tasks and store data. Long running.\n\nTask node: optional nodes that just run tasks. Typically spot instances.\n\nPurchasing options:\n\nOn demand\nReserved\nSpot\n\nClusters can be long running or transient (temporary)."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#quicksight",
    "href": "posts/software/aws/aws_saa_notes.html#quicksight",
    "title": "AWS Solutions Architect",
    "section": "19.5. QuickSight",
    "text": "19.5. QuickSight\n\n19.5.1. Overview\nServerless BI service to create interactive dashboards. Integrated with RDS, Aurora, Athena, Redshift, S3, etc.\nQuickSight integrates with:\n\nMost (all?) AWS data services\n3rd party applications like Salesforce or Jira\nImports for files, eg xlsx, csv, json\nOn-premises databases using JDBC\n\nIf data is imported into QuickSight, in-memory computation can be done using the SPICE engine.\nIt is possible to set up column-level security so users can only see columns that they are permissioned for.\n\n\n19.5.2. Dashboard and Analysis\nYou define Users and Groups in QuickSight; note that these are not the same as IAM users.\nA dashboard is a read-only snapshot of an analysis that can be shared and preserves the configuration (filtering, parameters, sort, etc).\nUsers who can see a dashboard can also access its underlying data."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#aws-glue",
    "href": "posts/software/aws/aws_saa_notes.html#aws-glue",
    "title": "AWS Solutions Architect",
    "section": "19.6. AWS Glue",
    "text": "19.6. AWS Glue\n\n19.6.1. Overview\nGlue is a managed ETL service (serverless).\nIt is commonly used to convert file formats, e.g. CSV -&gt; Parquet.\n\nGlue Job Bookmarks prevent reprocessing old data.\nGlue DataBrew cleans and normalises data using prebuilt transformations\nGlue Studio is a GUI to create and manage ETL jobs in Glue.\nGlue Streaming ETL is built on Apache Spark Structured Streaming.\n\n\n\n19.6.2. Glue Data Catalog\nAn AWS Glue Data Crawler crawls the different AWS data services (S3, RDS, DynamoDB, connected on-premises databases) and writes metadata to the AWS Glue Data Catalog.\nGlue jobs can then use this to know what tables and schemas exist.\nAthena, Redshift and EMR use this under the hood for data discovery.\n\n\n\nGlue Data Catalog Example"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#aws-lake-formation",
    "href": "posts/software/aws/aws_saa_notes.html#aws-lake-formation",
    "title": "AWS Solutions Architect",
    "section": "19.7. AWS Lake Formation",
    "text": "19.7. AWS Lake Formation\nA data lake is a central place for data. AWS Lake Formation is a managed service to set up data lakes. It allows you to discover, clean, catalog, transform and ingest data into your data lake, providing automated tools for this. Behind the scenes, it is a layer on top of AWS Glue.\nThere are blueprints for S3, RDS, relational databases, NoSQL databases, etc to make set up easy.\nAccess control is fine-grained at both row-level and column-level. Centralised permissions is a common use case for using Lake Formation. The underlying S3, RDS, Aurora sources might all use different IAM rules and access policies, so you can define the user access in Lake Formation.\n\n\n\nAWS Lake Formation"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#amazon-managed-service-for-apache-flink",
    "href": "posts/software/aws/aws_saa_notes.html#amazon-managed-service-for-apache-flink",
    "title": "AWS Solutions Architect",
    "section": "19.8. Amazon Managed Service for Apache Flink",
    "text": "19.8. Amazon Managed Service for Apache Flink\nFlink is a framework for processing data streams. AWS provides a managed service to handle provisioning and backups.\nFlink can read from Kinesis Data Streams or Amazon MSK (managed Kafka). Note that Flink does not read from Amazon Data Firehose."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#amazon-managed-streaming-for-apache-kafka",
    "href": "posts/software/aws/aws_saa_notes.html#amazon-managed-streaming-for-apache-kafka",
    "title": "AWS Solutions Architect",
    "section": "19.9. Amazon Managed Streaming for Apache Kafka",
    "text": "19.9. Amazon Managed Streaming for Apache Kafka\nThis is called MSK. It is a fully managed Apache Kafka service, and is an alternative to Amazon Kinesis. It is a message broker that sits between your data sources and downstream services.\nIt allows you to create, update and delete clusters, manage the Kafka broker nodes and zookeeper nodes, deploy in multi-AZ setup, automatic recovery and persist data to EBS volumes.\nThere is a serverless option to autoscale.\nExample of how Kafka works: \nDifference between Kinesis Data Streams and Amazon MSK:\n\n\n\nKinesis Data Streams vs MSK"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#big-data-ingestion-pipeline",
    "href": "posts/software/aws/aws_saa_notes.html#big-data-ingestion-pipeline",
    "title": "AWS Solutions Architect",
    "section": "19.10. Big Data Ingestion Pipeline",
    "text": "19.10. Big Data Ingestion Pipeline\nExample of a big data ingestion pipelines\n\n\n\nBig Data Ingestion Pipeline"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#amazon-rekognition",
    "href": "posts/software/aws/aws_saa_notes.html#amazon-rekognition",
    "title": "AWS Solutions Architect",
    "section": "20.1. Amazon Rekognition",
    "text": "20.1. Amazon Rekognition\nComputer vision. Find objects, people, text, scenes in images and videos.\nYou can create a database of familiar faces or compare against celebrities.\nOne use case is content moderation, to detect inappropriate or offensive content. You can set a minimum confidence threshold for flagging content. Flagged content can be manually reviewed in Amazon Augmented AI (A2I)."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#amazon-transcribe",
    "href": "posts/software/aws/aws_saa_notes.html#amazon-transcribe",
    "title": "AWS Solutions Architect",
    "section": "20.2. Amazon Transcribe",
    "text": "20.2. Amazon Transcribe\nConvert speech to text using automatic speech recognition.\nYou can automatically remove Personally Identifiable Information (PII) using Redaction, and automatically detect languages for multi-lingual audio."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#polly",
    "href": "posts/software/aws/aws_saa_notes.html#polly",
    "title": "AWS Solutions Architect",
    "section": "20.3. Polly",
    "text": "20.3. Polly\nText to speech.\nLexicon can be used to customise the pronunciation of words, or expand acronyms to the full name.\nSpeech Synthesis Markup Language allows more precise customisation, such as emphasis, breathing sounds, whispering."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#translate",
    "href": "posts/software/aws/aws_saa_notes.html#translate",
    "title": "AWS Solutions Architect",
    "section": "20.4. Translate",
    "text": "20.4. Translate\nLanguage translation. It can be used to localise content."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#lex-and-connect",
    "href": "posts/software/aws/aws_saa_notes.html#lex-and-connect",
    "title": "AWS Solutions Architect",
    "section": "20.5. Lex and Connect",
    "text": "20.5. Lex and Connect\nAmazon Lex is the technology that powers Alexa. Automatic Speech Recognition and natural language understanding. It can be used to build chatbots.\nAmazon Connect allows you to receive calls and create contact flows like a virtual call centre. It can integrate with CRM or AWS services.\n\n\n\nLex and Connect Example"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#amazon-comprehend",
    "href": "posts/software/aws/aws_saa_notes.html#amazon-comprehend",
    "title": "AWS Solutions Architect",
    "section": "20.6. Amazon Comprehend",
    "text": "20.6. Amazon Comprehend\nFully managed NLP service. Use cases include: detecting language, sentiment analysis, extract entities, organise text files by topic.\nComprehend Medical is specifically for a clinical setting. It automatically detects Protected Health Information (PHI)."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#sagemaker-ai",
    "href": "posts/software/aws/aws_saa_notes.html#sagemaker-ai",
    "title": "AWS Solutions Architect",
    "section": "20.7. SageMaker AI",
    "text": "20.7. SageMaker AI\nFully managed service for data scientists to build ML models."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#amazon-kendra",
    "href": "posts/software/aws/aws_saa_notes.html#amazon-kendra",
    "title": "AWS Solutions Architect",
    "section": "20.8. Amazon Kendra",
    "text": "20.8. Amazon Kendra\nFully managed document search service. Extract answers from within a document. Natural language search capability. Incremental learning from user interactions to promote preferred results."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#amazon-personalize",
    "href": "posts/software/aws/aws_saa_notes.html#amazon-personalize",
    "title": "AWS Solutions Architect",
    "section": "20.9. Amazon Personalize",
    "text": "20.9. Amazon Personalize\nFully managed ML service to build apps with real-time personalised recommendations. Same tech used for the Amazon website."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#amazon-textract",
    "href": "posts/software/aws/aws_saa_notes.html#amazon-textract",
    "title": "AWS Solutions Architect",
    "section": "20.10 Amazon Textract",
    "text": "20.10 Amazon Textract\nAutomatically extract text, handwriting and data from scanned documents."
  }
]