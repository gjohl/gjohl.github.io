[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Notes",
    "section": "",
    "text": "Series\n\nFastAI Series\nThis series contains notes from the fastai “Practical Deep Learning for Coders” course and related book.\n\n\nGen AI Series\nA series of posts on Generative AI techniques and projects.\n\n\nReact Series\nMaking pretty front-ends.\n\n\n\nAll Posts\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nExecutive Presence\n\n\n\n\n\n\nBusiness\n\n\nLeadership\n\n\n\nMy Presence is a Present\n\n\n\n\n\nMay 22, 2025\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nScience of Leadership\n\n\n\n\n\n\nBusiness\n\n\nLeadership\n\n\n\nNerds in Charge\n\n\n\n\n\nMay 20, 2025\n\n\n20 min\n\n\n\n\n\n\n\n\n\n\n\n\nConflict Resolution\n\n\n\n\n\n\nBusiness\n\n\nLeadership\n\n\n\nCheaper Than Real Therapy\n\n\n\n\n\nApr 28, 2025\n\n\n20 min\n\n\n\n\n\n\n\n\n\n\n\n\nAWS Solutions Architect\n\n\n\n\n\n\nDataEngineering\n\n\nEngineering\n\n\nSoftware\n\n\n\nSAA-C03\n\n\n\n\n\nMar 20, 2025\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On LLMs: Advanced Text Generation Techniques and Tools\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\nLLM\n\n\n\nPart 7: Chains and Agents\n\n\n\n\n\nMar 5, 2025\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nSnowflake: SnowPro Core\n\n\n\n\n\n\nSoftware\n\n\nDataEngineering\n\n\n\nSnowflake? Snow Problem.\n\n\n\n\n\nMar 4, 2025\n\n\n61 min\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series Analysis\n\n\n\n\n\n\nEngineering\n\n\nTimeseries\n\n\n\nTime well spent\n\n\n\n\n\nFeb 15, 2025\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nSQL\n\n\n\n\n\n\nInterviewPrep\n\n\nSoftware\n\n\nDataEngineering\n\n\n\nPart 1 of 1: No Sequel\n\n\n\n\n\nJan 27, 2025\n\n\n25 min\n\n\n\n\n\n\n\n\n\n\n\n\nDesigning Data Intensive Applications: Part 1\n\n\n\n\n\n\nDataEngineering\n\n\nEngineering\n\n\nSoftware\n\n\n\nData Systems\n\n\n\n\n\nJan 26, 2025\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On LLMs: Text Clustering and Topic Modeling\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\nLLM\n\n\n\nPart 5: Hot Topics\n\n\n\n\n\nJan 16, 2025\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On LLMs: Text Classification\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\nLLM\n\n\n\nPart 4: Stay Classy\n\n\n\n\n\nJan 15, 2025\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On LLMs: Prompt Engineering\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\nLLM\n\n\n\nPart 6: Professional prompts\n\n\n\n\n\nDec 13, 2024\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On LLMs: Tokens and Embeddings\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\nLLM\n\n\n\nPart 2: Tokens and Embeddings\n\n\n\n\n\nNov 28, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On LLMs: Intro\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\nLLM\n\n\n\nPart 1: Introduction to LLMs\n\n\n\n\n\nNov 26, 2024\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nGraph ML: Graph Algorithms\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGraphs\n\n\nGraphML\n\n\nAlgorithms\n\n\n\nPart 0.5: Traditional Graph Algorithms\n\n\n\n\n\nMay 8, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nGraph ML: What’s a Graph?\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGraphs\n\n\nGraphML\n\n\n\nPart 0: Intro to Graphs\n\n\n\n\n\nMay 7, 2024\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nSorting Algorithms\n\n\n\n\n\n\nEngineering\n\n\nComputerScience\n\n\nInterviewPrep\n\n\nAlgorithms\n\n\n\nAlgorithms? Sorted, mate.\n\n\n\n\n\nMay 6, 2024\n\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\n\nData Structures\n\n\n\n\n\n\nEngineering\n\n\nComputerScience\n\n\nInterviewPrep\n\n\n\nAn Array of Possibilities, Trie it Out, You’ll Have Heaps of Fun\n\n\n\n\n\nApr 23, 2024\n\n\n30 min\n\n\n\n\n\n\n\n\n\n\n\n\nAlgorithms\n\n\n\n\n\n\nEngineering\n\n\nComputerScience\n\n\nInterviewPrep\n\n\nAlgorithms\n\n\n\nBig G Explains Big O\n\n\n\n\n\nApr 17, 2024\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI: GANs\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\nGAN\n\n\n\nPart 4: Generative Adversarial Networks\n\n\n\n\n\nApr 10, 2024\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: TypeScript Essentials\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 7: I don’t got no type, bad code is the only thing that I like\n\n\n\n\n\nMar 21, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: Testing\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 6: Testing my patience\n\n\n\n\n\nMar 20, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: Deployment\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 5: Deploying React Apps\n\n\n\n\n\nMar 18, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: Debugging\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 4: A Bug’s Life\n\n\n\n\n\nMar 17, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: Styling\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 3: Styling it Out\n\n\n\n\n\nMar 16, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: JavaScript Essentials\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 2: WTF is JSX\n\n\n\n\n\nMar 14, 2024\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: A Gentle Introduction\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 1: Getting Started with React\n\n\n\n\n\nMar 12, 2024\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\nMeta Learning\n\n\n\n\n\n\nLearning\n\n\nAI\n\n\n\nMeta Meta Learning: What I Learned From Meta Learning by Radek Osmulski\n\n\n\n\n\nMar 10, 2024\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI: VAEs\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\nVAE\n\n\n\nPart 3: Variational Autoencoders\n\n\n\n\n\nMar 6, 2024\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 8: Convolutions\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 8\n\n\n\n\n\nMar 5, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 7: Collaborative Filtering\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 7\n\n\n\n\n\nMar 1, 2024\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI: Deep Learning Foundations\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\n\nPart 2: The Building Blocks for Generative AI\n\n\n\n\n\nFeb 26, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI: Intro\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\n\nPart 1: Introduction to Generative AI\n\n\n\n\n\nFeb 19, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 6.5: Road to the Top\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 6.5\n\n\n\n\n\nFeb 13, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 6: Random Forests\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 6\n\n\n\n\n\nOct 9, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 5: Natural Language Processing\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 5\n\n\n\n\n\nSep 23, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 4: Natural Language Processing\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 4\n\n\n\n\n\nSep 7, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 3: How Does a Neural Network Learn?\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 3\n\n\n\n\n\nSep 1, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 2: Deployment\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 2\n\n\n\n\n\nAug 30, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 1: Image Classification Models\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 1\n\n\n\n\n\nAug 23, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nSystem Design Notes\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\n\nNotes on System Design\n\n\n\n\n\nAug 14, 2023\n\n\n24 min\n\n\n\n\n\n\n\n\n\n\n\n\nPitching Notes\n\n\n\n\n\n\nBusiness\n\n\n\nNotes on Pitching\n\n\n\n\n\nAug 4, 2023\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nKalman Filter Notes\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\n\nNotes on Kalman filters\n\n\n\n\n\nJul 23, 2023\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nPublic Speaking Notes\n\n\n\n\n\n\nBusiness\n\n\n\nNotes on Public Speaking\n\n\n\n\n\nJul 20, 2023\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nMarketing Notes\n\n\n\n\n\n\nBusiness\n\n\n\nNotes on Marketing\n\n\n\n\n\nJul 18, 2023\n\n\n18 min\n\n\n\n\n\n\n\n\n\n\n\n\nSoftware Architecture Notes\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\n\nNotes on Software Architecture\n\n\n\n\n\nJun 23, 2023\n\n\n22 min\n\n\n\n\n\n\n\n\n\n\n\n\nTensorflow Notes\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\n\nNotes on Tensorflow\n\n\n\n\n\nFeb 23, 2023\n\n\n21 min\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n Back to top"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Projects\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBrazilian Jiu Jitsu Taxonomy\n\n\n\nBJJ\n\n\nWebDev\n\n\n\nGrappling with Web Development\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplainable AI in Healthcare\n\n\n\nResearch\n\n\nAI\n\n\nHealthcare\n\n\n\nOpening the black box in medical AI\n\n\n\n\n\n\n\n\n\n\n\n\n\nTradeIntel\n\n\n\nTrading\n\n\nApp\n\n\n\nNext-generation robo advisor\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/ml/kalman_filter/kalman_filter.html",
    "href": "posts/ml/kalman_filter/kalman_filter.html",
    "title": "Kalman Filter Notes",
    "section": "",
    "text": "Notes from https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python\n\n\n\n\nWith measurements from 2 noisy sensors, we can combine them to get a much more accurate measurement. Never throw data away.\nIf we have a prediction model for how the world evolves (e.g. predicted daily weight gain) we can combine this with noisy measurements in the same way. Treat the prediction as a noisy sensor and combine as before, so that our estimate is some way between the prediction and the estimate. How close it is to one or the other depends on the relative accuracy of each, and this is a hyperparameter we can set.\nprediction =  x_est + dx*dt \n\nwhere:\n`x_est` is an initial constant that gets updated\n`dx` is a predicted daily weight gain rate\n`dt` is the time step\n\n\n\nWe can create a single parameter filter, where the parameter g corresponds to how strongly we trust the prediction over the measurement\nresidual = prediction - measurement\nestimate = measurement + g * residual\n\ng=0 =&gt; estimate=measurement\ng=1 =&gt; estimate=prediction\nfor other values of g, the estimate will be somewhere between the measurement and prediction\n\n\n\n\n\n\n\nFigure 1: g-h filter diagram\n\n\n\n\n\n\nWe can go one step further and use our noisy estimates of weight to refine our the daily weight gain w used in our prediction\ndx_t+1 = dx_t + h * residual / dt\nThis gives us the g-h filter - a generic filter which allows us to set a parameter g for the weight measurement confidence and h for the weight change measurement confidence.\nWe can then update the estimates for weight and weight change on every time step (or every measurement if time steps are irregular).\nThis insight forms the basis for Kalman filters, where we will set g and h dynamically on each time step.\n\n\n\nKey takeaways:\n\nMultiple data points are more accurate than one data point, so throw nothing away no matter how inaccurate it is.\nAlways choose a number part way between two data points to create a more accurate estimate.\nPredict the next measurement and rate of change based on the current estimate and how much we think it will change.\nThe new estimate is then chosen as part way between the prediction and next measurement scaled by how accurate each is.\nThe filter is only as good as the mathematical model used to express the system.\nFilters are designed, not selected ad hoc. No choice of g and h applies to all scenarios.\n\nTerminology:\n\nSystem - the object we want to estimate\nState, x - the current configuration of the system. Hidden.\nMeasurement, z - measured value of the state from a noisy sensor. Observable.\nState estimate - our filter’s estimate of the state.\nProcess model - the model we use to predict the next state based on the current state.\nSystem propagation - the predict step\nMeasurement update - the update step\nEpoch - one iteration of system propagation and measurement update.\n\n\n\n\n\n\n\nBayesian statistics treats probability as a belief about a single event. Frequentist statistics describes past events based on their frequency. It has no bearing on future events.\nIf I flip a coin 100 times and get 50 heads and 50 tails, frequentist statistics states the probability of heads was 50% for those cases. On the next coin flip, frequentist statistics has nothing to say about the probability. The state is simply unknown. Bayesian statistics incorporates these past events as a prior belief, so that we can say the next coin flip has a 50% chance of landing heads. “Belief” is a measure of the strength of our knowledge.\nWhen talking about the probability of something, we are implicitly saying “the probability that this event is true given past events”. This is a Bayesian approach. In practice, we may incorporate frequentist techniques too, as in the example above when the 100 previous coin tosses were used to inform our prior.\n\nPrior is the probability distribution before including the measurement’s information. This corresponds to the prediction in the Kalman filter.\nPosterior is a probability distribution after incorporating the measurement’s information. This corresponds to the estimated state in the Kalman filter.\nLikelihood is the joint probability of the observed data - how likely is each position given the measurement. This is not a probability distribution as it does not sum to 1.\n\nThe filter will use Bayes theorem:\nposterior = likelihood * prior / normalization\nIn even simpler filter terms:\nupdated knowledge = || likelihood of new knowledge * prior knowledge ||\nIf we have a prior distribution of positions and system model of the subsequent movement, we can convolve the two to calculate the posterior.\n\n\n\nThe discrete Bayes filter is a form of g-h filter. It is useful for multimodal, discrete problems.\nThe equations are:\nPredict step:\nx_bar = x * f_x(.)\nwhere:\n- x is the current state\n- f_x(.) is the state propagation function for x, i.e. the system model.\n\n\nUpdate step:\nx = ||L . x_bar||\nwhere:\n- L is the likely function\nIn pseudocode this is:\nInitialisation:\n1. Initialise our belief in the state.\n\nPredict:\n1. Predict state for the next time step using the system model.\n2. Adjust belief to account for uncertainty in prediction.\n\nUpdate:\n1. Get a measurement and belief about its accuracy (noise estimate).\n2. Compute likelihood of measurement matching each state.\n3. Update posterior state belief based on likelihood.\nAlgorithms of this form a called “predictor correctors”; we make a prediction then correct it. The predict step will always degrade our knowledge due to the uncertainty in the second step of the predict stage. But adding another measurement, even if noisy, improves our knowledge again. So we can converge on the most likely result.\n\n\n\nThe algorithm is trivial to implement, debug and understand.\nLimitations:\n\nScaling - Tracking i state variables results in O(n^i) runtime complexity.\nDiscrete - Most real-world examples are continuous. We can increase the granularity to get a discrete approximation of continuous measurements, but this increases the scale again.\nMultimodal - Sometimes you require a single output value.\nNeeds a state change measurement.\n\nThe Kalman filter is based on the same idea that we can use Bayesian reasoning to combine measurements and system models. The fundamental insight in this chapter is that we multiply (convolve) probabilities when we measure and shift probabilities when we update, which leads to a converging solution.\n\n\n\n\nGaussian distributions address some of the limitations of the discrete Bayes filter, as they are continuous and unimodal.\n\n\n\nRandom variables - the possible values and associated probabilities of a particular event. Denoted with capital letter, e.g. X\nSample space - the range of values a random variable can take. This is not unique, e.g. for a dice roll could be {1,2,3,4,5,6}, or {even,odd} or {d}\nProbability distribution - the probability for the random variable to take any value in the sample space. Probability distribution denoted with lower case p, and probability of a single event denoted with upper case P, e.g. P(X=1) = p(1)\nTotal probability - Probabilities are non-negative and sum/integrate to 1.\nSummary statistics - mean, median, mode\nExpected value is the probability-weighted mean of values. This equals the mean if the probability distribution is uniform. E[X] = \\sum{p_i x_i} mean = \\sum{x_i}/n\nVariance - var(X) = E[(x-mu)^2] = \\sum{(x_i - mu)^2}/n The square in the variance (rather han absolute value) is somewhat arbitrary, and reflects that the sign of the deviation shouldn’t matter and larger deviations are “worse” than smaller deviations. Precision is sometimes used which is the inverse of the variance.\n\n\n\n\nContinuous, unimodal probability distributions.\nf(x, mu, sigma) = (1 / sigma sqrt(2*pi)) * e^(-(x-mu)^2/2*sigma^2)\n\nwhich, removing constants, is proportional to e^-x^2\nThe probability of a single point is infinitesimally small. We can think of this in Bayesian terms or frequentist terms. As a Bayesian, if the thermometer reads exactly 22°C, then our belief is described by the Gaussian curve; our belief that the actual (system) temperature is near 22°C is very high, and our belief that the actual temperature is, say, near 18 is very low. As a frequentist we would say that if we took 1 billion temperature measurements of a system at exactly 22°C, then a histogram of the measurements would look like a Gaussian curve.\nGaussians are nice to work with because the sum of independent Gaussian random variables is another Gaussian random variable. The product of Gaussian distributions will be a Gaussian function, i.e. it is Gaussian but may not sum to 1 so will need to be scaled. Sum of two Gaussians:\n\nmu = mu_1 + mu_2\nvar = var_1 + var_2\n\nProduct of two Gaussians:\n\nmu = (var_1mu_2 + var_2mu_1) / (var_1 + var_2)\nvar = (var_1*var_2) / (var_1 + var_2)\n\nThey also mean we can summarise/approximate large datasets with only two numbers: mean and variance.\nBayes theorem applies to probability distribution functions (p) just like it does to individual events (P). From the general form of Bayes theorem:\np(A|B) = p(B|A)p(A)/p(B)\nRecasting this for the sensor problem, where A=x_i (the position) and B=z (sensor reading):\np(x_i|z) = p(z|x_i)p(x_i)/p(z)\n\np(z|x_i) is the likelihood - the probability that we get the sensor measurement z at each position x_i\np(x_i) is the prior - our belief before incorporating the measurement\np(x_i) is the evidence - in practice this is often an intractable integral which we can sidestep by treating this as a normalisation factor, as we know the posterior pdf must sum to 1.\np(x_i|z) is the posterior - this is typically a hard question to answer, but using Bayes we can substitute it with the easier question answered by the likelihood and prior. Rather than answering “what is the probability of cancer given this test result” (hard problem) we instead answer “what is the probability of this test result given someone has cancer” (easier problem)\n\n\n\n\nA limitation of using Gaussians to model physical systems is that it has infinitely long tails, but in practice many physical quantities have hard bounds, e.g. weight cannot be negative. Under a Gaussian assumption, model people’s weight as a Gaussian would suggest there is a miniscule chance that someone weights 1000000kg, which is obviously wrong.\nThis can impact the performance of Kalman filters as they rely on assumptions of Gaussian noise, which aren’t strictly true.\n\n\n\n\nA filter to track one state variable, position. A Kalman filter is essentially a Bayesian filter that uses Gaussians, so combines the previous two chapters.\n\n\n\n\nKalman filter repo\nArtificial Intelligence for Robotics course\nThink Stats ebook\n\n\n\n\nFigure 1: g-h filter diagram"
  },
  {
    "objectID": "posts/ml/kalman_filter/kalman_filter.html#g-h-filters",
    "href": "posts/ml/kalman_filter/kalman_filter.html#g-h-filters",
    "title": "Kalman Filter Notes",
    "section": "",
    "text": "With measurements from 2 noisy sensors, we can combine them to get a much more accurate measurement. Never throw data away.\nIf we have a prediction model for how the world evolves (e.g. predicted daily weight gain) we can combine this with noisy measurements in the same way. Treat the prediction as a noisy sensor and combine as before, so that our estimate is some way between the prediction and the estimate. How close it is to one or the other depends on the relative accuracy of each, and this is a hyperparameter we can set.\nprediction =  x_est + dx*dt \n\nwhere:\n`x_est` is an initial constant that gets updated\n`dx` is a predicted daily weight gain rate\n`dt` is the time step\n\n\n\nWe can create a single parameter filter, where the parameter g corresponds to how strongly we trust the prediction over the measurement\nresidual = prediction - measurement\nestimate = measurement + g * residual\n\ng=0 =&gt; estimate=measurement\ng=1 =&gt; estimate=prediction\nfor other values of g, the estimate will be somewhere between the measurement and prediction\n\n\n\n\n\n\n\nFigure 1: g-h filter diagram\n\n\n\n\n\n\nWe can go one step further and use our noisy estimates of weight to refine our the daily weight gain w used in our prediction\ndx_t+1 = dx_t + h * residual / dt\nThis gives us the g-h filter - a generic filter which allows us to set a parameter g for the weight measurement confidence and h for the weight change measurement confidence.\nWe can then update the estimates for weight and weight change on every time step (or every measurement if time steps are irregular).\nThis insight forms the basis for Kalman filters, where we will set g and h dynamically on each time step.\n\n\n\nKey takeaways:\n\nMultiple data points are more accurate than one data point, so throw nothing away no matter how inaccurate it is.\nAlways choose a number part way between two data points to create a more accurate estimate.\nPredict the next measurement and rate of change based on the current estimate and how much we think it will change.\nThe new estimate is then chosen as part way between the prediction and next measurement scaled by how accurate each is.\nThe filter is only as good as the mathematical model used to express the system.\nFilters are designed, not selected ad hoc. No choice of g and h applies to all scenarios.\n\nTerminology:\n\nSystem - the object we want to estimate\nState, x - the current configuration of the system. Hidden.\nMeasurement, z - measured value of the state from a noisy sensor. Observable.\nState estimate - our filter’s estimate of the state.\nProcess model - the model we use to predict the next state based on the current state.\nSystem propagation - the predict step\nMeasurement update - the update step\nEpoch - one iteration of system propagation and measurement update."
  },
  {
    "objectID": "posts/ml/kalman_filter/kalman_filter.html#discrete-bayes-filter",
    "href": "posts/ml/kalman_filter/kalman_filter.html#discrete-bayes-filter",
    "title": "Kalman Filter Notes",
    "section": "",
    "text": "Bayesian statistics treats probability as a belief about a single event. Frequentist statistics describes past events based on their frequency. It has no bearing on future events.\nIf I flip a coin 100 times and get 50 heads and 50 tails, frequentist statistics states the probability of heads was 50% for those cases. On the next coin flip, frequentist statistics has nothing to say about the probability. The state is simply unknown. Bayesian statistics incorporates these past events as a prior belief, so that we can say the next coin flip has a 50% chance of landing heads. “Belief” is a measure of the strength of our knowledge.\nWhen talking about the probability of something, we are implicitly saying “the probability that this event is true given past events”. This is a Bayesian approach. In practice, we may incorporate frequentist techniques too, as in the example above when the 100 previous coin tosses were used to inform our prior.\n\nPrior is the probability distribution before including the measurement’s information. This corresponds to the prediction in the Kalman filter.\nPosterior is a probability distribution after incorporating the measurement’s information. This corresponds to the estimated state in the Kalman filter.\nLikelihood is the joint probability of the observed data - how likely is each position given the measurement. This is not a probability distribution as it does not sum to 1.\n\nThe filter will use Bayes theorem:\nposterior = likelihood * prior / normalization\nIn even simpler filter terms:\nupdated knowledge = || likelihood of new knowledge * prior knowledge ||\nIf we have a prior distribution of positions and system model of the subsequent movement, we can convolve the two to calculate the posterior.\n\n\n\nThe discrete Bayes filter is a form of g-h filter. It is useful for multimodal, discrete problems.\nThe equations are:\nPredict step:\nx_bar = x * f_x(.)\nwhere:\n- x is the current state\n- f_x(.) is the state propagation function for x, i.e. the system model.\n\n\nUpdate step:\nx = ||L . x_bar||\nwhere:\n- L is the likely function\nIn pseudocode this is:\nInitialisation:\n1. Initialise our belief in the state.\n\nPredict:\n1. Predict state for the next time step using the system model.\n2. Adjust belief to account for uncertainty in prediction.\n\nUpdate:\n1. Get a measurement and belief about its accuracy (noise estimate).\n2. Compute likelihood of measurement matching each state.\n3. Update posterior state belief based on likelihood.\nAlgorithms of this form a called “predictor correctors”; we make a prediction then correct it. The predict step will always degrade our knowledge due to the uncertainty in the second step of the predict stage. But adding another measurement, even if noisy, improves our knowledge again. So we can converge on the most likely result.\n\n\n\nThe algorithm is trivial to implement, debug and understand.\nLimitations:\n\nScaling - Tracking i state variables results in O(n^i) runtime complexity.\nDiscrete - Most real-world examples are continuous. We can increase the granularity to get a discrete approximation of continuous measurements, but this increases the scale again.\nMultimodal - Sometimes you require a single output value.\nNeeds a state change measurement.\n\nThe Kalman filter is based on the same idea that we can use Bayesian reasoning to combine measurements and system models. The fundamental insight in this chapter is that we multiply (convolve) probabilities when we measure and shift probabilities when we update, which leads to a converging solution."
  },
  {
    "objectID": "posts/ml/kalman_filter/kalman_filter.html#probability-and-gaussians",
    "href": "posts/ml/kalman_filter/kalman_filter.html#probability-and-gaussians",
    "title": "Kalman Filter Notes",
    "section": "",
    "text": "Gaussian distributions address some of the limitations of the discrete Bayes filter, as they are continuous and unimodal.\n\n\n\nRandom variables - the possible values and associated probabilities of a particular event. Denoted with capital letter, e.g. X\nSample space - the range of values a random variable can take. This is not unique, e.g. for a dice roll could be {1,2,3,4,5,6}, or {even,odd} or {d}\nProbability distribution - the probability for the random variable to take any value in the sample space. Probability distribution denoted with lower case p, and probability of a single event denoted with upper case P, e.g. P(X=1) = p(1)\nTotal probability - Probabilities are non-negative and sum/integrate to 1.\nSummary statistics - mean, median, mode\nExpected value is the probability-weighted mean of values. This equals the mean if the probability distribution is uniform. E[X] = \\sum{p_i x_i} mean = \\sum{x_i}/n\nVariance - var(X) = E[(x-mu)^2] = \\sum{(x_i - mu)^2}/n The square in the variance (rather han absolute value) is somewhat arbitrary, and reflects that the sign of the deviation shouldn’t matter and larger deviations are “worse” than smaller deviations. Precision is sometimes used which is the inverse of the variance.\n\n\n\n\nContinuous, unimodal probability distributions.\nf(x, mu, sigma) = (1 / sigma sqrt(2*pi)) * e^(-(x-mu)^2/2*sigma^2)\n\nwhich, removing constants, is proportional to e^-x^2\nThe probability of a single point is infinitesimally small. We can think of this in Bayesian terms or frequentist terms. As a Bayesian, if the thermometer reads exactly 22°C, then our belief is described by the Gaussian curve; our belief that the actual (system) temperature is near 22°C is very high, and our belief that the actual temperature is, say, near 18 is very low. As a frequentist we would say that if we took 1 billion temperature measurements of a system at exactly 22°C, then a histogram of the measurements would look like a Gaussian curve.\nGaussians are nice to work with because the sum of independent Gaussian random variables is another Gaussian random variable. The product of Gaussian distributions will be a Gaussian function, i.e. it is Gaussian but may not sum to 1 so will need to be scaled. Sum of two Gaussians:\n\nmu = mu_1 + mu_2\nvar = var_1 + var_2\n\nProduct of two Gaussians:\n\nmu = (var_1mu_2 + var_2mu_1) / (var_1 + var_2)\nvar = (var_1*var_2) / (var_1 + var_2)\n\nThey also mean we can summarise/approximate large datasets with only two numbers: mean and variance.\nBayes theorem applies to probability distribution functions (p) just like it does to individual events (P). From the general form of Bayes theorem:\np(A|B) = p(B|A)p(A)/p(B)\nRecasting this for the sensor problem, where A=x_i (the position) and B=z (sensor reading):\np(x_i|z) = p(z|x_i)p(x_i)/p(z)\n\np(z|x_i) is the likelihood - the probability that we get the sensor measurement z at each position x_i\np(x_i) is the prior - our belief before incorporating the measurement\np(x_i) is the evidence - in practice this is often an intractable integral which we can sidestep by treating this as a normalisation factor, as we know the posterior pdf must sum to 1.\np(x_i|z) is the posterior - this is typically a hard question to answer, but using Bayes we can substitute it with the easier question answered by the likelihood and prior. Rather than answering “what is the probability of cancer given this test result” (hard problem) we instead answer “what is the probability of this test result given someone has cancer” (easier problem)\n\n\n\n\nA limitation of using Gaussians to model physical systems is that it has infinitely long tails, but in practice many physical quantities have hard bounds, e.g. weight cannot be negative. Under a Gaussian assumption, model people’s weight as a Gaussian would suggest there is a miniscule chance that someone weights 1000000kg, which is obviously wrong.\nThis can impact the performance of Kalman filters as they rely on assumptions of Gaussian noise, which aren’t strictly true."
  },
  {
    "objectID": "posts/ml/kalman_filter/kalman_filter.html#one-dimensional-kalman-filter",
    "href": "posts/ml/kalman_filter/kalman_filter.html#one-dimensional-kalman-filter",
    "title": "Kalman Filter Notes",
    "section": "",
    "text": "A filter to track one state variable, position. A Kalman filter is essentially a Bayesian filter that uses Gaussians, so combines the previous two chapters."
  },
  {
    "objectID": "posts/ml/kalman_filter/kalman_filter.html#references",
    "href": "posts/ml/kalman_filter/kalman_filter.html#references",
    "title": "Kalman Filter Notes",
    "section": "",
    "text": "Kalman filter repo\nArtificial Intelligence for Robotics course\nThink Stats ebook\n\n\n\n\nFigure 1: g-h filter diagram"
  },
  {
    "objectID": "posts/ml/tensorflow/tensorflow.html",
    "href": "posts/ml/tensorflow/tensorflow.html",
    "title": "Tensorflow Notes",
    "section": "",
    "text": "These are notes taken primarily from the Complete Tensorflow and Keras Udemy course\n\n\n\nData sourcing - train_test_split\nExploratory data analysis - plot distributions, correlations\nData cleaning - drop redundant columns, handle missing data (drop or impute)\nFeature engineering - one hot encoding categories, scaling/normalising numerical values, combining columns, extracting info from columns (e.g. zip code from address)\nModel selection and training - model and hyperparameters\nModel tuning and evaluation metrics - classification: classification_report, confusion matrix, accuracy, recall, F1. Regression: error (RMSE, MAE, etc)\nPredictions\n\n\n\n\n\nSupervised learning\n\nLinear regression\nSVM\nNaive Bayes\nKNN\nRandom forests\n\nUnsupervised learning\n\nDimensionality reduction\nClustering\n\nReinforcement Learning\n\nSupervised learning tasks can be broadly broken into:\n\nRegression\nClassification\n\nSingle class\nMulti class\n\n\n\n\n\n\n\nGeneral idea of a neural network that can then be extended to specific cases, e.g. CNNs and RNNs.\n\nPerceptron: a weighted average of inputs passed through some activation gate function to arrive at an output decision\nNetwork of perceptrons\nActivation function: a non-linear transfer function f(wx+b)\nCost function and gradient descent: convex optimisation of a loss function using sub-gradient descent. The optimizer can be set in the compile method of the model.\nBackpropagation: use chain rule to determine partial gradients of each weight and bias. This means we only need a single forward pass followed by a single backward pass. Contrast this to if we perturbed each weight or bias to determine each partial gradient: in that case, for each epoch we would need to run a forward pass per weight/bias in the network, which is potentially millions!\nDropout: a technique to avoid overfitting by randomly dropping neurons in each epoch.\n\nGeneral structure:\n\nInput layer\nHidden layer(s)\nOutput layer\n\nInput and output layers are determined by the problem:\n\nInput size: number of features in the data\nOutput size number of targets to predict, i.e. one for single class classification or single target regression, or multiple for multiclass (one per class)\nOutput layer activation determined by problem. For single class classification activation='sigmoid', for multiclass classification activation='softmax'\nLoss function determined by problem. For single class classification loss='binary_crossentropy', for multiclass classification loss='categorical_crossentropy'\n\nHidden layers are less well-defined. Some heuristics here.\nVanishing gradients can be an issue for lower layers in particular, where the partial gradients of individual layers can be very small, so when multiplied together in chain rule the gradient is vanishingly small. Exploding gradients are a similar issue but where gradients get increasingly large when multiplied together. Some techniques to rectify vanishing/exploding gradients:\n\nUse different activation functions with larger gradients close to 0 and 1, e.g. leaky ReLU or ELU (exponential linear unit)\nBatch normalisation: scale each gradient by the mean and standard deviation of the batch\nDifferent weight initialisation methods, e.g. Xavier initialisation\n\nExample model outline\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\n\nmodel = Sequential()\n\n# Input layer\nmodel.add(Dense(78, activation='relu'))  # Number of input features\nmodel.add(Dropout(0.2))  # Optional dropout layer after each layer. Omitted for next layers for clarity\n\n# Hidden layers\nmodel.add(Dense(39, activation='relu'))\nmodel.add(Dense(19, activation='relu'))\n\n# Output layer\nmodel.add(Dense(1, activation='sigmoid'))  # Number of target classes (single class in this case)\n\n# Problem setup\nmodel.compile(loss='binary_crossentropy', optimizer='adam')  # Type of problem (single class classification in this case)\n\n# Training\nmodel.fit(\n    x=X_train,\n    y=y_train,\n    batch_size=256,\n    epochs=30,\n    validation_data=(X_test, y_test)\n)\n\n# Prediction\nmodel.predict(new_input)  # The new input needs to be shaped and scaled the sane as the training data\n\n\n\nThese are used for image classification problems where convolutional filters are useful for extracting features from input arrays.\n\nImage kernels/filters\n\nGrayscale 2D arrays\nRGB 3D tensors\n\nConvolutional layers\nPooling layers\n\nGeneral structure:\n\nInput layer\nConvolutional layer\nPooling layer\n(Optionally more pairs of convolutional and pooling layers)\nFlattening layer\nDense hidden layer(s)\nOutput layer\n\nExample model outline\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nmodel = Sequential()\n\n# Convolutional layer followed by pooling. Deeper networks may have multiple pairs of these.\nmodel.add(Conv2D(filters=32, kernel_size=(4,4),input_shape=(28, 28, 1), activation='relu',))  # Input shape determined by input data size\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\n# Flatten _images from 2D to 1D before final layer\nmodel.add(Flatten())\n\n# Dense hidden layer\nmodel.add(Dense(128, activation='relu'))\n\n# Output layer\nmodel.add(Dense(10, activation='softmax'))  # Size and activation determined by problem; multiclass classification in this case\n\n# Problem setup\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')  # Loss determined by problem, multiclass classification in this case\n\n# Training (with optional early stopping)\nearly_stop = EarlyStopping(monitor='val_loss',patience=2)\nmodel.fit(x_train,y_cat_train,epochs=10,validation_data=(x_test,y_cat_test),callbacks=[early_stop])\n\n# Prediction\nmodel.predict(new_input)  # The new input needs to be shaped and scaled the sane as the training data\n\n\n\nThese are used for modelling sequences with variable lengths of inputs and outputs.\nRecurrent neurons take as their input the current data AND the previous epoch’s output. We could try to pass EVERY previous epoch’s output as an input, but would run into issues of vanushing gradients.\nLSTMs take this idea a step further by incorporating the previous epochs input AND some longer lookback of epoch where some old outputs are “forgotten”.\nA basic RNN:\n\n            --------------------------------\n            |                              |\n H_t-1 ---&gt; |                              |---&gt; H_t\n            |                              |\n X_t   ---&gt; |                              |\n            |                              |\n            --------------------------------\n\nwhere:\nH_t is the neuron's output at current epoch t\nH_t-1 is the neuron's output from the previous epoch t-1\nX_t is the input data at current epoch t\n\nH_t = tanh(W[H_t-1, X_t] + b)\nAn LSTM (Long Short Term Memory) unit:\n\n            --------------------------------\n            |                              |\n H_t-1 ---&gt; |                              |---&gt; H_t\n            |                              |\n C_t-1 ---&gt; |                              |---&gt; C_t\n            |                              |\n X_t   ---&gt; |                              |\n            |                              |\n            --------------------------------\n            \nThe `forget gate` determines which part of the old short-term memory and current input is \"forgotten\" by the new long-term memory.\nThese are a set of weights (between 0 and 1) that get applied to the old long-term memory to downweight it.\nF_t = sigmoid(W_F[H_t-1, X_t] + b_F)\n\nThe `input gate` i_t similarly gates the input and old short-term memory.\nThis will later (in the update gate) get combined  with a candidate value for the new long-term memory `C_candidate_t`\nI_t = sigmoid(W_I[H_t-1, X_t] + b_I)\nC_candidate_t = tanh(W_C_cand[H_t-1, X_t] + b_C_cand) \n\nThe `update gate` for the new long-term memory `C_t` is then calculated as a sum of forgotten old memory and input-weighted candidate memory:\nC_t = F_t*C_t-1 + I_t*C_candidate_t \n\nThe `output gate` O_t is a combination of the old short-term memory and latest input data.\nThis is then combined with the latest long-term memory to produce the output of the recurrent neuron, which is also the updated short-term memory H_t:\nO_t = sigmoid(W_O[H_t-1, X_t] + b_O)\nH_t = O_t * tanh(C_t) \n\nwhere:\nH_t is short-term memory at epoch t\nC_t is long-term memory at epoch t\nX_t is the input data at current epoch t\n\nsigmoid is a sigmoid  activation function\nF_t is an intermediate forget gate weight\nI_t is an intermediate input gate weight\nO_t is an intermediate output gate weight\nThere are several variants of RNNs. RNNs with peepholes\nThis leaks long-term memory into the forget, input and output gates. Note that the forget gate and input gate each get the OLD long-term memory, whereas the output gate gets the NEW long-term memory.\nF_t = sigmoid(W_F[C_t-1, H_t-1, X_t] + b_F)\nI_t = sigmoid(W_I[C_t-1, H_t-1, X_t] + b_I)\nO_t = sigmoid(W_O[C_t, H_t-1, X_t] + b_O)\nGated Recurrent Unit (GRU)\nThis combines the forget and input gates into a single gate. It also has some other changes. This is simpler than a typical LSTM model as it has fewer parameters. This makes it more computationally efficient, and in practice they can have similar performance.\nz_t = sigmoid(W_z[H_t-1, X_t])\nr_t = sigmoid(W_r[H_t-1, X_t])\nH_candidate_t = tanh(W_H_candidate[r_t*h_t-1, x_t])\nH_t = (1 - z_t) * H_t-1 + z_t * H_candidate_t\nThe sequences modelled with RNNs can be:\n\nOne-to-many\nMany-to-many\nMany-to-one\n\nConsiderations for choosing the input sequence length:\n\nIt should be long enough to capture any patterns or seasonality in the data\nThe validation set and test set each need to have a size at least (input_length + output_length), so the longer the input length, the more data is required to be set aside.\n\nThe validation loss for RNNs/LSTMs can be volatile, so allow a higher patience when using early stopping.\nExample model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,LSTM\n\n# Generators for helper functions to chunk up the input series into an array of pairs of (input_batch, target_output)\ntrain_generator = TimeseriesGenerator(scaled_train, scaled_train, length=batch_length, batch_size=1)\nvalidation_generator = TimeseriesGenerator(scaled_test, scaled_test, length=batch_length, batch_size=1)\n\n# Create an LSTM model\nmodel = Sequential()\nmodel.add(LSTM(100, activation='relu', input_shape=(batch_length, n_features)))\nmodel.add(Dense(1))  # Output layer\nmodel.compile(optimizer='adam', loss='mse')  # Problem setup for regression\n\n# Fit the model\nearly_stop = EarlyStopping(monitor='val_loss', patience=10)  # RNNs can be volatile so if use a higher patience with an early stopping callback\nmodel.fit(train_generator, epochs=20, validation_data=validation_generator, callbacks=[early_stop])\n\n# Evaluate test data\ntest_predictions = []\nfinal_batch = scaled_train[-batch_length:, :]\ncurrent_batch = final_batch.reshape((1, batch_length, n_features))\n\nfor test_val in scaled_test:\n    pred_val = model.predict(current_batch)  # These will later need to use the scaler.inverse_transform of the scaler originally used on the training data\n    test_predictions.append(pred_val[0])\n    current_batch = np.append(current_batch[:,1:,:], pred_val.reshape(1,1,1), axis=1)\n\n\n\nCharacter-based generative NLP: given an input string, e.g. [‘h’, ‘e’, ‘l’, ‘l’], predict the sequence shifted by one character, e.g. [‘e’, ‘l’, ‘l’, ‘o’]\nThe embedding, GRU and dense layers work on the sequence in the following way: \nSteps:\n\nRead in text data\nText processing and vectorisation - one-hot encoding\nCreate batches\nCreate the model\nTrain the model\nGenerate new text\n\nStep 1: Read in text data\n\nA corpus of &gt;1 million characters is a good size\nThe more distinct the style of your corpus, the more obvious it will be if your model has worked.\n\nGiven some structured input_text string, we can get the one-hot encoded vocab list of unique characters.\nvocab_list = sorted(set(input_text))\nStep 2: Text processing\nVectorise the text - create a mapping between character and integer. This is the encoding dictionary used to vectorise the corpus.\n# Mappings back and forth between characters and their encoded integers\nchar_to_ind = {char: ind for ind, char in enumerate(vocab_list)}\nind_to_char = {ind: char for ind, char in enumerate(vocab_list)}\n\nencoded_text = np.array([char_to_ind[char] for char in input_text])\nStep 3: Create batches\nThe sequence length should be long enough to capture structure, e.g. for poetry with rhyming couplets it should be the number of characters in 3 lines to capture the rhyme and non-rhyme. Too long a sequence makes the model take longer to train and captures too much historical noise.\nCreate a shuffled dataset where:\n\ninput sequence is the first n characters\noutput sequence is n characters lagged by one\n\nWe want to shuffle these sequence pairs into a random order so the model doesn’t overfit to any section of the text, but can instead generate characters given any seed text.\nseq_len = 120\ntotal_num_seq = len(input_text) // (seq_len + 1)\n\n# Create Training Sequences\nchar_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\nsequences = char_dataset.batch(seq_len + 1, drop_remainder=True)  # drop_remainder drops the last incomplete sequence\n\n# Create tuples of input and output sequences\ndef create_seq_targets(seq):\n    input_seq = seq[:-1]\n    output_seq = seq[-1:]\n    return input_seq, output_seq\ndataset = sequences.map(create_seq_targets)\n\n# Generate training batches\nbatch_size = 128\nbuffer_size = 10000  # Shuffle this amount of batches rather than shuffling the entire dataset in memory\ndataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\nStep 4: Create the model\nSet up the loss function and layers.\nThis model uses 3 layers:\n\nEmbedding\n\nEmbed positive integers, e.g. “a”&lt;-&gt;1, as dense vectors of fixed size. This embedding size is a hyperparameter for the user to decide.\nNumber of layers should be smaller but a similar scale to the vocab size; typically use the power of 2 that is just smaller than the vocab size.\n\nGRU\n\nThis is the main thing to vary in the model: number of hidden layers and number of neurons in each, types of neurons (RNN, LSTM, GRU), etc. Typically use lots of layers here.\n\nDense\n\nOne neuron per character (one-hot encoded), so this produces a probability per character. The user can vary the “temperature”, choosing less probable characters more/less often.\n\n\nLoss function:\n\nSparse categorical cross-entropy\nUse sparse categorical cross-entropy because the classes are mutually exclusive. Use regular categorical cross-entropy when one smaple can have multiple classes, or the labels are soft probabilities.\nSee link in NLP appendix for discussion.\nUse logits=True because vocab input is one hot encoded.\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, GRU\nfrom tensorflow.keras.losses import sparse_categorical_crossentropy\n\n# Hyperparameters\nvocab_size = len(vocab_list)\nembed_dim = 64\nrnn_neurons = 1026\n\n# Create model \nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embed_dim, batch_input_shape=[batch_size, None]))\nmodel.add(GRU(rnn_neurons, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'))\nmodel.add(Dense(vocab_size))\nsparse_cat_loss = lambda y_true,y_pred: sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)  # We want a custom loss function with logits=True\nmodel.compile(optimizer='adam', loss=sparse_cat_loss) \nStep 5: Train the model\nepochs = 30\nmodel.fit(dataset,epochs=epochs)\nStep 6: Generate text\nAllow the model to accept a batch size of 1 to allow us to give an input seed text from which to generate text from.\nWe format the seed text so it can be fed into the network, then loop through generating one character at a time and feeding that back into the model.\nmodel.build(tf.TensorShape([1, None]))\n\ndef generate_text(model, input_seed_text, gen_size=100, temperature=1.0):\n    \"\"\"\n    model: tensorflow.model\n    input_seed_text: str\n    gen_size: int\n    temperature: float\n    \"\"\"\n    generated_text_result = []\n    vectorised_seed = tf.expand_dims([char_to_ind[s] for s in input_seed_text], 0)\n    model.reset_states()\n    \n    for k in range(gen_size):\n        raw_predictions = model(vectorised_seed)\n        predictions = tf.squeeze(raw_predictions, 0) / temperature\n        predicted_index = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n        generated_text_result.append(ind_to_char[predicted_index])\n        \n        # Set a new vectorised seed to generate the next character in the loop\n        vectorised_seed = tf.expand_dims([predicted_index], 0)\n        \n    return (input_seed_text + ''.join(generated_text_result))\n\n\n\nThis is an unsupervised learning problem, therefore evaluation metrics are different. Sometimes called semi-supervised as labels may be used in training the model, but not available when it’s used to predict new values.\nApplications:\n\nDimensionality reduction\nNoise removal\n\nDesigned to reproduce its input in the output layer. Number of input neurons is the same as the number of output layers. The encoder reduces dimensionality to the hidden layer. The hidden layer should contain the most relevant information required to reconstruct the input. The decoder reconstructs a high dimensional output from a low dimensional input.\n\nEncoder: Input layer -&gt; Hidden layer\nDecoder: Hidden layer -&gt; Output\n\nStacked autoencoders include multiple hidden layers.\nAutoencoder for dimensionality reduction\nLet’s try to reduce an input dataset from 3 dimensions to 2. We want to train an autoencoder to go from 3 -&gt; 2 -&gt; 3.\nWhen we scale data, we fit and transform on the entire dataset rather than a train/test split, because there is no ground truth for the “correct” dimensionality reduction.\nUsing SGD in Autoencoders allows the learning rate to be varied as a hyperparameter to affect how well the hidden layer learns. Larger values will train quicker but potentially perform worse.\nTo retrieve the lower dimensionality representation, use just the encoder half of the trained autoencoder to predict the input.\n# Scale data\nscaler = MinMaxScaler()\nscaled_data = scaler.fit_transform(input_features)\n\n# Create model\nencoder = Sequential()\nencoder.add(Dense(units=2, acivation='relu', input_shape=[3]))\n\ndecoder = Sequential()\ndecoder.add(Dense(units=3, acivation='relu', input_shape=[2]))\n\nautoencoder = Sequential([encoder, decoder])\nautoencoder.compile(loss='mse', optimizer=SGD(lr=1.5))\n\n# Train the full autoencoder model\nautoencoder.fit(scaled_data, scaled_data, epochs=5)\n\n# Reduce dimensionality by using just the encoder part of the trained model\nencoded_2dim = encoder.predict(scaled_data)\nAutoencoder for noise removal\nStarting with clean images, we want to train an autoencoder to learn to remove noise. We create a noisy dataset by adding noise to our clean images. We then try to reconstruct that input, and compare to the original clean images as the ground truth.\nStarting with 28x28 images (e.g. MNIST dataset), this gives 784 input features when flattened. Use multiple hidden layers as there are a large number of input features that would be difficult to learn in one layer. The numbers of layers and number of neurons in each are hyperparameters to tune; decreasing in powers of 2 is a good rule of thumb.\nAdd a Gaussian noise layer to train the model to remove noise. Without the Gaussian layer, the model would be used for dimensionality reduction rather than noise removal.\nThe final layer uses a sigmoid activation because it is a binary classification problem - does the output match the input?\nencoder = Sequential()\nencoder.add(Flatten(input_shape=[28, 28]))\nencoder.add(Gaussian(0.2))  # Optional layer if training for noise removal rather than dimensionality reduction\nencoder.add(Dense(400, activation='relu'))\nencoder.add(Dense(200, activation='relu'))\nencoder.add(Dense(100, activation='relu'))\nencoder.add(Dense(50, activation='relu'))\nencoder.add(Dense(25, activation='relu'))\n\ndecoder = Sequential()\ndecoder.add(Dense(50, input_shape=[25], activation='relu'))\ndecoder.add(Dense(100, activation='relu'))\ndecoder.add(Dense(200, activation='relu'))\ndecoder.add(Dense(400, activation='relu'))\ndecoder.add(Dense(28*28, activation='sigmoid'))\n\nautoencoder = Sequential([encoder, decoder])\nautoencoder.compile(loss='binary_crossentropy', optimizer=SGD(lr=1.5), metrics=['accuracy'])\nautoencoder.fit(X_train, X_train, epochs=5)\n\n\n\nUse 2 networks competing against each other to generate data - counterfeiter (generator) vs detective (discriminator).\n\nGenerator: recieves random noise\nDiscriminator: receives data set containing real data and fake generated data, and attempts to calssify real vs fake (binary classificaiton).\n\nTwo training phases, each only training one of the networks:\n\nTrain discriminator - real images (labeled 1) and generated images (labeled 0) are fed to the discriminator network.\nTrain generator - only feed generated images to discriminator, ALL labeled 1.\n\nThe generator never sees real images, it creates images based only off of the gradients flowing back from the discriminator.\nDifficulties with GANs:\n\nTraining resources - GPUs generally required\nMode collapse - generator learns an image that fools the discriminator, then only produces this image. Deep convolutional GANs and mini-batch discrimination are two solution to this problem.\nInstability - True performance is hard to measure; just because the discriminator was fooled, it doesn’t mean that the generated image was actually realistic.\n\nCreating the model\nWe create the generator and discriminator as separate models, then join them into a GAN object. This is analogous to how we joined an encoder and decoder into an autoencoder.\nThe discriminator is trained on a binary classification problem, real or fake, so uses a binary cross entropy loss function. Backpropagation only alters the weights of the discriminator in the first phase, not the generator.\ndiscriminator = Sequential()\n\n# Flatten the image\ndiscriminator.add(Flatten(input_shape=[28,28]))\n\n# Some hidden layers\ndiscriminator.add(Dense(150,activation='relu'))\ndiscriminator.add(Dense(100,activation='relu'))\n\n# Binary classification - real vs fake\ndiscriminator.add(Dense(1,activation=\"sigmoid\"))\ndiscriminator.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\nThe generator is trained in the second phase.\nWe set a codings_size, which is the size of the latent representation from which the generator will create a full image. For example, if we want to create a 28x28 image (784 pixels), we may want to generate this from a 100 element input. This is analogous to the decoder part of an autoencoder.\nThe codings_size should be smaller than the total number of features but large enough so that there is something to learn from.\nThe generator is NOT compiled at this step, it is compiled in the combined GAN later so that it is only trained at that step.\ncodings_size = 100\ngenerator = Sequential()\n\n# Input shape of first layer is the codings_size\ngenerator.add(Dense(150, activation=\"relu\", input_shape=[codings_size]))\n\n# Some hidden layers\ngenerator.add(Dense(200,activation='relu'))\n\n# Output is the size of the image we want to create\ngenerator.add(Dense(784, activation=\"sigmoid\"))\ngenerator.add(Reshape([28,28]))\nThe GAN combines the generator and discriminator.\nThe discriminator is only trained in the first phase, not the second phase.\nGAN = Sequential([generator, discriminator])\ndiscriminator.trainable = False\nGAN.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\nTraining the model\nWe first create batches of images from our input data, similarly to previous models.\nbatch_size = 32  # Size of training input batches\nbuffer_size = 1000  # Don't load the entire dataset into memory\nepochs = 1\n\nraw_dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(buffer_size=buffer_size)\nbatched_dataset = raw_dataset.batch(batch_size, drop_remainder=True).prefetch(1)\nThe training loop for each epoch trains the discriminator to distinguish real vs fake images (binary classification). Then it trains the generator to generate fake images using ONLY the gradients learned in the discriminator training step; the generator NEVER sees any real images.\ngenerator, discriminator = GAN.layers\n\nfor epoch in range(epochs):\n    for index, X_batch in enumerate(batched_dataset):\n        # Phase 1: Train the discriminator\n        noise = tf.random.normal(shape=[batch_size, codings_size])\n        generated_images = generator(noise)\n        real_images = tf.dtypes.cast(X_batch,tf.float32)\n        X_train_discriminator = tf.concat([generated_images, real_images], axis=0)\n        y_train_discriminator = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)  # Targets set to zero for fake _images and 1 for real _images\n        discriminator.trainable = True\n        discriminator.train_on_batch(X_train_discriminator, y_train_discriminator)\n\n        # Phase 2: Train the generator\n        noise = tf.random.normal(shape=[batch_size, codings_size])\n        y_train_generator = tf.constant([[1.]] * batch_size)  # We want discriminator to believe that fake _images are real\n        discriminator.trainable = False\n        GAN.train_on_batch(noise, y_train_generator)    \nDeep convolutional GANs\nThese are GANs which use convolutional layers inside the discriminator and generator models.\nThe models are almost identical to the models above, just with additional Conv2D, Conv2DTranspose and BatchNormalization layers.\nChanges compared to regular GANs:\n\nAdditional convolutional layers in model.\nReshape the training set to match the images.\nRescale the training data to be between -1 and 1 so that tanh activation function works.\nActivation of discriminator output layer is tanh rather than sigmoid.\n\n\n\n\n\n\n\n\nComplete Tensorflow and Keras Udemy course\nIntuition behind neural networks\nThe deep learning bible (with lectures)\nHeuristics for choosing hidden layers\nAlternatives to the deprecated model predict for different classification problems\nMIT course\n\n\n\n\n\nImage kernels explained\nChoosing CNN layers\n\n\n\n\n\nOverview of RNNs\nWikipedia page contains the equations of LSTMs and peepholes\nLSTMs vs GRUs\nWorked example of LSTM\nRNN cheatsheet\n\n\n\n\n\nThe unreasonable effectiveness of RNNs - essay on RNNs applied to NLP\nSparse vs dense categorical crossentropy loss function\n\n\n\n\n\nbuffer_size argument in tensorflow prefetch and shuffle\nMode collapse"
  },
  {
    "objectID": "posts/ml/tensorflow/tensorflow.html#end-end-process",
    "href": "posts/ml/tensorflow/tensorflow.html#end-end-process",
    "title": "Tensorflow Notes",
    "section": "",
    "text": "Data sourcing - train_test_split\nExploratory data analysis - plot distributions, correlations\nData cleaning - drop redundant columns, handle missing data (drop or impute)\nFeature engineering - one hot encoding categories, scaling/normalising numerical values, combining columns, extracting info from columns (e.g. zip code from address)\nModel selection and training - model and hyperparameters\nModel tuning and evaluation metrics - classification: classification_report, confusion matrix, accuracy, recall, F1. Regression: error (RMSE, MAE, etc)\nPredictions"
  },
  {
    "objectID": "posts/ml/tensorflow/tensorflow.html#machine-learning-categories",
    "href": "posts/ml/tensorflow/tensorflow.html#machine-learning-categories",
    "title": "Tensorflow Notes",
    "section": "",
    "text": "Supervised learning\n\nLinear regression\nSVM\nNaive Bayes\nKNN\nRandom forests\n\nUnsupervised learning\n\nDimensionality reduction\nClustering\n\nReinforcement Learning\n\nSupervised learning tasks can be broadly broken into:\n\nRegression\nClassification\n\nSingle class\nMulti class"
  },
  {
    "objectID": "posts/ml/tensorflow/tensorflow.html#deep-learning-models",
    "href": "posts/ml/tensorflow/tensorflow.html#deep-learning-models",
    "title": "Tensorflow Notes",
    "section": "",
    "text": "General idea of a neural network that can then be extended to specific cases, e.g. CNNs and RNNs.\n\nPerceptron: a weighted average of inputs passed through some activation gate function to arrive at an output decision\nNetwork of perceptrons\nActivation function: a non-linear transfer function f(wx+b)\nCost function and gradient descent: convex optimisation of a loss function using sub-gradient descent. The optimizer can be set in the compile method of the model.\nBackpropagation: use chain rule to determine partial gradients of each weight and bias. This means we only need a single forward pass followed by a single backward pass. Contrast this to if we perturbed each weight or bias to determine each partial gradient: in that case, for each epoch we would need to run a forward pass per weight/bias in the network, which is potentially millions!\nDropout: a technique to avoid overfitting by randomly dropping neurons in each epoch.\n\nGeneral structure:\n\nInput layer\nHidden layer(s)\nOutput layer\n\nInput and output layers are determined by the problem:\n\nInput size: number of features in the data\nOutput size number of targets to predict, i.e. one for single class classification or single target regression, or multiple for multiclass (one per class)\nOutput layer activation determined by problem. For single class classification activation='sigmoid', for multiclass classification activation='softmax'\nLoss function determined by problem. For single class classification loss='binary_crossentropy', for multiclass classification loss='categorical_crossentropy'\n\nHidden layers are less well-defined. Some heuristics here.\nVanishing gradients can be an issue for lower layers in particular, where the partial gradients of individual layers can be very small, so when multiplied together in chain rule the gradient is vanishingly small. Exploding gradients are a similar issue but where gradients get increasingly large when multiplied together. Some techniques to rectify vanishing/exploding gradients:\n\nUse different activation functions with larger gradients close to 0 and 1, e.g. leaky ReLU or ELU (exponential linear unit)\nBatch normalisation: scale each gradient by the mean and standard deviation of the batch\nDifferent weight initialisation methods, e.g. Xavier initialisation\n\nExample model outline\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\n\nmodel = Sequential()\n\n# Input layer\nmodel.add(Dense(78, activation='relu'))  # Number of input features\nmodel.add(Dropout(0.2))  # Optional dropout layer after each layer. Omitted for next layers for clarity\n\n# Hidden layers\nmodel.add(Dense(39, activation='relu'))\nmodel.add(Dense(19, activation='relu'))\n\n# Output layer\nmodel.add(Dense(1, activation='sigmoid'))  # Number of target classes (single class in this case)\n\n# Problem setup\nmodel.compile(loss='binary_crossentropy', optimizer='adam')  # Type of problem (single class classification in this case)\n\n# Training\nmodel.fit(\n    x=X_train,\n    y=y_train,\n    batch_size=256,\n    epochs=30,\n    validation_data=(X_test, y_test)\n)\n\n# Prediction\nmodel.predict(new_input)  # The new input needs to be shaped and scaled the sane as the training data\n\n\n\nThese are used for image classification problems where convolutional filters are useful for extracting features from input arrays.\n\nImage kernels/filters\n\nGrayscale 2D arrays\nRGB 3D tensors\n\nConvolutional layers\nPooling layers\n\nGeneral structure:\n\nInput layer\nConvolutional layer\nPooling layer\n(Optionally more pairs of convolutional and pooling layers)\nFlattening layer\nDense hidden layer(s)\nOutput layer\n\nExample model outline\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nmodel = Sequential()\n\n# Convolutional layer followed by pooling. Deeper networks may have multiple pairs of these.\nmodel.add(Conv2D(filters=32, kernel_size=(4,4),input_shape=(28, 28, 1), activation='relu',))  # Input shape determined by input data size\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\n# Flatten _images from 2D to 1D before final layer\nmodel.add(Flatten())\n\n# Dense hidden layer\nmodel.add(Dense(128, activation='relu'))\n\n# Output layer\nmodel.add(Dense(10, activation='softmax'))  # Size and activation determined by problem; multiclass classification in this case\n\n# Problem setup\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')  # Loss determined by problem, multiclass classification in this case\n\n# Training (with optional early stopping)\nearly_stop = EarlyStopping(monitor='val_loss',patience=2)\nmodel.fit(x_train,y_cat_train,epochs=10,validation_data=(x_test,y_cat_test),callbacks=[early_stop])\n\n# Prediction\nmodel.predict(new_input)  # The new input needs to be shaped and scaled the sane as the training data\n\n\n\nThese are used for modelling sequences with variable lengths of inputs and outputs.\nRecurrent neurons take as their input the current data AND the previous epoch’s output. We could try to pass EVERY previous epoch’s output as an input, but would run into issues of vanushing gradients.\nLSTMs take this idea a step further by incorporating the previous epochs input AND some longer lookback of epoch where some old outputs are “forgotten”.\nA basic RNN:\n\n            --------------------------------\n            |                              |\n H_t-1 ---&gt; |                              |---&gt; H_t\n            |                              |\n X_t   ---&gt; |                              |\n            |                              |\n            --------------------------------\n\nwhere:\nH_t is the neuron's output at current epoch t\nH_t-1 is the neuron's output from the previous epoch t-1\nX_t is the input data at current epoch t\n\nH_t = tanh(W[H_t-1, X_t] + b)\nAn LSTM (Long Short Term Memory) unit:\n\n            --------------------------------\n            |                              |\n H_t-1 ---&gt; |                              |---&gt; H_t\n            |                              |\n C_t-1 ---&gt; |                              |---&gt; C_t\n            |                              |\n X_t   ---&gt; |                              |\n            |                              |\n            --------------------------------\n            \nThe `forget gate` determines which part of the old short-term memory and current input is \"forgotten\" by the new long-term memory.\nThese are a set of weights (between 0 and 1) that get applied to the old long-term memory to downweight it.\nF_t = sigmoid(W_F[H_t-1, X_t] + b_F)\n\nThe `input gate` i_t similarly gates the input and old short-term memory.\nThis will later (in the update gate) get combined  with a candidate value for the new long-term memory `C_candidate_t`\nI_t = sigmoid(W_I[H_t-1, X_t] + b_I)\nC_candidate_t = tanh(W_C_cand[H_t-1, X_t] + b_C_cand) \n\nThe `update gate` for the new long-term memory `C_t` is then calculated as a sum of forgotten old memory and input-weighted candidate memory:\nC_t = F_t*C_t-1 + I_t*C_candidate_t \n\nThe `output gate` O_t is a combination of the old short-term memory and latest input data.\nThis is then combined with the latest long-term memory to produce the output of the recurrent neuron, which is also the updated short-term memory H_t:\nO_t = sigmoid(W_O[H_t-1, X_t] + b_O)\nH_t = O_t * tanh(C_t) \n\nwhere:\nH_t is short-term memory at epoch t\nC_t is long-term memory at epoch t\nX_t is the input data at current epoch t\n\nsigmoid is a sigmoid  activation function\nF_t is an intermediate forget gate weight\nI_t is an intermediate input gate weight\nO_t is an intermediate output gate weight\nThere are several variants of RNNs. RNNs with peepholes\nThis leaks long-term memory into the forget, input and output gates. Note that the forget gate and input gate each get the OLD long-term memory, whereas the output gate gets the NEW long-term memory.\nF_t = sigmoid(W_F[C_t-1, H_t-1, X_t] + b_F)\nI_t = sigmoid(W_I[C_t-1, H_t-1, X_t] + b_I)\nO_t = sigmoid(W_O[C_t, H_t-1, X_t] + b_O)\nGated Recurrent Unit (GRU)\nThis combines the forget and input gates into a single gate. It also has some other changes. This is simpler than a typical LSTM model as it has fewer parameters. This makes it more computationally efficient, and in practice they can have similar performance.\nz_t = sigmoid(W_z[H_t-1, X_t])\nr_t = sigmoid(W_r[H_t-1, X_t])\nH_candidate_t = tanh(W_H_candidate[r_t*h_t-1, x_t])\nH_t = (1 - z_t) * H_t-1 + z_t * H_candidate_t\nThe sequences modelled with RNNs can be:\n\nOne-to-many\nMany-to-many\nMany-to-one\n\nConsiderations for choosing the input sequence length:\n\nIt should be long enough to capture any patterns or seasonality in the data\nThe validation set and test set each need to have a size at least (input_length + output_length), so the longer the input length, the more data is required to be set aside.\n\nThe validation loss for RNNs/LSTMs can be volatile, so allow a higher patience when using early stopping.\nExample model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,LSTM\n\n# Generators for helper functions to chunk up the input series into an array of pairs of (input_batch, target_output)\ntrain_generator = TimeseriesGenerator(scaled_train, scaled_train, length=batch_length, batch_size=1)\nvalidation_generator = TimeseriesGenerator(scaled_test, scaled_test, length=batch_length, batch_size=1)\n\n# Create an LSTM model\nmodel = Sequential()\nmodel.add(LSTM(100, activation='relu', input_shape=(batch_length, n_features)))\nmodel.add(Dense(1))  # Output layer\nmodel.compile(optimizer='adam', loss='mse')  # Problem setup for regression\n\n# Fit the model\nearly_stop = EarlyStopping(monitor='val_loss', patience=10)  # RNNs can be volatile so if use a higher patience with an early stopping callback\nmodel.fit(train_generator, epochs=20, validation_data=validation_generator, callbacks=[early_stop])\n\n# Evaluate test data\ntest_predictions = []\nfinal_batch = scaled_train[-batch_length:, :]\ncurrent_batch = final_batch.reshape((1, batch_length, n_features))\n\nfor test_val in scaled_test:\n    pred_val = model.predict(current_batch)  # These will later need to use the scaler.inverse_transform of the scaler originally used on the training data\n    test_predictions.append(pred_val[0])\n    current_batch = np.append(current_batch[:,1:,:], pred_val.reshape(1,1,1), axis=1)\n\n\n\nCharacter-based generative NLP: given an input string, e.g. [‘h’, ‘e’, ‘l’, ‘l’], predict the sequence shifted by one character, e.g. [‘e’, ‘l’, ‘l’, ‘o’]\nThe embedding, GRU and dense layers work on the sequence in the following way: \nSteps:\n\nRead in text data\nText processing and vectorisation - one-hot encoding\nCreate batches\nCreate the model\nTrain the model\nGenerate new text\n\nStep 1: Read in text data\n\nA corpus of &gt;1 million characters is a good size\nThe more distinct the style of your corpus, the more obvious it will be if your model has worked.\n\nGiven some structured input_text string, we can get the one-hot encoded vocab list of unique characters.\nvocab_list = sorted(set(input_text))\nStep 2: Text processing\nVectorise the text - create a mapping between character and integer. This is the encoding dictionary used to vectorise the corpus.\n# Mappings back and forth between characters and their encoded integers\nchar_to_ind = {char: ind for ind, char in enumerate(vocab_list)}\nind_to_char = {ind: char for ind, char in enumerate(vocab_list)}\n\nencoded_text = np.array([char_to_ind[char] for char in input_text])\nStep 3: Create batches\nThe sequence length should be long enough to capture structure, e.g. for poetry with rhyming couplets it should be the number of characters in 3 lines to capture the rhyme and non-rhyme. Too long a sequence makes the model take longer to train and captures too much historical noise.\nCreate a shuffled dataset where:\n\ninput sequence is the first n characters\noutput sequence is n characters lagged by one\n\nWe want to shuffle these sequence pairs into a random order so the model doesn’t overfit to any section of the text, but can instead generate characters given any seed text.\nseq_len = 120\ntotal_num_seq = len(input_text) // (seq_len + 1)\n\n# Create Training Sequences\nchar_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\nsequences = char_dataset.batch(seq_len + 1, drop_remainder=True)  # drop_remainder drops the last incomplete sequence\n\n# Create tuples of input and output sequences\ndef create_seq_targets(seq):\n    input_seq = seq[:-1]\n    output_seq = seq[-1:]\n    return input_seq, output_seq\ndataset = sequences.map(create_seq_targets)\n\n# Generate training batches\nbatch_size = 128\nbuffer_size = 10000  # Shuffle this amount of batches rather than shuffling the entire dataset in memory\ndataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\nStep 4: Create the model\nSet up the loss function and layers.\nThis model uses 3 layers:\n\nEmbedding\n\nEmbed positive integers, e.g. “a”&lt;-&gt;1, as dense vectors of fixed size. This embedding size is a hyperparameter for the user to decide.\nNumber of layers should be smaller but a similar scale to the vocab size; typically use the power of 2 that is just smaller than the vocab size.\n\nGRU\n\nThis is the main thing to vary in the model: number of hidden layers and number of neurons in each, types of neurons (RNN, LSTM, GRU), etc. Typically use lots of layers here.\n\nDense\n\nOne neuron per character (one-hot encoded), so this produces a probability per character. The user can vary the “temperature”, choosing less probable characters more/less often.\n\n\nLoss function:\n\nSparse categorical cross-entropy\nUse sparse categorical cross-entropy because the classes are mutually exclusive. Use regular categorical cross-entropy when one smaple can have multiple classes, or the labels are soft probabilities.\nSee link in NLP appendix for discussion.\nUse logits=True because vocab input is one hot encoded.\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, GRU\nfrom tensorflow.keras.losses import sparse_categorical_crossentropy\n\n# Hyperparameters\nvocab_size = len(vocab_list)\nembed_dim = 64\nrnn_neurons = 1026\n\n# Create model \nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embed_dim, batch_input_shape=[batch_size, None]))\nmodel.add(GRU(rnn_neurons, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'))\nmodel.add(Dense(vocab_size))\nsparse_cat_loss = lambda y_true,y_pred: sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)  # We want a custom loss function with logits=True\nmodel.compile(optimizer='adam', loss=sparse_cat_loss) \nStep 5: Train the model\nepochs = 30\nmodel.fit(dataset,epochs=epochs)\nStep 6: Generate text\nAllow the model to accept a batch size of 1 to allow us to give an input seed text from which to generate text from.\nWe format the seed text so it can be fed into the network, then loop through generating one character at a time and feeding that back into the model.\nmodel.build(tf.TensorShape([1, None]))\n\ndef generate_text(model, input_seed_text, gen_size=100, temperature=1.0):\n    \"\"\"\n    model: tensorflow.model\n    input_seed_text: str\n    gen_size: int\n    temperature: float\n    \"\"\"\n    generated_text_result = []\n    vectorised_seed = tf.expand_dims([char_to_ind[s] for s in input_seed_text], 0)\n    model.reset_states()\n    \n    for k in range(gen_size):\n        raw_predictions = model(vectorised_seed)\n        predictions = tf.squeeze(raw_predictions, 0) / temperature\n        predicted_index = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n        generated_text_result.append(ind_to_char[predicted_index])\n        \n        # Set a new vectorised seed to generate the next character in the loop\n        vectorised_seed = tf.expand_dims([predicted_index], 0)\n        \n    return (input_seed_text + ''.join(generated_text_result))\n\n\n\nThis is an unsupervised learning problem, therefore evaluation metrics are different. Sometimes called semi-supervised as labels may be used in training the model, but not available when it’s used to predict new values.\nApplications:\n\nDimensionality reduction\nNoise removal\n\nDesigned to reproduce its input in the output layer. Number of input neurons is the same as the number of output layers. The encoder reduces dimensionality to the hidden layer. The hidden layer should contain the most relevant information required to reconstruct the input. The decoder reconstructs a high dimensional output from a low dimensional input.\n\nEncoder: Input layer -&gt; Hidden layer\nDecoder: Hidden layer -&gt; Output\n\nStacked autoencoders include multiple hidden layers.\nAutoencoder for dimensionality reduction\nLet’s try to reduce an input dataset from 3 dimensions to 2. We want to train an autoencoder to go from 3 -&gt; 2 -&gt; 3.\nWhen we scale data, we fit and transform on the entire dataset rather than a train/test split, because there is no ground truth for the “correct” dimensionality reduction.\nUsing SGD in Autoencoders allows the learning rate to be varied as a hyperparameter to affect how well the hidden layer learns. Larger values will train quicker but potentially perform worse.\nTo retrieve the lower dimensionality representation, use just the encoder half of the trained autoencoder to predict the input.\n# Scale data\nscaler = MinMaxScaler()\nscaled_data = scaler.fit_transform(input_features)\n\n# Create model\nencoder = Sequential()\nencoder.add(Dense(units=2, acivation='relu', input_shape=[3]))\n\ndecoder = Sequential()\ndecoder.add(Dense(units=3, acivation='relu', input_shape=[2]))\n\nautoencoder = Sequential([encoder, decoder])\nautoencoder.compile(loss='mse', optimizer=SGD(lr=1.5))\n\n# Train the full autoencoder model\nautoencoder.fit(scaled_data, scaled_data, epochs=5)\n\n# Reduce dimensionality by using just the encoder part of the trained model\nencoded_2dim = encoder.predict(scaled_data)\nAutoencoder for noise removal\nStarting with clean images, we want to train an autoencoder to learn to remove noise. We create a noisy dataset by adding noise to our clean images. We then try to reconstruct that input, and compare to the original clean images as the ground truth.\nStarting with 28x28 images (e.g. MNIST dataset), this gives 784 input features when flattened. Use multiple hidden layers as there are a large number of input features that would be difficult to learn in one layer. The numbers of layers and number of neurons in each are hyperparameters to tune; decreasing in powers of 2 is a good rule of thumb.\nAdd a Gaussian noise layer to train the model to remove noise. Without the Gaussian layer, the model would be used for dimensionality reduction rather than noise removal.\nThe final layer uses a sigmoid activation because it is a binary classification problem - does the output match the input?\nencoder = Sequential()\nencoder.add(Flatten(input_shape=[28, 28]))\nencoder.add(Gaussian(0.2))  # Optional layer if training for noise removal rather than dimensionality reduction\nencoder.add(Dense(400, activation='relu'))\nencoder.add(Dense(200, activation='relu'))\nencoder.add(Dense(100, activation='relu'))\nencoder.add(Dense(50, activation='relu'))\nencoder.add(Dense(25, activation='relu'))\n\ndecoder = Sequential()\ndecoder.add(Dense(50, input_shape=[25], activation='relu'))\ndecoder.add(Dense(100, activation='relu'))\ndecoder.add(Dense(200, activation='relu'))\ndecoder.add(Dense(400, activation='relu'))\ndecoder.add(Dense(28*28, activation='sigmoid'))\n\nautoencoder = Sequential([encoder, decoder])\nautoencoder.compile(loss='binary_crossentropy', optimizer=SGD(lr=1.5), metrics=['accuracy'])\nautoencoder.fit(X_train, X_train, epochs=5)\n\n\n\nUse 2 networks competing against each other to generate data - counterfeiter (generator) vs detective (discriminator).\n\nGenerator: recieves random noise\nDiscriminator: receives data set containing real data and fake generated data, and attempts to calssify real vs fake (binary classificaiton).\n\nTwo training phases, each only training one of the networks:\n\nTrain discriminator - real images (labeled 1) and generated images (labeled 0) are fed to the discriminator network.\nTrain generator - only feed generated images to discriminator, ALL labeled 1.\n\nThe generator never sees real images, it creates images based only off of the gradients flowing back from the discriminator.\nDifficulties with GANs:\n\nTraining resources - GPUs generally required\nMode collapse - generator learns an image that fools the discriminator, then only produces this image. Deep convolutional GANs and mini-batch discrimination are two solution to this problem.\nInstability - True performance is hard to measure; just because the discriminator was fooled, it doesn’t mean that the generated image was actually realistic.\n\nCreating the model\nWe create the generator and discriminator as separate models, then join them into a GAN object. This is analogous to how we joined an encoder and decoder into an autoencoder.\nThe discriminator is trained on a binary classification problem, real or fake, so uses a binary cross entropy loss function. Backpropagation only alters the weights of the discriminator in the first phase, not the generator.\ndiscriminator = Sequential()\n\n# Flatten the image\ndiscriminator.add(Flatten(input_shape=[28,28]))\n\n# Some hidden layers\ndiscriminator.add(Dense(150,activation='relu'))\ndiscriminator.add(Dense(100,activation='relu'))\n\n# Binary classification - real vs fake\ndiscriminator.add(Dense(1,activation=\"sigmoid\"))\ndiscriminator.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\nThe generator is trained in the second phase.\nWe set a codings_size, which is the size of the latent representation from which the generator will create a full image. For example, if we want to create a 28x28 image (784 pixels), we may want to generate this from a 100 element input. This is analogous to the decoder part of an autoencoder.\nThe codings_size should be smaller than the total number of features but large enough so that there is something to learn from.\nThe generator is NOT compiled at this step, it is compiled in the combined GAN later so that it is only trained at that step.\ncodings_size = 100\ngenerator = Sequential()\n\n# Input shape of first layer is the codings_size\ngenerator.add(Dense(150, activation=\"relu\", input_shape=[codings_size]))\n\n# Some hidden layers\ngenerator.add(Dense(200,activation='relu'))\n\n# Output is the size of the image we want to create\ngenerator.add(Dense(784, activation=\"sigmoid\"))\ngenerator.add(Reshape([28,28]))\nThe GAN combines the generator and discriminator.\nThe discriminator is only trained in the first phase, not the second phase.\nGAN = Sequential([generator, discriminator])\ndiscriminator.trainable = False\nGAN.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\nTraining the model\nWe first create batches of images from our input data, similarly to previous models.\nbatch_size = 32  # Size of training input batches\nbuffer_size = 1000  # Don't load the entire dataset into memory\nepochs = 1\n\nraw_dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(buffer_size=buffer_size)\nbatched_dataset = raw_dataset.batch(batch_size, drop_remainder=True).prefetch(1)\nThe training loop for each epoch trains the discriminator to distinguish real vs fake images (binary classification). Then it trains the generator to generate fake images using ONLY the gradients learned in the discriminator training step; the generator NEVER sees any real images.\ngenerator, discriminator = GAN.layers\n\nfor epoch in range(epochs):\n    for index, X_batch in enumerate(batched_dataset):\n        # Phase 1: Train the discriminator\n        noise = tf.random.normal(shape=[batch_size, codings_size])\n        generated_images = generator(noise)\n        real_images = tf.dtypes.cast(X_batch,tf.float32)\n        X_train_discriminator = tf.concat([generated_images, real_images], axis=0)\n        y_train_discriminator = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)  # Targets set to zero for fake _images and 1 for real _images\n        discriminator.trainable = True\n        discriminator.train_on_batch(X_train_discriminator, y_train_discriminator)\n\n        # Phase 2: Train the generator\n        noise = tf.random.normal(shape=[batch_size, codings_size])\n        y_train_generator = tf.constant([[1.]] * batch_size)  # We want discriminator to believe that fake _images are real\n        discriminator.trainable = False\n        GAN.train_on_batch(noise, y_train_generator)    \nDeep convolutional GANs\nThese are GANs which use convolutional layers inside the discriminator and generator models.\nThe models are almost identical to the models above, just with additional Conv2D, Conv2DTranspose and BatchNormalization layers.\nChanges compared to regular GANs:\n\nAdditional convolutional layers in model.\nReshape the training set to match the images.\nRescale the training data to be between -1 and 1 so that tanh activation function works.\nActivation of discriminator output layer is tanh rather than sigmoid."
  },
  {
    "objectID": "posts/ml/tensorflow/tensorflow.html#a.-appendix",
    "href": "posts/ml/tensorflow/tensorflow.html#a.-appendix",
    "title": "Tensorflow Notes",
    "section": "",
    "text": "Complete Tensorflow and Keras Udemy course\nIntuition behind neural networks\nThe deep learning bible (with lectures)\nHeuristics for choosing hidden layers\nAlternatives to the deprecated model predict for different classification problems\nMIT course\n\n\n\n\n\nImage kernels explained\nChoosing CNN layers\n\n\n\n\n\nOverview of RNNs\nWikipedia page contains the equations of LSTMs and peepholes\nLSTMs vs GRUs\nWorked example of LSTM\nRNN cheatsheet\n\n\n\n\n\nThe unreasonable effectiveness of RNNs - essay on RNNs applied to NLP\nSparse vs dense categorical crossentropy loss function\n\n\n\n\n\nbuffer_size argument in tensorflow prefetch and shuffle\nMode collapse"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter5/lesson.html#the-goal",
    "href": "posts/ml/hands_on_llm/chapter5/lesson.html#the-goal",
    "title": "Hands-On LLMs: Text Clustering and Topic Modeling",
    "section": "1.1 The goal",
    "text": "1.1 The goal\nText clustering is an unsupervised technique that aims to group similar texts based on their content, meaning and relationships.\n\n\n\nClusters in embedding space\n\n\nThe clusters can the be used for applications such as outlier detection, speeding up labelling and finding mislabelled data.\nIt also has applications in topic modelling, where we assign a label or keywords to a cluster describing its constituents.\nWe apply this to an example data set of Arxiv articles. Let’s load the Arxiv data we’ll use for this:\n\nfrom datasets import load_dataset\n\n\ndataset = load_dataset(\"maartengr/arxiv_nlp\")[\"train\"]\nabstracts = dataset[\"Abstracts\"]\ntitles = dataset[\"Titles\"]"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter5/lesson.html#text-clustering-pipeline",
    "href": "posts/ml/hands_on_llm/chapter5/lesson.html#text-clustering-pipeline",
    "title": "Hands-On LLMs: Text Clustering and Topic Modeling",
    "section": "1.2. Text clustering pipeline",
    "text": "1.2. Text clustering pipeline\nThere are many approaches to text clustering, including GNNs and centroid-based clustering. A common approach is:\n\nConvert input documents to embeddings, using an embedding model\nReduce the dimensionality of those embeddings, using a dimensionality reduction model\nGroup similar documents, using a cluster model\n\n\n1.2.1. The embedding model\nWe convert our text to embedding vectors using an embedding model. We should choose one that was trained to optimise semantic similarity (which most are). We can use the MTEB leaderboard to help select a good model.\nWe can load a pre-trained model and create our embeddings.\n\nfrom sentence_transformers import SentenceTransformer \n\nembedding_model = SentenceTransformer(\"thenlper/gte-small\") \nembeddings = embedding_model.encode(abstracts, show_progress_bar=True)\n\n\n\n\n\n\n\n\n\n\n/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.2.2. The dimensionality reduction model\nHigh-dimensional data can suffer from the curse of dimensionality, making it difficult to find meaningful clusters.\nWe can use a dimensionality reduction model to compress (not remove) dimensions which makes the downstream clustering easier.\nThis is, by it’s nature, a lossy transformation. But we hope that enough of the information is retained to be useful.\nStandard dimensionality reduction techniques include Principal Component Analysis (PCA) and Uniform Manifold Approximation (UMAP). We’ll use UMAP as it tends to handle nonlinear relationships better.\nThe following code reduces our embeddings from 384 -&gt; 5 dimensions. We set min_dist=0 as this allows embedded points to be arbitrarily close together, which results in tighter clusters, and metric=‘cosine’ generally performs better than Euclidean methods for high-dimensional data.\n\nfrom umap import UMAP\n\numap_model = UMAP(n_components=5, min_dist=0.0, metric='cosine', random_state=42)\nreduced_embeddings = umap_model.fit_transform(embeddings)\n\n/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n  warn(\nOMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n\n\n\n\n1.2.3. The cluster model\nCentroid-based algorithms like K-Nearest Neighbours (KNN) are popular in other settings but require us to specify the number of clusters ahead of time (which we don’t know) and forces all data points to be part of a cluster (there can’t be unassigned points). This makes them less useful for our use case.\nDensity-based algorithms calculate the number of clusters freely and do not force all points into a cluster. We’ll use HDBSCAN for our case.\n\n\n\nCentroid vs density approaches\n\n\nWe can cluster the data with the following code. We can vary min_cluster_size to change the number of clusters produced.\n\nfrom hdbscan import HDBSCAN \n\nhdbscan_model = HDBSCAN(min_cluster_size=50, metric=\"euclidean\", cluster_selection_method=\"eom\").fit(reduced_embeddings)\nclusters = hdbscan_model.labels_\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\nWe can inspect the clusters and plot the data in the reduced dimension space. Although helpful, it’s worth remembering that this is just an approximation of the real embeddings; some information is lost.\nFirst, let’s observe a selection of 3 documents from the first cluster to see if they seem similar:\n\nimport numpy as np \n\n# Print first three documents in cluster 0 \ncluster = 0 \nfor index in np.where(clusters==cluster)[0][:3]:\n    print(abstracts[index][:300] + \"... \\n\")\n\n  This works aims to design a statistical machine translation from English text\nto American Sign Language (ASL). The system is based on Moses tool with some\nmodifications and the results are synthesized through a 3D avatar for\ninterpretation. First, we translate the input text to gloss, a written fo... \n\n  Researches on signed languages still strongly dissociate lin- guistic issues\nrelated on phonological and phonetic aspects, and gesture studies for\nrecognition and synthesis purposes. This paper focuses on the imbrication of\nmotion and meaning for the analysis, synthesis and evaluation of sign lang... \n\n  Modern computational linguistic software cannot produce important aspects of\nsign language translation. Using some researches we deduce that the majority of\nautomatic sign language translation systems ignore many aspects when they\ngenerate animation; therefore the interpretation lost the truth inf... \n\n\n\nThey do! Now we can plot the data in the reduced embedding space:\n\nimport pandas as pd \n\n# Reduce 384-dimensional embeddings to two dimensions for easier visualization \nreduced_embeddings = UMAP(n_components=2, min_dist=0.0, metric=\"cosine\", random_state=42 ).fit_transform(embeddings) \n\ndf = pd.DataFrame(reduced_embeddings, columns=[\"x\", \"y\"]) \ndf[\"title\"] = titles \ndf[\"cluster\"] = [str(c) for c in clusters] \n\n# Select outliers and non-outliers (clusters) \nto_plot = df.loc[df.cluster != \"-1\", :] \noutliers = df.loc[df.cluster == \"-1\", :]\n\n/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n  warn(\n\n\n\nimport matplotlib.pyplot as plt \n\n# Plot outliers and non-outliers separately \nplt.scatter(outliers.x, outliers.y, alpha=0.05, s=2, c=\"grey\") \nplt.scatter(to_plot.x, to_plot.y, c=to_plot.cluster.astype(int), alpha=0.6, s=2, cmap=\"tab20b\" ) \nplt.axis(\"off\")\n\n(-7.562705826759339,\n 10.960084271430969,\n -3.4470335602760316,\n 18.276195919513704)"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter5/lesson.html#bertopic",
    "href": "posts/ml/hands_on_llm/chapter5/lesson.html#bertopic",
    "title": "Hands-On LLMs: Text Clustering and Topic Modeling",
    "section": "2.1. BERTopic",
    "text": "2.1. BERTopic\nThe first step is to perform text clustering using the same 3 steps outlined in the previous section.\nWe then use a bag-of-words approach per cluster (instead of per document as would usually be the case) to model a distribution over words per class. This is the CountVectorizer step.\nWe similarly use a class-specific variant of term-frequency inverse document-frequency called c-TF-IDF, which puts more weight on the meaningful words of that cluster.\nWe now have a generic text clustering pipeline:\n\n\n\n\n\nflowchart LR\n\n  A(Embeddings) --&gt; B(Dimensionality Reduction) --&gt; C(Clustering)\n\n\n\n\n\n\nAnd a topic modeling pipeline:\n\n\n\n\n\nflowchart LR\n\n  D(Cluster Bag-of-Words) --&gt; E(Keyword Selection)\n\n\n\n\n\n\nPutting this all together with our choice of components:\n\n\n\n\n\nflowchart LR\n\n  A(SBERT) --&gt; B(UMAP) --&gt; C(HDBSCAN) --&gt; D(CountVectorizer) --&gt; E(c-TF-IDF)\n\n\n\n\n\n\nThe idea behind BERTopic is that these components are modular, so each can be swapped out like lego blocks. For example, if you prefer K-means clusters or PCA over UMAP, just swap it.\n\n\n\nBERTopic modular components\n\n\nThis modularity also means the same base model can be used and adapted for different tasks and use cases by adding/removing components downstream of the base model.\n\n2.1.1. Create a BERTopic model\nWe can run this end-to-end pipeline using the previously defined models in BERTopic:\n\nfrom bertopic import BERTopic \n\n# Train our BERTopic model with our previously defined component models \ntopic_model = (BERTopic(embedding_model=embedding_model, \n                        umap_model=umap_model,\n                        hdbscan_model=hdbscan_model,\n                        verbose=True)\n               .fit(abstracts, embeddings))\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n2025-01-15 11:48:19,327 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n2025-01-15 11:48:47,848 - BERTopic - Dimensionality - Completed ✓\n2025-01-15 11:48:47,851 - BERTopic - Cluster - Start clustering the reduced embeddings\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n2025-01-15 11:48:49,745 - BERTopic - Cluster - Completed ✓\n2025-01-15 11:48:49,757 - BERTopic - Representation - Extracting topics from clusters using representation models.\n2025-01-15 11:48:51,583 - BERTopic - Representation - Completed ✓\n\n\n\n\n2.1.2. Explore the topics\nWe can then explore the topics found by the model.\nNote that the topic labelled -1 is a bucket for outliers that do not fit in any other cluster.\n\ntopic_model.get_topic_info()\n\n\n\n\n\n\n\n\nTopic\nCount\nName\nRepresentation\nRepresentative_Docs\n\n\n\n\n0\n-1\n13779\n-1_of_the_and_to\n[of, the, and, to, in, we, language, that, for...\n[ Language models have emerged as a central c...\n\n\n1\n0\n2224\n0_speech_asr_recognition_end\n[speech, asr, recognition, end, acoustic, spea...\n[ The amount of labeled data to train models ...\n\n\n2\n1\n2104\n1_question_qa_questions_answer\n[question, qa, questions, answer, answering, a...\n[ Multi-hop question answering (QA) requires ...\n\n\n3\n2\n1428\n2_medical_clinical_biomedical_patient\n[medical, clinical, biomedical, patient, healt...\n[ Clinical texts, such as admission notes, di...\n\n\n4\n3\n986\n3_translation_nmt_machine_neural\n[translation, nmt, machine, neural, bleu, engl...\n[ In this paper, we introduce a hybrid search...\n\n\n...\n...\n...\n...\n...\n...\n\n\n146\n145\n54\n145_gans_gan_adversarial_generation\n[gans, gan, adversarial, generation, generativ...\n[ Text generation is of particular interest i...\n\n\n147\n146\n54\n146_emoji_emojis_emoticons_sentiment\n[emoji, emojis, emoticons, sentiment, twitter,...\n[ The frequent use of Emojis on social media ...\n\n\n148\n147\n51\n147_prompt_prompts_optimization_prompting\n[prompt, prompts, optimization, prompting, llm...\n[ Prompt optimization aims to find the best p...\n\n\n149\n148\n51\n148_coherence_discourse_paragraph_text\n[coherence, discourse, paragraph, text, cohesi...\n[ While there has been significant progress t...\n\n\n150\n149\n51\n149_long_context_window_length\n[long, context, window, length, llms, memory, ...\n[ We present a series of long-context LLMs th...\n\n\n\n\n151 rows × 5 columns\n\n\n\nWe can explore a particular topic by its topic number:\n\ntopic_model.get_topic(2)\n\n[('medical', 0.0220337946328463),\n ('clinical', 0.02092442350104087),\n ('biomedical', 0.014552038344966458),\n ('patient', 0.010048801098837407),\n ('health', 0.008769124731484461),\n ('notes', 0.008421182820081155),\n ('patients', 0.0067969193810322485),\n ('healthcare', 0.0067470745955792765),\n ('and', 0.006483211946307094),\n ('drug', 0.006111735386306484)]\n\n\nWe can also search for clusters which match a given search term:\n\ntopic_model.find_topics(\"rocket science\")\n\n([131, 46, 28, -1, 9], [0.8482957, 0.8474297, 0.8343923, 0.8332416, 0.8269581])\n\n\nTopic number 131 allegedly matches the term, so we can look closer at this topic:\n\ntopic_model.get_topic(131)\n\n[('materials', 0.050279482237225254),\n ('science', 0.02243336305054669),\n ('chemistry', 0.0215702079363354),\n ('chemical', 0.019510674137408444),\n ('scientific', 0.019096261213199146),\n ('material', 0.01734997997000861),\n ('synthesis', 0.013922383987668636),\n ('literature', 0.011377588070962407),\n ('reaction', 0.010392948527677913),\n ('extraction', 0.009880316014163601)]\n\n\n\n\n2.1.3. Visualise the topics\nAs we did in the “manual” example, we can visualise the text clusters. The library provides a handy convenience method for interactive plotting.\n\nfig = topic_model.visualize_documents( titles, reduced_embeddings=reduced_embeddings, width=1200, hide_annotations=True ) \nfig.update_layout(font=dict(size=16))\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\n\nCluster embedding space\n\n\nWe can also plot keywords per topic:\n\ntopic_model.visualize_barchart()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\n\nTopic word scores\n\n\nWe can plot the similarity between topics as a heatmap:\n\ntopic_model.visualize_heatmap(n_clusters=30)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\n\nTopic similarity heatmap\n\n\nWe can also see the hierarchies within topics:\n\ntopic_model.visualize_hierarchy()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\n\nTopic hierarchies"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter5/lesson.html#re-ranking",
    "href": "posts/ml/hands_on_llm/chapter5/lesson.html#re-ranking",
    "title": "Hands-On LLMs: Text Clustering and Topic Modeling",
    "section": "2.2. Re-ranking",
    "text": "2.2. Re-ranking\nThe pipeline so far relied on the bag-of-words (BoW) approach to identify key words. This is fast, but does not take the semantic structure of a sentence into account.\nWe could swap the bag-of-words “lego block” for something more sophisticated. Another approach is to instead keep it as is but add an extra “re-ranker” block at the end to fine-tune the ordering of the keywords. This can be a slower algorithm, but it only processes the list of words identified by BoW for each topic (tens or hundreds), not the entire dcoument corpus (millions or more).\nThese re-rankers are referred to as representation models.\nSo our overall pipeline is extended to:\n\n\n\n\n\nflowchart LR\n\n  A(Embeddings) --&gt; B(Dimensionality Reduction) --&gt; C(Clustering)  --&gt; D(Tokenization) --&gt; E(Topic Modeling) --&gt; F(Reranker)\n\n\n\n\n\n\n\n\n\nRe-ranker block\n\n\n\n2.2.1. KeyBERTInspired\nThe idea behind this apporach is to use the similarity between embedding and words vectors to give a score, then order key words by their match score.\n\nAverage document embedding: Calculate embeddings for each document, then average\nCalculate embeddings for each keyword\nCalculate cosine similarity between each keyword and the average document embedding\nOrder by the most similar\n\n\n\n\nKeyBERTInspired\n\n\nAs well as reranking, KeyBERTInspired is effective at removing stopwords.\nWe will save the representations from the previousl model so we can compare them to the re-ranked versions:\n\n# Save original representations \nfrom copy import deepcopy \n\noriginal_topics = deepcopy(topic_model.topic_representations_)\n\nThe following convenience function helps to compare topics between the original and reranked versions.\n\ndef topic_differences(model, original_topics, num_topics=5):\n    \"\"\"Show the differences in topic representations between two models\"\"\"\n    topic_words = []\n    for topic in range(num_topics): \n        # Extract top 4 words per topic per model \n        og_words = \" | \".join(list(zip(*original_topics[topic])) [0][:4]) \n        reranked_words = \" | \".join(list(zip(*model.get_topic(topic))) [0][:4]) \n        topic_words.append((topic, og_words, reranked_words,))\n    \n    return pd.DataFrame(columns=[\"Topic\", \"Original\", \"Reranked\"], data=topic_words)\n\n\nfrom bertopic.representation import KeyBERTInspired \n\n# Update our topic representations using KeyBERTInspired \nrepresentation_model = KeyBERTInspired() \ntopic_model.update_topics(abstracts, representation_model=representation_model)\n\nWe can observe the topic differences:\n\ntopic_differences(topic_model, original_topics, num_topics=10)\n\n\n\n\n\n\n\n\nTopic\nOriginal\nReranked\n\n\n\n\n0\n0\nspeech | asr | recognition | end\ntranscription | phonetic | speech | language\n\n\n1\n1\nquestion | qa | questions | answer\nanswering | comprehension | retrieval | questions\n\n\n2\n2\nmedical | clinical | biomedical | patient\nnlp | clinical | text | language\n\n\n3\n3\ntranslation | nmt | machine | neural\ntranslation | translate | translations | multi...\n\n\n4\n4\nsummarization | summaries | summary | abstractive\nsummarization | summarizers | summaries | abst...\n\n\n5\n5\nhate | offensive | speech | detection\nhate | hateful | language | cyberbullying\n\n\n6\n6\ngender | bias | biases | debiasing\ngendered | gender | bias | biases\n\n\n7\n7\nrelation | extraction | relations | re\nrelation | relations | relational | extracting\n\n\n8\n8\nner | entity | named | recognition\nentity | entities | labeled | name\n\n\n9\n9\nagents | agent | game | games\nreasoning | ai | game | agents\n\n\n\n\n\n\n\nThere is still some redundancy in the re-ranked topics. For example, translation | translate | translations and relation | relations | relational. The next approach seeks to address this…\n\n\n2.2.2. Maximal Marginal Relevance\nMaximal Marginal Relevance (MMR) can be used to diversify the topic keywords, so that we do not end up with translation | translate | translations. It attempts to find keywords that are diverse from one another but still related to the topic.\nThe approach is to start with a set of candidate keywords and iteratively add the next best keyword that is “diverse enough” according to a user-defined diversity parameter.\n\nfrom bertopic.representation import MaximalMarginalRelevance \n\n# Update our topic representations to MaximalMarginalRelevance \nrepresentation_model = MaximalMarginalRelevance(diversity=0.2) \ntopic_model.update_topics(abstracts, representation_model=representation_model)\n\nAgain we can plot the topic differences and see the effect of MMR reranking:\n\ntopic_differences(topic_model, original_topics, num_topics=10)\n\n\n\n\n\n\n\n\nTopic\nOriginal\nReranked\n\n\n\n\n0\n0\nspeech | asr | recognition | end\nspeech | asr | audio | error\n\n\n1\n1\nquestion | qa | questions | answer\nquestions | retrieval | comprehension | passage\n\n\n2\n2\nmedical | clinical | biomedical | patient\nmedical | clinical | biomedical | patient\n\n\n3\n3\ntranslation | nmt | machine | neural\ntranslation | nmt | neural | bleu\n\n\n4\n4\nsummarization | summaries | summary | abstractive\nsummarization | summaries | abstractive | docu...\n\n\n5\n5\nhate | offensive | speech | detection\nhate | offensive | toxic | abusive\n\n\n6\n6\ngender | bias | biases | debiasing\ngender | bias | biases | debiasing\n\n\n7\n7\nrelation | extraction | relations | re\nrelation | extraction | relations | entities\n\n\n8\n8\nner | entity | named | recognition\nner | recognition | entities | data\n\n\n9\n9\nagents | agent | game | games\nagents | games | planning | environments\n\n\n\n\n\n\n\nThis does seem to give more diverse keywords."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter5/lesson.html#text-generation",
    "href": "posts/ml/hands_on_llm/chapter5/lesson.html#text-generation",
    "title": "Hands-On LLMs: Text Clustering and Topic Modeling",
    "section": "2.3. Text Generation",
    "text": "2.3. Text Generation\nOne more “lego block” we can add to our modular set up is a text generation module that will generate a label for the topic.\nWe can prompt the text generation LLM as follows to produce a label:\n\nI have a topic that contains the following documents: [DOCUMENTS]\nThe topic is described by the following keywords: [KEYWORDS]\nGive a short label of the topic.\n\nNote we only pass a small subset of the highest matching documents, not all of them.\nNow our pipeline extends further to:\n\n\n\n\n\nflowchart LR\n\n  A(Embeddings) --&gt; B(Dimensionality Reduction) --&gt; C(Clustering)  --&gt; D(Tokenization) --&gt; E(Topic Modeling) --&gt; F(Reranker) --&gt; G(Label Generation)\n\n\n\n\n\n\nWe only need to call the LLM for each topic, not each document.\n\nfrom transformers import pipeline \nfrom bertopic.representation import TextGeneration \n\nprompt = \"\"\"\n    I have a topic that contains the following documents: [DOCUMENTS] \n    \n    The topic is described by the following keywords: '[KEYWORDS]'. \n    \n    Based on the documents and keywords, what is this topic about?\n\"\"\" \n\n# Update our topic representations using Flan-T5 \ngenerator = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\") \nrepresentation_model = TextGeneration(generator, prompt=prompt, doc_length=50, tokenizer=\"whitespace\") \ntopic_model.update_topics(abstracts, representation_model=representation_model)\n\n100%|██████████| 151/151 [00:19&lt;00:00,  7.74it/s]\n\n\nNow we can see the LLM-generated keywords for each topic:\n\ntopic_differences(topic_model, original_topics, num_topics=10)\n\n\n\n\n\n\n\n\nTopic\nOriginal\nReranked\n\n\n\n\n0\n0\nspeech | asr | recognition | end\nSpeech-to-text translation | | |\n\n\n1\n1\nquestion | qa | questions | answer\nQuestion answering | | |\n\n\n2\n2\nmedical | clinical | biomedical | patient\nScience/Tech | | |\n\n\n3\n3\ntranslation | nmt | machine | neural\nLearning neural machine translation | | |\n\n\n4\n4\nsummarization | summaries | summary | abstractive\nAbstractive summarization | | |\n\n\n5\n5\nhate | offensive | speech | detection\nScience/Tech | | |\n\n\n6\n6\ngender | bias | biases | debiasing\nScience/Tech | | |\n\n\n7\n7\nrelation | extraction | relations | re\nrelation extraction | | |\n\n\n8\n8\nner | entity | named | recognition\nScience/Tech | | |\n\n\n9\n9\nagents | agent | game | games\nScience/Tech | | |\n\n\n\n\n\n\n\nSome are pretty good, e.g. “Speech-to-text translation”. Some are a bit too broad, e.g. a lot of “Science/Tech”.\nThis was with a pretty tiny LLM. A more capable model would have fared better. But we get the general idea.\nAs a cherry on top, we can get a nice plot of our clusters with the topic labels overlaid:\n\nfig = topic_model.visualize_document_datamap(\n    titles, topics=list(range(20)), reduced_embeddings=reduced_embeddings, width=1200, label_font_size=11, label_wrap_width=20, use_medoids=True\n)"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter6/lesson.html",
    "href": "posts/ml/hands_on_llm/chapter6/lesson.html",
    "title": "Hands-On LLMs: Prompt Engineering",
    "section": "",
    "text": "Prompt engineering is an umbrella term for techniques that can improve the quality of a response from an LLM.\nIt extends beyond this as a tool to evaluate the model’s outputs and implement guardrails on the response."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter6/lesson.html#special-tokens",
    "href": "posts/ml/hands_on_llm/chapter6/lesson.html#special-tokens",
    "title": "Hands-On LLMs: Prompt Engineering",
    "section": "1.1. Special Tokens",
    "text": "1.1. Special Tokens\nWe can explore the tokens used by a model in a transformers pipeline by inspecting the chat template.\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False)\nThis gives a prompt in the form:\n&lt;s&gt;&lt;|user|&gt; Create a funny joke about chickens.&lt;|end|&gt; &lt;|assistant|&gt; \nThere are special tokens to indicate:\n\n&lt;s&gt; - the start of the prompt\n&lt;|user|&gt; - when a user (i.e. you) begins their message\n&lt;|end|&gt; - when that message ends\n&lt;|assistant|&gt; - when the assistant (the LLM) begins their message.\n\nThis gets passed to the model to complete the sequence, one token and a time until it generates an &lt;|end|&gt; token.\nThese special tokens help the model keep track of the context."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter6/lesson.html#temperature",
    "href": "posts/ml/hands_on_llm/chapter6/lesson.html#temperature",
    "title": "Hands-On LLMs: Prompt Engineering",
    "section": "1.2. Temperature",
    "text": "1.2. Temperature\nLLMs are fundamentally trained neural networks, so their output should be (and is) deterministic.\nBut we don’t want to always get the same response to a given prompt. That would be boring. We want a bit of pizazz.\nWhen the model is predicting the next token, what it actually does is create a probability distribution over all possible tokens in the vocabulary. It then samples from this distribution to choose the next word.\nTemperature defines how likely it is to choose less probable tokens.\nA temperature=0 will always choose the most likely token; this is greedy sampling.\nHigher temperatures lead to more creative outputs."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter6/lesson.html#constraining-the-sample-space-top-p-and-top-k",
    "href": "posts/ml/hands_on_llm/chapter6/lesson.html#constraining-the-sample-space-top-p-and-top-k",
    "title": "Hands-On LLMs: Prompt Engineering",
    "section": "1.3. Constraining the sample space: Top p and Top k",
    "text": "1.3. Constraining the sample space: Top p and Top k\nA related concept in sampling is to restrict the sample space, so rather than having the possibility (however small) of selecting any token in the vocab list, we constrain the possibilities.\nThe top_p parameter considers tokens from most to least probable until the cumulative probability reaches the given value. So top_p=1 considers all tokens, and a lower value filters out less probable tokens. This is known as nucleus sampling.\nThe top_k parameter restrict the sample space to the k most probable tokens."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter6/lesson.html#example-use-cases",
    "href": "posts/ml/hands_on_llm/chapter6/lesson.html#example-use-cases",
    "title": "Hands-On LLMs: Prompt Engineering",
    "section": "1.4. Example use cases",
    "text": "1.4. Example use cases\n\n\n\n\n\n\n\n\n\nUse case\ntemperature\ntop_p\nDescription\n\n\n\n\nBrainstorming\nHigh\nHigh\nCreative and unexpected responses\n\n\nEmail generation\nLow\nLow\nPredictable and focused responses that aren’t too out there\n\n\nCreative writing\nHigh\nLow\nCreative but still coherent\n\n\nTranslation\nLow\nHigh\nDeterministic output but with linguistic variety"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter6/lesson.html#the-ingredients-of-a-prompt",
    "href": "posts/ml/hands_on_llm/chapter6/lesson.html#the-ingredients-of-a-prompt",
    "title": "Hands-On LLMs: Prompt Engineering",
    "section": "2.1. The ingredients of a prompt",
    "text": "2.1. The ingredients of a prompt\nThe most basic prompt is simply an input, without even an instruction. The LLM will simply complete the sequence. E.g.\n\nThe sky is\n\n\nblue\n\nWe can extend this to an instruction prompt where we now have two components:\n\nInstruction - Classify the text into negative or positive\nData - “This is a great movie!”\n\nThe model may have seen similar instructions in its training data, or at least seen similar enough example to allow it to generalise.\nThis is called instruction-based prompting."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter6/lesson.html#instruction-based-tasks",
    "href": "posts/ml/hands_on_llm/chapter6/lesson.html#instruction-based-tasks",
    "title": "Hands-On LLMs: Prompt Engineering",
    "section": "2.2. Instruction-based tasks",
    "text": "2.2. Instruction-based tasks\nIt’s helpful to understand common tasks LLMs are used to perform, as they may have been trained on such examples, so if you use the phrasing they are familiar with, they are more likely to give you the desired output.\n\nClassification - Classify the text into positive, neutral or negative.\nSearch - Find the ___ in the following text\nSummarization - Summarise the following text\nCode generation - Generate python code for…\nNamed entity recognition - An entity is… Extract the named entities from the following text\n\nThe common features of these prompts for these different tasks are:\n\nSpecificity - accurately describe what you want to achieve\nHallucination - LLMs are confident, not correct. We can ask the LLM to generate an answer if it knows the answer, otherwise respond with I don’t know\nOrder - The instruction should come either at the beginning or end. LLMs tend to focus at either extreme, known as the primacy effect and the recency effect respectively."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter6/lesson.html#zero-shot",
    "href": "posts/ml/hands_on_llm/chapter6/lesson.html#zero-shot",
    "title": "Hands-On LLMs: Prompt Engineering",
    "section": "4.1. Zero-shot",
    "text": "4.1. Zero-shot\nZero-shot prompts provide no examples but outline the shape of the desired response. E.g.\n\nClassify the text into positive, neutral or negative Text: The food was great! Sentiment:"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter6/lesson.html#one-shot-and-few-shot",
    "href": "posts/ml/hands_on_llm/chapter6/lesson.html#one-shot-and-few-shot",
    "title": "Hands-On LLMs: Prompt Engineering",
    "section": "4.2. One-shot and Few-shot",
    "text": "4.2. One-shot and Few-shot\nFew-shot prompts provide two or more examples, one-shot prompts provide one example.\nWe can use the user and assistant roles in the prompt to distinguish the user prompt from the exemplar assistant output.\none_shot_prompt = [  \n    {\"role\": \"user\",\n    \"content\": \"A 'Gigamuru' is a type of Japanese musical instrument. An example of a sentence that uses the word Gigamuru is:\"  },\n    {\"role\": \"assistant\",\n     \"content\": \"I have a Gigamuru that my uncle gave me as a gift. I love to play it at home.\"  },\n    {\"role\": \"user\",\n     \"content\": \"To 'screeg' something is to swing a sword at it. An example of a sentence that uses the word screeg is:\"}\n]"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter6/lesson.html#chain-of-thought",
    "href": "posts/ml/hands_on_llm/chapter6/lesson.html#chain-of-thought",
    "title": "Hands-On LLMs: Prompt Engineering",
    "section": "6.1. Chain of thought",
    "text": "6.1. Chain of thought\nWe can coax this behaviour which mimics reasoning out of the LLM to improve the output.\nThis is analogous to the System 1 and System 2 thinking of Kahneman and Tversky; by default the LLM will give a knee-jerk System 1 response, but if we ask it to reason it will give an more considered System 2 response.\nWe can encourage this with a one-shot chain-of-thought prompt demonstrating reasoning. E.g.\ncot_prompt = [\n    {\"role\": \"user\",\n     \"content\": \"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\"},\n    {\"role\": \"assistant\", \n     \"content\": \"Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\"},\n     {\"role\": \"user\",\n      \"content\": \"The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\"}\n]\nThis guides the model towards providing an explanation as well as the answer. “Show your working!”\nThere is a zero-shot chain-of-thought approach that doesn’t require us to give an example of reasoning. A common and effective method is use this phrase to prime reasoning:\n\nLet’s think set-by-step\n\n(AKA the Bobby Valentino approach: slow down!)"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter6/lesson.html#self-consistency",
    "href": "posts/ml/hands_on_llm/chapter6/lesson.html#self-consistency",
    "title": "Hands-On LLMs: Prompt Engineering",
    "section": "6.2. Self-consistency",
    "text": "6.2. Self-consistency\nWe can use the same prompt multiple times and get different responses (with differing quality) due to the sampling nature of LLMs.\nWe can sample multiple responses and ask the LLM to give the majority vote as the response.\nThis does make it slower and more expensive though, since we’re prompting n times for each prompt."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter6/lesson.html#tree-of-thought",
    "href": "posts/ml/hands_on_llm/chapter6/lesson.html#tree-of-thought",
    "title": "Hands-On LLMs: Prompt Engineering",
    "section": "6.3. Tree of thought",
    "text": "6.3. Tree of thought\nThis is a combination of the previous ideas.\nTree of thought = Chain of thought + Self consistency \nWe break the problem down into multiple steps. At each step, we ask the model to explore different solutions, then vote for the best solution(s) and move on to the next step. The thoughts are rates, with the most promising kept and the least promising pruned.\nThe disadvantage is that it requires even more calls to the model, slowing things down.\nA zero-shot tree-of-thought approach is to ask the model to emulate a “discussion between multiple experts”. E.g.\nzeroshot_tot_prompt = [\n    {\"role\": \"user\",\n     \"content\": \"Imagine three different experts are answering this question. All experts will write down 1 step of their thinking, then share it with the group. Then all experts will go on to the next step, etc. If any expert realizes they're wrong at any point then they leave. The question is 'The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?' Make sure to discuss the results.\"}\n]\nAn extention of Three-of-thought is Graph-of-thought, where each prompt is treated like a node in a graph. Rather than a tree following a linear train of thought, the prompts can be reused in different orders and combinations in a graph."
  },
  {
    "objectID": "posts/ml/fastai/lesson1/lesson.html",
    "href": "posts/ml/fastai/lesson1/lesson.html",
    "title": "FastAI Lesson 1: Image Classification Models",
    "section": "",
    "text": "These are notes from lesson 1 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\nTrain an image classifier: see car classification notebook\n\n\n\n\nThere is a companion course focusing on AI ethics here\nThoughts on education: play the whole game first to get an idea of the big picture. Don’t become afflicted with “elementitis” getting too bogged down in details too early.\n\nColoured cups - green, amber, red - for each student to indicate how well they are following\nMeta Learning by Radek Osmulski - a book based on the author’s experience of learning about deep learning (via the fastai course)\nMathematician’s Lament by Paul Lockhart - a book about the state of mathematics education\nMaking Learning Whole by David Perkins - a book about apporaches to holistic learning\n\nRISE is a jupyter notebook extension to turn notebooks into slides. Jeremy uses notebooks for: source code, book, blogging, CI/CD.\n\n\n\nBefore deep learning, the approach to machine learning was to enlist many domain experts to handcraft features and feed this into a constrained linear model (e.g. ridge regression). This is time-consuming, expensive and requires many domain experts.\nNeural networks learn these features. Looking inside a CNN, for example, shows that these learned features match interpretable features that an expert might handcraft. An illustration of the features learned is given in this paper by Zeiler and Fergus.\nFor image classifiers, you don’t need particularly large images as inputs. GPUs are so quick now that if you use large images, most of the time is spent on opening the file rather than computations. So often we resize images down to 400x400 pixels or smaller.\nFor most use cases, there are pre-trained models and sensible default values that we can use. In practice, most of the time is spent on the input layer and output layer. For most models the middle layers are identical.\n\n\n\nData blocks structure the input to learners. An overview of the DataBlock class:\n\nblocks determines the input and output type as a tuple. For multi-target classification this tuple can be arbitrary length.\nget_items is a function that returns a list of all the inputs.\nsplitter defines how to split the training/validation set.\nget_y is a function that returns the label of a given input image.\nitem_tfms defines what transforms to apply to the inputs before training, e.g. resize.\ndataloaders is a method that parallelises loading the data.\n\nA learner combines the model (e.g. resnet or something from timm library) and the data to run that model on (the dataloaders from the DataBlock).\nThe fine_tune method starts with a pretrained model weights rather than randomised weights, and only needs to learn the differences between your data and the original model.\nOther image problems that can utilise deep learning:\n\nImage classification\nImage segmentation\n\nOther problem types use the same process, just with different DataBlock blocks types and the rest is the same. For example, tabular data, collaborative filtering.\n\n\n\n\nTraditional computer programs are essentially:\n\n\n\n\n\n\nFigure 1: Traditional computer programs\n\n\n\nDeep learning models are:\n\n\n\n\n\n\nFigure 2: Deep learning model\n\n\n\n\n\n\n\nCourse lesson page\nVisualizing and Understanding Convolutional Networks\nTIMM library\nStatistical Modeling: The Two Cultures\n\n\n\n\nFigure 1: Traditional computer programs\nFigure 2: Deep learning model"
  },
  {
    "objectID": "posts/ml/fastai/lesson1/lesson.html#notes-on-learning",
    "href": "posts/ml/fastai/lesson1/lesson.html#notes-on-learning",
    "title": "FastAI Lesson 1: Image Classification Models",
    "section": "",
    "text": "There is a companion course focusing on AI ethics here\nThoughts on education: play the whole game first to get an idea of the big picture. Don’t become afflicted with “elementitis” getting too bogged down in details too early.\n\nColoured cups - green, amber, red - for each student to indicate how well they are following\nMeta Learning by Radek Osmulski - a book based on the author’s experience of learning about deep learning (via the fastai course)\nMathematician’s Lament by Paul Lockhart - a book about the state of mathematics education\nMaking Learning Whole by David Perkins - a book about apporaches to holistic learning\n\nRISE is a jupyter notebook extension to turn notebooks into slides. Jeremy uses notebooks for: source code, book, blogging, CI/CD."
  },
  {
    "objectID": "posts/ml/fastai/lesson1/lesson.html#background-on-deep-learning-and-image-classification",
    "href": "posts/ml/fastai/lesson1/lesson.html#background-on-deep-learning-and-image-classification",
    "title": "FastAI Lesson 1: Image Classification Models",
    "section": "",
    "text": "Before deep learning, the approach to machine learning was to enlist many domain experts to handcraft features and feed this into a constrained linear model (e.g. ridge regression). This is time-consuming, expensive and requires many domain experts.\nNeural networks learn these features. Looking inside a CNN, for example, shows that these learned features match interpretable features that an expert might handcraft. An illustration of the features learned is given in this paper by Zeiler and Fergus.\nFor image classifiers, you don’t need particularly large images as inputs. GPUs are so quick now that if you use large images, most of the time is spent on opening the file rather than computations. So often we resize images down to 400x400 pixels or smaller.\nFor most use cases, there are pre-trained models and sensible default values that we can use. In practice, most of the time is spent on the input layer and output layer. For most models the middle layers are identical."
  },
  {
    "objectID": "posts/ml/fastai/lesson1/lesson.html#overview-of-the-fastai-learner",
    "href": "posts/ml/fastai/lesson1/lesson.html#overview-of-the-fastai-learner",
    "title": "FastAI Lesson 1: Image Classification Models",
    "section": "",
    "text": "Data blocks structure the input to learners. An overview of the DataBlock class:\n\nblocks determines the input and output type as a tuple. For multi-target classification this tuple can be arbitrary length.\nget_items is a function that returns a list of all the inputs.\nsplitter defines how to split the training/validation set.\nget_y is a function that returns the label of a given input image.\nitem_tfms defines what transforms to apply to the inputs before training, e.g. resize.\ndataloaders is a method that parallelises loading the data.\n\nA learner combines the model (e.g. resnet or something from timm library) and the data to run that model on (the dataloaders from the DataBlock).\nThe fine_tune method starts with a pretrained model weights rather than randomised weights, and only needs to learn the differences between your data and the original model.\nOther image problems that can utilise deep learning:\n\nImage classification\nImage segmentation\n\nOther problem types use the same process, just with different DataBlock blocks types and the rest is the same. For example, tabular data, collaborative filtering."
  },
  {
    "objectID": "posts/ml/fastai/lesson1/lesson.html#deep-learning-vs-traditional-computer-programs",
    "href": "posts/ml/fastai/lesson1/lesson.html#deep-learning-vs-traditional-computer-programs",
    "title": "FastAI Lesson 1: Image Classification Models",
    "section": "",
    "text": "Traditional computer programs are essentially:\n\n\n\n\n\n\nFigure 1: Traditional computer programs\n\n\n\nDeep learning models are:\n\n\n\n\n\n\nFigure 2: Deep learning model"
  },
  {
    "objectID": "posts/ml/fastai/lesson1/lesson.html#references",
    "href": "posts/ml/fastai/lesson1/lesson.html#references",
    "title": "FastAI Lesson 1: Image Classification Models",
    "section": "",
    "text": "Course lesson page\nVisualizing and Understanding Convolutional Networks\nTIMM library\nStatistical Modeling: The Two Cultures\n\n\n\n\nFigure 1: Traditional computer programs\nFigure 2: Deep learning model"
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html",
    "href": "posts/ml/fastai/lesson7/lesson.html",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "These are notes from lesson 7 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\n\nCreate a collaborative filtering model in a spreadsheet\n\n\n\n\n\nWe have users ratings of movies.\nSay we had “embeddings” of a set of categories for each. So for a given movie, we have a vector of [action, sci-fi, romance] and for a given user we have their preference for [action, sci-fi, romance]. Then we could do the dot product between user embedding and movie embedding to get the probability that the user likes that movie. That is, the predicted user rating.\nSo the problem boils down to:\n\nWhat are the embeddings? i.e. the salient factors ([action, sci-fi, romance] in the example above)\nHow do we get them?\n\nThe answer to both questions is: we just let the model learn them.\nLet’s just pick a randomised embedding for each movie and each user. Then we have a loss function which is the MAE between predicted user rating for a movie and actual rating. Now we can use SGD to optimise those embeddings to find the best values.\n\n\n\nTo gain an intuition behind the calculations behind a collaborative filter, we can work through a (smaller) example in excel. This allows us to see the logic and dig into the calculations before we create them “for real” in Python.\n\n\n\n\n\n\nTip\n\n\n\nThis can be found in this spreadsheet.\n\n\nWe first look at an example where the results are in a cross-table and we can take the dot product of user embeddings and movie embeddings.\nThen we reshape the problem slightly by placing all of the embeddings in a matrix and doing a lookup. This is essentially what pytorch does, although it uses matrix multiplication by one-hot encoded vectors rather than array lookups for computational efficiency.\nWe then add a bias term to account for some users who love all movies, or hate all movies. And also movies that are universally beloved.\n\n\n\nThe broad idea behind collaborative filtering is:\n\nIf we could quantify the most salient “latent factors” about a movie, and…\nQuantify how much a user cares about that factor, then…\nIf we multiplied the two (dot product) it would give a measure of their rating.\n\nBut what are those latent factors? We let the model learn it. 1. We initialise randomised latent factors (called embeddings) 2. We use that to predict the user’s rating for each move. Initially, those randomised weights will give terrible predictions. 3. Our loss function is the MSE of the ground truth actual predictions and the prediction rating. 4. We can optimise the embedding values to minimise this loss function.\n\n\nWe use data on user ratings of movies sourced from MovieLens. The ml-latest-small data set is downloaded and saved in the DATA_DIR folder.\n\nfrom pathlib import Path\n\nfrom fastai.collab import CollabDataLoaders, Module, Embedding, collab_learner\nfrom fastai.tabular.all import one_hot, sigmoid_range, MSELossFlat, Learner, get_emb_sz\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd \nimport torch\n\n\nDATA_DIR = Path(\"/Users/gurpreetjohl/workspace/python/ml-practice/ml-practice/datasets/ml-latest-small\")\n\nLoad the ratings data which we will use for this task:\n\nratings = pd.read_csv(DATA_DIR / 'ratings.csv')\nratings\n\n\n\n\n\n\n\n\nuserId\nmovieId\nrating\ntimestamp\n\n\n\n\n0\n1\n1\n4.0\n964982703\n\n\n1\n1\n3\n4.0\n964981247\n\n\n2\n1\n6\n4.0\n964982224\n\n\n3\n1\n47\n5.0\n964983815\n\n\n4\n1\n50\n5.0\n964982931\n\n\n...\n...\n...\n...\n...\n\n\n100831\n610\n166534\n4.0\n1493848402\n\n\n100832\n610\n168248\n5.0\n1493850091\n\n\n100833\n610\n168250\n5.0\n1494273047\n\n\n100834\n610\n168252\n5.0\n1493846352\n\n\n100835\n610\n170875\n3.0\n1493846415\n\n\n\n\n100836 rows × 4 columns\n\n\n\nThe users and movies are encoded as integers.\nFor reference, we can load the movies data to see what each movieId corresponds to:\n\nmovies = pd.read_csv(DATA_DIR / 'movies.csv')\nmovies\n\n\n\n\n\n\n\n\nmovieId\ntitle\ngenres\n\n\n\n\n0\n1\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n1\n2\nJumanji (1995)\nAdventure|Children|Fantasy\n\n\n2\n3\nGrumpier Old Men (1995)\nComedy|Romance\n\n\n3\n4\nWaiting to Exhale (1995)\nComedy|Drama|Romance\n\n\n4\n5\nFather of the Bride Part II (1995)\nComedy\n\n\n...\n...\n...\n...\n\n\n9737\n193581\nBlack Butler: Book of the Atlantic (2017)\nAction|Animation|Comedy|Fantasy\n\n\n9738\n193583\nNo Game No Life: Zero (2017)\nAnimation|Comedy|Fantasy\n\n\n9739\n193585\nFlint (2017)\nDrama\n\n\n9740\n193587\nBungo Stray Dogs: Dead Apple (2018)\nAction|Animation\n\n\n9741\n193609\nAndrew Dice Clay: Dice Rules (1991)\nComedy\n\n\n\n\n9742 rows × 3 columns\n\n\n\nWe’ll merge the two for easier human readability.\n\nratings = ratings.merge(movies)\nratings\n\n\n\n\n\n\n\n\nuserId\nmovieId\nrating\ntimestamp\ntitle\ngenres\n\n\n\n\n0\n1\n1\n4.0\n964982703\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n1\n1\n3\n4.0\n964981247\nGrumpier Old Men (1995)\nComedy|Romance\n\n\n2\n1\n6\n4.0\n964982224\nHeat (1995)\nAction|Crime|Thriller\n\n\n3\n1\n47\n5.0\n964983815\nSeven (a.k.a. Se7en) (1995)\nMystery|Thriller\n\n\n4\n1\n50\n5.0\n964982931\nUsual Suspects, The (1995)\nCrime|Mystery|Thriller\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n100831\n610\n166534\n4.0\n1493848402\nSplit (2017)\nDrama|Horror|Thriller\n\n\n100832\n610\n168248\n5.0\n1493850091\nJohn Wick: Chapter Two (2017)\nAction|Crime|Thriller\n\n\n100833\n610\n168250\n5.0\n1494273047\nGet Out (2017)\nHorror\n\n\n100834\n610\n168252\n5.0\n1493846352\nLogan (2017)\nAction|Sci-Fi\n\n\n100835\n610\n170875\n3.0\n1493846415\nThe Fate of the Furious (2017)\nAction|Crime|Drama|Thriller\n\n\n\n\n100836 rows × 6 columns\n\n\n\n\n\n\n\ndls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\ndls.show_batch()\n\n\n\n\n\nuserId\ntitle\nrating\n\n\n\n\n0\n4\nMighty Aphrodite (1995)\n3.0\n\n\n1\n573\nDark Knight, The (2008)\n5.0\n\n\n2\n116\nAmadeus (1984)\n3.0\n\n\n3\n380\nAddams Family, The (1991)\n5.0\n\n\n4\n353\nBrothers McMullen, The (1995)\n4.0\n\n\n5\n37\nFugitive, The (1993)\n4.0\n\n\n6\n356\nUnbreakable (2000)\n4.0\n\n\n7\n489\nAlien³ (a.k.a. Alien 3) (1992)\n3.5\n\n\n8\n174\nNell (1994)\n5.0\n\n\n9\n287\nPanic Room (2002)\n2.5\n\n\n\n\n\nInitialise randomised 5-dimensional embeddings.\nHow should we choose the number of latent factors? (5 in the example above). Jeremy wrote down some ballpark values for models of different sizes in excel, then fit a function to it to get a heuristic measure. This is the default used by fast AI.\n\nn_users  = len(dls.classes['userId'])\nn_movies = len(dls.classes['title'])\nn_factors = 5\n\nuser_factors = torch.randn(n_users, n_factors)\nmovie_factors = torch.randn(n_movies, n_factors)\n\nDoing a matrix multiply by a one hot encoded vector is the same as doing a lookup in an array, just in a more computationally efficient way. Recall the softmax example.\nAn embedding is essentially just “look up in an array”.\n\n\n\n\nPutting a sigmoid_range on the final layer to squish ratings to fit 0 to 5 means “the model doesn’t have to work as hard” to get movies in the right range. In practice we use 5.5 as the sigmoid scale value as a sigmoid can never hit 1, but we want ratings to be able to hit 5.\n\nfrom fastai.collab import CollabDataLoaders, Module, Embedding\nfrom fastai.tabular.all import one_hot, sigmoid_range, MSELossFlat, Learner\n\n\nclass DotProduct(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.y_range = y_range\n\n    def forward(self, x):\n        users = self.user_factors(x[:, 0])\n        movies = self.movie_factors(x[:, 1])\n        # Apply a sigmoid to the raw_output\n        raw_output = (users * movies).sum(dim=1)\n        return sigmoid_range(raw_output, *self.y_range)\n\nWe can now fit a model\n\nembedding_dim = 50\nnum_epochs = 5\nmax_learning_rate = 5e-3\n\nmodel = DotProduct(n_users, n_movies, embedding_dim)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(num_epochs, max_learning_rate)\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\nAdding a user bias term and a movie bias term to the prediction call helps account for the fact that some users always rate high (4 or 5) but other users always rate low. And similarly for movies if everyone always rates it a 5 or a 1.\n\nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.user_bias = Embedding(n_users, 1)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.movie_bias = Embedding(n_movies, 1)\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        raw_output = (users * movies).sum(dim=1, keepdim=True)\n        raw_output += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n        return sigmoid_range(raw_output, *self.y_range)\n\n\nmodel = DotProductBias(n_users, n_movies, embedding_dim)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(num_epochs, max_learning_rate)\n\n\n\n\n\n\n    \n      \n      0.00% [0/5 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/1260 00:00&lt;?]\n    \n    \n\n\n\n\n\nThe validation loss in the previous model decreases then icnreases, which is a clear indication of overfitting.\nWe want to avoid overfitting, but data augmentation isn’t possible here. One approach is to use weight decay AKA L2 regularisation. We add sum of weights squared to the loss function.\nHow does this prevent overfitting? The larger the coefficients, the sharper the canyons the model is able to produce, which allows it to fit individual data points. By penalising larger weights, it will only produce sharp changes if this causes the model to fit many points well, so it should generalise better.\nWe essentially want to modify our loss function with an additional term dependent on the magnitude of the weights:\nloss_with_weight_decay = loss + weight_decay * (parameters**2).sum()\nIn practice, these values would be large and numerically unstable. We only actually care about the gradient of the loss, so we can add the gradient of the additional term to the existing gradient.\nparameters.grad += weight_decay * 2 * parameters\nBut weight_decay is just a constant that we choose, so we can fold the 2* term into it.\n\nweight_decay = 0.1\n\nmodel = DotProductBias(n_users, n_movies, embedding_dim)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(num_epochs, max_learning_rate, wd=weight_decay)\n\n\n\n\n\n\n    \n      \n      0.00% [0/5 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n\n\n\n    \n      \n      0.16% [2/1260 00:00&lt;00:20 1.4480]\n    \n    \n\n\n\n\n\nIn the previous section, we used that pytorch (technically the fastai version) Embedding module.\nLet’s briefly take a look at this and create our own Embedding module from scratch.\n\n\nThe way pytorch knows if a tensor is a parameter (and therefore can calculate gradients on it) is if it inherits from nn.Parameter. Then a Module’s .parameters() method will list this tensor.\nAs an example of this behaviour, let’s create a module with some parameters but WITHOUT declaring these as Parameters:\n\nclass MyModule(Module):\n    def __init__(self): \n        self.a = torch.ones(3)\n\nmm = MyModule()\nlist(mm.parameters())\n\n[]\n\n\nWe declared a tensor a in MyModule but we don’t see it! Which means it wouldn’t be trained by Pytorch.\nInstead, let’s declare is as a Parameter:\n\nclass MyModuleWithParams(Module):\n    def __init__(self): \n        self.a = torch.nn.Parameter(torch.ones(3))\n\nmm_params = MyModuleWithParams()\nlist(mm_params.parameters())\n\n[Parameter containing:\n tensor([1., 1., 1.], requires_grad=True)]\n\n\nPytorch’s builtin modules all use Parameter for any trainable parameters, so we haven’t needed to explicitly declare this.\nAs an example, if we use Pytorch’s Linear layer, it will automatically appear as a parameter:\n\nclass MyModuleWithLinear(Module):\n    def __init__(self): \n        self.a = torch.nn.Linear(1, 3, bias=False)\n\nmm_linear = MyModuleWithLinear()\nlist(mm_linear.parameters())\n\n[Parameter containing:\n tensor([[-0.6689],\n         [-0.0181],\n         [ 0.8172]], requires_grad=True)]\n\n\n\ntype(mm_linear.a.weight)\n\ntorch.nn.parameter.Parameter\n\n\n\n\n\nAn Embedding object essentially instantiates a tensor of random weights of the given dimensions and declares this as a Parameter. Pytorch can then modify the weights when training.\n\ndef create_params(tensor_dims):\n    \"\"\"Create a tensor of the required size and fill it with random values.\"\"\"\n    embedding_tensor = torch.zeros(*tensor_dims).normal_(0, 0.01)\n    return torch.nn.Parameter(embedding_tensor)\n\nNow we can replace the import Embedding module with our custom implementation create_params in the DotProductBias module:\n\nclass DotProductBiasCustomEmbedding(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = create_params([n_users, n_factors])\n        self.user_bias = create_params([n_users])\n        self.movie_factors = create_params([n_movies, n_factors])\n        self.movie_bias = create_params([n_movies])\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors[x[:,0]]\n        movies = self.movie_factors[x[:,1]]\n        raw_output = (users * movies).sum(dim=1)\n        raw_output += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]\n        return sigmoid_range(raw_output, *self.y_range)\n\n\nmodel = DotProductBiasCustomEmbedding(n_users, n_movies, embedding_dim)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(num_epochs, max_learning_rate, wd=weight_decay)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.795456\n0.787701\n00:07\n\n\n1\n0.693971\n0.728376\n00:06\n\n\n2\n0.546227\n0.711909\n00:07\n\n\n3\n0.402994\n0.707349\n00:06\n\n\n4\n0.282765\n0.708693\n00:06\n\n\n\n\n\n\n\n\n\nWe can interrogate the model to learn more about these embeddings it has learned.\n\n\nWe can visualise the biases of our collaborative filter model to see:\n\nMovie biases: Which movies are bad even compared to other similar movies of that type? Lawnmower man 2 is crap even compared to similar action movies. But people love titanic even if they don’t normally like romance dramas.\nUser biases: Which users love any and all movies? Users who give a high rating to all movies.\n\nAccording to our biases, these movies are crap even for those who like that style of movie:\n\nmovie_bias = learn.model.movie_bias.squeeze()\nidxs = movie_bias.argsort()[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Karate Kid, Part III, The (1989)',\n 'Catwoman (2004)',\n 'Stuart Saves His Family (1995)',\n 'Speed 2: Cruise Control (1997)',\n 'Dungeons & Dragons (2000)']\n\n\nWhereas these are highly rated, even when users don’t normally like that type of movie:\n\nidxs = movie_bias.argsort()[-5:]\n[dls.classes['title'][i] for i in idxs]\n\n['Star Wars: Episode IV - A New Hope (1977)',\n 'Dark Knight, The (2008)',\n 'Green Mile, The (1999)',\n 'Forrest Gump (1994)',\n 'Shawshank Redemption, The (1994)']\n\n\n\n\n\nWe can visualise the weights to see what human-interpretable features the model is learning.\nWe can condense our embedding to 2 axes with PCA. we get a critically-acclaimed -&gt; popular x-axis and a action-dialog y-axis.\n\n\n\n\n\n\nTip\n\n\n\nFor more details on PCA and related methods, see computational linear algebra fastai course\n\n\n\ng = ratings.groupby('title')['rating'].count()\ntop_movies = g.sort_values(ascending=False).index.values[:1000]\ntop_idxs = torch.tensor([learn.dls.classes['title'].o2i[m] for m in top_movies])\nmovie_w = learn.model.movie_factors[top_idxs].cpu().detach()\nmovie_pca = movie_w.pca(3)\nfac0,fac1,fac2 = movie_pca.t()\nidxs = list(range(50))\nX = fac0[idxs]\nY = fac2[idxs]\nplt.figure(figsize=(12,12))\nplt.scatter(X, Y)\nfor i, x, y in zip(top_movies[idxs], X, Y):\n    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWe can also use the “embedding distance” (distance in the latent space) to see when two movies are similar. We use cosine similarity to determine this distance, which is similar in principle to Euclidean distance but normalised.\nIn the example below, we start with the movie Forrest Gump and find the closest movie to it in our embedding:\n\nmovie_idx = dls.classes['title'].o2i['Forrest Gump (1994)']\n\nmovie_factors = learn.model.movie_factors\ndistances = torch.nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[movie_idx][None])\nclosest_distance_idx = distances.argsort(descending=True)[1]\ndls.classes['title'][closest_distance_idx]\n\n'Beautiful Mind, A (2001)'\n\n\n\n\n\n\n\nWe can repeat the same exercise using the collaborative filter from the fastai library to see how it compares to our from-scratch implementation.\n\nlearn_fast = collab_learner(dls, n_factors=embedding_dim, y_range=(0, 5.5))\nlearn_fast.fit_one_cycle(num_epochs, max_learning_rate, wd=weight_decay)\n\n\n\n\n\n\n    \n      \n      0.00% [0/5 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n\n\n\n    \n      \n      0.40% [5/1260 00:00&lt;00:20 1.5528]\n    \n    \n\n\nWe can repeat any of the analysis of our from-scratch model. For example, the movies with the highest bias:\n\nmovie_bias = learn_fast.model.i_bias.weight.squeeze()\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Shawshank Redemption, The (1994)',\n 'Forrest Gump (1994)',\n 'Green Mile, The (1999)',\n 'Star Wars: Episode IV - A New Hope (1977)',\n 'Dark Knight, The (2008)']\n\n\n\n\n\nHow do you start off a collaborative filtering model? For example, when you first start out, you have no data on users or items.\nOr even for established companies, what happens when we have a new user or a new item, so the entire row or column is null?\nThere is no hard and fast solution, they all boil down to “use common sense”.\n\nA tempting option is to fill NaNs with the median latent vectors. But this might result in an odd combination that doesn’t exist in practice, i.e. the latent space isn’t continuous so this could be where a gap in the latent space lies. For example, a medium action, medium sci-fi film with medium romance and medium comedy that is medium popular and medium critically acclaimed.\nAnother option is to pick a user/item that is representative of the average taste.\nCreate a tabular model using answers to a new user survey. Ask the user some questions when they sign up, then create a model where the user’s embedding vector is the dependent variable and their answers, along with any other relevant signup metadata, are the independent variables.\n\nIt is important to be careful of a small number of extremely enthusiastic users dominating the recommendations. For example, people who watch anime watch a LOT of it, and rate a lot of it highly. So this could end up getting recommended to users outside of this niche.\nThis can create positive feedback loops that change the behaviour of your product in unexpected ways.\n\n\n\nThe matrix completion approach used previously is known as Probabilistic Matrix Factorization (PMF). An alternative approach is to use deep learning.\nIn practice the two approaches are often stacked in an ensemble.\nThis section explores the deep learning collaborative filtering approach from scratch, then recreates it using fastai’s library.\n\n\nWe are concatenating the embedding matrices together, rather than taking the dot product, so that we can pass it through a dense ANN.\nThese matrices can be different sizes, and the size of embedding to use for each depends on the number of classes in the data. Fastai has a heuristic method for this which we use here:\n\n(user_num_classes, user_num_embeddings), (item_num_classes, item_num_embeddings) = get_emb_sz(dls)\n\nWe can then use this in a simple neural network with one hidden layer:\n\nclass CollabNN(Module):\n    def __init__(self, user_embedding_size, item_embedding_size, y_range=(0, 5.5), n_activations=100):\n        self.user_factors = Embedding(*user_embedding_size)\n        self.item_factors = Embedding(*item_embedding_size)\n        self.layers = torch.nn.Sequential(\n            torch.nn.Linear(user_embedding_size[1] + item_embedding_size[1], n_activations),\n            torch.nn.ReLU(),\n            torch.nn.Linear(n_activations, 1))\n        self.y_range = y_range\n        \n    def forward(self, x):\n        embs = self.user_factors(x[:, 0]), self.item_factors(x[:, 1])\n        x = self.layers(torch.cat(embs, dim=1))\n        return sigmoid_range(x, *self.y_range)\n    \ncollab_nn_model = CollabNN(user_embedding_size=(user_num_classes, user_num_embeddings),\n                           item_embedding_size=(item_num_classes, item_num_embeddings))\n\nNow train this model on the data:\n\nlearn_nn = Learner(dls, collab_nn_model, loss_func=MSELossFlat())\nlearn_nn.fit_one_cycle(num_epochs, max_learning_rate, wd=weight_decay)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.799004\n0.792579\n00:10\n\n\n1\n0.747623\n0.755708\n00:10\n\n\n2\n0.706981\n0.723887\n00:10\n\n\n3\n0.650337\n0.719642\n00:10\n\n\n4\n0.569418\n0.734302\n00:10\n\n\n\n\n\n\n\n\nWe can repeat the same exercise using fastai’s implementation.\nThis is almost identical to the PMF approach, simply with an additional argument use_nn=True.\n\nlearn_nn_fast = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100, 50])\nlearn_nn_fast.fit_one_cycle(num_epochs, max_learning_rate, wd=weight_decay)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.838047\n0.801519\n00:14\n\n\n1\n0.761085\n0.744033\n00:12\n\n\n2\n0.709788\n0.734091\n00:13\n\n\n3\n0.653415\n0.728950\n00:13\n\n\n4\n0.545074\n0.743957\n00:12\n\n\n\n\n\n\n\n\n\nThe recommender problem is one where we have some users and their ratings of some items. We want to know which unseen items a user may like.\nWe implemented two approaches to collaborative filtering:\n\nProbabilistic Matrix Factorization (PMF)\nA neural network\n\nFor each approach, we build a model from scratch in Pytorch, then compared that with fastai’s implementation. For the PMF approach, we even gained some intuition by creating a spreadsheet implementation first!\nIn practice, both approaches can be stacked for improved recommendations.\nThe idea we explored here of user some (initially random) embeddings to represent an entity and then letting our model learn them is a powerful one and it is not limited to collaborative learning. NLP uses embeddings to represent each unique token (i.e. each word with word-level tokenisation). It can then understand relationships between similar words, much like we were able to use embedding distances to identify similar movies.\n\n\n\n\nCourse lesson page\nCollaborative filtering notebook\nComputational linear algebra fastai course"
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html#the-intuition-behind-collaborative-filtering",
    "href": "posts/ml/fastai/lesson7/lesson.html#the-intuition-behind-collaborative-filtering",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "We have users ratings of movies.\nSay we had “embeddings” of a set of categories for each. So for a given movie, we have a vector of [action, sci-fi, romance] and for a given user we have their preference for [action, sci-fi, romance]. Then we could do the dot product between user embedding and movie embedding to get the probability that the user likes that movie. That is, the predicted user rating.\nSo the problem boils down to:\n\nWhat are the embeddings? i.e. the salient factors ([action, sci-fi, romance] in the example above)\nHow do we get them?\n\nThe answer to both questions is: we just let the model learn them.\nLet’s just pick a randomised embedding for each movie and each user. Then we have a loss function which is the MAE between predicted user rating for a movie and actual rating. Now we can use SGD to optimise those embeddings to find the best values."
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html#a-deep-learning-spreadsheet",
    "href": "posts/ml/fastai/lesson7/lesson.html#a-deep-learning-spreadsheet",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "To gain an intuition behind the calculations behind a collaborative filter, we can work through a (smaller) example in excel. This allows us to see the logic and dig into the calculations before we create them “for real” in Python.\n\n\n\n\n\n\nTip\n\n\n\nThis can be found in this spreadsheet.\n\n\nWe first look at an example where the results are in a cross-table and we can take the dot product of user embeddings and movie embeddings.\nThen we reshape the problem slightly by placing all of the embeddings in a matrix and doing a lookup. This is essentially what pytorch does, although it uses matrix multiplication by one-hot encoded vectors rather than array lookups for computational efficiency.\nWe then add a bias term to account for some users who love all movies, or hate all movies. And also movies that are universally beloved."
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html#input-data-and-factors",
    "href": "posts/ml/fastai/lesson7/lesson.html#input-data-and-factors",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "The broad idea behind collaborative filtering is:\n\nIf we could quantify the most salient “latent factors” about a movie, and…\nQuantify how much a user cares about that factor, then…\nIf we multiplied the two (dot product) it would give a measure of their rating.\n\nBut what are those latent factors? We let the model learn it. 1. We initialise randomised latent factors (called embeddings) 2. We use that to predict the user’s rating for each move. Initially, those randomised weights will give terrible predictions. 3. Our loss function is the MSE of the ground truth actual predictions and the prediction rating. 4. We can optimise the embedding values to minimise this loss function.\n\n\nWe use data on user ratings of movies sourced from MovieLens. The ml-latest-small data set is downloaded and saved in the DATA_DIR folder.\n\nfrom pathlib import Path\n\nfrom fastai.collab import CollabDataLoaders, Module, Embedding, collab_learner\nfrom fastai.tabular.all import one_hot, sigmoid_range, MSELossFlat, Learner, get_emb_sz\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd \nimport torch\n\n\nDATA_DIR = Path(\"/Users/gurpreetjohl/workspace/python/ml-practice/ml-practice/datasets/ml-latest-small\")\n\nLoad the ratings data which we will use for this task:\n\nratings = pd.read_csv(DATA_DIR / 'ratings.csv')\nratings\n\n\n\n\n\n\n\n\nuserId\nmovieId\nrating\ntimestamp\n\n\n\n\n0\n1\n1\n4.0\n964982703\n\n\n1\n1\n3\n4.0\n964981247\n\n\n2\n1\n6\n4.0\n964982224\n\n\n3\n1\n47\n5.0\n964983815\n\n\n4\n1\n50\n5.0\n964982931\n\n\n...\n...\n...\n...\n...\n\n\n100831\n610\n166534\n4.0\n1493848402\n\n\n100832\n610\n168248\n5.0\n1493850091\n\n\n100833\n610\n168250\n5.0\n1494273047\n\n\n100834\n610\n168252\n5.0\n1493846352\n\n\n100835\n610\n170875\n3.0\n1493846415\n\n\n\n\n100836 rows × 4 columns\n\n\n\nThe users and movies are encoded as integers.\nFor reference, we can load the movies data to see what each movieId corresponds to:\n\nmovies = pd.read_csv(DATA_DIR / 'movies.csv')\nmovies\n\n\n\n\n\n\n\n\nmovieId\ntitle\ngenres\n\n\n\n\n0\n1\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n1\n2\nJumanji (1995)\nAdventure|Children|Fantasy\n\n\n2\n3\nGrumpier Old Men (1995)\nComedy|Romance\n\n\n3\n4\nWaiting to Exhale (1995)\nComedy|Drama|Romance\n\n\n4\n5\nFather of the Bride Part II (1995)\nComedy\n\n\n...\n...\n...\n...\n\n\n9737\n193581\nBlack Butler: Book of the Atlantic (2017)\nAction|Animation|Comedy|Fantasy\n\n\n9738\n193583\nNo Game No Life: Zero (2017)\nAnimation|Comedy|Fantasy\n\n\n9739\n193585\nFlint (2017)\nDrama\n\n\n9740\n193587\nBungo Stray Dogs: Dead Apple (2018)\nAction|Animation\n\n\n9741\n193609\nAndrew Dice Clay: Dice Rules (1991)\nComedy\n\n\n\n\n9742 rows × 3 columns\n\n\n\nWe’ll merge the two for easier human readability.\n\nratings = ratings.merge(movies)\nratings\n\n\n\n\n\n\n\n\nuserId\nmovieId\nrating\ntimestamp\ntitle\ngenres\n\n\n\n\n0\n1\n1\n4.0\n964982703\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n1\n1\n3\n4.0\n964981247\nGrumpier Old Men (1995)\nComedy|Romance\n\n\n2\n1\n6\n4.0\n964982224\nHeat (1995)\nAction|Crime|Thriller\n\n\n3\n1\n47\n5.0\n964983815\nSeven (a.k.a. Se7en) (1995)\nMystery|Thriller\n\n\n4\n1\n50\n5.0\n964982931\nUsual Suspects, The (1995)\nCrime|Mystery|Thriller\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n100831\n610\n166534\n4.0\n1493848402\nSplit (2017)\nDrama|Horror|Thriller\n\n\n100832\n610\n168248\n5.0\n1493850091\nJohn Wick: Chapter Two (2017)\nAction|Crime|Thriller\n\n\n100833\n610\n168250\n5.0\n1494273047\nGet Out (2017)\nHorror\n\n\n100834\n610\n168252\n5.0\n1493846352\nLogan (2017)\nAction|Sci-Fi\n\n\n100835\n610\n170875\n3.0\n1493846415\nThe Fate of the Furious (2017)\nAction|Crime|Drama|Thriller\n\n\n\n\n100836 rows × 6 columns\n\n\n\n\n\n\n\ndls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\ndls.show_batch()\n\n\n\n\n\nuserId\ntitle\nrating\n\n\n\n\n0\n4\nMighty Aphrodite (1995)\n3.0\n\n\n1\n573\nDark Knight, The (2008)\n5.0\n\n\n2\n116\nAmadeus (1984)\n3.0\n\n\n3\n380\nAddams Family, The (1991)\n5.0\n\n\n4\n353\nBrothers McMullen, The (1995)\n4.0\n\n\n5\n37\nFugitive, The (1993)\n4.0\n\n\n6\n356\nUnbreakable (2000)\n4.0\n\n\n7\n489\nAlien³ (a.k.a. Alien 3) (1992)\n3.5\n\n\n8\n174\nNell (1994)\n5.0\n\n\n9\n287\nPanic Room (2002)\n2.5\n\n\n\n\n\nInitialise randomised 5-dimensional embeddings.\nHow should we choose the number of latent factors? (5 in the example above). Jeremy wrote down some ballpark values for models of different sizes in excel, then fit a function to it to get a heuristic measure. This is the default used by fast AI.\n\nn_users  = len(dls.classes['userId'])\nn_movies = len(dls.classes['title'])\nn_factors = 5\n\nuser_factors = torch.randn(n_users, n_factors)\nmovie_factors = torch.randn(n_movies, n_factors)\n\nDoing a matrix multiply by a one hot encoded vector is the same as doing a lookup in an array, just in a more computationally efficient way. Recall the softmax example.\nAn embedding is essentially just “look up in an array”."
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html#building-a-collaborative-filter-from-scratch",
    "href": "posts/ml/fastai/lesson7/lesson.html#building-a-collaborative-filter-from-scratch",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "Putting a sigmoid_range on the final layer to squish ratings to fit 0 to 5 means “the model doesn’t have to work as hard” to get movies in the right range. In practice we use 5.5 as the sigmoid scale value as a sigmoid can never hit 1, but we want ratings to be able to hit 5.\n\nfrom fastai.collab import CollabDataLoaders, Module, Embedding\nfrom fastai.tabular.all import one_hot, sigmoid_range, MSELossFlat, Learner\n\n\nclass DotProduct(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.y_range = y_range\n\n    def forward(self, x):\n        users = self.user_factors(x[:, 0])\n        movies = self.movie_factors(x[:, 1])\n        # Apply a sigmoid to the raw_output\n        raw_output = (users * movies).sum(dim=1)\n        return sigmoid_range(raw_output, *self.y_range)\n\nWe can now fit a model\n\nembedding_dim = 50\nnum_epochs = 5\nmax_learning_rate = 5e-3\n\nmodel = DotProduct(n_users, n_movies, embedding_dim)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(num_epochs, max_learning_rate)\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\nAdding a user bias term and a movie bias term to the prediction call helps account for the fact that some users always rate high (4 or 5) but other users always rate low. And similarly for movies if everyone always rates it a 5 or a 1.\n\nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.user_bias = Embedding(n_users, 1)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.movie_bias = Embedding(n_movies, 1)\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        raw_output = (users * movies).sum(dim=1, keepdim=True)\n        raw_output += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n        return sigmoid_range(raw_output, *self.y_range)\n\n\nmodel = DotProductBias(n_users, n_movies, embedding_dim)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(num_epochs, max_learning_rate)\n\n\n\n\n\n\n    \n      \n      0.00% [0/5 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/1260 00:00&lt;?]\n    \n    \n\n\n\n\n\nThe validation loss in the previous model decreases then icnreases, which is a clear indication of overfitting.\nWe want to avoid overfitting, but data augmentation isn’t possible here. One approach is to use weight decay AKA L2 regularisation. We add sum of weights squared to the loss function.\nHow does this prevent overfitting? The larger the coefficients, the sharper the canyons the model is able to produce, which allows it to fit individual data points. By penalising larger weights, it will only produce sharp changes if this causes the model to fit many points well, so it should generalise better.\nWe essentially want to modify our loss function with an additional term dependent on the magnitude of the weights:\nloss_with_weight_decay = loss + weight_decay * (parameters**2).sum()\nIn practice, these values would be large and numerically unstable. We only actually care about the gradient of the loss, so we can add the gradient of the additional term to the existing gradient.\nparameters.grad += weight_decay * 2 * parameters\nBut weight_decay is just a constant that we choose, so we can fold the 2* term into it.\n\nweight_decay = 0.1\n\nmodel = DotProductBias(n_users, n_movies, embedding_dim)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(num_epochs, max_learning_rate, wd=weight_decay)\n\n\n\n\n\n\n    \n      \n      0.00% [0/5 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n\n\n\n    \n      \n      0.16% [2/1260 00:00&lt;00:20 1.4480]\n    \n    \n\n\n\n\n\nIn the previous section, we used that pytorch (technically the fastai version) Embedding module.\nLet’s briefly take a look at this and create our own Embedding module from scratch.\n\n\nThe way pytorch knows if a tensor is a parameter (and therefore can calculate gradients on it) is if it inherits from nn.Parameter. Then a Module’s .parameters() method will list this tensor.\nAs an example of this behaviour, let’s create a module with some parameters but WITHOUT declaring these as Parameters:\n\nclass MyModule(Module):\n    def __init__(self): \n        self.a = torch.ones(3)\n\nmm = MyModule()\nlist(mm.parameters())\n\n[]\n\n\nWe declared a tensor a in MyModule but we don’t see it! Which means it wouldn’t be trained by Pytorch.\nInstead, let’s declare is as a Parameter:\n\nclass MyModuleWithParams(Module):\n    def __init__(self): \n        self.a = torch.nn.Parameter(torch.ones(3))\n\nmm_params = MyModuleWithParams()\nlist(mm_params.parameters())\n\n[Parameter containing:\n tensor([1., 1., 1.], requires_grad=True)]\n\n\nPytorch’s builtin modules all use Parameter for any trainable parameters, so we haven’t needed to explicitly declare this.\nAs an example, if we use Pytorch’s Linear layer, it will automatically appear as a parameter:\n\nclass MyModuleWithLinear(Module):\n    def __init__(self): \n        self.a = torch.nn.Linear(1, 3, bias=False)\n\nmm_linear = MyModuleWithLinear()\nlist(mm_linear.parameters())\n\n[Parameter containing:\n tensor([[-0.6689],\n         [-0.0181],\n         [ 0.8172]], requires_grad=True)]\n\n\n\ntype(mm_linear.a.weight)\n\ntorch.nn.parameter.Parameter\n\n\n\n\n\nAn Embedding object essentially instantiates a tensor of random weights of the given dimensions and declares this as a Parameter. Pytorch can then modify the weights when training.\n\ndef create_params(tensor_dims):\n    \"\"\"Create a tensor of the required size and fill it with random values.\"\"\"\n    embedding_tensor = torch.zeros(*tensor_dims).normal_(0, 0.01)\n    return torch.nn.Parameter(embedding_tensor)\n\nNow we can replace the import Embedding module with our custom implementation create_params in the DotProductBias module:\n\nclass DotProductBiasCustomEmbedding(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = create_params([n_users, n_factors])\n        self.user_bias = create_params([n_users])\n        self.movie_factors = create_params([n_movies, n_factors])\n        self.movie_bias = create_params([n_movies])\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors[x[:,0]]\n        movies = self.movie_factors[x[:,1]]\n        raw_output = (users * movies).sum(dim=1)\n        raw_output += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]\n        return sigmoid_range(raw_output, *self.y_range)\n\n\nmodel = DotProductBiasCustomEmbedding(n_users, n_movies, embedding_dim)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(num_epochs, max_learning_rate, wd=weight_decay)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.795456\n0.787701\n00:07\n\n\n1\n0.693971\n0.728376\n00:06\n\n\n2\n0.546227\n0.711909\n00:07\n\n\n3\n0.402994\n0.707349\n00:06\n\n\n4\n0.282765\n0.708693\n00:06\n\n\n\n\n\n\n\n\n\nWe can interrogate the model to learn more about these embeddings it has learned.\n\n\nWe can visualise the biases of our collaborative filter model to see:\n\nMovie biases: Which movies are bad even compared to other similar movies of that type? Lawnmower man 2 is crap even compared to similar action movies. But people love titanic even if they don’t normally like romance dramas.\nUser biases: Which users love any and all movies? Users who give a high rating to all movies.\n\nAccording to our biases, these movies are crap even for those who like that style of movie:\n\nmovie_bias = learn.model.movie_bias.squeeze()\nidxs = movie_bias.argsort()[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Karate Kid, Part III, The (1989)',\n 'Catwoman (2004)',\n 'Stuart Saves His Family (1995)',\n 'Speed 2: Cruise Control (1997)',\n 'Dungeons & Dragons (2000)']\n\n\nWhereas these are highly rated, even when users don’t normally like that type of movie:\n\nidxs = movie_bias.argsort()[-5:]\n[dls.classes['title'][i] for i in idxs]\n\n['Star Wars: Episode IV - A New Hope (1977)',\n 'Dark Knight, The (2008)',\n 'Green Mile, The (1999)',\n 'Forrest Gump (1994)',\n 'Shawshank Redemption, The (1994)']\n\n\n\n\n\nWe can visualise the weights to see what human-interpretable features the model is learning.\nWe can condense our embedding to 2 axes with PCA. we get a critically-acclaimed -&gt; popular x-axis and a action-dialog y-axis.\n\n\n\n\n\n\nTip\n\n\n\nFor more details on PCA and related methods, see computational linear algebra fastai course\n\n\n\ng = ratings.groupby('title')['rating'].count()\ntop_movies = g.sort_values(ascending=False).index.values[:1000]\ntop_idxs = torch.tensor([learn.dls.classes['title'].o2i[m] for m in top_movies])\nmovie_w = learn.model.movie_factors[top_idxs].cpu().detach()\nmovie_pca = movie_w.pca(3)\nfac0,fac1,fac2 = movie_pca.t()\nidxs = list(range(50))\nX = fac0[idxs]\nY = fac2[idxs]\nplt.figure(figsize=(12,12))\nplt.scatter(X, Y)\nfor i, x, y in zip(top_movies[idxs], X, Y):\n    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWe can also use the “embedding distance” (distance in the latent space) to see when two movies are similar. We use cosine similarity to determine this distance, which is similar in principle to Euclidean distance but normalised.\nIn the example below, we start with the movie Forrest Gump and find the closest movie to it in our embedding:\n\nmovie_idx = dls.classes['title'].o2i['Forrest Gump (1994)']\n\nmovie_factors = learn.model.movie_factors\ndistances = torch.nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[movie_idx][None])\nclosest_distance_idx = distances.argsort(descending=True)[1]\ndls.classes['title'][closest_distance_idx]\n\n'Beautiful Mind, A (2001)'"
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html#building-a-collaborative-filter-with-fastais-library",
    "href": "posts/ml/fastai/lesson7/lesson.html#building-a-collaborative-filter-with-fastais-library",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "We can repeat the same exercise using the collaborative filter from the fastai library to see how it compares to our from-scratch implementation.\n\nlearn_fast = collab_learner(dls, n_factors=embedding_dim, y_range=(0, 5.5))\nlearn_fast.fit_one_cycle(num_epochs, max_learning_rate, wd=weight_decay)\n\n\n\n\n\n\n    \n      \n      0.00% [0/5 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n\n\n\n    \n      \n      0.40% [5/1260 00:00&lt;00:20 1.5528]\n    \n    \n\n\nWe can repeat any of the analysis of our from-scratch model. For example, the movies with the highest bias:\n\nmovie_bias = learn_fast.model.i_bias.weight.squeeze()\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Shawshank Redemption, The (1994)',\n 'Forrest Gump (1994)',\n 'Green Mile, The (1999)',\n 'Star Wars: Episode IV - A New Hope (1977)',\n 'Dark Knight, The (2008)']"
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html#bootstrapping-a-collaborative-filtering-model",
    "href": "posts/ml/fastai/lesson7/lesson.html#bootstrapping-a-collaborative-filtering-model",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "How do you start off a collaborative filtering model? For example, when you first start out, you have no data on users or items.\nOr even for established companies, what happens when we have a new user or a new item, so the entire row or column is null?\nThere is no hard and fast solution, they all boil down to “use common sense”.\n\nA tempting option is to fill NaNs with the median latent vectors. But this might result in an odd combination that doesn’t exist in practice, i.e. the latent space isn’t continuous so this could be where a gap in the latent space lies. For example, a medium action, medium sci-fi film with medium romance and medium comedy that is medium popular and medium critically acclaimed.\nAnother option is to pick a user/item that is representative of the average taste.\nCreate a tabular model using answers to a new user survey. Ask the user some questions when they sign up, then create a model where the user’s embedding vector is the dependent variable and their answers, along with any other relevant signup metadata, are the independent variables.\n\nIt is important to be careful of a small number of extremely enthusiastic users dominating the recommendations. For example, people who watch anime watch a LOT of it, and rate a lot of it highly. So this could end up getting recommended to users outside of this niche.\nThis can create positive feedback loops that change the behaviour of your product in unexpected ways."
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html#deep-learning-for-collaborative-filtering",
    "href": "posts/ml/fastai/lesson7/lesson.html#deep-learning-for-collaborative-filtering",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "The matrix completion approach used previously is known as Probabilistic Matrix Factorization (PMF). An alternative approach is to use deep learning.\nIn practice the two approaches are often stacked in an ensemble.\nThis section explores the deep learning collaborative filtering approach from scratch, then recreates it using fastai’s library.\n\n\nWe are concatenating the embedding matrices together, rather than taking the dot product, so that we can pass it through a dense ANN.\nThese matrices can be different sizes, and the size of embedding to use for each depends on the number of classes in the data. Fastai has a heuristic method for this which we use here:\n\n(user_num_classes, user_num_embeddings), (item_num_classes, item_num_embeddings) = get_emb_sz(dls)\n\nWe can then use this in a simple neural network with one hidden layer:\n\nclass CollabNN(Module):\n    def __init__(self, user_embedding_size, item_embedding_size, y_range=(0, 5.5), n_activations=100):\n        self.user_factors = Embedding(*user_embedding_size)\n        self.item_factors = Embedding(*item_embedding_size)\n        self.layers = torch.nn.Sequential(\n            torch.nn.Linear(user_embedding_size[1] + item_embedding_size[1], n_activations),\n            torch.nn.ReLU(),\n            torch.nn.Linear(n_activations, 1))\n        self.y_range = y_range\n        \n    def forward(self, x):\n        embs = self.user_factors(x[:, 0]), self.item_factors(x[:, 1])\n        x = self.layers(torch.cat(embs, dim=1))\n        return sigmoid_range(x, *self.y_range)\n    \ncollab_nn_model = CollabNN(user_embedding_size=(user_num_classes, user_num_embeddings),\n                           item_embedding_size=(item_num_classes, item_num_embeddings))\n\nNow train this model on the data:\n\nlearn_nn = Learner(dls, collab_nn_model, loss_func=MSELossFlat())\nlearn_nn.fit_one_cycle(num_epochs, max_learning_rate, wd=weight_decay)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.799004\n0.792579\n00:10\n\n\n1\n0.747623\n0.755708\n00:10\n\n\n2\n0.706981\n0.723887\n00:10\n\n\n3\n0.650337\n0.719642\n00:10\n\n\n4\n0.569418\n0.734302\n00:10\n\n\n\n\n\n\n\n\nWe can repeat the same exercise using fastai’s implementation.\nThis is almost identical to the PMF approach, simply with an additional argument use_nn=True.\n\nlearn_nn_fast = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100, 50])\nlearn_nn_fast.fit_one_cycle(num_epochs, max_learning_rate, wd=weight_decay)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.838047\n0.801519\n00:14\n\n\n1\n0.761085\n0.744033\n00:12\n\n\n2\n0.709788\n0.734091\n00:13\n\n\n3\n0.653415\n0.728950\n00:13\n\n\n4\n0.545074\n0.743957\n00:12"
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html#summary",
    "href": "posts/ml/fastai/lesson7/lesson.html#summary",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "The recommender problem is one where we have some users and their ratings of some items. We want to know which unseen items a user may like.\nWe implemented two approaches to collaborative filtering:\n\nProbabilistic Matrix Factorization (PMF)\nA neural network\n\nFor each approach, we build a model from scratch in Pytorch, then compared that with fastai’s implementation. For the PMF approach, we even gained some intuition by creating a spreadsheet implementation first!\nIn practice, both approaches can be stacked for improved recommendations.\nThe idea we explored here of user some (initially random) embeddings to represent an entity and then letting our model learn them is a powerful one and it is not limited to collaborative learning. NLP uses embeddings to represent each unique token (i.e. each word with word-level tokenisation). It can then understand relationships between similar words, much like we were able to use embedding distances to identify similar movies."
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html#references",
    "href": "posts/ml/fastai/lesson7/lesson.html#references",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "Course lesson page\nCollaborative filtering notebook\nComputational linear algebra fastai course"
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html",
    "href": "posts/ml/fastai/lesson2/lesson.html",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "These are notes from lesson 2 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\n\nDeploy a model to Huggingface Spaces: see car classifier model\nDeploy a model to a Github Pages website: see car classifier website\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nA website for quizzes based on the book: AI quizzes\n\n\n\n\nIt can be useful to train a model on the data BEFORE you clean it\n\nCounterintuitive!\nThe confusion matrix output of the learner gives you a good intuition about which classifications are hard\nplot_top_losses shows which examples were hardest for the model to classify. This can find (1) when the model is correct but not confident, and (2) when the model was confident but incorrect\nImageClassifierCleaner shows the examples in the training and validation set ordered by loss, so we can choose to keep, reclassify or remove them\n\nFor image resizing, random resize crop can often be more effective.\n\nSquishing can result in weird, unrealistic images.\nPadding or mirroring can add false information that the model will erroneously learn.\nRandom crops give different sections of the image which acts as a form of data augmentation.\naug_transforms can be use for more sophisticated data augmentation like warping and recoloring images.\n\n\n\n\nOnce you are happy with the model you’ve trained, you can pickle the learner object and save it.\nlearn.export('model.pkl')\nThen you can add the saved model to the hugging face space. To use the model to make predictions, any external functions you used to create the model will need to be instantiated too.\nlearn = load_learner('model.pkl')\nlearn.predict(image)\n\n\n\nHugging face spaces hosts models with a choice of pre-canned interfaces to quickly deploy a model to the public. Gradio is used in the example in the lecture. Streamlit is an alternative to Gradio that is more flexible.\nGradio requires a dict of classes as keys and probabilities (as floats not tensors) as the values. To go from the Gradio prototype to a production app, you can view the Gradio API from the huggingface space which will show you the API. The API exposes an endpoint which you can then hit from your own frontend app.\nGithub pages is a free and simple way to host a public website. See this fastai repo as an example of a minimal example html website which issues GET requests to the Gradio API\n\n\n\nTo convert a notebook to a python script, you can add #|export to the top of any cells to include in the script, then use:\nfrom nbdev.export import notebook2script\nnotebook2script('name_of_output_file.py')\nUse #|default_exp app in the first cell of the notebook to set the default name of the exported python file.\n\n\n\nHow do we choose the number of epochs to train for? Whenever it is “good enough” for your use case.\nIf you need to train for longer, you may need to use data augmentation to prevent overfitting. Keep an eye on the validation error to check overfitting.\n\n\n\n\nCourse lesson page\nHuggingFace Spaces"
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html#data-pre-processing",
    "href": "posts/ml/fastai/lesson2/lesson.html#data-pre-processing",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "It can be useful to train a model on the data BEFORE you clean it\n\nCounterintuitive!\nThe confusion matrix output of the learner gives you a good intuition about which classifications are hard\nplot_top_losses shows which examples were hardest for the model to classify. This can find (1) when the model is correct but not confident, and (2) when the model was confident but incorrect\nImageClassifierCleaner shows the examples in the training and validation set ordered by loss, so we can choose to keep, reclassify or remove them\n\nFor image resizing, random resize crop can often be more effective.\n\nSquishing can result in weird, unrealistic images.\nPadding or mirroring can add false information that the model will erroneously learn.\nRandom crops give different sections of the image which acts as a form of data augmentation.\naug_transforms can be use for more sophisticated data augmentation like warping and recoloring images."
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html#saving-a-model",
    "href": "posts/ml/fastai/lesson2/lesson.html#saving-a-model",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "Once you are happy with the model you’ve trained, you can pickle the learner object and save it.\nlearn.export('model.pkl')\nThen you can add the saved model to the hugging face space. To use the model to make predictions, any external functions you used to create the model will need to be instantiated too.\nlearn = load_learner('model.pkl')\nlearn.predict(image)"
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html#hosting-a-model",
    "href": "posts/ml/fastai/lesson2/lesson.html#hosting-a-model",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "Hugging face spaces hosts models with a choice of pre-canned interfaces to quickly deploy a model to the public. Gradio is used in the example in the lecture. Streamlit is an alternative to Gradio that is more flexible.\nGradio requires a dict of classes as keys and probabilities (as floats not tensors) as the values. To go from the Gradio prototype to a production app, you can view the Gradio API from the huggingface space which will show you the API. The API exposes an endpoint which you can then hit from your own frontend app.\nGithub pages is a free and simple way to host a public website. See this fastai repo as an example of a minimal example html website which issues GET requests to the Gradio API"
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html#using-nbdev",
    "href": "posts/ml/fastai/lesson2/lesson.html#using-nbdev",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "To convert a notebook to a python script, you can add #|export to the top of any cells to include in the script, then use:\nfrom nbdev.export import notebook2script\nnotebook2script('name_of_output_file.py')\nUse #|default_exp app in the first cell of the notebook to set the default name of the exported python file."
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html#training-the-model-and-when-to-stop",
    "href": "posts/ml/fastai/lesson2/lesson.html#training-the-model-and-when-to-stop",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "How do we choose the number of epochs to train for? Whenever it is “good enough” for your use case.\nIf you need to train for longer, you may need to use data augmentation to prevent overfitting. Keep an eye on the validation error to check overfitting."
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html#references",
    "href": "posts/ml/fastai/lesson2/lesson.html#references",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "Course lesson page\nHuggingFace Spaces"
  },
  {
    "objectID": "posts/ml/fastai/lesson4/lesson.html",
    "href": "posts/ml/fastai/lesson4/lesson.html",
    "title": "FastAI Lesson 4: Natural Language Processing",
    "section": "",
    "text": "These are notes from lesson 4 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\nKaggle NLP pattern similarity notebook: see notebook\n\n\n\n\nNLP applications: categorising documents, translation, text generation.\nWe are using the Huggingface transformers library for this lesson. It is now incorporated into the fastai library.\nULMFit is an algorithm which uses fine-tuning, in this example to train a positve/negative sentiment classifier in 3 steps: 1. Train an RNN on wikipedia to predict the next word. No labels required. 2. Fine-tune this for IMDb reviews to predict the next word of a movie review. Still no labels required. 3. Fine-tune this to classify the sentiment.\nTransformers have overtaken ULMFit as the state-of-the-art.\nLooking “inside” a CNN, the first layer contains elementary detectors like edge detectors, blob detectors, gradient detectors etc. These get combined in non-linear ways to make increasingly complex detectors. Layer 2 might combine vertical and horizontal edge detectors into a corner detector. By the later layers, it is detecting rich features like lizard eyes, dog faces etc. See Zeiler and Fegus.\nFor the fine-tuning process, the earlier layers are unlikely to need changing because they are more general. So we only need to fine-tune (i.e. re-train) the later layers.\n\n\n\nAs an example of NLP in practice, we walk through this notebook for U.S. Patent Phrase to Phrase Matching.\nReshape the input to fit a standard NLP task\n\nWe want to learn the similarity between two fields and are provided with similarity scores.\nWe concat the fields of interest (with identifiers in between). The identifiers themselves are not important, they just need to be consistent.\nThe NLP model is then a supervised regression task to predict the score given the concatendated string.\n\ndf['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor\n\n\nSplit the text into tokens (words). Tokens are, broadly speaking, words.\nThere are some caveats to that, as some languages like Chinese don’t fit nicely into that model. We don’t want the vocabulary to be too big. Alternative approaches use character tokenization rather than word tokenization.\nIn practice, we tokenize into subwords.\n\n\n\nMap each unique token to a number. One-hot encoding.\nThe choice of tokenization and numericalization depends on the model you use. Whoever trained the model chose a convention for tokenizing. We need to be consistent with that if we want the model to work correctly.\n\n\n\nThe Huggingface model hub contains thousands of pretrained models.\nFor NLP tasks, it is useful to choose a model that was trained on a similar corpus, so you can search the model hub. In this case, we search for “patent”.\nSome models are general purpose, e.g. deberta-v3 used in the lesson.\nULMFit handles large documents better as it can split up the document. Transformer approaches require loading the whole document into GPU memory, so struggle for larger documents.\n\n\n\nIf a model is too simple (i.e. not flexible enough) then it cannot fit the data and be biased, leading to underfitting.\nIf the model fits the data points too closely, it is overfitting.\nA good validation set, and monitoring validation error rather than training error as a metric, is key to avoiding overfitting. See this article for an in-depth discussion on the importance of choosing a good validation set.\nOften people will default to using a random train/test split. This is what scikit-learn uses. This is a BAD idea very often.\nFor time-series data, it’s easier to infer gaps than it is to predict a block in the future. The latter is the more common task but a random split simulates the former, giving unrealistically good performance.\nFor image data, there may be people, boats, etc that are in the training set but not the test set. By failing to have new people in the validation set, the model can learn things about specific people/boats that it can’t rely on in practice.\n\n\n\nMetrics are things that are human-understandable.\nLoss functions should be smooth and differentiable to aid in training.\nThese can sometimes be the same thing, but not in general. For example, accuracy is a good metric in image classification. We could tweak the weights in such a way that it improves the model slightly, but not so much that it now correctly classifies a previously incorrect image. This means the metric function is bumpy, therefore a bad loss function.\nThis article goes into more detail on the choice of metrics.\nAI can be particularly dangerous at confirming systematic biases, because it is so good at optimising metrics, so it will conform to any biases present in the training data. Making decisions based on the model then reinforces those biases.\n\nGoodhart’s law applies: If a metric becomes a target it’s no longer a good metric\n\n\n\n\nThe best way to understand a metric is not to look at the mathematical formula, but to plot some data for which the metric is high, medium and low, then see what that tells you.\nAfter that, look at the equation to see if your intuition matches the logic.\n\n\n\nFast AI has a function to find a good starting point. Otherwise, pick a small value and keep doubling it until it falls apart.\n\n\n\n\n\nCourse lesson page\nHuggingFace Transformers\nVisualizing and Understanding Convolutional Networks\nHuggingFace models\nValidation sets\nModel metrics"
  },
  {
    "objectID": "posts/ml/fastai/lesson4/lesson.html#nlp-background-and-transformers",
    "href": "posts/ml/fastai/lesson4/lesson.html#nlp-background-and-transformers",
    "title": "FastAI Lesson 4: Natural Language Processing",
    "section": "",
    "text": "NLP applications: categorising documents, translation, text generation.\nWe are using the Huggingface transformers library for this lesson. It is now incorporated into the fastai library.\nULMFit is an algorithm which uses fine-tuning, in this example to train a positve/negative sentiment classifier in 3 steps: 1. Train an RNN on wikipedia to predict the next word. No labels required. 2. Fine-tune this for IMDb reviews to predict the next word of a movie review. Still no labels required. 3. Fine-tune this to classify the sentiment.\nTransformers have overtaken ULMFit as the state-of-the-art.\nLooking “inside” a CNN, the first layer contains elementary detectors like edge detectors, blob detectors, gradient detectors etc. These get combined in non-linear ways to make increasingly complex detectors. Layer 2 might combine vertical and horizontal edge detectors into a corner detector. By the later layers, it is detecting rich features like lizard eyes, dog faces etc. See Zeiler and Fegus.\nFor the fine-tuning process, the earlier layers are unlikely to need changing because they are more general. So we only need to fine-tune (i.e. re-train) the later layers."
  },
  {
    "objectID": "posts/ml/fastai/lesson4/lesson.html#kaggle-competition-walkthrough",
    "href": "posts/ml/fastai/lesson4/lesson.html#kaggle-competition-walkthrough",
    "title": "FastAI Lesson 4: Natural Language Processing",
    "section": "",
    "text": "As an example of NLP in practice, we walk through this notebook for U.S. Patent Phrase to Phrase Matching.\nReshape the input to fit a standard NLP task\n\nWe want to learn the similarity between two fields and are provided with similarity scores.\nWe concat the fields of interest (with identifiers in between). The identifiers themselves are not important, they just need to be consistent.\nThe NLP model is then a supervised regression task to predict the score given the concatendated string.\n\ndf['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor\n\n\nSplit the text into tokens (words). Tokens are, broadly speaking, words.\nThere are some caveats to that, as some languages like Chinese don’t fit nicely into that model. We don’t want the vocabulary to be too big. Alternative approaches use character tokenization rather than word tokenization.\nIn practice, we tokenize into subwords.\n\n\n\nMap each unique token to a number. One-hot encoding.\nThe choice of tokenization and numericalization depends on the model you use. Whoever trained the model chose a convention for tokenizing. We need to be consistent with that if we want the model to work correctly.\n\n\n\nThe Huggingface model hub contains thousands of pretrained models.\nFor NLP tasks, it is useful to choose a model that was trained on a similar corpus, so you can search the model hub. In this case, we search for “patent”.\nSome models are general purpose, e.g. deberta-v3 used in the lesson.\nULMFit handles large documents better as it can split up the document. Transformer approaches require loading the whole document into GPU memory, so struggle for larger documents.\n\n\n\nIf a model is too simple (i.e. not flexible enough) then it cannot fit the data and be biased, leading to underfitting.\nIf the model fits the data points too closely, it is overfitting.\nA good validation set, and monitoring validation error rather than training error as a metric, is key to avoiding overfitting. See this article for an in-depth discussion on the importance of choosing a good validation set.\nOften people will default to using a random train/test split. This is what scikit-learn uses. This is a BAD idea very often.\nFor time-series data, it’s easier to infer gaps than it is to predict a block in the future. The latter is the more common task but a random split simulates the former, giving unrealistically good performance.\nFor image data, there may be people, boats, etc that are in the training set but not the test set. By failing to have new people in the validation set, the model can learn things about specific people/boats that it can’t rely on in practice.\n\n\n\nMetrics are things that are human-understandable.\nLoss functions should be smooth and differentiable to aid in training.\nThese can sometimes be the same thing, but not in general. For example, accuracy is a good metric in image classification. We could tweak the weights in such a way that it improves the model slightly, but not so much that it now correctly classifies a previously incorrect image. This means the metric function is bumpy, therefore a bad loss function.\nThis article goes into more detail on the choice of metrics.\nAI can be particularly dangerous at confirming systematic biases, because it is so good at optimising metrics, so it will conform to any biases present in the training data. Making decisions based on the model then reinforces those biases.\n\nGoodhart’s law applies: If a metric becomes a target it’s no longer a good metric\n\n\n\n\nThe best way to understand a metric is not to look at the mathematical formula, but to plot some data for which the metric is high, medium and low, then see what that tells you.\nAfter that, look at the equation to see if your intuition matches the logic.\n\n\n\nFast AI has a function to find a good starting point. Otherwise, pick a small value and keep doubling it until it falls apart."
  },
  {
    "objectID": "posts/ml/fastai/lesson4/lesson.html#references",
    "href": "posts/ml/fastai/lesson4/lesson.html#references",
    "title": "FastAI Lesson 4: Natural Language Processing",
    "section": "",
    "text": "Course lesson page\nHuggingFace Transformers\nVisualizing and Understanding Convolutional Networks\nHuggingFace models\nValidation sets\nModel metrics"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson1/lesson.html",
    "href": "posts/ml/generative_deep_learning/lesson1/lesson.html",
    "title": "Generative AI: Intro",
    "section": "",
    "text": "The 30000ft view of generative AI.\n\n\n\nGenerative modeling is a branch of machine learning that involves training a model to produce new data that is similar ot a given dataset.\n\nThis means we require some training data which we wish to imitate. The model should be probabilistic rather than deterministic, so that we can sample different variations of the output.\nA helpful way to define generative modeling is by contrasting it with its counterpart discriminative modeling:\n\nDiscriminative modeling estimates \\(p(y|x)\\) - the probability of a label \\(y\\) given some observation \\(x\\)\nGenerative modeling estimates \\(p(x)\\) - the probability of seeing an observation \\(x\\). By sampling from this distribution we can generate new observations.\n\nWe can also create a conditional generative model to estimate \\(p(x|y)\\), the probability of seeing an observation \\(x\\) with label \\(y\\). For ample, if we train a model to generate a given type of fruit.\n\n\n\n\n\nThe aim:\n\nWe have training data of observations \\(X\\)\nWe assume the training data has been generated by an unknown distribution \\(p_{data}\\)\nWe want to build a generative model \\(p_{model}\\) that mimics \\(p_{data}\\)\n\nIf we succeed, we can sample from \\(p_{model}\\) to generate synthetic observations\n\n\nDesirable properties of \\(p_{model}\\) are:\n\nAccuracy - If \\(p_{model}\\) is a high value it should look like a real observation, and likewise low values should look fake.\nGeneration - It should be easy to sample from it.\nRepresentation - It should be possible to understand how high-level features are represented by the model.\n\n\n\n\nWe want to be able to describe things in terms of high-level features. For example, when describing appearance, we might talk about hair colour, length, eye colour, etc. Not RGB values pixel by pixel…\nThis is the idea behind representation learning. We describe each training data observation using some lower-dimensional latent space. Then we learn a maaping function from the latent space to the original domain.\nEncoder-decoder techniques try to transform a high-dimensional nonlinear manifold into a simpler latent space.\n\n\n\n\n\nThe complete set of all values that an observation \\(x\\) can take.\n\n\n\nA function \\(p(x)\\) that maps a point \\(x\\) in the sample space to a number between 0 and 1. The integral over the sample space must equal 1 for it to be a well-define probability distribtution.\nThere is one true data distribution \\(p_{data}\\) but infinitely many approximations \\(p_{model}\\) that we can find.\n\n\n\nWe can structure our approach to finding \\(p_{model}\\) by restricting ourselves to a family of density functions \\(p_{\\theta}(x)\\) which can be described with a finite set of parameters \\(\\theta\\).\nFor example, restricting our search to a linear model \\(y = w*x + b\\) where we need to find the parameters \\(w\\) and \\(b\\).\n\n\n\nThe likelihood, \\(L(\\theta|x)\\), of a parameter set \\(\\theta\\) is a function which measures the plausability of the parameters having observed some data point \\(x\\). It is the probability of seeing the data \\(x\\) if the true data-generating distribution was the model parameterized by \\(\\theta\\).\nIt is defined as the parametric model density: \\[\nL(\\theta|x) = p_{\\theta}(x)\n\\]\nIf we have a dataset of independent observations \\(X\\) then the probabilities multiply: \\[\nL(\\theta|X) = \\prod_{x \\in X} p_{\\theta}(x)\n\\]\nThe product of a lot of decimals can get unwieldy, so we often take the log-likelihood \\(l\\) to turn the product into a sum: \\[\nl(\\theta|X) = \\sum_{x \\in X} \\log{p_{\\theta}(x) }\n\\]\nThe focus of parametric modeling is therefore to find the optimal parameter set \\(\\hat{\\theta}\\), which leads to…\n\n\n\nA technique to estimate \\(\\hat{\\theta}\\), the set of parameters that is most likely to explain the observed data \\(X\\): \\[\n\\hat{\\theta} = \\arg\\max_x l(\\theta|X)\n\\]\nNeural networks often work with a loss function, so this is equivalent to minimising the negative log-likelihood: \\[\n\\hat{\\theta} = \\arg\\min_\\theta -l(\\theta|X) = \\arg\\min_\\theta -\\log{p_{\\theta}(X)}\n\\]\n\n\n\nGenerative modeling is a form of MLE where the parameters are the neural network weights. We want to find the weights that maximise the likelihood of observing the training data.\nBut for high-dimensional problems \\(p_{\\theta}(X)\\) is intractable, it cannot be calculated directly.\nThere are different approaches to maing the problem tractable. These are summarised in the following section and explored in more detail in subsequent chapters.\n\n\n\n\nAll types of generative model are seeking to solve the same problem, but they take different approaches to modeling the intractable density function \\(p_{\\theta}(x)\\).\nThere are three general approaches:\n\nExplicitly model the density function but constrain the model in some way.\nExplicitly model a tractable approximation of the density function.\nImplicitly model the density function, using a stochastic process that generates the data directly.\n\n\n\n\n\n\nflowchart LR\n\n\n  A(Generative models) --&gt; B1(Explicit density)\n  A(Generative models) --&gt; B2(Implicit density)\n\n  B1 --&gt; C1(Approximate density)\n  B1 --&gt; C2(Tractable density)\n\n  C1 --&gt; D1(VAE)\n  C1 --&gt; D2(Energy-based model)\n  C1 --&gt; D3(Diffusion model)\n\n  C2 --&gt; D4(Autoregressive model)\n  C2 --&gt; D5(Normalizing flow model)\n\n  B2 --&gt; D6(GAN)\n\n\n\n\n\n\n\nImplicit density models don’t even try to estimate the probability density, instead focusing on producing a stochastic data generation process directly.\nTractable models place constraints on the model architecture (i.e. the parameters \\(\\theta\\)).\n\n\n\n\n\nChapter 1 of Generative Deep Learning by David Foster."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson1/lesson.html#what-is-generative-modeling",
    "href": "posts/ml/generative_deep_learning/lesson1/lesson.html#what-is-generative-modeling",
    "title": "Generative AI: Intro",
    "section": "",
    "text": "Generative modeling is a branch of machine learning that involves training a model to produce new data that is similar ot a given dataset.\n\nThis means we require some training data which we wish to imitate. The model should be probabilistic rather than deterministic, so that we can sample different variations of the output.\nA helpful way to define generative modeling is by contrasting it with its counterpart discriminative modeling:\n\nDiscriminative modeling estimates \\(p(y|x)\\) - the probability of a label \\(y\\) given some observation \\(x\\)\nGenerative modeling estimates \\(p(x)\\) - the probability of seeing an observation \\(x\\). By sampling from this distribution we can generate new observations.\n\nWe can also create a conditional generative model to estimate \\(p(x|y)\\), the probability of seeing an observation \\(x\\) with label \\(y\\). For ample, if we train a model to generate a given type of fruit."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson1/lesson.html#the-generative-modeling-framework",
    "href": "posts/ml/generative_deep_learning/lesson1/lesson.html#the-generative-modeling-framework",
    "title": "Generative AI: Intro",
    "section": "",
    "text": "The aim:\n\nWe have training data of observations \\(X\\)\nWe assume the training data has been generated by an unknown distribution \\(p_{data}\\)\nWe want to build a generative model \\(p_{model}\\) that mimics \\(p_{data}\\)\n\nIf we succeed, we can sample from \\(p_{model}\\) to generate synthetic observations\n\n\nDesirable properties of \\(p_{model}\\) are:\n\nAccuracy - If \\(p_{model}\\) is a high value it should look like a real observation, and likewise low values should look fake.\nGeneration - It should be easy to sample from it.\nRepresentation - It should be possible to understand how high-level features are represented by the model."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson1/lesson.html#representation-learning",
    "href": "posts/ml/generative_deep_learning/lesson1/lesson.html#representation-learning",
    "title": "Generative AI: Intro",
    "section": "",
    "text": "We want to be able to describe things in terms of high-level features. For example, when describing appearance, we might talk about hair colour, length, eye colour, etc. Not RGB values pixel by pixel…\nThis is the idea behind representation learning. We describe each training data observation using some lower-dimensional latent space. Then we learn a maaping function from the latent space to the original domain.\nEncoder-decoder techniques try to transform a high-dimensional nonlinear manifold into a simpler latent space."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson1/lesson.html#core-probability-theory",
    "href": "posts/ml/generative_deep_learning/lesson1/lesson.html#core-probability-theory",
    "title": "Generative AI: Intro",
    "section": "",
    "text": "The complete set of all values that an observation \\(x\\) can take.\n\n\n\nA function \\(p(x)\\) that maps a point \\(x\\) in the sample space to a number between 0 and 1. The integral over the sample space must equal 1 for it to be a well-define probability distribtution.\nThere is one true data distribution \\(p_{data}\\) but infinitely many approximations \\(p_{model}\\) that we can find.\n\n\n\nWe can structure our approach to finding \\(p_{model}\\) by restricting ourselves to a family of density functions \\(p_{\\theta}(x)\\) which can be described with a finite set of parameters \\(\\theta\\).\nFor example, restricting our search to a linear model \\(y = w*x + b\\) where we need to find the parameters \\(w\\) and \\(b\\).\n\n\n\nThe likelihood, \\(L(\\theta|x)\\), of a parameter set \\(\\theta\\) is a function which measures the plausability of the parameters having observed some data point \\(x\\). It is the probability of seeing the data \\(x\\) if the true data-generating distribution was the model parameterized by \\(\\theta\\).\nIt is defined as the parametric model density: \\[\nL(\\theta|x) = p_{\\theta}(x)\n\\]\nIf we have a dataset of independent observations \\(X\\) then the probabilities multiply: \\[\nL(\\theta|X) = \\prod_{x \\in X} p_{\\theta}(x)\n\\]\nThe product of a lot of decimals can get unwieldy, so we often take the log-likelihood \\(l\\) to turn the product into a sum: \\[\nl(\\theta|X) = \\sum_{x \\in X} \\log{p_{\\theta}(x) }\n\\]\nThe focus of parametric modeling is therefore to find the optimal parameter set \\(\\hat{\\theta}\\), which leads to…\n\n\n\nA technique to estimate \\(\\hat{\\theta}\\), the set of parameters that is most likely to explain the observed data \\(X\\): \\[\n\\hat{\\theta} = \\arg\\max_x l(\\theta|X)\n\\]\nNeural networks often work with a loss function, so this is equivalent to minimising the negative log-likelihood: \\[\n\\hat{\\theta} = \\arg\\min_\\theta -l(\\theta|X) = \\arg\\min_\\theta -\\log{p_{\\theta}(X)}\n\\]\n\n\n\nGenerative modeling is a form of MLE where the parameters are the neural network weights. We want to find the weights that maximise the likelihood of observing the training data.\nBut for high-dimensional problems \\(p_{\\theta}(X)\\) is intractable, it cannot be calculated directly.\nThere are different approaches to maing the problem tractable. These are summarised in the following section and explored in more detail in subsequent chapters."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson1/lesson.html#generative-model-taxonomy",
    "href": "posts/ml/generative_deep_learning/lesson1/lesson.html#generative-model-taxonomy",
    "title": "Generative AI: Intro",
    "section": "",
    "text": "All types of generative model are seeking to solve the same problem, but they take different approaches to modeling the intractable density function \\(p_{\\theta}(x)\\).\nThere are three general approaches:\n\nExplicitly model the density function but constrain the model in some way.\nExplicitly model a tractable approximation of the density function.\nImplicitly model the density function, using a stochastic process that generates the data directly.\n\n\n\n\n\n\nflowchart LR\n\n\n  A(Generative models) --&gt; B1(Explicit density)\n  A(Generative models) --&gt; B2(Implicit density)\n\n  B1 --&gt; C1(Approximate density)\n  B1 --&gt; C2(Tractable density)\n\n  C1 --&gt; D1(VAE)\n  C1 --&gt; D2(Energy-based model)\n  C1 --&gt; D3(Diffusion model)\n\n  C2 --&gt; D4(Autoregressive model)\n  C2 --&gt; D5(Normalizing flow model)\n\n  B2 --&gt; D6(GAN)\n\n\n\n\n\n\n\nImplicit density models don’t even try to estimate the probability density, instead focusing on producing a stochastic data generation process directly.\nTractable models place constraints on the model architecture (i.e. the parameters \\(\\theta\\))."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson1/lesson.html#references",
    "href": "posts/ml/generative_deep_learning/lesson1/lesson.html#references",
    "title": "Generative AI: Intro",
    "section": "",
    "text": "Chapter 1 of Generative Deep Learning by David Foster."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson3/chapter3.html",
    "href": "posts/ml/generative_deep_learning/lesson3/chapter3.html",
    "title": "Generative AI: VAEs",
    "section": "",
    "text": "Notes on VAEs.\n\n\n\n\n\n\nStory Time\n\n\n\nImagine an infinite wardrobe organised by “type” of clothing.\nShoes would be close together, but formal shoes might be closer to the suits and trainers closer to the sports gear. Shirts and t-shirts would be close together. Coats might be nearby; the shirt-&gt;coat vector applied to t-shirts might lead you to “invent” gilets.\nThis encapsulates the idea of using a lower dimensional (2D in this case) latent space to encode the representation of more complex objects.\nWe could sample from some of the empty spaces to invent new hybrids of clothing. This generative step is decoding the latent space.\n\n\n\n\nThe idea of autoencoders (read: self-encoders) is that they learn to simplify the input then reconstruct it; the input and target output are the same.\n\nThe encoder learns to compress high-dimensional input data into a lower dimensional representation called the embedding.\nThe decoder takes an embedding and recreates a higher-dimensional image. This should be an accurate reconstruction of the input.\n\nThis can be used as a generative model because we can the sample and decode new points from the latent space to generate novel outputs. The goal of training an autoencoder is to learn a meaningful embedding \\(z\\).\n\n\n\n\n\nflowchart LR\n\n  A(Encoder) --&gt; B(z)\n  B(z) --&gt; c(Decoder)\n\n\n\n\n\n\nThis also makes autoencoders useful as denoising models, because the embedding should retain the salient information but “lose” the noise.\n\n\n\nWe will implement an autoencoder to learn lower-dimensional embeddings for the fashion MNIST data set.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nimport tensorflow as tf\nfrom tensorflow.keras import (\n    layers,\n    models,\n    datasets,\n    callbacks,\n    losses,\n    optimizers,\n    metrics,\n)\n\n\n# Parameters\nIMAGE_SIZE = 32\nCHANNELS = 1\nBATCH_SIZE = 100\nBUFFER_SIZE = 1000\nVALIDATION_SPLIT = 0.2\nEMBEDDING_DIM = 2\nEPOCHS = 3\n\n\n\n\nScale the pixel values and reshape the images.\n\n\nCode\n(x_train, y_train), (x_test, y_test) = datasets.fashion_mnist.load_data()\n\ndef preprocess(images):\n    images = images.astype(\"float32\") / 255.0\n    images = np.pad(images, ((0, 0), (2, 2), (2, 2)), constant_values=0.0)\n    images = np.expand_dims(images, -1)\n    return images\n\nx_train = preprocess(x_train)\nx_test = preprocess(x_test)\n\n\nWe can see an example from our training set:\n\n\nCode\nplt.imshow(x_train[0])\n\n\n\n\n\n\n\n\n\n\n\n\nThe encoder compresses the dimensionality on the input to a smaller embedding dimension.\n\n\nCode\n# Input\nencoder_input = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS),name=\"encoder_input\")\n\n# Conv layers\nx = layers.Conv2D(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(encoder_input)\nx = layers.Conv2D(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2D(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\npre_flatten_shape = tf.keras.backend.int_shape(x)[1:]  # Used by the decoder later\n\n# Output\nx = layers.Flatten()(x)\nencoder_output = layers.Dense(EMBEDDING_DIM, name=\"encoder_output\")(x)\n\n# Model\nencoder = models.Model(encoder_input, encoder_output)\nencoder.summary()\n\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n encoder_input (InputLayer)  [(None, 32, 32, 1)]       0         \n                                                                 \n conv2d (Conv2D)             (None, 16, 16, 32)        320       \n                                                                 \n conv2d_1 (Conv2D)           (None, 8, 8, 64)          18496     \n                                                                 \n conv2d_2 (Conv2D)           (None, 4, 4, 128)         73856     \n                                                                 \n flatten (Flatten)           (None, 2048)              0         \n                                                                 \n encoder_output (Dense)      (None, 2)                 4098      \n                                                                 \n=================================================================\nTotal params: 96770 (378.01 KB)\nTrainable params: 96770 (378.01 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\nThe decoder reconstructs the original image from the embedding.\n\n\nIn a standard convolutional layer, if we have stride=2 it will half the image size.\nIn a convolutional transpose layer, we are increasing the image size. The stride parameter determines the amount of zero padding to add between each pixel. A kernel is then applied to this “internally padded” image to expand the image size.\n\n\nCode\n# Input\ndecoder_input = layers.Input(shape=(EMBEDDING_DIM,),name=\"decoder_input\")\n\n# Reshape the input using the pre-flattening shape from the encoder\nx = layers.Dense(np.prod(pre_flatten_shape))(decoder_input)\nx = layers.Reshape(pre_flatten_shape)(x)\n\n# Scale up the image back to its original size. These are the reverse of the conv layers applied in the encoder.\nx = layers.Conv2DTranspose(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2DTranspose(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n\n# Output\ndecoder_output = layers.Conv2D(\n    CHANNELS,\n    (3, 3),\n    strides=1,\n    activation='sigmoid',\n    padding=\"same\",\n    name=\"decoder_output\",\n)(x)\n\n# Model\ndecoder = models.Model(decoder_input, decoder_output)\ndecoder.summary()\n\n\nModel: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n decoder_input (InputLayer)  [(None, 2)]               0         \n                                                                 \n dense (Dense)               (None, 2048)              6144      \n                                                                 \n reshape (Reshape)           (None, 4, 4, 128)         0         \n                                                                 \n conv2d_transpose (Conv2DTr  (None, 8, 8, 128)         147584    \n anspose)                                                        \n                                                                 \n conv2d_transpose_1 (Conv2D  (None, 16, 16, 64)        73792     \n Transpose)                                                      \n                                                                 \n conv2d_transpose_2 (Conv2D  (None, 32, 32, 32)        18464     \n Transpose)                                                      \n                                                                 \n decoder_output (Conv2D)     (None, 32, 32, 1)         289       \n                                                                 \n=================================================================\nTotal params: 246273 (962.00 KB)\nTrainable params: 246273 (962.00 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\n\nCombine the encoder and decoder into a single model.\n\n\nCode\nautoencoder = models.Model(encoder_input, decoder(encoder_output))\nautoencoder.summary()\n\n\nModel: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n encoder_input (InputLayer)  [(None, 32, 32, 1)]       0         \n                                                                 \n conv2d (Conv2D)             (None, 16, 16, 32)        320       \n                                                                 \n conv2d_1 (Conv2D)           (None, 8, 8, 64)          18496     \n                                                                 \n conv2d_2 (Conv2D)           (None, 4, 4, 128)         73856     \n                                                                 \n flatten (Flatten)           (None, 2048)              0         \n                                                                 \n encoder_output (Dense)      (None, 2)                 4098      \n                                                                 \n model_1 (Functional)        (None, 32, 32, 1)         246273    \n                                                                 \n=================================================================\nTotal params: 343043 (1.31 MB)\nTrainable params: 343043 (1.31 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\nThe autoencoder is trained with the source images as both input and target output.\nThe loss function is usually chosen as either RMSE or binary cross-entropy between pixels of original image vs reconstruction.\n\n\nCode\nautoencoder.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\nautoencoder.fit(\n    x_train,\n    x_train,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    validation_data=(x_test, x_test)\n)\n\n\nEpoch 1/3\n600/600 [==============================] - 35s 58ms/step - loss: 0.2910 - val_loss: 0.2610\nEpoch 2/3\n600/600 [==============================] - 36s 60ms/step - loss: 0.2569 - val_loss: 0.2561\nEpoch 3/3\n600/600 [==============================] - 34s 57ms/step - loss: 0.2536 - val_loss: 0.2540\n\n\n&lt;keras.src.callbacks.History at 0x156e4a310&gt;\n\n\n\n\n\n\nWe can use our trained autoencoder to:\n\nReconstruct images\nAnalyse embeddings\nGenerate new images\n\n\n\nReconstruct a sample of test images using the autoencoder.\nThe reconstruction isn’t perfect; some information is lost when reducing down to just 2 dimensions. But it does a surprisingly good job of compressing 32x32 pixel values into just 2 embedding values.\n\n\nCode\nNUM_IMAGES_TO_RECONSTRUCT = 5000\nexample_images = x_test[:NUM_IMAGES_TO_RECONSTRUCT]\nexample_labels = y_test[:NUM_IMAGES_TO_RECONSTRUCT]\n\npredictions = autoencoder.predict(example_images)\n\n\n  7/157 [&gt;.............................] - ETA: 1s 157/157 [==============================] - 1s 8ms/step\n\n\nOriginal images:\n\n\nCode\ndef plot_sample_images(images, n=10, size=(20, 3), cmap=\"gray_r\"):\n    plt.figure(figsize=size)\n    for i in range(n):\n        _ = plt.subplot(1, n, i + 1)\n        plt.imshow(images[i].astype(\"float32\"), cmap=cmap)\n        plt.axis(\"off\")\n    \n    plt.show()\n\nplot_sample_images(example_images)\n\n\n\n\n\n\n\n\n\nReconstructed images:\n\n\nCode\nplot_sample_images(predictions)\n\n\n\n\n\n\n\n\n\n\n\n\nEach of the images above has been encoded as a 2-dimensional embedding.\nWe can look at these embeddings to gain some insight into how the autoencoder works.\nThe embedding vectors for our sample images above:\n\n\nCode\n# Encode the example images\nembeddings = encoder.predict(example_images)\nprint(embeddings[:10])\n\n\n102/157 [==================&gt;...........] - ETA: 0s157/157 [==============================] - 0s 2ms/step\n[[ 2.2441912  -2.711683  ]\n [ 6.1558456   6.0202003 ]\n [-3.787192    7.3368516 ]\n [-2.5938551   4.2098355 ]\n [ 3.8645594   2.7229536 ]\n [-2.0130231   6.0485506 ]\n [ 1.2749226   2.1347647 ]\n [ 2.8239484   2.898773  ]\n [-0.48542604 -1.0869933 ]\n [ 0.30643728 -2.6099105 ]]\n\n\nWe can plot the 2D latent space, colouring each point by its label. This shows how similar items are clustered together in latent space.\nThis is impressive! Remember, we never showed the model the labels when training, so it has learned to cluster images that look alike.\n\n\nCode\n# Colour the embeddings by their label\nexample_labels = y_test[:NUM_IMAGES_TO_RECONSTRUCT]\n\n# Plot the latent space\nfigsize = 8\nplt.figure(figsize=(figsize, figsize))\nplt.scatter(\n    embeddings[:, 0],\n    embeddings[:, 1],\n    cmap=\"rainbow\",\n    c=example_labels,\n    alpha=0.6,\n    s=3,\n)\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nWe can sample from the latent space and decode these sampled points to generate new images.\nFirst we sample some random points in the latent space:\n\n\nCode\n# Get the range of existing embedding values so we can sample sensible points within the latent space.\nembedding_min = np.min(embeddings, axis=0)\nembedding_max = np.max(embeddings, axis=0)\n\n# Sample some points\ngrid_width = 6\ngrid_height = 3\nsample = np.random.uniform(\n    embedding_min, embedding_max, size=(grid_width * grid_height, EMBEDDING_DIM)\n)\nprint(sample)\n\n\n[[ 1.47862929  9.28394749]\n [-3.19389344 -3.04713146]\n [-0.57161452 -0.35644389]\n [10.97632621 -2.12482484]\n [ 4.05160668  9.04420005]\n [ 9.50105167  5.71270956]\n [ 3.24765456  4.95969011]\n [-3.68217634  4.52120851]\n [-1.7067196   5.87696959]\n [ 5.99883565 -2.11597183]\n [ 1.84553131  6.04266323]\n [ 0.15552252  1.98655625]\n [ 3.55479856  2.35587959]\n [-0.32278762  6.07537408]\n [ 8.98977414 -1.15893539]\n [ 2.1476981   4.97819188]\n [-2.0896675   3.9166368 ]\n [ 6.49229371 -4.75611412]]\n\n\nWe can then decode these sampled points.\n\n\nCode\n# Decode the sampled points\nreconstructions = decoder.predict(sample)\n\n\n1/1 [==============================] - 0s 59ms/step\n\n\n\n\nCode\nfigsize = 8\nplt.figure(figsize=(figsize, figsize))\n\n# Plot the latent space and overlay the positions of the sampled points\nplt.scatter(embeddings[:, 0], embeddings[:, 1], c=\"black\", alpha=0.5, s=2)\nplt.scatter(sample[:, 0], sample[:, 1], c=\"#00B0F0\", alpha=1, s=40)\nplt.show()\n\n# Plot a grid of the reconstructed images which decode those sampled points\nfig = plt.figure(figsize=(figsize, grid_height * 2))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i in range(grid_width * grid_height):\n    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n    ax.axis(\"off\")\n    ax.text(\n        0.5,\n        -0.35,\n        str(np.round(sample[i, :], 1)),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow let’s see what happens when we regularly sample the latent space.\n\n\nCode\n# Colour the embeddings by their label (clothing type - see table)\nfigsize = 12\ngrid_size = 15\nplt.figure(figsize=(figsize, figsize))\nplt.scatter(\n    embeddings[:, 0],\n    embeddings[:, 1],\n    cmap=\"rainbow\",\n    c=example_labels,\n    alpha=0.8,\n    s=300,\n)\nplt.colorbar()\n\nx = np.linspace(min(embeddings[:, 0]), max(embeddings[:, 0]), grid_size)\ny = np.linspace(max(embeddings[:, 1]), min(embeddings[:, 1]), grid_size)\nxv, yv = np.meshgrid(x, y)\nxv = xv.flatten()\nyv = yv.flatten()\ngrid = np.array(list(zip(xv, yv)))\n\nreconstructions = decoder.predict(grid)\n# plt.scatter(grid[:, 0], grid[:, 1], c=\"black\", alpha=1, s=10)\nplt.show()\n\nfig = plt.figure(figsize=(figsize, figsize))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\nfor i in range(grid_size**2):\n    ax = fig.add_subplot(grid_size, grid_size, i + 1)\n    ax.axis(\"off\")\n    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")\n\n\n8/8 [==============================] - 0s 9ms/step\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe latent space exploration above yields some interesting insights into “regular” autoencoders that motivate the use of variational autoencoders to address these shortcomings.\n\nDifferent categories occupy varying amounts of area in latent space.\nThe latent space distribution is not symmetrical or bounded.\nThere are gaps in the latent space.\n\nThis makes it difficult for us to sample from this latent space effectively. We could sample a “gap” and get a nonsensical image. If a category (say, trousers) occupies a larger area in latent space, we are more likely to generate images of trousers than of categories which occupy a small area (say, shoes).\n\n\n\n\n\n\n\n\n\n\nStory Time\n\n\n\nIf we revisit our wardrobe, rather than assigning each item to a specific location, let’s assign it to a general region of the wardrobe.\nAnd let’s also insist that this region should be as close to the centre of the wardrobe as possible, otherwise we are penalised. This should yield a more uniform latent space.\nThis is the idea behind variational autoencoders (VAE).\n\n\n\n\nIn a standard autoencoder, each image is mapped directly to one point in the latent space.\nIn a variational autoencoder, each image is mapped to a multivariate Normal distribution around a point in the latent space. Variational autoencoders assume their is no correlation between latent space dimensions.\nSo we will typically use isotropic Normal distributions, meaning the covariance matrix is diagonal so the distribution is independent in each dimension. The encoder only needs to map each input to a mean vector and a variance vector; it does not need to worry about covariances.\nIn practice we choose to map to log variances because this can be any value in the range \\((-\\infty, \\infty)\\) which gives a smoother value to learn rather than variances whihc are positive.\nIn summary, the encoder maps \\(image \\rightarrow (z_{mean}, z_{log\\_var})\\)\nWe can then sample a point \\(z\\) from this distribution using:\n\\[\nz = z_{mean} + z_{sigma} * epsilon\n\\]\nwhere: \\[\nz_{sigma} = e^{z_{log\\_var} * 0.5}\n\\] \\[\nepsilon \\sim \\mathcal{N}(0, I)\n\\]\n\n\n\nThis is identical to the standard autoencoder.\n\n\n\nPutting these together, we get the overall architecture:\n\n\n\n\n\nflowchart LR\n\n\n  A[Encoder] --&gt; B1(z_mean)\n  A[Encoder] --&gt; B2(z_log_var)\n\n  B1(z_mean) --&gt; C[sample]\n  B2(z_log_var) --&gt; C[sample]\n\n  C[sample] --&gt; D(z)\n  D(z) --&gt; E[Decoder]\n\n\n\n\n\n\nWhy does this change to the encoder help?\nIn the standard autoencoder, there is no requirement for the latent space to be continuous. So we could sample a point, say, \\((1, 2)\\) and decode it to a well-formed image. But there is no guarantee that a point next to it \\((1.1, 2.1)\\) would look similar or even be intelligible.\nThe “variational” part of the VAE addresses this problem. We now sample from an area around z_mean, so the decoder must ensure that all points in that region produce similar images to keep the reconstruction loss small.\n\n\n\nRather than sample directly from a Normal distribution parameterised by z_mean and z_log_var, we can sample epsilon from a standard Normal distribution and manually adjust the sample to correct its mean and variance.\nThis means gradients can backpropagate freely through the layer. The randomness in the layer is all encapsulated in epsilon, so the partial derivative of the layer output w.r.t. the layer input is deterministic, making backpropagation possible.\n\n\n\nThe loss function of the standard autoencoder was the reconstruction loss between original image and its decoded version.\nFor VAEs, we add an additional term which encourages points to have small mean and variance by penalising z_mean and z_log_var variables that differ significantly from 0.\nThis is the Kullback-Leibler (KL) divergence. It measures how much one probability distribution differs from another. We use it to measure how much our Normal distribution, with parameters z_mean and z_log_var, differs from a standard Normal distribution.\nFor this special case of KL divergence between our Normal distribution and a standard Normal, the closed form solution is: \\[\nD_{KL}[\\mathcal{N}(\\mu, \\sigma) || \\mathcal{N}(0, 1)] = -\\frac{1}{2} \\sum (1 + \\log(\\sigma ^2) - \\mu ^2 - \\sigma ^ 2)\n\\]\nSo using our variables, we can describe this in code as:\nkl_loss = -0.5 * sum(1 + z_log_var - z_mean ** 2 - exp(z_log_var))\nThis loss is minimised when z_mean=0 and z_log_var=0, i.e. it encourages our distrubution towards a stand Normal distribution, thus using the space around the origin symmetrically and efficently with few gaps.\nThe original paper simply summed the reconstruction_loss and the kl_loss. A variant of this includes a hyperparameter \\(\\beta\\) to vary the weight of the KL divergence term. This is called a “\\(\\beta-VAE\\)”:\nvae_loss = reconstruction_error + beta * kl_loss\n\n\n\n\n\n\nWe need a sampling layer which allows us to sample \\(z\\) from the distribution defined by \\(z_{mean}\\) and \\(z_{log\\_var}\\).\n\n\nCode\nclass Sampling(layers.Layer):\n    def call(self, z_mean, z_log_var):\n        batch = tf.shape(z_mean)[0]\n        dim = tf.shape(z_mean)[1]\n        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n\n\n\n\n\nThe encoder incorporates the Sampling layer as the final step. This is what is passed to the decoder.\n\n\nCode\n# Encoder\nencoder_input = layers.Input(\n    shape=(IMAGE_SIZE, IMAGE_SIZE, 1), name=\"encoder_input\"\n)\nx = layers.Conv2D(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(encoder_input)\nx = layers.Conv2D(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2D(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nshape_before_flattening = tf.keras.backend.int_shape(x)[1:]  # the decoder will need this!\n\nx = layers.Flatten()(x)\nz_mean = layers.Dense(EMBEDDING_DIM, name=\"z_mean\")(x)\nz_log_var = layers.Dense(EMBEDDING_DIM, name=\"z_log_var\")(x)\nz = Sampling()(z_mean, z_log_var)\n\nencoder = models.Model(encoder_input, [z_mean, z_log_var, z], name=\"encoder\")\nencoder.summary()\n\n\nModel: \"encoder\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n encoder_input (InputLayer)  [(None, 32, 32, 1)]          0         []                            \n                                                                                                  \n conv2d_3 (Conv2D)           (None, 16, 16, 32)           320       ['encoder_input[0][0]']       \n                                                                                                  \n conv2d_4 (Conv2D)           (None, 8, 8, 64)             18496     ['conv2d_3[0][0]']            \n                                                                                                  \n conv2d_5 (Conv2D)           (None, 4, 4, 128)            73856     ['conv2d_4[0][0]']            \n                                                                                                  \n flatten_1 (Flatten)         (None, 2048)                 0         ['conv2d_5[0][0]']            \n                                                                                                  \n z_mean (Dense)              (None, 2)                    4098      ['flatten_1[0][0]']           \n                                                                                                  \n z_log_var (Dense)           (None, 2)                    4098      ['flatten_1[0][0]']           \n                                                                                                  \n sampling (Sampling)         (None, 2)                    0         ['z_mean[0][0]',              \n                                                                     'z_log_var[0][0]']           \n                                                                                                  \n==================================================================================================\nTotal params: 100868 (394.02 KB)\nTrainable params: 100868 (394.02 KB)\nNon-trainable params: 0 (0.00 Byte)\n__________________________________________________________________________________________________\n\n\n\n\n\nThe decoder is the same as a standard autoencoder.\n\n\nCode\n# Decoder\ndecoder_input = layers.Input(shape=(EMBEDDING_DIM,), name=\"decoder_input\")\nx = layers.Dense(np.prod(shape_before_flattening))(decoder_input)\nx = layers.Reshape(shape_before_flattening)(x)\nx = layers.Conv2DTranspose(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2DTranspose(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n\ndecoder_output = layers.Conv2D(1, (3, 3), strides=1, activation=\"sigmoid\", padding=\"same\", name=\"decoder_output\")(x)\n\ndecoder = models.Model(decoder_input, decoder_output)\ndecoder.summary()\n\n\nModel: \"model_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n decoder_input (InputLayer)  [(None, 2)]               0         \n                                                                 \n dense_1 (Dense)             (None, 2048)              6144      \n                                                                 \n reshape_1 (Reshape)         (None, 4, 4, 128)         0         \n                                                                 \n conv2d_transpose_3 (Conv2D  (None, 8, 8, 128)         147584    \n Transpose)                                                      \n                                                                 \n conv2d_transpose_4 (Conv2D  (None, 16, 16, 64)        73792     \n Transpose)                                                      \n                                                                 \n conv2d_transpose_5 (Conv2D  (None, 32, 32, 32)        18464     \n Transpose)                                                      \n                                                                 \n decoder_output (Conv2D)     (None, 32, 32, 1)         289       \n                                                                 \n=================================================================\nTotal params: 246273 (962.00 KB)\nTrainable params: 246273 (962.00 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\nPutting the encoder and decoder together.\n\n\nCode\nEPOCHS = 5\nBETA = 500\n\n\n\n\nCode\nclass VAE(models.Model):\n    def __init__(self, encoder, decoder, **kwargs):\n        super(VAE, self).__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n        self.total_loss_tracker = metrics.Mean(name=\"total_loss\")\n        self.reconstruction_loss_tracker = metrics.Mean(name=\"reconstruction_loss\")\n        self.kl_loss_tracker = metrics.Mean(name=\"kl_loss\")\n\n    @property\n    def metrics(self):\n        return [\n            self.total_loss_tracker,\n            self.reconstruction_loss_tracker,\n            self.kl_loss_tracker,\n        ]\n\n    def call(self, inputs):\n        \"\"\"Call the model on a particular input.\"\"\"\n        z_mean, z_log_var, z = encoder(inputs)\n        reconstruction = decoder(z)\n        return z_mean, z_log_var, reconstruction\n\n    def train_step(self, data):\n        \"\"\"Step run during training.\"\"\"\n        with tf.GradientTape() as tape:\n            z_mean, z_log_var, reconstruction = self(data)\n            reconstruction_loss = tf.reduce_mean(\n                BETA * losses.binary_crossentropy(data, reconstruction, axis=(1, 2, 3))\n            )\n            kl_loss = tf.reduce_mean(\n                tf.reduce_sum(-0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)), axis=1)\n            )\n            total_loss = reconstruction_loss + kl_loss\n\n        grads = tape.gradient(total_loss, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n\n        self.total_loss_tracker.update_state(total_loss)\n        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n        self.kl_loss_tracker.update_state(kl_loss)\n\n        return {m.name: m.result() for m in self.metrics}\n\n    def test_step(self, data):\n        \"\"\"Step run during validation.\"\"\"\n        if isinstance(data, tuple):\n            data = data[0]\n\n        z_mean, z_log_var, reconstruction = self(data)\n        reconstruction_loss = tf.reduce_mean(\n            BETA * losses.binary_crossentropy(data, reconstruction, axis=(1, 2, 3))\n        )\n        kl_loss = tf.reduce_mean(\n            tf.reduce_sum(\n                -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)),\n                axis=1,\n            )\n        )\n        total_loss = reconstruction_loss + kl_loss\n\n        return {\n            \"loss\": total_loss,\n            \"reconstruction_loss\": reconstruction_loss,\n            \"kl_loss\": kl_loss,\n        }\n\n\nInstantiate the VAE model and compile it.\n\n\nCode\nvae = VAE(encoder, decoder)\n\n# optimizer = optimizers.Adam(learning_rate=0.0005)\noptimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0005)\nvae.compile(optimizer=optimizer)\n\n\n\n\n\nTrain the VAE as before.\n\n\nCode\nvae.fit(\n    x_train,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    validation_data=(x_test, x_test),\n)\n\n\nEpoch 1/5\n600/600 [==============================] - 37s 61ms/step - total_loss: 160.4693 - reconstruction_loss: 155.9913 - kl_loss: 4.4779 - val_loss: 141.2442 - val_reconstruction_loss: 136.1877 - val_kl_loss: 5.0565\nEpoch 2/5\n600/600 [==============================] - 34s 57ms/step - total_loss: 135.9397 - reconstruction_loss: 130.9409 - kl_loss: 4.9988 - val_loss: 138.5623 - val_reconstruction_loss: 133.5856 - val_kl_loss: 4.9767\nEpoch 3/5\n600/600 [==============================] - 34s 56ms/step - total_loss: 134.3719 - reconstruction_loss: 129.3381 - kl_loss: 5.0338 - val_loss: 137.1351 - val_reconstruction_loss: 132.1540 - val_kl_loss: 4.9811\nEpoch 4/5\n600/600 [==============================] - 34s 56ms/step - total_loss: 133.4455 - reconstruction_loss: 128.3819 - kl_loss: 5.0637 - val_loss: 136.5461 - val_reconstruction_loss: 131.4780 - val_kl_loss: 5.0681\nEpoch 5/5\n600/600 [==============================] - 34s 57ms/step - total_loss: 132.7808 - reconstruction_loss: 127.6688 - kl_loss: 5.1120 - val_loss: 135.8917 - val_reconstruction_loss: 130.7375 - val_kl_loss: 5.1542\n\n\n&lt;keras.src.callbacks.History at 0x2c59e9f10&gt;\n\n\n\n\n\n\n\n\nAs before, we can eyeball the reconstructions from our model.\n\n\nCode\n# Select a subset of the test set\nn_to_predict = 5000\nexample_images = x_test[:n_to_predict]\nexample_labels = y_test[:n_to_predict]\n\n# Create autoencoder predictions and display\nz_mean, z_log_var, reconstructions = vae.predict(example_images)\nprint(\"Example real clothing items\")\nplot_sample_images(example_images)\nprint(\"Reconstructions\")\nplot_sample_images(reconstructions)\n\n\n 42/157 [=======&gt;......................] - ETA: 1s157/157 [==============================] - 1s 9ms/step\nExample real clothing items\nReconstructions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can sample and decode points in the latent space to observe how the model generates images. We note that:\n\nThe latent space has more even coverage and does not stray to far from a standard Normal distribution. If this is not the case, we can vary the \\(\\beta\\) value used to give more weight to the KL loss term.\nWe do not see as many poorly formed images as we did when sampling a “gap” in a standard autoencoder.\n\n\n\nCode\n# Encode the example images\nz_mean, z_var, z = encoder.predict(example_images)\n\n# Sample some points in the latent space, from the standard normal distribution\ngrid_width, grid_height = (6, 3)\nz_sample = np.random.normal(size=(grid_width * grid_height, 2))\n# Decode the sampled points\nreconstructions = decoder.predict(z_sample)\n# Convert original embeddings and sampled embeddings to p-values\np = norm.cdf(z)\np_sample = norm.cdf(z_sample)\n# Draw a plot of...\nfigsize = 8\nplt.figure(figsize=(figsize, figsize))\n\n# ... the original embeddings ...\nplt.scatter(z[:, 0], z[:, 1], c=\"black\", alpha=0.5, s=2)\n\n# ... and the newly generated points in the latent space\nplt.scatter(z_sample[:, 0], z_sample[:, 1], c=\"#00B0F0\", alpha=1, s=40)\nplt.show()\n\n# Add underneath a grid of the decoded images\nfig = plt.figure(figsize=(figsize, grid_height * 2))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i in range(grid_width * grid_height):\n    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n    ax.axis(\"off\")\n    ax.text(\n        0.5,\n        -0.35,\n        str(np.round(z_sample[i, :], 1)),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")\n\n\n1/1 [==============================] - 0s 14ms/step\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe plots below show the latent space coloured by clothing type. The left plot shows this in terms of z-values and the right in terms of p-values.\nThe latent space is more continuous with fewer gaps, and different categories take similar amounts of space.\n\n\nCode\n# Colour the embeddings by their label (clothing type - see table)\nfigsize = 8\nfig = plt.figure(figsize=(figsize * 2, figsize))\nax = fig.add_subplot(1, 2, 1)\nplot_1 = ax.scatter(\n    z[:, 0], z[:, 1], cmap=\"rainbow\", c=example_labels, alpha=0.8, s=3\n)\nplt.colorbar(plot_1)\nax = fig.add_subplot(1, 2, 2)\nplot_2 = ax.scatter(\n    p[:, 0], p[:, 1], cmap=\"rainbow\", c=example_labels, alpha=0.8, s=3\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nNext we see what happens when we sample from the latent space in a regular grid.\n\n\nCode\n# Colour the embeddings by their label (clothing type - see table)\nfigsize = 12\ngrid_size = 15\nplt.figure(figsize=(figsize, figsize))\nplt.scatter(\n    p[:, 0], p[:, 1], cmap=\"rainbow\", c=example_labels, alpha=0.8, s=300\n)\nplt.colorbar()\n\nx = norm.ppf(np.linspace(0, 1, grid_size))\ny = norm.ppf(np.linspace(1, 0, grid_size))\nxv, yv = np.meshgrid(x, y)\nxv = xv.flatten()\nyv = yv.flatten()\ngrid = np.array(list(zip(xv, yv)))\n\nreconstructions = decoder.predict(grid)\n# plt.scatter(grid[:, 0], grid[:, 1], c=\"black\", alpha=1, s=10)\nplt.show()\n\nfig = plt.figure(figsize=(figsize, figsize))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\nfor i in range(grid_size**2):\n    ax = fig.add_subplot(grid_size, grid_size, i + 1)\n    ax.axis(\"off\")\n    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")\n\n\n8/8 [==============================] - 0s 6ms/step\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 3 of Generative Deep Learning by David Foster."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson3/chapter3.html#autoencoders",
    "href": "posts/ml/generative_deep_learning/lesson3/chapter3.html#autoencoders",
    "title": "Generative AI: VAEs",
    "section": "",
    "text": "The idea of autoencoders (read: self-encoders) is that they learn to simplify the input then reconstruct it; the input and target output are the same.\n\nThe encoder learns to compress high-dimensional input data into a lower dimensional representation called the embedding.\nThe decoder takes an embedding and recreates a higher-dimensional image. This should be an accurate reconstruction of the input.\n\nThis can be used as a generative model because we can the sample and decode new points from the latent space to generate novel outputs. The goal of training an autoencoder is to learn a meaningful embedding \\(z\\).\n\n\n\n\n\nflowchart LR\n\n  A(Encoder) --&gt; B(z)\n  B(z) --&gt; c(Decoder)\n\n\n\n\n\n\nThis also makes autoencoders useful as denoising models, because the embedding should retain the salient information but “lose” the noise."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson3/chapter3.html#building-an-autoencoder",
    "href": "posts/ml/generative_deep_learning/lesson3/chapter3.html#building-an-autoencoder",
    "title": "Generative AI: VAEs",
    "section": "",
    "text": "We will implement an autoencoder to learn lower-dimensional embeddings for the fashion MNIST data set.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nimport tensorflow as tf\nfrom tensorflow.keras import (\n    layers,\n    models,\n    datasets,\n    callbacks,\n    losses,\n    optimizers,\n    metrics,\n)\n\n\n# Parameters\nIMAGE_SIZE = 32\nCHANNELS = 1\nBATCH_SIZE = 100\nBUFFER_SIZE = 1000\nVALIDATION_SPLIT = 0.2\nEMBEDDING_DIM = 2\nEPOCHS = 3\n\n\n\n\nScale the pixel values and reshape the images.\n\n\nCode\n(x_train, y_train), (x_test, y_test) = datasets.fashion_mnist.load_data()\n\ndef preprocess(images):\n    images = images.astype(\"float32\") / 255.0\n    images = np.pad(images, ((0, 0), (2, 2), (2, 2)), constant_values=0.0)\n    images = np.expand_dims(images, -1)\n    return images\n\nx_train = preprocess(x_train)\nx_test = preprocess(x_test)\n\n\nWe can see an example from our training set:\n\n\nCode\nplt.imshow(x_train[0])\n\n\n\n\n\n\n\n\n\n\n\n\nThe encoder compresses the dimensionality on the input to a smaller embedding dimension.\n\n\nCode\n# Input\nencoder_input = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS),name=\"encoder_input\")\n\n# Conv layers\nx = layers.Conv2D(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(encoder_input)\nx = layers.Conv2D(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2D(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\npre_flatten_shape = tf.keras.backend.int_shape(x)[1:]  # Used by the decoder later\n\n# Output\nx = layers.Flatten()(x)\nencoder_output = layers.Dense(EMBEDDING_DIM, name=\"encoder_output\")(x)\n\n# Model\nencoder = models.Model(encoder_input, encoder_output)\nencoder.summary()\n\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n encoder_input (InputLayer)  [(None, 32, 32, 1)]       0         \n                                                                 \n conv2d (Conv2D)             (None, 16, 16, 32)        320       \n                                                                 \n conv2d_1 (Conv2D)           (None, 8, 8, 64)          18496     \n                                                                 \n conv2d_2 (Conv2D)           (None, 4, 4, 128)         73856     \n                                                                 \n flatten (Flatten)           (None, 2048)              0         \n                                                                 \n encoder_output (Dense)      (None, 2)                 4098      \n                                                                 \n=================================================================\nTotal params: 96770 (378.01 KB)\nTrainable params: 96770 (378.01 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\nThe decoder reconstructs the original image from the embedding.\n\n\nIn a standard convolutional layer, if we have stride=2 it will half the image size.\nIn a convolutional transpose layer, we are increasing the image size. The stride parameter determines the amount of zero padding to add between each pixel. A kernel is then applied to this “internally padded” image to expand the image size.\n\n\nCode\n# Input\ndecoder_input = layers.Input(shape=(EMBEDDING_DIM,),name=\"decoder_input\")\n\n# Reshape the input using the pre-flattening shape from the encoder\nx = layers.Dense(np.prod(pre_flatten_shape))(decoder_input)\nx = layers.Reshape(pre_flatten_shape)(x)\n\n# Scale up the image back to its original size. These are the reverse of the conv layers applied in the encoder.\nx = layers.Conv2DTranspose(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2DTranspose(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n\n# Output\ndecoder_output = layers.Conv2D(\n    CHANNELS,\n    (3, 3),\n    strides=1,\n    activation='sigmoid',\n    padding=\"same\",\n    name=\"decoder_output\",\n)(x)\n\n# Model\ndecoder = models.Model(decoder_input, decoder_output)\ndecoder.summary()\n\n\nModel: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n decoder_input (InputLayer)  [(None, 2)]               0         \n                                                                 \n dense (Dense)               (None, 2048)              6144      \n                                                                 \n reshape (Reshape)           (None, 4, 4, 128)         0         \n                                                                 \n conv2d_transpose (Conv2DTr  (None, 8, 8, 128)         147584    \n anspose)                                                        \n                                                                 \n conv2d_transpose_1 (Conv2D  (None, 16, 16, 64)        73792     \n Transpose)                                                      \n                                                                 \n conv2d_transpose_2 (Conv2D  (None, 32, 32, 32)        18464     \n Transpose)                                                      \n                                                                 \n decoder_output (Conv2D)     (None, 32, 32, 1)         289       \n                                                                 \n=================================================================\nTotal params: 246273 (962.00 KB)\nTrainable params: 246273 (962.00 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\n\nCombine the encoder and decoder into a single model.\n\n\nCode\nautoencoder = models.Model(encoder_input, decoder(encoder_output))\nautoencoder.summary()\n\n\nModel: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n encoder_input (InputLayer)  [(None, 32, 32, 1)]       0         \n                                                                 \n conv2d (Conv2D)             (None, 16, 16, 32)        320       \n                                                                 \n conv2d_1 (Conv2D)           (None, 8, 8, 64)          18496     \n                                                                 \n conv2d_2 (Conv2D)           (None, 4, 4, 128)         73856     \n                                                                 \n flatten (Flatten)           (None, 2048)              0         \n                                                                 \n encoder_output (Dense)      (None, 2)                 4098      \n                                                                 \n model_1 (Functional)        (None, 32, 32, 1)         246273    \n                                                                 \n=================================================================\nTotal params: 343043 (1.31 MB)\nTrainable params: 343043 (1.31 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\nThe autoencoder is trained with the source images as both input and target output.\nThe loss function is usually chosen as either RMSE or binary cross-entropy between pixels of original image vs reconstruction.\n\n\nCode\nautoencoder.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\nautoencoder.fit(\n    x_train,\n    x_train,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    validation_data=(x_test, x_test)\n)\n\n\nEpoch 1/3\n600/600 [==============================] - 35s 58ms/step - loss: 0.2910 - val_loss: 0.2610\nEpoch 2/3\n600/600 [==============================] - 36s 60ms/step - loss: 0.2569 - val_loss: 0.2561\nEpoch 3/3\n600/600 [==============================] - 34s 57ms/step - loss: 0.2536 - val_loss: 0.2540\n\n\n&lt;keras.src.callbacks.History at 0x156e4a310&gt;"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson3/chapter3.html#analysing-the-autoencoder",
    "href": "posts/ml/generative_deep_learning/lesson3/chapter3.html#analysing-the-autoencoder",
    "title": "Generative AI: VAEs",
    "section": "",
    "text": "We can use our trained autoencoder to:\n\nReconstruct images\nAnalyse embeddings\nGenerate new images\n\n\n\nReconstruct a sample of test images using the autoencoder.\nThe reconstruction isn’t perfect; some information is lost when reducing down to just 2 dimensions. But it does a surprisingly good job of compressing 32x32 pixel values into just 2 embedding values.\n\n\nCode\nNUM_IMAGES_TO_RECONSTRUCT = 5000\nexample_images = x_test[:NUM_IMAGES_TO_RECONSTRUCT]\nexample_labels = y_test[:NUM_IMAGES_TO_RECONSTRUCT]\n\npredictions = autoencoder.predict(example_images)\n\n\n  7/157 [&gt;.............................] - ETA: 1s 157/157 [==============================] - 1s 8ms/step\n\n\nOriginal images:\n\n\nCode\ndef plot_sample_images(images, n=10, size=(20, 3), cmap=\"gray_r\"):\n    plt.figure(figsize=size)\n    for i in range(n):\n        _ = plt.subplot(1, n, i + 1)\n        plt.imshow(images[i].astype(\"float32\"), cmap=cmap)\n        plt.axis(\"off\")\n    \n    plt.show()\n\nplot_sample_images(example_images)\n\n\n\n\n\n\n\n\n\nReconstructed images:\n\n\nCode\nplot_sample_images(predictions)\n\n\n\n\n\n\n\n\n\n\n\n\nEach of the images above has been encoded as a 2-dimensional embedding.\nWe can look at these embeddings to gain some insight into how the autoencoder works.\nThe embedding vectors for our sample images above:\n\n\nCode\n# Encode the example images\nembeddings = encoder.predict(example_images)\nprint(embeddings[:10])\n\n\n102/157 [==================&gt;...........] - ETA: 0s157/157 [==============================] - 0s 2ms/step\n[[ 2.2441912  -2.711683  ]\n [ 6.1558456   6.0202003 ]\n [-3.787192    7.3368516 ]\n [-2.5938551   4.2098355 ]\n [ 3.8645594   2.7229536 ]\n [-2.0130231   6.0485506 ]\n [ 1.2749226   2.1347647 ]\n [ 2.8239484   2.898773  ]\n [-0.48542604 -1.0869933 ]\n [ 0.30643728 -2.6099105 ]]\n\n\nWe can plot the 2D latent space, colouring each point by its label. This shows how similar items are clustered together in latent space.\nThis is impressive! Remember, we never showed the model the labels when training, so it has learned to cluster images that look alike.\n\n\nCode\n# Colour the embeddings by their label\nexample_labels = y_test[:NUM_IMAGES_TO_RECONSTRUCT]\n\n# Plot the latent space\nfigsize = 8\nplt.figure(figsize=(figsize, figsize))\nplt.scatter(\n    embeddings[:, 0],\n    embeddings[:, 1],\n    cmap=\"rainbow\",\n    c=example_labels,\n    alpha=0.6,\n    s=3,\n)\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nWe can sample from the latent space and decode these sampled points to generate new images.\nFirst we sample some random points in the latent space:\n\n\nCode\n# Get the range of existing embedding values so we can sample sensible points within the latent space.\nembedding_min = np.min(embeddings, axis=0)\nembedding_max = np.max(embeddings, axis=0)\n\n# Sample some points\ngrid_width = 6\ngrid_height = 3\nsample = np.random.uniform(\n    embedding_min, embedding_max, size=(grid_width * grid_height, EMBEDDING_DIM)\n)\nprint(sample)\n\n\n[[ 1.47862929  9.28394749]\n [-3.19389344 -3.04713146]\n [-0.57161452 -0.35644389]\n [10.97632621 -2.12482484]\n [ 4.05160668  9.04420005]\n [ 9.50105167  5.71270956]\n [ 3.24765456  4.95969011]\n [-3.68217634  4.52120851]\n [-1.7067196   5.87696959]\n [ 5.99883565 -2.11597183]\n [ 1.84553131  6.04266323]\n [ 0.15552252  1.98655625]\n [ 3.55479856  2.35587959]\n [-0.32278762  6.07537408]\n [ 8.98977414 -1.15893539]\n [ 2.1476981   4.97819188]\n [-2.0896675   3.9166368 ]\n [ 6.49229371 -4.75611412]]\n\n\nWe can then decode these sampled points.\n\n\nCode\n# Decode the sampled points\nreconstructions = decoder.predict(sample)\n\n\n1/1 [==============================] - 0s 59ms/step\n\n\n\n\nCode\nfigsize = 8\nplt.figure(figsize=(figsize, figsize))\n\n# Plot the latent space and overlay the positions of the sampled points\nplt.scatter(embeddings[:, 0], embeddings[:, 1], c=\"black\", alpha=0.5, s=2)\nplt.scatter(sample[:, 0], sample[:, 1], c=\"#00B0F0\", alpha=1, s=40)\nplt.show()\n\n# Plot a grid of the reconstructed images which decode those sampled points\nfig = plt.figure(figsize=(figsize, grid_height * 2))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i in range(grid_width * grid_height):\n    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n    ax.axis(\"off\")\n    ax.text(\n        0.5,\n        -0.35,\n        str(np.round(sample[i, :], 1)),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow let’s see what happens when we regularly sample the latent space.\n\n\nCode\n# Colour the embeddings by their label (clothing type - see table)\nfigsize = 12\ngrid_size = 15\nplt.figure(figsize=(figsize, figsize))\nplt.scatter(\n    embeddings[:, 0],\n    embeddings[:, 1],\n    cmap=\"rainbow\",\n    c=example_labels,\n    alpha=0.8,\n    s=300,\n)\nplt.colorbar()\n\nx = np.linspace(min(embeddings[:, 0]), max(embeddings[:, 0]), grid_size)\ny = np.linspace(max(embeddings[:, 1]), min(embeddings[:, 1]), grid_size)\nxv, yv = np.meshgrid(x, y)\nxv = xv.flatten()\nyv = yv.flatten()\ngrid = np.array(list(zip(xv, yv)))\n\nreconstructions = decoder.predict(grid)\n# plt.scatter(grid[:, 0], grid[:, 1], c=\"black\", alpha=1, s=10)\nplt.show()\n\nfig = plt.figure(figsize=(figsize, figsize))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\nfor i in range(grid_size**2):\n    ax = fig.add_subplot(grid_size, grid_size, i + 1)\n    ax.axis(\"off\")\n    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")\n\n\n8/8 [==============================] - 0s 9ms/step\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe latent space exploration above yields some interesting insights into “regular” autoencoders that motivate the use of variational autoencoders to address these shortcomings.\n\nDifferent categories occupy varying amounts of area in latent space.\nThe latent space distribution is not symmetrical or bounded.\nThere are gaps in the latent space.\n\nThis makes it difficult for us to sample from this latent space effectively. We could sample a “gap” and get a nonsensical image. If a category (say, trousers) occupies a larger area in latent space, we are more likely to generate images of trousers than of categories which occupy a small area (say, shoes)."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson3/chapter3.html#variational-autoencoders-1",
    "href": "posts/ml/generative_deep_learning/lesson3/chapter3.html#variational-autoencoders-1",
    "title": "Generative AI: VAEs",
    "section": "",
    "text": "Story Time\n\n\n\nIf we revisit our wardrobe, rather than assigning each item to a specific location, let’s assign it to a general region of the wardrobe.\nAnd let’s also insist that this region should be as close to the centre of the wardrobe as possible, otherwise we are penalised. This should yield a more uniform latent space.\nThis is the idea behind variational autoencoders (VAE).\n\n\n\n\nIn a standard autoencoder, each image is mapped directly to one point in the latent space.\nIn a variational autoencoder, each image is mapped to a multivariate Normal distribution around a point in the latent space. Variational autoencoders assume their is no correlation between latent space dimensions.\nSo we will typically use isotropic Normal distributions, meaning the covariance matrix is diagonal so the distribution is independent in each dimension. The encoder only needs to map each input to a mean vector and a variance vector; it does not need to worry about covariances.\nIn practice we choose to map to log variances because this can be any value in the range \\((-\\infty, \\infty)\\) which gives a smoother value to learn rather than variances whihc are positive.\nIn summary, the encoder maps \\(image \\rightarrow (z_{mean}, z_{log\\_var})\\)\nWe can then sample a point \\(z\\) from this distribution using:\n\\[\nz = z_{mean} + z_{sigma} * epsilon\n\\]\nwhere: \\[\nz_{sigma} = e^{z_{log\\_var} * 0.5}\n\\] \\[\nepsilon \\sim \\mathcal{N}(0, I)\n\\]\n\n\n\nThis is identical to the standard autoencoder.\n\n\n\nPutting these together, we get the overall architecture:\n\n\n\n\n\nflowchart LR\n\n\n  A[Encoder] --&gt; B1(z_mean)\n  A[Encoder] --&gt; B2(z_log_var)\n\n  B1(z_mean) --&gt; C[sample]\n  B2(z_log_var) --&gt; C[sample]\n\n  C[sample] --&gt; D(z)\n  D(z) --&gt; E[Decoder]\n\n\n\n\n\n\nWhy does this change to the encoder help?\nIn the standard autoencoder, there is no requirement for the latent space to be continuous. So we could sample a point, say, \\((1, 2)\\) and decode it to a well-formed image. But there is no guarantee that a point next to it \\((1.1, 2.1)\\) would look similar or even be intelligible.\nThe “variational” part of the VAE addresses this problem. We now sample from an area around z_mean, so the decoder must ensure that all points in that region produce similar images to keep the reconstruction loss small.\n\n\n\nRather than sample directly from a Normal distribution parameterised by z_mean and z_log_var, we can sample epsilon from a standard Normal distribution and manually adjust the sample to correct its mean and variance.\nThis means gradients can backpropagate freely through the layer. The randomness in the layer is all encapsulated in epsilon, so the partial derivative of the layer output w.r.t. the layer input is deterministic, making backpropagation possible.\n\n\n\nThe loss function of the standard autoencoder was the reconstruction loss between original image and its decoded version.\nFor VAEs, we add an additional term which encourages points to have small mean and variance by penalising z_mean and z_log_var variables that differ significantly from 0.\nThis is the Kullback-Leibler (KL) divergence. It measures how much one probability distribution differs from another. We use it to measure how much our Normal distribution, with parameters z_mean and z_log_var, differs from a standard Normal distribution.\nFor this special case of KL divergence between our Normal distribution and a standard Normal, the closed form solution is: \\[\nD_{KL}[\\mathcal{N}(\\mu, \\sigma) || \\mathcal{N}(0, 1)] = -\\frac{1}{2} \\sum (1 + \\log(\\sigma ^2) - \\mu ^2 - \\sigma ^ 2)\n\\]\nSo using our variables, we can describe this in code as:\nkl_loss = -0.5 * sum(1 + z_log_var - z_mean ** 2 - exp(z_log_var))\nThis loss is minimised when z_mean=0 and z_log_var=0, i.e. it encourages our distrubution towards a stand Normal distribution, thus using the space around the origin symmetrically and efficently with few gaps.\nThe original paper simply summed the reconstruction_loss and the kl_loss. A variant of this includes a hyperparameter \\(\\beta\\) to vary the weight of the KL divergence term. This is called a “\\(\\beta-VAE\\)”:\nvae_loss = reconstruction_error + beta * kl_loss"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson3/chapter3.html#building-a-variational-autoencoder-vae",
    "href": "posts/ml/generative_deep_learning/lesson3/chapter3.html#building-a-variational-autoencoder-vae",
    "title": "Generative AI: VAEs",
    "section": "",
    "text": "We need a sampling layer which allows us to sample \\(z\\) from the distribution defined by \\(z_{mean}\\) and \\(z_{log\\_var}\\).\n\n\nCode\nclass Sampling(layers.Layer):\n    def call(self, z_mean, z_log_var):\n        batch = tf.shape(z_mean)[0]\n        dim = tf.shape(z_mean)[1]\n        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n\n\n\n\n\nThe encoder incorporates the Sampling layer as the final step. This is what is passed to the decoder.\n\n\nCode\n# Encoder\nencoder_input = layers.Input(\n    shape=(IMAGE_SIZE, IMAGE_SIZE, 1), name=\"encoder_input\"\n)\nx = layers.Conv2D(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(encoder_input)\nx = layers.Conv2D(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2D(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nshape_before_flattening = tf.keras.backend.int_shape(x)[1:]  # the decoder will need this!\n\nx = layers.Flatten()(x)\nz_mean = layers.Dense(EMBEDDING_DIM, name=\"z_mean\")(x)\nz_log_var = layers.Dense(EMBEDDING_DIM, name=\"z_log_var\")(x)\nz = Sampling()(z_mean, z_log_var)\n\nencoder = models.Model(encoder_input, [z_mean, z_log_var, z], name=\"encoder\")\nencoder.summary()\n\n\nModel: \"encoder\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n encoder_input (InputLayer)  [(None, 32, 32, 1)]          0         []                            \n                                                                                                  \n conv2d_3 (Conv2D)           (None, 16, 16, 32)           320       ['encoder_input[0][0]']       \n                                                                                                  \n conv2d_4 (Conv2D)           (None, 8, 8, 64)             18496     ['conv2d_3[0][0]']            \n                                                                                                  \n conv2d_5 (Conv2D)           (None, 4, 4, 128)            73856     ['conv2d_4[0][0]']            \n                                                                                                  \n flatten_1 (Flatten)         (None, 2048)                 0         ['conv2d_5[0][0]']            \n                                                                                                  \n z_mean (Dense)              (None, 2)                    4098      ['flatten_1[0][0]']           \n                                                                                                  \n z_log_var (Dense)           (None, 2)                    4098      ['flatten_1[0][0]']           \n                                                                                                  \n sampling (Sampling)         (None, 2)                    0         ['z_mean[0][0]',              \n                                                                     'z_log_var[0][0]']           \n                                                                                                  \n==================================================================================================\nTotal params: 100868 (394.02 KB)\nTrainable params: 100868 (394.02 KB)\nNon-trainable params: 0 (0.00 Byte)\n__________________________________________________________________________________________________\n\n\n\n\n\nThe decoder is the same as a standard autoencoder.\n\n\nCode\n# Decoder\ndecoder_input = layers.Input(shape=(EMBEDDING_DIM,), name=\"decoder_input\")\nx = layers.Dense(np.prod(shape_before_flattening))(decoder_input)\nx = layers.Reshape(shape_before_flattening)(x)\nx = layers.Conv2DTranspose(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2DTranspose(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n\ndecoder_output = layers.Conv2D(1, (3, 3), strides=1, activation=\"sigmoid\", padding=\"same\", name=\"decoder_output\")(x)\n\ndecoder = models.Model(decoder_input, decoder_output)\ndecoder.summary()\n\n\nModel: \"model_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n decoder_input (InputLayer)  [(None, 2)]               0         \n                                                                 \n dense_1 (Dense)             (None, 2048)              6144      \n                                                                 \n reshape_1 (Reshape)         (None, 4, 4, 128)         0         \n                                                                 \n conv2d_transpose_3 (Conv2D  (None, 8, 8, 128)         147584    \n Transpose)                                                      \n                                                                 \n conv2d_transpose_4 (Conv2D  (None, 16, 16, 64)        73792     \n Transpose)                                                      \n                                                                 \n conv2d_transpose_5 (Conv2D  (None, 32, 32, 32)        18464     \n Transpose)                                                      \n                                                                 \n decoder_output (Conv2D)     (None, 32, 32, 1)         289       \n                                                                 \n=================================================================\nTotal params: 246273 (962.00 KB)\nTrainable params: 246273 (962.00 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\nPutting the encoder and decoder together.\n\n\nCode\nEPOCHS = 5\nBETA = 500\n\n\n\n\nCode\nclass VAE(models.Model):\n    def __init__(self, encoder, decoder, **kwargs):\n        super(VAE, self).__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n        self.total_loss_tracker = metrics.Mean(name=\"total_loss\")\n        self.reconstruction_loss_tracker = metrics.Mean(name=\"reconstruction_loss\")\n        self.kl_loss_tracker = metrics.Mean(name=\"kl_loss\")\n\n    @property\n    def metrics(self):\n        return [\n            self.total_loss_tracker,\n            self.reconstruction_loss_tracker,\n            self.kl_loss_tracker,\n        ]\n\n    def call(self, inputs):\n        \"\"\"Call the model on a particular input.\"\"\"\n        z_mean, z_log_var, z = encoder(inputs)\n        reconstruction = decoder(z)\n        return z_mean, z_log_var, reconstruction\n\n    def train_step(self, data):\n        \"\"\"Step run during training.\"\"\"\n        with tf.GradientTape() as tape:\n            z_mean, z_log_var, reconstruction = self(data)\n            reconstruction_loss = tf.reduce_mean(\n                BETA * losses.binary_crossentropy(data, reconstruction, axis=(1, 2, 3))\n            )\n            kl_loss = tf.reduce_mean(\n                tf.reduce_sum(-0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)), axis=1)\n            )\n            total_loss = reconstruction_loss + kl_loss\n\n        grads = tape.gradient(total_loss, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n\n        self.total_loss_tracker.update_state(total_loss)\n        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n        self.kl_loss_tracker.update_state(kl_loss)\n\n        return {m.name: m.result() for m in self.metrics}\n\n    def test_step(self, data):\n        \"\"\"Step run during validation.\"\"\"\n        if isinstance(data, tuple):\n            data = data[0]\n\n        z_mean, z_log_var, reconstruction = self(data)\n        reconstruction_loss = tf.reduce_mean(\n            BETA * losses.binary_crossentropy(data, reconstruction, axis=(1, 2, 3))\n        )\n        kl_loss = tf.reduce_mean(\n            tf.reduce_sum(\n                -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)),\n                axis=1,\n            )\n        )\n        total_loss = reconstruction_loss + kl_loss\n\n        return {\n            \"loss\": total_loss,\n            \"reconstruction_loss\": reconstruction_loss,\n            \"kl_loss\": kl_loss,\n        }\n\n\nInstantiate the VAE model and compile it.\n\n\nCode\nvae = VAE(encoder, decoder)\n\n# optimizer = optimizers.Adam(learning_rate=0.0005)\noptimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0005)\nvae.compile(optimizer=optimizer)\n\n\n\n\n\nTrain the VAE as before.\n\n\nCode\nvae.fit(\n    x_train,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    validation_data=(x_test, x_test),\n)\n\n\nEpoch 1/5\n600/600 [==============================] - 37s 61ms/step - total_loss: 160.4693 - reconstruction_loss: 155.9913 - kl_loss: 4.4779 - val_loss: 141.2442 - val_reconstruction_loss: 136.1877 - val_kl_loss: 5.0565\nEpoch 2/5\n600/600 [==============================] - 34s 57ms/step - total_loss: 135.9397 - reconstruction_loss: 130.9409 - kl_loss: 4.9988 - val_loss: 138.5623 - val_reconstruction_loss: 133.5856 - val_kl_loss: 4.9767\nEpoch 3/5\n600/600 [==============================] - 34s 56ms/step - total_loss: 134.3719 - reconstruction_loss: 129.3381 - kl_loss: 5.0338 - val_loss: 137.1351 - val_reconstruction_loss: 132.1540 - val_kl_loss: 4.9811\nEpoch 4/5\n600/600 [==============================] - 34s 56ms/step - total_loss: 133.4455 - reconstruction_loss: 128.3819 - kl_loss: 5.0637 - val_loss: 136.5461 - val_reconstruction_loss: 131.4780 - val_kl_loss: 5.0681\nEpoch 5/5\n600/600 [==============================] - 34s 57ms/step - total_loss: 132.7808 - reconstruction_loss: 127.6688 - kl_loss: 5.1120 - val_loss: 135.8917 - val_reconstruction_loss: 130.7375 - val_kl_loss: 5.1542\n\n\n&lt;keras.src.callbacks.History at 0x2c59e9f10&gt;"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson3/chapter3.html#analysing-the-vae",
    "href": "posts/ml/generative_deep_learning/lesson3/chapter3.html#analysing-the-vae",
    "title": "Generative AI: VAEs",
    "section": "",
    "text": "As before, we can eyeball the reconstructions from our model.\n\n\nCode\n# Select a subset of the test set\nn_to_predict = 5000\nexample_images = x_test[:n_to_predict]\nexample_labels = y_test[:n_to_predict]\n\n# Create autoencoder predictions and display\nz_mean, z_log_var, reconstructions = vae.predict(example_images)\nprint(\"Example real clothing items\")\nplot_sample_images(example_images)\nprint(\"Reconstructions\")\nplot_sample_images(reconstructions)\n\n\n 42/157 [=======&gt;......................] - ETA: 1s157/157 [==============================] - 1s 9ms/step\nExample real clothing items\nReconstructions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can sample and decode points in the latent space to observe how the model generates images. We note that:\n\nThe latent space has more even coverage and does not stray to far from a standard Normal distribution. If this is not the case, we can vary the \\(\\beta\\) value used to give more weight to the KL loss term.\nWe do not see as many poorly formed images as we did when sampling a “gap” in a standard autoencoder.\n\n\n\nCode\n# Encode the example images\nz_mean, z_var, z = encoder.predict(example_images)\n\n# Sample some points in the latent space, from the standard normal distribution\ngrid_width, grid_height = (6, 3)\nz_sample = np.random.normal(size=(grid_width * grid_height, 2))\n# Decode the sampled points\nreconstructions = decoder.predict(z_sample)\n# Convert original embeddings and sampled embeddings to p-values\np = norm.cdf(z)\np_sample = norm.cdf(z_sample)\n# Draw a plot of...\nfigsize = 8\nplt.figure(figsize=(figsize, figsize))\n\n# ... the original embeddings ...\nplt.scatter(z[:, 0], z[:, 1], c=\"black\", alpha=0.5, s=2)\n\n# ... and the newly generated points in the latent space\nplt.scatter(z_sample[:, 0], z_sample[:, 1], c=\"#00B0F0\", alpha=1, s=40)\nplt.show()\n\n# Add underneath a grid of the decoded images\nfig = plt.figure(figsize=(figsize, grid_height * 2))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i in range(grid_width * grid_height):\n    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n    ax.axis(\"off\")\n    ax.text(\n        0.5,\n        -0.35,\n        str(np.round(z_sample[i, :], 1)),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")\n\n\n1/1 [==============================] - 0s 14ms/step\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe plots below show the latent space coloured by clothing type. The left plot shows this in terms of z-values and the right in terms of p-values.\nThe latent space is more continuous with fewer gaps, and different categories take similar amounts of space.\n\n\nCode\n# Colour the embeddings by their label (clothing type - see table)\nfigsize = 8\nfig = plt.figure(figsize=(figsize * 2, figsize))\nax = fig.add_subplot(1, 2, 1)\nplot_1 = ax.scatter(\n    z[:, 0], z[:, 1], cmap=\"rainbow\", c=example_labels, alpha=0.8, s=3\n)\nplt.colorbar(plot_1)\nax = fig.add_subplot(1, 2, 2)\nplot_2 = ax.scatter(\n    p[:, 0], p[:, 1], cmap=\"rainbow\", c=example_labels, alpha=0.8, s=3\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nNext we see what happens when we sample from the latent space in a regular grid.\n\n\nCode\n# Colour the embeddings by their label (clothing type - see table)\nfigsize = 12\ngrid_size = 15\nplt.figure(figsize=(figsize, figsize))\nplt.scatter(\n    p[:, 0], p[:, 1], cmap=\"rainbow\", c=example_labels, alpha=0.8, s=300\n)\nplt.colorbar()\n\nx = norm.ppf(np.linspace(0, 1, grid_size))\ny = norm.ppf(np.linspace(1, 0, grid_size))\nxv, yv = np.meshgrid(x, y)\nxv = xv.flatten()\nyv = yv.flatten()\ngrid = np.array(list(zip(xv, yv)))\n\nreconstructions = decoder.predict(grid)\n# plt.scatter(grid[:, 0], grid[:, 1], c=\"black\", alpha=1, s=10)\nplt.show()\n\nfig = plt.figure(figsize=(figsize, figsize))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\nfor i in range(grid_size**2):\n    ax = fig.add_subplot(grid_size, grid_size, i + 1)\n    ax.axis(\"off\")\n    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")\n\n\n8/8 [==============================] - 0s 6ms/step"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson3/chapter3.html#references",
    "href": "posts/ml/generative_deep_learning/lesson3/chapter3.html#references",
    "title": "Generative AI: VAEs",
    "section": "",
    "text": "Chapter 3 of Generative Deep Learning by David Foster."
  },
  {
    "objectID": "posts/ml/graph_ml/part_1_2/graph_algorithms.html",
    "href": "posts/ml/graph_ml/part_1_2/graph_algorithms.html",
    "title": "Graph ML: Graph Algorithms",
    "section": "",
    "text": "The following are standard graph algorithms."
  },
  {
    "objectID": "posts/ml/graph_ml/part_1_2/graph_algorithms.html#graph-search",
    "href": "posts/ml/graph_ml/part_1_2/graph_algorithms.html#graph-search",
    "title": "Graph ML: Graph Algorithms",
    "section": "1. Graph Search",
    "text": "1. Graph Search\nWe are searching for a particular vertex in the graph.\nFor DFS and BFS, we need to iterate over each of the \\(V\\) nodes and each of its edges \\(E\\). We visit the edge from both sides, i.e. from each of the nodes attached to it, so we perform \\(V + 2E\\) steps.\nThe complexity is therefore \\(O(V+ E)\\).\n\n1.1. Depth-First Search (DFS)\nStart at the root node and go as deep as possible before backtracking.\nDFS is a recursive algorithm where we keep track of the nodes visited so far.\nIt is similar to the binary tree traversal algorithm, which shouldn’t be surprising since a tree is a type of graph and we want to traverse it to find our target node.\nSteps:\n\nInitialise: Start at any (random) node.\nVisit the node: Add the current node to the visited_nodes hash table.\nRecursively visit neighbors: Iterate through the current node’s neighbors. If the neighbor has already been visited, ignore it. Otherwise, recursively call DFS on the unvisited neighbor, i.e. go back to Step 2.\n\nDFS can be used to find cycles, because a node will be visited twice during traversal if and only if a cycle exists.\n\n\n1.2. Breadth-First Search (BFS)\nStart at the root node and explore all neighbors before going deeper.\nBFS does not rely on recursion, instead using a queue to track the execution order.\nSteps:\n\nInitialise: Start at any (random) node.\nVisit the node: Add the current node to the visited_nodes hash table and to the node_queue.\nIterate through the queue: Run a while-loop to iterate while the queue is populated.\nProcess the queue: Pop the first node from node_queue. Iterate over its neighbors. If the neighbor has been visited already, ignore it. Otherwise, visit the node - add it to the visited_nodes hash table and to the node_queue. Repeat until the queue is empty.\n\n\n\n1.3. A* Algorithm\n\n\n1.4. Bidirectional Search"
  },
  {
    "objectID": "posts/ml/graph_ml/part_1_2/graph_algorithms.html#shortest-path-algorithms",
    "href": "posts/ml/graph_ml/part_1_2/graph_algorithms.html#shortest-path-algorithms",
    "title": "Graph ML: Graph Algorithms",
    "section": "2. Shortest Path Algorithms",
    "text": "2. Shortest Path Algorithms\nWe are trying to find the shortest path between two nodes.\n\n2.1. Dijkstra’s Algorithm\nDijkstra’s algorithm maintains\n\nA hash map shortest_distances containing shortest known distances from the starting node to each node visited.\nA hash map shortest_path_previous_node tracking the previous node visited on the shortest knonw path to a node.\n\nA nice bonus is that it ends up finding the shortest path from the source node to all other nodes, not just the target node we care about.\nSteps:\n\nInitialise: Start at the source node and make it the current_node.\nCheck the immediate neighbours: If the distance to a neighbor from the source node is shorter than the current known shortest_distances, then update shortest_distances and shortest_path_previous_node.\nVisit the new node closest to source: Visit whichever unvisited node has the shortest known distance. Make it the current_node and repeat Step 2."
  },
  {
    "objectID": "posts/ml/graph_ml/part_1_2/graph_algorithms.html#minimum-spanning-tree",
    "href": "posts/ml/graph_ml/part_1_2/graph_algorithms.html#minimum-spanning-tree",
    "title": "Graph ML: Graph Algorithms",
    "section": "3. Minimum Spanning Tree",
    "text": "3. Minimum Spanning Tree\n\n3.1. Kruskal’s Algorithm\n\n\n3.2. Prim’s Algorithm"
  },
  {
    "objectID": "posts/ml/graph_ml/part_1_2/graph_algorithms.html#further-topics-in-graph-theory",
    "href": "posts/ml/graph_ml/part_1_2/graph_algorithms.html#further-topics-in-graph-theory",
    "title": "Graph ML: Graph Algorithms",
    "section": "4. Further Topics in Graph Theory",
    "text": "4. Further Topics in Graph Theory\n\nTarjan’s strongly connected components algorithm\nTopological sort\nFloyd-Warshall algorithms\nBellman-Ford algorithm\nGraph coloring\nMin-cut max-flow"
  },
  {
    "objectID": "posts/ml/timeseries/timeseries.html",
    "href": "posts/ml/timeseries/timeseries.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "Time series data has certain properties that make it different from other tabular or sequential data:\n\nOrdering is important: The order of data points is important so you need to be wary of information leakage. For example, unless you really know what you’re doing and have a good reason, you probably shouldn’t shuffle your train/test split.\nLook-ahead bias: Related to the above. We often want to forecast future data points, so we need to make sure our model is not inadvertently looking ahead.\nIrregular sampling: The data is not always at regular intervals. for example, tick-level financial data or heart beats in medical data.\nInformative sampling: The presence/timing of a sample contains information in and of itself. For example, more ticks in a short time window indicates more trading activity, or more heart beats in a time period indicates unusual activity. Resampling to a regular interval risks losing this information.\n\n\n\n\nA time series may exhibit a trend over time. That is to say, it’s rolling average is monotonically increasing/decreasing.\nWe can model this with a simple linear relationship w.r.t. time \\(t\\): \\[\ntarget = a t + b\n\\]\nIf the trend is non-linear, we can transform the time variable so we can still apply linear models. For example, if we think the trend is quadratic with time, we can pass \\(t\\) and \\(t^2\\) as independent variables to a linear model: \\[\ntarget = a t^2 + b t + c\n\\]\nWe may want to split the trend and residual components of the data and model them separately. If the residuals are stations (more on this later) then there are more models that would be applicable.\nA moving average term can be useful to eyeball changes in trend.\n\nplot_ma_df = df[['BTCUSD']].copy()\nplot_ma_df['100_day_MA'] = plot_ma_df['BTCUSD'].rolling(100).mean()\n\nplot_ma_df.plot()\n\n                                                \n\n\n\n\n\nSeasonality is when there are regular, periodic changes in the mean of a time series. They often happen at “human-interpretable” intervals, e.g. daily, weekly, monthly, etc.\nA seasonal plot can help identify such seasonality. If we suspect day-of-week seasonality, we can plot the day of week vs target value to see if there is a common behaviour.\nThis time series doesn’t actually exhibit any strong seasonality, but let’s see how we’d check.\n\nplot_seasonal_df = df[['BTCUSD']].copy()\nplot_seasonal_df['day_of_week'] = plot_seasonal_df.index.dayofweek\n\nplot_seasonal_df.tail(50).set_index('day_of_week').plot()\n\n                                                \n\n\nSeasonal indicators are binary features that represent the seasonality level of interest.\nFor example, if we believed there was weekly seasonality, we could one-hot encode each day of the week as a feature.\n\nplot_seasonal_df['Monday'] = (plot_seasonal_df['day_of_week'] == 0) * 1.\nplot_seasonal_df['Tuesday'] = (plot_seasonal_df['day_of_week'] == 1) * 1.\nplot_seasonal_df['Wednesday'] = (plot_seasonal_df['day_of_week'] == 2) * 1.\nplot_seasonal_df['Thursday'] = (plot_seasonal_df['day_of_week'] == 3) * 1.\nplot_seasonal_df['Friday'] = (plot_seasonal_df['day_of_week'] == 4) * 1.\nplot_seasonal_df['Saturday'] = (plot_seasonal_df['day_of_week'] == 5) * 1.\n\nplot_seasonal_df\n\n\n\n\n\n\n\n\nBTCUSD\nday_of_week\nMonday\nTuesday\nWednesday\nThursday\nFriday\nSaturday\n\n\ntimestamp\n\n\n\n\n\n\n\n\n\n\n\n\n2018-01-01\n13657.200195\n0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2018-01-02\n14982.099609\n1\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n2018-01-03\n15201.000000\n2\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n2018-01-04\n15599.200195\n3\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n2018-01-05\n17429.500000\n4\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-07-05\n30514.166016\n2\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n2023-07-06\n29909.337891\n3\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n2023-07-07\n30342.265625\n4\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n2023-07-08\n30292.541016\n5\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n2023-07-09\n30280.958984\n6\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n2016 rows × 8 columns\n\n\n\nWe can decompose a timeseries into trend + seasonal + residual components.\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndecomp = seasonal_decompose(df['BTCUSD'], model='additive', filt=None, period=None, two_sided=False, extrapolate_trend=0)\nfig = decomp.plot()\n\n\n\n\n\n\n\n\nFourier analysis can be useful in determining frequencies of seasonality. More on this later.\nIn brief, we can plot the periodogram to determine the strength of different frequencies.\n\n# From https://www.kaggle.com/code/ryanholbrook/seasonality\ndef plot_periodogram(ts, detrend='linear', ax=None):\n    from scipy.signal import periodogram\n    fs = pd.Timedelta(\"365D\") / pd.Timedelta(\"1D\")\n    freqencies, spectrum = periodogram(\n        ts,\n        fs=fs,\n        detrend=detrend,\n        window=\"boxcar\",\n        scaling='spectrum',\n    )\n    if ax is None:\n        _, ax = plt.subplots()\n    ax.step(freqencies, spectrum, color=\"purple\")\n    ax.set_xscale(\"log\")\n    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104])\n    ax.set_xticklabels(\n        [\n            \"Annual (1)\",\n            \"Semiannual (2)\",\n            \"Quarterly (4)\",\n            \"Bimonthly (6)\",\n            \"Monthly (12)\",\n            \"Biweekly (26)\",\n            \"Weekly (52)\",\n            \"Semiweekly (104)\",\n        ],\n        rotation=30,\n    )\n    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n    ax.set_ylabel(\"Variance\")\n    ax.set_title(\"Periodogram\")\n    return ax\n\n\nplot_periodogram(df['BTCUSD'])\n\n\n\n\n\n\n\n\n\n\n\n\nA stationary time series is one whose properties do not depend on the time at which the series is observed\n\nFrom Forecasting: Principles and Practice\nIn other words, the mean and variance do not change over time.\nIn the context of financial time series, it is often the case that price is non-stationary, but returns (the lag-1 difference) is stationary.\nThe Augmented Dickey-Fuller test provides a test statistic to quantify stationarity. \nUsing the BTCUSD time series as an example, the price series is definitely not stationary. We can see this from a plot, but the ADF test corroborates this with a p-value of 0.58.\n\nfrom statsmodels.tsa.stattools import adfuller\n\nadf_test = adfuller(df['BTCUSD'])\nadf_results = {\n    \"ADF Statistic\": adf_test[0],\n    \"p-value\": adf_test[1],\n    \"Critical Values\": adf_test[4],\n}\n\nadf_results\n\n{'ADF Statistic': -1.4058608330365159,\n 'p-value': 0.5794631585252685,\n 'Critical Values': {'1%': -3.4336386745240652,\n  '5%': -2.8629927557359443,\n  '10%': -2.5675433856598793}}\n\n\nNext we take the differences. This is stationary, with a tiny p-value.\n\nbtc_rets = df['BTCUSD'].diff().dropna()\n\nadf_test = adfuller(btc_rets)\nadf_results = {\n    \"ADF Statistic\": adf_test[0],\n    \"p-value\": adf_test[1],\n    \"Critical Values\": adf_test[4],\n}\n\nadf_results\n\n{'ADF Statistic': -7.742323245600769,\n 'p-value': 1.0538877703747789e-11,\n 'Critical Values': {'1%': -3.433643643742798,\n  '5%': -2.862994949652858,\n  '10%': -2.5675445538118042}}"
  },
  {
    "objectID": "posts/ml/timeseries/timeseries.html#considerations-for-time-series-data",
    "href": "posts/ml/timeseries/timeseries.html#considerations-for-time-series-data",
    "title": "Time Series Analysis",
    "section": "",
    "text": "Time series data has certain properties that make it different from other tabular or sequential data:\n\nOrdering is important: The order of data points is important so you need to be wary of information leakage. For example, unless you really know what you’re doing and have a good reason, you probably shouldn’t shuffle your train/test split.\nLook-ahead bias: Related to the above. We often want to forecast future data points, so we need to make sure our model is not inadvertently looking ahead.\nIrregular sampling: The data is not always at regular intervals. for example, tick-level financial data or heart beats in medical data.\nInformative sampling: The presence/timing of a sample contains information in and of itself. For example, more ticks in a short time window indicates more trading activity, or more heart beats in a time period indicates unusual activity. Resampling to a regular interval risks losing this information."
  },
  {
    "objectID": "posts/ml/timeseries/timeseries.html#trend",
    "href": "posts/ml/timeseries/timeseries.html#trend",
    "title": "Time Series Analysis",
    "section": "",
    "text": "A time series may exhibit a trend over time. That is to say, it’s rolling average is monotonically increasing/decreasing.\nWe can model this with a simple linear relationship w.r.t. time \\(t\\): \\[\ntarget = a t + b\n\\]\nIf the trend is non-linear, we can transform the time variable so we can still apply linear models. For example, if we think the trend is quadratic with time, we can pass \\(t\\) and \\(t^2\\) as independent variables to a linear model: \\[\ntarget = a t^2 + b t + c\n\\]\nWe may want to split the trend and residual components of the data and model them separately. If the residuals are stations (more on this later) then there are more models that would be applicable.\nA moving average term can be useful to eyeball changes in trend.\n\nplot_ma_df = df[['BTCUSD']].copy()\nplot_ma_df['100_day_MA'] = plot_ma_df['BTCUSD'].rolling(100).mean()\n\nplot_ma_df.plot()"
  },
  {
    "objectID": "posts/ml/timeseries/timeseries.html#seasonality",
    "href": "posts/ml/timeseries/timeseries.html#seasonality",
    "title": "Time Series Analysis",
    "section": "",
    "text": "Seasonality is when there are regular, periodic changes in the mean of a time series. They often happen at “human-interpretable” intervals, e.g. daily, weekly, monthly, etc.\nA seasonal plot can help identify such seasonality. If we suspect day-of-week seasonality, we can plot the day of week vs target value to see if there is a common behaviour.\nThis time series doesn’t actually exhibit any strong seasonality, but let’s see how we’d check.\n\nplot_seasonal_df = df[['BTCUSD']].copy()\nplot_seasonal_df['day_of_week'] = plot_seasonal_df.index.dayofweek\n\nplot_seasonal_df.tail(50).set_index('day_of_week').plot()\n\n                                                \n\n\nSeasonal indicators are binary features that represent the seasonality level of interest.\nFor example, if we believed there was weekly seasonality, we could one-hot encode each day of the week as a feature.\n\nplot_seasonal_df['Monday'] = (plot_seasonal_df['day_of_week'] == 0) * 1.\nplot_seasonal_df['Tuesday'] = (plot_seasonal_df['day_of_week'] == 1) * 1.\nplot_seasonal_df['Wednesday'] = (plot_seasonal_df['day_of_week'] == 2) * 1.\nplot_seasonal_df['Thursday'] = (plot_seasonal_df['day_of_week'] == 3) * 1.\nplot_seasonal_df['Friday'] = (plot_seasonal_df['day_of_week'] == 4) * 1.\nplot_seasonal_df['Saturday'] = (plot_seasonal_df['day_of_week'] == 5) * 1.\n\nplot_seasonal_df\n\n\n\n\n\n\n\n\nBTCUSD\nday_of_week\nMonday\nTuesday\nWednesday\nThursday\nFriday\nSaturday\n\n\ntimestamp\n\n\n\n\n\n\n\n\n\n\n\n\n2018-01-01\n13657.200195\n0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2018-01-02\n14982.099609\n1\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n2018-01-03\n15201.000000\n2\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n2018-01-04\n15599.200195\n3\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n2018-01-05\n17429.500000\n4\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-07-05\n30514.166016\n2\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n2023-07-06\n29909.337891\n3\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n2023-07-07\n30342.265625\n4\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n2023-07-08\n30292.541016\n5\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n2023-07-09\n30280.958984\n6\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n2016 rows × 8 columns\n\n\n\nWe can decompose a timeseries into trend + seasonal + residual components.\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndecomp = seasonal_decompose(df['BTCUSD'], model='additive', filt=None, period=None, two_sided=False, extrapolate_trend=0)\nfig = decomp.plot()\n\n\n\n\n\n\n\n\nFourier analysis can be useful in determining frequencies of seasonality. More on this later.\nIn brief, we can plot the periodogram to determine the strength of different frequencies.\n\n# From https://www.kaggle.com/code/ryanholbrook/seasonality\ndef plot_periodogram(ts, detrend='linear', ax=None):\n    from scipy.signal import periodogram\n    fs = pd.Timedelta(\"365D\") / pd.Timedelta(\"1D\")\n    freqencies, spectrum = periodogram(\n        ts,\n        fs=fs,\n        detrend=detrend,\n        window=\"boxcar\",\n        scaling='spectrum',\n    )\n    if ax is None:\n        _, ax = plt.subplots()\n    ax.step(freqencies, spectrum, color=\"purple\")\n    ax.set_xscale(\"log\")\n    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104])\n    ax.set_xticklabels(\n        [\n            \"Annual (1)\",\n            \"Semiannual (2)\",\n            \"Quarterly (4)\",\n            \"Bimonthly (6)\",\n            \"Monthly (12)\",\n            \"Biweekly (26)\",\n            \"Weekly (52)\",\n            \"Semiweekly (104)\",\n        ],\n        rotation=30,\n    )\n    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n    ax.set_ylabel(\"Variance\")\n    ax.set_title(\"Periodogram\")\n    return ax\n\n\nplot_periodogram(df['BTCUSD'])"
  },
  {
    "objectID": "posts/ml/timeseries/timeseries.html#stationarity",
    "href": "posts/ml/timeseries/timeseries.html#stationarity",
    "title": "Time Series Analysis",
    "section": "",
    "text": "A stationary time series is one whose properties do not depend on the time at which the series is observed\n\nFrom Forecasting: Principles and Practice\nIn other words, the mean and variance do not change over time.\nIn the context of financial time series, it is often the case that price is non-stationary, but returns (the lag-1 difference) is stationary.\nThe Augmented Dickey-Fuller test provides a test statistic to quantify stationarity. \nUsing the BTCUSD time series as an example, the price series is definitely not stationary. We can see this from a plot, but the ADF test corroborates this with a p-value of 0.58.\n\nfrom statsmodels.tsa.stattools import adfuller\n\nadf_test = adfuller(df['BTCUSD'])\nadf_results = {\n    \"ADF Statistic\": adf_test[0],\n    \"p-value\": adf_test[1],\n    \"Critical Values\": adf_test[4],\n}\n\nadf_results\n\n{'ADF Statistic': -1.4058608330365159,\n 'p-value': 0.5794631585252685,\n 'Critical Values': {'1%': -3.4336386745240652,\n  '5%': -2.8629927557359443,\n  '10%': -2.5675433856598793}}\n\n\nNext we take the differences. This is stationary, with a tiny p-value.\n\nbtc_rets = df['BTCUSD'].diff().dropna()\n\nadf_test = adfuller(btc_rets)\nadf_results = {\n    \"ADF Statistic\": adf_test[0],\n    \"p-value\": adf_test[1],\n    \"Critical Values\": adf_test[4],\n}\n\nadf_results\n\n{'ADF Statistic': -7.742323245600769,\n 'p-value': 1.0538877703747789e-11,\n 'Critical Values': {'1%': -3.433643643742798,\n  '5%': -2.862994949652858,\n  '10%': -2.5675445538118042}}"
  },
  {
    "objectID": "posts/software/sql/notes.html",
    "href": "posts/software/sql/notes.html",
    "title": "SQL",
    "section": "",
    "text": "Structured Query Language (SQL) is used to manage and query data stored in a Relational Database Management System (RDMBS). Essentially, put data in tables and get it out again.\nAn RDMBS organises data into tables with defined schemas (column names and data types).\nSQL is a declarative language. You specify what you want to happen, not how to achieve it. The SQL query engine optimises how the query run internally, e.g. what order to execute commands, what indexes to use.\n\n\n\nThere are different “flavours” of SQL. For example, MySQL, PostegreSQL, SQLite, SQL Server. These are often, incorrectly, used interchangeably with SQL. SQL is a general, high-level language for querying RDMBSes.\nThe “flavours” are each a specific RDBMS which you query with the corresponding language. Postgres is an RDBMS which you can query by writing PostgreSQL. MySQL is and RDBMS which you can query by writing MySQL. Etc. In practice, the difference is pretty minimal. If you can use one, you’ll learn the others pretty quickly.\n\n\n\nWe use PostgreSQL and its associated Postgres RDBMS.\nSome of the advantages of PostgreSQL:\n\nPopularity: One of the most popular behind MySQL\nOpen Source: BSD-style license is not too restrictive\nExtensible: Postgres has extensions like PostGIS for geospatial data, etc\nANSI Compliant: American National Standards Institute (ANSI) define standards and PostgreSQL mostly conforms to these. One of the least quirky flavours of SQL."
  },
  {
    "objectID": "posts/software/sql/notes.html#sql-and-rdbms",
    "href": "posts/software/sql/notes.html#sql-and-rdbms",
    "title": "SQL",
    "section": "",
    "text": "Structured Query Language (SQL) is used to manage and query data stored in a Relational Database Management System (RDMBS). Essentially, put data in tables and get it out again.\nAn RDMBS organises data into tables with defined schemas (column names and data types).\nSQL is a declarative language. You specify what you want to happen, not how to achieve it. The SQL query engine optimises how the query run internally, e.g. what order to execute commands, what indexes to use."
  },
  {
    "objectID": "posts/software/sql/notes.html#sql-flavours",
    "href": "posts/software/sql/notes.html#sql-flavours",
    "title": "SQL",
    "section": "",
    "text": "There are different “flavours” of SQL. For example, MySQL, PostegreSQL, SQLite, SQL Server. These are often, incorrectly, used interchangeably with SQL. SQL is a general, high-level language for querying RDMBSes.\nThe “flavours” are each a specific RDBMS which you query with the corresponding language. Postgres is an RDBMS which you can query by writing PostgreSQL. MySQL is and RDBMS which you can query by writing MySQL. Etc. In practice, the difference is pretty minimal. If you can use one, you’ll learn the others pretty quickly."
  },
  {
    "objectID": "posts/software/sql/notes.html#postgresql",
    "href": "posts/software/sql/notes.html#postgresql",
    "title": "SQL",
    "section": "",
    "text": "We use PostgreSQL and its associated Postgres RDBMS.\nSome of the advantages of PostgreSQL:\n\nPopularity: One of the most popular behind MySQL\nOpen Source: BSD-style license is not too restrictive\nExtensible: Postgres has extensions like PostGIS for geospatial data, etc\nANSI Compliant: American National Standards Institute (ANSI) define standards and PostgreSQL mostly conforms to these. One of the least quirky flavours of SQL."
  },
  {
    "objectID": "posts/software/sql/notes.html#select",
    "href": "posts/software/sql/notes.html#select",
    "title": "SQL",
    "section": "2.1. SELECT",
    "text": "2.1. SELECT\nUse SELECT to read specific columns (or all with *) from a given table.\nSELECT column1, column2\nFROM table_name;\nWe can optional use LIMIT to return a set number of rows. This can be helpful if we’re querying a massive table that might be a huge query.\nSELECT *\nFROM table_name\nLIMIT 10\nWe can use the AS keyword to alias a column name.\nSELECT column1, column2 AS skibidi  -- using a stupid alias\nFROM table_name;\nWe can also add comments with -- as above."
  },
  {
    "objectID": "posts/software/sql/notes.html#where",
    "href": "posts/software/sql/notes.html#where",
    "title": "SQL",
    "section": "2.2. WHERE",
    "text": "2.2. WHERE\nUse WHERE to filter the result on a specific condition. Conditions can be: =, !=, &lt;, &gt;, &lt;=, &gt;=\nSELECT column1, column2\nFROM table_name\nWHERE condition;"
  },
  {
    "objectID": "posts/software/sql/notes.html#combining-conditions",
    "href": "posts/software/sql/notes.html#combining-conditions",
    "title": "SQL",
    "section": "2.3. Combining Conditions",
    "text": "2.3. Combining Conditions\nUse logical operators AND, OR, NOT to chain multiple conditions.\nSELECT *\nFROM table\nWHERE condition1\n  AND condition2\n  AND NOT condition3;"
  },
  {
    "objectID": "posts/software/sql/notes.html#between",
    "href": "posts/software/sql/notes.html#between",
    "title": "SQL",
    "section": "2.4. BETWEEN",
    "text": "2.4. BETWEEN\nThe BETWEEN operator can also be used as a condition, and is equivalent to a combination of &gt;= AND &lt;=. Note that both sides are inclusive.\nFor example, the following BETWEEN condition:\nSELECT column1, column2\nFROM table_name\nWHERE column1 BETWEEN 0 AND 100;\nis equivalent to\nSELECT column1, column2\nFROM table_name\nWHERE column1 &gt;= 0 AND column2 &lt;= 100;"
  },
  {
    "objectID": "posts/software/sql/notes.html#in",
    "href": "posts/software/sql/notes.html#in",
    "title": "SQL",
    "section": "2.5. IN",
    "text": "2.5. IN\nThe IN operator is another implicit combined condition. It saves us the hassle of writing out multiple OR conditions.\nFor example, the following IN condition:\nSELECT column1, column2\nFROM table_name\nWHERE column1 (1, 2);\nis equivalent to\nSELECT column1, column2\nFROM table_name\nWHERE column1 = 1 OR column1 = 2;"
  },
  {
    "objectID": "posts/software/sql/notes.html#like",
    "href": "posts/software/sql/notes.html#like",
    "title": "SQL",
    "section": "2.6. LIKE",
    "text": "2.6. LIKE\nThe LIKE operator is another implicit condition. Similarly to IN, it save us the hassle of writing out multiple OR conditions.\nIt allows us to match patterns using the wildcards _ (to represent a single character) or % (to represent arbitrary number of characters).\nThe LIKE command is case-sensitive. The ILIKE command is a case-insensitive variant (Insensitive LIKE)\nSELECT product_id,\n       manufacturer,\n       drug\nFROM pharmacy_sales\nWHERE drug LIKE '%Relief%';\nWe can use multiple underscores to match a specific number of unknown characters. For example, this will match “a” followed by any 3 characters.\nWHERE word LIKE 'a___'\nThe wildcards can be at multiple points in the pattern, e.g.\nWHERE word LIKE 'f_c_'"
  },
  {
    "objectID": "posts/software/sql/notes.html#order-by",
    "href": "posts/software/sql/notes.html#order-by",
    "title": "SQL",
    "section": "2.7. ORDER BY",
    "text": "2.7. ORDER BY\nThe order of rows saved in the database is not guaranteed. Executing the same SELECT twice in a row can give a different ordering.\nIf we want a specific order, we can specify the ORDER BY column(s).\nSELECT column1, column2\nFROM table_name\nORDER BY column1;\nBy default, this is in ascending order (ASC). We can pass DESC to instead return items in descending order. This can be column-specific.\nSELECT column1, column2\nFROM table_name\nORDER BY column1 ASC, column2 DESC;\nWe can also pass the column numbers rather than names.\nSELECT policy_holder_id, call_category, call_received\nFROM callers\nORDER BY 1,3 DESC;\nWe can use ORDER BY in conjunction with LIMIT where we need the top N highest/lowest results. We can also use OFFSET to skip a number of results.\nFor example, we can skip the first 10 rows and then return the next 5, so the following query returns the 11th-15th ordered results.\nSELECT *\nFROM callers\nORDER BY call_received DESC\nOFFSET 10\nLIMIT 5;"
  },
  {
    "objectID": "posts/software/sql/notes.html#aggregate-functions",
    "href": "posts/software/sql/notes.html#aggregate-functions",
    "title": "SQL",
    "section": "3.1. Aggregate Functions",
    "text": "3.1. Aggregate Functions\nWe can aggregate data with SUM, MIN, MAX, AVG, COUNT.\nSELECT COUNT(*)\nFROM table_name;"
  },
  {
    "objectID": "posts/software/sql/notes.html#group-by",
    "href": "posts/software/sql/notes.html#group-by",
    "title": "SQL",
    "section": "3.2. GROUP BY",
    "text": "3.2. GROUP BY\nThe aggregate functions can be run on the entire table as above. But they come into their own when grouping by particular fields.\nWe can GROUP BY one or more columns.\nSELECT\ncategory,\n    SUM(spend) AS total_spend\nFROM product_spend\nGROUP BY category;"
  },
  {
    "objectID": "posts/software/sql/notes.html#having",
    "href": "posts/software/sql/notes.html#having",
    "title": "SQL",
    "section": "3.3. HAVING",
    "text": "3.3. HAVING\nSuppose we want to filter the data on the aggregated value. For example, in the previous example, we want to only return categories with total_spend &gt; 10.\nNaively, we might try to use WHERE. But WHERE filters individual rows. Trying this will give some variation of the following error message\n\naggregate functions are not allowed in WHERE\n\nThe HAVING clause is essentially the analog of WHERE, but operates on grouped data rather than individual rows.\nSELECT ticker, AVG(open)\nFROM stock_prices\nGROUP BY ticker\nHAVING AVG(open) &gt; 200;"
  },
  {
    "objectID": "posts/software/sql/notes.html#distinct",
    "href": "posts/software/sql/notes.html#distinct",
    "title": "SQL",
    "section": "3.4. DISTINCT",
    "text": "3.4. DISTINCT\nThe DISTINCT keyword can specify that only rows where the column(s) are distinct. If we pass multiple columns, we will get all of the distinct pairs (or tuples in the general case) of those columns.\nSELECT DISTINCT col1, col2\nFROM table_name;\nDISTINCT can be combined with aggregate functions, typically COUNT.\nSELECT COUNT(DISTINCT user_id)\nFROM trades;"
  },
  {
    "objectID": "posts/software/sql/notes.html#arithmetic",
    "href": "posts/software/sql/notes.html#arithmetic",
    "title": "SQL",
    "section": "3.5. Arithmetic",
    "text": "3.5. Arithmetic\nWe can use standard mathematical operations +, -, /, *, ^, %\nSELECT salary + bonus AS total_compensation\nFROM employees;\nWe have the modulus operator % which returns the remainder of a division. This is often helpful in problems where we need to find odd or even values.\nSELECT *\nFROM measurements\nWHERE measurement_num % 2 = 1\nThese operations follow the usual BODMAS rule (or PEMDAS if you’re an asshole)."
  },
  {
    "objectID": "posts/software/sql/notes.html#mathematical-functions",
    "href": "posts/software/sql/notes.html#mathematical-functions",
    "title": "SQL",
    "section": "3.6. Mathematical Functions",
    "text": "3.6. Mathematical Functions\nThe following built-in maths functions are useful:\n\nABS() - absolute value\nCEIL() - round up\nFLOOR() - round down\nROUND(column_name, N) - round to N decimal places\nPOWER(column_name, exponent) - equivalent to column_name ^ exponent\nMOD(column_name, divisor) - equivalent tocolumn_name % divisor`"
  },
  {
    "objectID": "posts/software/sql/notes.html#division",
    "href": "posts/software/sql/notes.html#division",
    "title": "SQL",
    "section": "3.7. Division",
    "text": "3.7. Division\nDivision is SQL can be deceptively tricky. Naively, we might think we just do col1 / col2, job done.\nBut in practice, we can get weird results depending on the data types of the numerator or denominator.\n\n\n\nInput\nSQL Output\nExpected\n\n\n\n\nSELECT 10/4\n2\n2.5\n\n\nSELECT 10/2\n5\n5\n\n\nSELECT 10/6\n1\n1.6666666667\n\n\nSELECT 10.0/4\n2.5000000000000000\n2.5\n\n\nSELECT 10/3.0\n3.3333333333333333\n3.333333333\n\n\n\nWe can coerce values to floats by:\n\nUsing the CAST(column_name AS FLOAT) function\nMultiplying * 1.0\nExplicitly using types with ::\n\nSELECT \n  CAST(10 AS DECIMAL)/4,\n  CAST(10 AS FLOAT)/4,\n  10 * 1.0 / 4\n  10::DECIMAL / 4"
  },
  {
    "objectID": "posts/software/sql/notes.html#nulls",
    "href": "posts/software/sql/notes.html#nulls",
    "title": "SQL",
    "section": "3.8. Nulls",
    "text": "3.8. Nulls\nA NULL value indicates the absence of a value. Missing data is different to data which is populated but empty, like a 0 or an empty string.\nWe can identify null and non-null values with IS NULL and IS NOT NULL.\nSELECT *\nFROM goodreads\nWHERE book_title IS NULL;\nThe COALESCE keyword allows us to pass multiple inputs and return the first non-null value. We can pass multiple columns, or a mix of columns and a hard-coded default value. This makes it useful to fill nulls. Think of it like the pandas fillna method.\nSELECT COALESCE(book_rating, 0)  -- fill NULL values with 0\nFROM goodreads;\nWe can also use the IFNULL keyword to fill null values.\nSELECT \n  book_title, \n  IFNULL(book_rating, 0) AS rated_books  - fill NULL values with 0\nFROM goodreads;\nIn the above examples, IFNULL and COALESCE are interchangeable. In general, use COALESCE when checking multiple columns, e.g. COALESCE(col1, col2, col3). If only checking one column, IFNULL is more concise.\nThe above examples are to replace null with values. We can do the opposite – conditionally replace values with nulls – using the NULLIF command. NULLIF(expr1, expr2) will return NULL if the two expressions are equal."
  },
  {
    "objectID": "posts/software/sql/notes.html#case",
    "href": "posts/software/sql/notes.html#case",
    "title": "SQL",
    "section": "3.9. CASE",
    "text": "3.9. CASE\nThe CASE statement is used to create new columns, categorize data, or perform calculations based on specified conditions.\nSELECT\n  column_1,\n  column_2, \n  CASE \n    WHEN condition_1 THEN result_1\n    WHEN condition_2 THEN result_2\n    ELSE result_3 -- If condition_1 and condition_2 are not met, return result_3 in ELSE clause\n  END AS column_3_name -- Give your new column an alias\nFROM table_1;  \nWe can also use CASE inside a WHERE clause to filter rows based on specific conditions.\nSELECT\n  column_1,\n  column_2\nFROM table_1\nWHERE CASE \n    WHEN condition_1 THEN result_1\n    WHEN condition_2 THEN result_2\n    ELSE result_3 -- If condition_1 and condition_2 are not met, return result_3 in ELSE clause\n  END; \nAs a concrete example of filtering, we may want to filter based on number of followers, where the threshold for followers depends on the platform\nSELECT \n  actor, \n  character, \n  platform\nFROM marvel_avengers\nWHERE \n  CASE \n    WHEN platform = 'Instagram' THEN followers &gt;= 500000\n    WHEN platform = 'Twitter' THEN followers &gt;= 200000\n    ELSE followers &gt;= 100000\n  END;\nWe can also use aggregate functions like COUNT, AVG, SUM around a CASE statement to only include rows which meet a certain criteria.\nSELECT\n  platform,\n  SUM(CASE \n    WHEN engagement_rate &gt;= 8.0 THEN followers\n    ELSE 0\n  END) AS high_engagement_followers_sum,\n  SUM(CASE \n    WHEN engagement_rate &lt; 8.0 THEN followers\n    ELSE 0\n  END) AS low_engagement_followers_sum\nFROM marvel_avengers\nGROUP BY platform;"
  },
  {
    "objectID": "posts/software/sql/notes.html#join",
    "href": "posts/software/sql/notes.html#join",
    "title": "SQL",
    "section": "3.10. JOIN",
    "text": "3.10. JOIN\nJoining multiple tables is the bread and butter of relational databases. We specify the tables to JOIN and the keys to join ON. We optionally define the type of join; this defaults to INNER if not specified.\nSELECT *\nFROM artists\nJOIN songs\n  ON artists.artist_id = songs.artist_id;\nThere are 4 types of join:\n\nINNER JOIN - Returns only the rows with matching values from both tables.\nLEFT JOIN - Returns all the rows from the left table and the matching rows from the right table. NULL values where there is no match in the right table.\nRIGHT JOIN - Returns all the rows from the right table and the matching rows from the left table. NULL values where there is no match in the left table.\nFULL OUTER JOIN - Returns all rows from either table. Where there is no match in either left or right, return a NULL value.\n\nWe can perform conditional joins where we filter the tables as we join them. This avoids the need to join two, potentially large tables, then filter the very large result after joining.\nThis example will filter the orders table before joining it.\nSELECT \n  g.book_title, \n  o.quantity\nFROM goodreads AS g\nINNER JOIN orders AS o \n  ON g.book_id = o.book_id\n    AND o.quantity &gt; 2;\nSimilarly, we can also use CASE statements in the JOIN to apply the CASE on the source table before joining, rather than on the potentially larger result set. This may be particularly helpful when we also want to filter on that CASE statement before joining."
  },
  {
    "objectID": "posts/software/sql/notes.html#datetimes",
    "href": "posts/software/sql/notes.html#datetimes",
    "title": "SQL",
    "section": "3.11. Datetimes",
    "text": "3.11. Datetimes\nDates and timestamps are handled as specific data types in SQL.\nThe following functions are useful for getting the current date, time or datetime: - CURRENT_DATE - CURRENT_TIME - CURRENT_TIMESTAMP or NOW()\nWe can use comparison operators =, !=, &gt;, &lt; to compare datetimes. Aggregate functions like MIN and MAX also work.\nSELECT *\nFROM messages\nWHERE sent_date &gt;= '2022-08-10 00:00:00';\nWe can extract parts of the date with either EXTRACT or DATE_PART. These are equivalent, and can be used to extract year, month, day, hour, minute.\nSELECT \n  EXTRACT(YEAR FROM sent_date) AS extracted_year,\n  DATE_PART('year', sent_date) AS part_year\nFROM messages;\nWe can truncate a datetime to a specified granularity with DATE_TRUNC.\nSELECT \n  DATE_TRUNC('day', sent_date) AS truncated_to_day\nFROM messages;\nWe can add and subtract datetimes using INTERVAL.\nSELECT \n  sent_date + INTERVAL '2 days' AS add_2days,\n  sent_date - INTERVAL '10 minutes' AS minus_10mins\nFROM messages;\nTimestamps can be converted to strings with a specified format using TO_CHAR.\nSELECT \n  TO_CHAR(sent_date, 'YYYY-MM-DD HH:MI:SS') AS formatted_iso8601\nFROM messages;\nThere are different formatting options for the string.\n\n\n\n\n\n\n\n\nFormat Name\nFormat\nExample\n\n\n\n\nISO 8601 Date and Time\n‘YYYY-MM-DD HH24:MI:SS’\n‘2023-08-27 14:30:00’\n\n\nDate and Time with 12-hour Format\n‘YYYY-MM-DD HH:MI:SS AM’\n‘2023-08-27 02:30:00 PM’\n\n\nLong Month Name, Day and Year\n‘Month DDth, YYYY’\n‘August 27th, 2023’\n\n\nShort Month Name, Day and Year\n‘Mon DD, YYYY’\n‘Aug 27, 2023’\n\n\nDay, Month, and Year\n‘DD Month YYYY’\n‘27 August 2023’\n\n\nDay of the Month\n‘Month’\n‘August’\n\n\nDay of the Week\n‘Day’\n‘Saturday’\n\n\n\nStrings can be converted to timestamps by casting. There are two equivalent syntaxes. For dates we can use ::DATE or TO_DATE(). For timestamps we can use ::TIMESTAMP or TO_TIMESTAMP().\nSELECT \n  sent_date::DATE AS casted_date,\n  TO_DATE('2023-08-27', 'YYYY-MM-DD') AS converted_to_date,\n  sent_date::TIMESTAMP AS casted_timestamp,\n  TO_TIMESTAMP('2023-08-27 10:30:00', 'YYYY-MM-DD HH:MI:SS') AS converted_to_timestamp\nFROM messages;"
  },
  {
    "objectID": "posts/software/sql/notes.html#cte-vs-subquery",
    "href": "posts/software/sql/notes.html#cte-vs-subquery",
    "title": "SQL",
    "section": "4.1. CTE vs Subquery",
    "text": "4.1. CTE vs Subquery\nA Common Table Expression (CTE) is like a query within a query using a WITH statement.\nA Subquery is a query within a query using parentheses.\n\n4.1.1. CTE\nWe declare CTEs at the beginning of the query, which can help break down more complex queries to improve readability.\n-- Start of a CTE\nWITH genre_revenue_cte AS (\n  SELECT\n    genre,\n    SUM(concert_revenue) AS total_revenue\n  FROM concerts\n  GROUP BY genre\n)\n-- End of a CTE\n\nSELECT\n  g.genre,\n  g.total_revenue,\n  AVG(c.concert_revenue) AS avg_concert_revenue\nFROM genre_revenue_cte AS g\nINNER JOIN concerts AS c \n  ON g.genre = c.genre\nWHERE c.concert_revenue &gt; g.total_revenue * 0.5\nGROUP BY g.genre, g.total_revenue;\nWe can reuse the same CTE result multiple times, which can avoid redundant calculations.\nThey also allow for recursive queries.\nWITH recursive_cte AS (\n  SELECT \n    employee_id, \n    name, \n    manager_id\n  FROM employees\n  WHERE manager_id = @manager_id\n  \n  UNION ALL\n  \n  SELECT \n    e.employee_id, \n    e.name, \n    e.manager_id\n  FROM employees AS e\n  INNER JOIN recursive_cte AS r -- The RECURSIVE CTE is utilized here within the main CTE.\n    ON e.ManagerID = r.employee_id\n)\n\nSELECT * \nFROM recursive_cte;\n\n\n4.1.2. Subquery\nSubqueries are generally favoured where we need the result from some other small query. Think of it like lambda functions compared to defining a function in python; if the inner query is small then a subquery is more readable.\nSELECT artist_name\nFROM concerts\nWHERE concert_revenue &gt; (\n  SELECT AVG(concert_revenue) FROM concerts);\nIn a similar vein, we can use subqueries to create and aggregate columns on the fly without sacrificing readability.\nSELECT \n  artist_name, \n  genre, \n  concert_revenue,\n  (SELECT AVG(concert_revenue) FROM concerts) AS avg_concert_revenue,\n  (SELECT MAX(concert_revenue) FROM concerts) AS max_concert_revenue\nFROM concerts;\nThey can be useful for filtering on conditions in another query.\nSELECT artist_name\nFROM concerts\nWHERE artist_id IN (\n  SELECT artist_id FROM concert_revenue WHERE concert_revenue &gt; 500000);\nCorrelated subqueries are when we want to query one table based on a dynamic result in another (or itself). For example, return fields for the highest-grossing concerts of each genre.\nSELECT \n  artist_name, \n  genre, \n  concert_revenue\nFROM concerts AS c1\nWHERE concert_revenue = (\n  SELECT MAX(concert_revenue)\n  FROM concerts AS c2\n  WHERE c1.genre = c2.genre\n);"
  },
  {
    "objectID": "posts/software/sql/notes.html#window",
    "href": "posts/software/sql/notes.html#window",
    "title": "SQL",
    "section": "4.2. WINDOW",
    "text": "4.2. WINDOW\nWe have already calculated aggregates over the whole table or certain groups.\nWindow functions allow us to create our own partitions, or “virtual windows”, and perform cumulative aggregates over these.\nFor example, if we want a cumulative total of revenue for each product type:\nSELECT\n  spend,\n   SUM(spend) OVER (\n     PARTITION BY product\n     ORDER BY transaction_date) AS running_total\n  FROM product_spend;\nThe PARTITION BY clause defines our virtual windows. We can partition by multiple fields. ORDER BY determines the ordering for a running total. SUM(spend) OVER defines what aggreagtion we perform per partition. The OVER keyword is required for window function.\nCommon aggregates are COUNT, SUM, AVG, MIN, MAX, FIRST_VALUE, LAST_VALUE.\nThe distinction between PARTITION BY vs GROUP BY can feel subtle. Think of it like .rolling() vs .groupby() in pandas.\n\nGROUP BY normally reduces the number of rows returned by grouping them up and calculating averages or sums for each group.\nPARTITION BY does not affect the number of rows returned, but it changes how a window function’s result is calculated."
  },
  {
    "objectID": "posts/software/sql/notes.html#ranking",
    "href": "posts/software/sql/notes.html#ranking",
    "title": "SQL",
    "section": "4.3. Ranking",
    "text": "4.3. Ranking\nThere are several ranking functions – RANK, DENSE_RANK, ROW_NUMBER – we can use to ranks rows based on specific criteria. This assigns numbers indicating the position of the data within a certain “window”.\nThe syntax for all of them follows the same pattern:\nSELECT \n  RANK() / DENSE_RANK() / ROW_NUMBER() OVER ( -- Compulsory expression\n    PARTITION BY partitioning_expression -- Optional expression\n    ORDER BY order_expression) -- Compulsory expression\nFROM table_name;\nThe differences between the ranking functions:\n\nROW_NUMBER(): Essentially numbers the rows in order.\nRANK(): Tied values are given the same rank, skipping subsequent ranks which leaves gaps in the sequence.\nDENSE_RANK(): Tied values are given the same rank, without skipping subsequent ranks so there are no gaps in the sequence."
  },
  {
    "objectID": "posts/software/sql/notes.html#leadlag",
    "href": "posts/software/sql/notes.html#leadlag",
    "title": "SQL",
    "section": "4.4. Lead/Lag",
    "text": "4.4. Lead/Lag\nThe time series window functions LAG and LEAD allow us to access data from rows before or after the current row.\nLEAD(column_name, offset) OVER (  -- Compulsory expression\n  PARTITION BY partition_column -- Optional expression\n  ORDER BY order_column) -- Compulsory expression\nThis can “feel” similar to the OFFSET function. The difference is OFFSET skips rows in the output, resulting in a different result set. LAG accesses previous rows within the same result set, allowing for row-wise comparisons within a window. OFFSET wouldn’t work in a window function."
  },
  {
    "objectID": "posts/software/sql/notes.html#self-joins",
    "href": "posts/software/sql/notes.html#self-joins",
    "title": "SQL",
    "section": "4.5. Self Joins",
    "text": "4.5. Self Joins\nWe can join a table to itself to match on the data within the table.\nAs an example, the query below will take each book in the table and give suggestions of other books in the same genre.\nSELECT\n  b1.genre,\n  b1.book_title AS current_book,\n  b2.book_title AS suggested_book\nFROM goodreads AS b1\nINNER JOIN goodreads AS b2\n  ON b1.genre = b2.genre\nWHERE b1.book_id != b2.book_id  -- Don't suggest the same book\nORDER BY b1.book_title;"
  },
  {
    "objectID": "posts/software/sql/notes.html#set-operations---union-intersect-except",
    "href": "posts/software/sql/notes.html#set-operations---union-intersect-except",
    "title": "SQL",
    "section": "4.6. Set operations - UNION, INTERSECT, EXCEPT",
    "text": "4.6. Set operations - UNION, INTERSECT, EXCEPT\nThe set operation UNION allows us to combine data vertically, i.e. appending columns.\nIn the same way that JOINs allow us to combine data horizontally, i.e. appending rows.\nFor all set operations – UNION, INTERSECT, EXCEPT – the number of columns, data types, and order of columns must match between the two SELECT statements.\n\n\n\nJOIN vs UNION. From datalemur.com\n\n\nThe UNION keyword gives all unique rows, UNION ALL retains duplicate rows.\nSELECT col1, col2\nFROM table1\nUNION ALL\nSELECT col1, col2\nFROM table2;\n\n\n\nUNION vs UNION ALL. From datalemur.com\n\n\nThe INTERSECT command gives the intersection between two sets, i.e. the common rows present in both.\nSELECT order_id\nFROM orders\nWHERE quantity &gt;= 2\nINTERSECT\nSELECT order_id\nFROM deliveries\nWHERE delivery_status = 'Delivered';\n\n\n\nINTERSECT. From datalemur.com\n\n\nEXCEPT gives all the unique rows in A that are not present in B.\nSELECT ingredient\nFROM recipe_1\nEXCEPT\nSELECT ingredient\nFROM recipe_2;\n\n\n\nEXCEPT. From datalemur.com"
  },
  {
    "objectID": "posts/software/sql/notes.html#sql-code-best-practices",
    "href": "posts/software/sql/notes.html#sql-code-best-practices",
    "title": "SQL",
    "section": "4.7. SQL Code Best Practices",
    "text": "4.7. SQL Code Best Practices\n\nUPPERCASE for keywords:\nlowercase or snake_case for names\nDescriptive and concise aliases\nConsistent formatting and indentation\nAvoid SELECT *, explicitly specify columns\nUse JOINs explicitly for clarity\n\nRather than relying on FROM table1, table2 syntax\nSpecify the type of join (INNER, LEFT, RIGHT, OUTER)\n\nFormat dates consistently - YYYY-MM-DD\nComment wisely\n\nUse -- for inline comments and /* ... */ for multiline"
  },
  {
    "objectID": "posts/software/sql/notes.html#execution-order",
    "href": "posts/software/sql/notes.html#execution-order",
    "title": "SQL",
    "section": "4.8. Execution Order",
    "text": "4.8. Execution Order\nThe query engine for the database handles the order of execution of commands in the query. This is helpful to understand what happens in the database under the hood so we can optimise our queries better.\n\n\n\n\n\n\n\n\nClause\nOrder\nDescription\n\n\n\n\nFROM / JOIN\n1\nThe query begins with the FROM clause, where the database identifies the tables involved and accesses the necessary data.\n\n\nWHERE\n2\nThe database applies the conditions specified in the WHERE clause to filter the data retrieved from the tables in the FROM clause.\n\n\nGROUP BY\n3\nIf a GROUP BY clause is present, the data is grouped based on the specified columns, and aggregation functions (such as SUM(), AVG(), COUNT()) are applied to each group.\n\n\nHAVING\n4\nThe HAVING clause filters the aggregated data based on specified conditions.\n\n\nSELECT / DISTINCT\n5\nThe SELECT clause defines the columns to be included in the final result set.\n\n\nORDER BY\n6\nIf an ORDER BY clause is used, the result set is sorted according to the specified columns.\n\n\nLIMIT / OFFSET\n7\nIf LIMIT or OFFSET clause is present, the result set is restricted to the specified number of rows and optionally offset by a certain number of rows."
  },
  {
    "objectID": "posts/software/sql/notes.html#string-functions",
    "href": "posts/software/sql/notes.html#string-functions",
    "title": "SQL",
    "section": "4.9. String Functions",
    "text": "4.9. String Functions\nString functions are useful for cleaning and manipulating text data.\nWe can change case with UPPER and LOWER.\nSELECT \n  UPPER(text_col) AS upper_case_text,\n  LOWER(text_col) AS lower_case_text\nFROM table_name;\nWe can extract the first or last N characters of a string with LEFT(col, N) and RIGHT(col, N). If N is greater than the length of the string, it will return the whole string.\nSELECT \n  LEFT(text_col, 5) AS left_substring,\n  RIGHT(text_col, 5) AS right_substring\nFROM table_name;\nWe can calculate the LENGTH of a string.\nSELECT \n  LENGTH(text_col) AS text_length\nFROM table_name;\nWe can find the index of a substring within a larger string using POSITION(substring IN string). This returns 0 if the substring is not found.\nSELECT POSITION('substring' IN text_col) AS position_result\nFROM table_name;\nWhite spaces (and other characters) can be trimmed from the left, right or both sides using LTRIM, RTRIM, BTRIM. They can each take an optional second argument specifying the character to trim. The TRIM command removes spaces from both sides of the string; essentially a shorthand for BTRIM(text_col, ' ').\nSELECT \n  TRIM('     Spiderman') AS full_trim,\n  LTRIM('Iron Man', 'Iron ') AS left_trim,\n  RTRIM('Scarlet Witch', ' Witch') AS right_trim,\n  BTRIM('   Falcon   ', ' ') AS combination_trim1,\n  BTRIM('...Iron Man...', '.') AS combination_trim2\nFROM marvel_avengers;\nWe can combine multiple string fields with CONCAT.\nSELECT \n    CONCAT(col1, col2)\nFROM table_name;\nWe can also “concatenate with separator” using CONCAT_WS. So these two queries would be equivalent.\nSELECT \n    CONCAT(col1, '-', col2),\n    CONCAT_WS('-', col1,col2)\nFROM table_name;\nWe can extract a SUBSTRING from a larger string using the following syntax: SUBSTRING(string, start_position, length [optional]). The start_position argument can be a negative index, meaning it counts from the end of the string. The length argument is optional. If not provided it will return the rest of the string.\nSELECT \n    SUBSTRING(col1, 2),  -- From second chracter onwards\n    SUBSTRING(col1, 2, 5)  -- 5 characters, starting from the 2nd\nFROM table_name;\nWe can split text into segments based on a specific delimiter using SPLIT_PART(string, delimiter, part_number).\nSELECT \n  SPLIT_PART('Spider-Man', '-', 1) AS split_part_1, -- Extracting the first part: 'Spider'\n  SPLIT_PART('Spider-Man', '-', 2) AS split_part_2, -- Extracting the second part: 'Man'\n  SPLIT_PART('Black Widow', ' ', -1) AS split_part_3 -- Extracting the last part: 'Widow'\nFROM marvel_avengers;"
  },
  {
    "objectID": "posts/software/react/1_getting_started/post.html",
    "href": "posts/software/react/1_getting_started/post.html",
    "title": "React: A Gentle Introduction",
    "section": "",
    "text": "React is a Javascript library for building user interfaces. It is less cumbersome and error-prone than using vanilla JS.\nCode sandbox is an in-browser environment to experiment with UIs. As an example, the same page is implemented in pure Javascript and React. The latter is much easier to follow, modularise and requires less boilerplate.\n\nWith React, you write declarative code: you define the goal, not the steps to get there.\nWith vanilla JS, you write imperative code, defining the steps, not the goal.\n\nA build tool (like Vite or Next.js) is necessary because the Javascript (specifically the JSX) must be transformed. React uses JSX which allows us to “mix” HTML and JS, so that we can define layout and functionality in the same place. This isn’t natively supported by the browser, so a build tool transforms this to pure html and JS.\n\n\n\nKey concepts in React are: components, JSX, props, and state.\n\n\nComponents are a core concept. They bundle html, CSS and JS into reusable blocks.\nIn vanilla JS, the JavaScript and HTML are in different files, so it can be hard to follow what needs to be changed where. Related code lives together, which is a key benefit of React and component style coding.\nJSX is a JavaScript syntax extension that allows us to write HTML in JavaScript files. This is not natively supported by browsers, so requires transformation by the build system, such as Vite.\nThe build process (of some but not all build tools) relies on the jsx file extension to indicate a JSX file that needs transformation. The browser does not care, as it never sees (and cannot read) these jsx files directly. Similarly, some build processes require the file extension in the import statements but others don’t.\nComponents must:\n\nStart with an upper case letter - so they do not clash with built-ins like header\nReturn a renderable object\n\nReact creates a component tree for your app. Your components do not end up in the source code directly. The build process traverses the tree until each component is resolved into built ins, and then these appear in the source code.\n\n\n\nUse curly braces to indicate dynamic values in JSX.\nIdeally declare constants rather than having complicated inline expressions.\nImages should be exported then the dynamic value passed as the src of the image. This prevents the image being lost in the build process if the build ignores files with certain extensions.\nimport myImage from './assets/exampleImage.png'\n\n&lt;img src={myImage}/&gt;\nDynamic values can be passed to components as props. React components take a single argument called props, which is an object of key:value pairs passed to the component.\nIf you have an object of props to pass, you can use the spread operator to avoid writing them out individually. Also use object destructuring inside the component to pick out the variables.\n\n\n\nIt is good practice that each component is in its own file. File name should match the component name and be the default export.\nAlso split out style CSS files and keep these alongside the component. CSS files need to be imported by each component file that uses it.\nThe styles are NOT automatically scoped to the component that uses them. They will apply to all components with that name. For example, if you apply header styling to a custom Header component, it will also apply to the built in header html component.\n\n\n\nThe children prop is passed by all components and it is the value between the component tags. It can be used for HTML tag-style syntax.\nWe can react to events by passing a function to onClick or similar. In vanilla JS, we would need to select the element and add an event listener, but react is declarative. We can define the handleClick function inside the component so that it has access to the component’s props and state. We can pass functions as props. This is useful as we can pass state setter functions down to nested components. This should be a pointer to the function, not the executed function itself, e.g. handleClick NOT handleClick()\nIf we want to modify the args that we pass to the function in onClick, use an anonymous arrow function () =&gt; handleSelect(arg) That doesn’t actually get executed until onClick is called.\nWe can set default values of props by putting the default value in the function signature.\n\n\n\nBy default, React components only execute once, even if an internal variable changes. You have to “tell” React to execute something again. This is where state comes in useful. React checks if UI updates are needed by comparing old output with new and applying the difference. So we use states rather than regular variables to indicate that a re-render is required if the state changes. State is essentially a special registered variable that react handles differently. If the state of a component changes, that component and its children in the component tree re-render.\nThe useState function is a “hook”. Hooks must be declared in the top level of a component function, they can’t be nested in internal functions such as event handlers, and they also can’t be declared outside of functions. It returns an array of two elements, the state value and a setter. A default state value can be passed to use state. The setter “schedules” an update, but that isn’t necessarily immediate. So you can see unexpected things when logging a value after the setter in code, where the logged value is still the “old” state value because the UI update has been scheduled but not completed yet.\nIf setting a state value based on its previous value, pass it as a function. For example, if on a button click we want to invert the value of a Boolean, use\nsetIsEditing((editing) =&gt; {!editing})\nNOT\nsetIsEditing(!isEditing)\nThis is because React schedules when to change state, it doesn’t necessarily do it immediately. So you could get unexpected behaviour. But when passed as a function, this triggers a re-render, similarly to how a hook does.\n\n\n\n\n\n\nOne option is to use JSX with a ternary expression. It is valid for null to be used in place of a component.\n{ selectedState ? ComponentA : ComponentB }\nAn alternative is the and operator which can also be used for this. In JavaScript, if the first term is truthy then it returns the second term, which is what we want.\n{ selectedState && ComponentA }\nA third option is to save the component as a variable and conditionally reassign it.\n// The default component\nlet tabContent = &lt;p&gt;Please select a topic&lt;/p&gt;\n\nif (selectedTopic) {\n    tabContent = (\n        // The component displayed if a topic is selected\n        &lt;div&gt;\n            &lt;h3&gt;selectedTopic.title&lt;/h3&gt;\n             &lt;p&gt;selectedTopic.description&lt;/p&gt;\n        &lt;/div&gt;\n    )\n}\n\n\n\nWe can set className as a JSX expression and use a ternary expression. For example, check if the button is selected and apply a different className depending on whether it’s selected.\n\n\n\nJSX is capable of outputting lists of renderable components. We do this by using map over the array:\nmyArray.map((item) =&gt; (&lt;Component item={item}/&gt;))\nAdd a key prop to the Component which uniquely identifies the item to avoid warnings raised by React.\n\n\n\n\nThe following sections are a collection of less essential topics but provide useful background in React.\n\n\nYou don’t NEED JSX, but it makes life easier.\nThe following is JSX, which is easy to read but requires a build transformation process.\n&lt;div id=\"content\"&gt;\n    &lt;p&gt;Hello World!&lt;/p&gt;\n&lt;/div&gt;\nThe alternative in plain JavaScript is to manipulate the DOM directly. It’s not as clear. But it avoids the need for a build process because it IS valid JavaScript supported by the browser.\nReact.createElement(\n    'div',\n    { id: 'content'},\n    React.createElement(\n        'p',\n        null,\n        'Hello World!'\n    )\n)\n\n\n\nA JavaScript function must return one value, it cannot return multiple values. This is true of React components, since they are really just syntactic sugar around JavaScript functions.\nSo if we have two or more sibling components being returned, we must wrap them in a parent component. Naively, we could just use a div, but this adds an unnecessary extra component to our tree.\nAn alternative is to use Fragment. This can be imported from react and used as a parent component without actually creating any new component when built. In newer versions of react, we can skip the import and just use empty opening and closing tags &lt;&gt; &lt;/&gt; to create a Fragment.\n\n\n\nRemember, React will re-render a component and all its child components when a state changes. So if a state is too high up the component tree, it will cause unnecessary re-rendering of many other components.\nThis is an indication that a component needs to be split out and its state managed lower down the tree.\n\n\n\nReact doesn’t auto-forward props to nested components.\nWe can forward an arbitrary number of props without having to write out each manually. Use the rest operator …extraProps in the function signature. Then use the spread operator (same syntax) to pass them to the inner component that you want …extraProps.\nThis is like **kwargs in Python.\n\n\n\nWe’ve seen how we can pass the special children prop to pass arbitrary JSX to our components.\nWhat if we wanted 2 or more slots in our component with arbitrary content? Components are ultimately just JavaScript code, so we can pass the components for the other slot as a prop (and if there are multiple siblings we can wrap them in a fragment).\n\n\n\nWe can pass a prop (with capital letter since it will be used as a custom component) which we can then vary in the nested component. For example, pass a prop called ButtonContainer which can be:\n\na string for a built in type like “menu”\na function for a custom component like Section (without calling it or using angled brackets)\n\n\n\n\nNot all content needs to go in components. Remember you can modify index.html directly if there is static content that makes more sense there.\n\n\n\nAny files (typically images) stored in the public directory of the root of the project are made publicly available, so anybody can navigate to them.\nIf you want files to be private until used on the website, store them in a folder in src, usually src/assets. Anything in src is not publicly accessible.\n\n\n\n\n\n\nIf we have an input tag, we can manage the user input value as a new state playerName.\nWe use the onChange prop of the input to handle this. We pass a handleChange function to it that takes the event (from the user input) and updates our state based on it.\nfunction handleChange (event) {\n    setPlayerName(event.target.value)\n    }\nWe then pass the value and onChange to the input\n&lt;input value={playerName} onChange={handleChange} /&gt;\nThis technique of passing a value to the input then allowing it to change the value is called a two-way binding.\n\n\n\nWhen your state is an object or array (an array is just a subset of object in JavaScript) then you should create a deep copy of it before altering its value. Objects are passed by reference so you may see unintended side effects otherwise if you try to update the object directly. Use the spread operator to copy to a new variable.\n\n\n\nIf two or more sibling components all need access to the same state, the state should be handled by the closest ancestor component.\n\n\n\nDon’t have two different states in different places which refer to the same state / data. Potential for conflicts and bugs.\nAlso avoid using logic based on state A for the setter logic of state B. For the same reason we use the functional form of the setter when it’s based on previous values; the state may be outdated as it is scheduled to updated but not yet recalculated and rendered.\n\n\n\nManage as few states as possible and pass them as props where needed, then derive any further states from those instead of managing another set of states.\n\n\n\nIf you initialise a state as an object or array, remember that JavaScript passes these by reference.\nSo if you then modify that array, you are modifying the initial array. If you later want to reset the state by setting the state back to that initial array, it won’t work because the original was mutated.\nTo work around this, take a deep copy using the spread operator. If it is a nested array, map for each inner array.\n[…initialArray.map(array =&gt; […array])]\n\n\n\nIf lifting up the state breaks the modularity of the nested components or would trigger the parent function to rerender unnecessarily, avoid lifting.\n\n\n\n\n\nSections 1, 3 and 4 of “React: The Complete Guide” Udemy course"
  },
  {
    "objectID": "posts/software/react/1_getting_started/post.html#what-is-react",
    "href": "posts/software/react/1_getting_started/post.html#what-is-react",
    "title": "React: A Gentle Introduction",
    "section": "",
    "text": "React is a Javascript library for building user interfaces. It is less cumbersome and error-prone than using vanilla JS.\nCode sandbox is an in-browser environment to experiment with UIs. As an example, the same page is implemented in pure Javascript and React. The latter is much easier to follow, modularise and requires less boilerplate.\n\nWith React, you write declarative code: you define the goal, not the steps to get there.\nWith vanilla JS, you write imperative code, defining the steps, not the goal.\n\nA build tool (like Vite or Next.js) is necessary because the Javascript (specifically the JSX) must be transformed. React uses JSX which allows us to “mix” HTML and JS, so that we can define layout and functionality in the same place. This isn’t natively supported by the browser, so a build tool transforms this to pure html and JS."
  },
  {
    "objectID": "posts/software/react/1_getting_started/post.html#key-react-concepts",
    "href": "posts/software/react/1_getting_started/post.html#key-react-concepts",
    "title": "React: A Gentle Introduction",
    "section": "",
    "text": "Key concepts in React are: components, JSX, props, and state.\n\n\nComponents are a core concept. They bundle html, CSS and JS into reusable blocks.\nIn vanilla JS, the JavaScript and HTML are in different files, so it can be hard to follow what needs to be changed where. Related code lives together, which is a key benefit of React and component style coding.\nJSX is a JavaScript syntax extension that allows us to write HTML in JavaScript files. This is not natively supported by browsers, so requires transformation by the build system, such as Vite.\nThe build process (of some but not all build tools) relies on the jsx file extension to indicate a JSX file that needs transformation. The browser does not care, as it never sees (and cannot read) these jsx files directly. Similarly, some build processes require the file extension in the import statements but others don’t.\nComponents must:\n\nStart with an upper case letter - so they do not clash with built-ins like header\nReturn a renderable object\n\nReact creates a component tree for your app. Your components do not end up in the source code directly. The build process traverses the tree until each component is resolved into built ins, and then these appear in the source code.\n\n\n\nUse curly braces to indicate dynamic values in JSX.\nIdeally declare constants rather than having complicated inline expressions.\nImages should be exported then the dynamic value passed as the src of the image. This prevents the image being lost in the build process if the build ignores files with certain extensions.\nimport myImage from './assets/exampleImage.png'\n\n&lt;img src={myImage}/&gt;\nDynamic values can be passed to components as props. React components take a single argument called props, which is an object of key:value pairs passed to the component.\nIf you have an object of props to pass, you can use the spread operator to avoid writing them out individually. Also use object destructuring inside the component to pick out the variables.\n\n\n\nIt is good practice that each component is in its own file. File name should match the component name and be the default export.\nAlso split out style CSS files and keep these alongside the component. CSS files need to be imported by each component file that uses it.\nThe styles are NOT automatically scoped to the component that uses them. They will apply to all components with that name. For example, if you apply header styling to a custom Header component, it will also apply to the built in header html component.\n\n\n\nThe children prop is passed by all components and it is the value between the component tags. It can be used for HTML tag-style syntax.\nWe can react to events by passing a function to onClick or similar. In vanilla JS, we would need to select the element and add an event listener, but react is declarative. We can define the handleClick function inside the component so that it has access to the component’s props and state. We can pass functions as props. This is useful as we can pass state setter functions down to nested components. This should be a pointer to the function, not the executed function itself, e.g. handleClick NOT handleClick()\nIf we want to modify the args that we pass to the function in onClick, use an anonymous arrow function () =&gt; handleSelect(arg) That doesn’t actually get executed until onClick is called.\nWe can set default values of props by putting the default value in the function signature.\n\n\n\nBy default, React components only execute once, even if an internal variable changes. You have to “tell” React to execute something again. This is where state comes in useful. React checks if UI updates are needed by comparing old output with new and applying the difference. So we use states rather than regular variables to indicate that a re-render is required if the state changes. State is essentially a special registered variable that react handles differently. If the state of a component changes, that component and its children in the component tree re-render.\nThe useState function is a “hook”. Hooks must be declared in the top level of a component function, they can’t be nested in internal functions such as event handlers, and they also can’t be declared outside of functions. It returns an array of two elements, the state value and a setter. A default state value can be passed to use state. The setter “schedules” an update, but that isn’t necessarily immediate. So you can see unexpected things when logging a value after the setter in code, where the logged value is still the “old” state value because the UI update has been scheduled but not completed yet.\nIf setting a state value based on its previous value, pass it as a function. For example, if on a button click we want to invert the value of a Boolean, use\nsetIsEditing((editing) =&gt; {!editing})\nNOT\nsetIsEditing(!isEditing)\nThis is because React schedules when to change state, it doesn’t necessarily do it immediately. So you could get unexpected behaviour. But when passed as a function, this triggers a re-render, similarly to how a hook does."
  },
  {
    "objectID": "posts/software/react/1_getting_started/post.html#dynamic-content",
    "href": "posts/software/react/1_getting_started/post.html#dynamic-content",
    "title": "React: A Gentle Introduction",
    "section": "",
    "text": "One option is to use JSX with a ternary expression. It is valid for null to be used in place of a component.\n{ selectedState ? ComponentA : ComponentB }\nAn alternative is the and operator which can also be used for this. In JavaScript, if the first term is truthy then it returns the second term, which is what we want.\n{ selectedState && ComponentA }\nA third option is to save the component as a variable and conditionally reassign it.\n// The default component\nlet tabContent = &lt;p&gt;Please select a topic&lt;/p&gt;\n\nif (selectedTopic) {\n    tabContent = (\n        // The component displayed if a topic is selected\n        &lt;div&gt;\n            &lt;h3&gt;selectedTopic.title&lt;/h3&gt;\n             &lt;p&gt;selectedTopic.description&lt;/p&gt;\n        &lt;/div&gt;\n    )\n}\n\n\n\nWe can set className as a JSX expression and use a ternary expression. For example, check if the button is selected and apply a different className depending on whether it’s selected.\n\n\n\nJSX is capable of outputting lists of renderable components. We do this by using map over the array:\nmyArray.map((item) =&gt; (&lt;Component item={item}/&gt;))\nAdd a key prop to the Component which uniquely identifies the item to avoid warnings raised by React."
  },
  {
    "objectID": "posts/software/react/1_getting_started/post.html#going-deeper",
    "href": "posts/software/react/1_getting_started/post.html#going-deeper",
    "title": "React: A Gentle Introduction",
    "section": "",
    "text": "The following sections are a collection of less essential topics but provide useful background in React.\n\n\nYou don’t NEED JSX, but it makes life easier.\nThe following is JSX, which is easy to read but requires a build transformation process.\n&lt;div id=\"content\"&gt;\n    &lt;p&gt;Hello World!&lt;/p&gt;\n&lt;/div&gt;\nThe alternative in plain JavaScript is to manipulate the DOM directly. It’s not as clear. But it avoids the need for a build process because it IS valid JavaScript supported by the browser.\nReact.createElement(\n    'div',\n    { id: 'content'},\n    React.createElement(\n        'p',\n        null,\n        'Hello World!'\n    )\n)\n\n\n\nA JavaScript function must return one value, it cannot return multiple values. This is true of React components, since they are really just syntactic sugar around JavaScript functions.\nSo if we have two or more sibling components being returned, we must wrap them in a parent component. Naively, we could just use a div, but this adds an unnecessary extra component to our tree.\nAn alternative is to use Fragment. This can be imported from react and used as a parent component without actually creating any new component when built. In newer versions of react, we can skip the import and just use empty opening and closing tags &lt;&gt; &lt;/&gt; to create a Fragment.\n\n\n\nRemember, React will re-render a component and all its child components when a state changes. So if a state is too high up the component tree, it will cause unnecessary re-rendering of many other components.\nThis is an indication that a component needs to be split out and its state managed lower down the tree.\n\n\n\nReact doesn’t auto-forward props to nested components.\nWe can forward an arbitrary number of props without having to write out each manually. Use the rest operator …extraProps in the function signature. Then use the spread operator (same syntax) to pass them to the inner component that you want …extraProps.\nThis is like **kwargs in Python.\n\n\n\nWe’ve seen how we can pass the special children prop to pass arbitrary JSX to our components.\nWhat if we wanted 2 or more slots in our component with arbitrary content? Components are ultimately just JavaScript code, so we can pass the components for the other slot as a prop (and if there are multiple siblings we can wrap them in a fragment).\n\n\n\nWe can pass a prop (with capital letter since it will be used as a custom component) which we can then vary in the nested component. For example, pass a prop called ButtonContainer which can be:\n\na string for a built in type like “menu”\na function for a custom component like Section (without calling it or using angled brackets)\n\n\n\n\nNot all content needs to go in components. Remember you can modify index.html directly if there is static content that makes more sense there.\n\n\n\nAny files (typically images) stored in the public directory of the root of the project are made publicly available, so anybody can navigate to them.\nIf you want files to be private until used on the website, store them in a folder in src, usually src/assets. Anything in src is not publicly accessible."
  },
  {
    "objectID": "posts/software/react/1_getting_started/post.html#more-on-states",
    "href": "posts/software/react/1_getting_started/post.html#more-on-states",
    "title": "React: A Gentle Introduction",
    "section": "",
    "text": "If we have an input tag, we can manage the user input value as a new state playerName.\nWe use the onChange prop of the input to handle this. We pass a handleChange function to it that takes the event (from the user input) and updates our state based on it.\nfunction handleChange (event) {\n    setPlayerName(event.target.value)\n    }\nWe then pass the value and onChange to the input\n&lt;input value={playerName} onChange={handleChange} /&gt;\nThis technique of passing a value to the input then allowing it to change the value is called a two-way binding.\n\n\n\nWhen your state is an object or array (an array is just a subset of object in JavaScript) then you should create a deep copy of it before altering its value. Objects are passed by reference so you may see unintended side effects otherwise if you try to update the object directly. Use the spread operator to copy to a new variable.\n\n\n\nIf two or more sibling components all need access to the same state, the state should be handled by the closest ancestor component.\n\n\n\nDon’t have two different states in different places which refer to the same state / data. Potential for conflicts and bugs.\nAlso avoid using logic based on state A for the setter logic of state B. For the same reason we use the functional form of the setter when it’s based on previous values; the state may be outdated as it is scheduled to updated but not yet recalculated and rendered.\n\n\n\nManage as few states as possible and pass them as props where needed, then derive any further states from those instead of managing another set of states.\n\n\n\nIf you initialise a state as an object or array, remember that JavaScript passes these by reference.\nSo if you then modify that array, you are modifying the initial array. If you later want to reset the state by setting the state back to that initial array, it won’t work because the original was mutated.\nTo work around this, take a deep copy using the spread operator. If it is a nested array, map for each inner array.\n[…initialArray.map(array =&gt; […array])]\n\n\n\nIf lifting up the state breaks the modularity of the nested components or would trigger the parent function to rerender unnecessarily, avoid lifting."
  },
  {
    "objectID": "posts/software/react/1_getting_started/post.html#references",
    "href": "posts/software/react/1_getting_started/post.html#references",
    "title": "React: A Gentle Introduction",
    "section": "",
    "text": "Sections 1, 3 and 4 of “React: The Complete Guide” Udemy course"
  },
  {
    "objectID": "posts/software/react/29_testing/post.html",
    "href": "posts/software/react/29_testing/post.html",
    "title": "React: Testing",
    "section": "",
    "text": "We should run tests at different levels of granularity:\n\nUnit tests\nIntegration tests\nEnd-to-end tests\n\nWe need some build tools to help us run tests.\nJest and React Testing Library are useful for unit and integration tests, which are the main focus of this page. End-to-end testing can be done with tools like Selenium or Cypress.\n\n\nJest is a testing framework that allows us to run JavaScript tests.\nThe test function creates a unit test. It takes a name of the test and an anonymous function which runs the test code.\nThe expect function then defines some behaviour to assert. For pure JavaScript util functions, this is all we need.\nFor React components, we need to test the rendered component with the help of React Testing Library.\n\n\n\nReact testing library is a library that lets us simulate rendered components and assert characteristics of them.\nThe render function is used to simulate the dom to render a component in a test. The screen function is used to get properties from the simulated DOM, eg screen.getByText to assert a particular passage of text appears on the screen.\nThe get functions return an error if the object does not exist. The query functions return null instead. The latter is useful if we want to test when something should NOT be rendered.\nThe getByRole function is useful to pick out specific elements. See available roles here.\nThe userEvent object from React Testing Library simulates user actions like click or hover, so we can test interactive behaviour of components.\n\n\n\n\nThe general pattern for testing is: Arrange, Act, Assert.\n\nArrange: Render the component.\nAct: Any user events or interaction (if applicable for the test).\nAssert: Check the desired output is in the DOM.\n\n\n\n\nWe may want to group tests for related features/components together for readability.\nUse the describe function to define a test suite.\nIt takes a test suite description string and anonymous function as argumens. Then each of the tests are defined within it.\n\n\n\nThe get and query functions attempt to retrieve elements from the DOM immediately, as soon as the component is rendered. This is not the behaviour we want for components with asynchronous elements which may take time to fetch data.\nThe findByRole function, and related “find” functions, return a promise which will wait and re-attempt to find the element before failing. This allows us to test async code. You can pass optional args to the find functions to set how long they should wait, when to retry etc.\nWe generally want to avoid sending external requests as part of unit tests, so this is more relevant to integration or end-to-end tests.\nFor unit tests, we can mock the results.\n\n\n\nWe want to mock out external calls like fetch when we run them is our unit tests.\nWe can overwrite the window.fetch method:\nwindow.fetch = jest.fn();\nwindow.fetch.mockResolvedValueOnce({\n    json: async() =&gt; {\n        // The mock values\n        [{id: 1, text: 'example text goes here'}] \n    }\n});\n\n\n\n\nSection 29 of “React: The Complete Guide” Udemy course\nJest\nReact Testing Library\nReact Testing Library with custom hooks"
  },
  {
    "objectID": "posts/software/react/29_testing/post.html#what-to-test-and-how",
    "href": "posts/software/react/29_testing/post.html#what-to-test-and-how",
    "title": "React: Testing",
    "section": "",
    "text": "We should run tests at different levels of granularity:\n\nUnit tests\nIntegration tests\nEnd-to-end tests\n\nWe need some build tools to help us run tests.\nJest and React Testing Library are useful for unit and integration tests, which are the main focus of this page. End-to-end testing can be done with tools like Selenium or Cypress.\n\n\nJest is a testing framework that allows us to run JavaScript tests.\nThe test function creates a unit test. It takes a name of the test and an anonymous function which runs the test code.\nThe expect function then defines some behaviour to assert. For pure JavaScript util functions, this is all we need.\nFor React components, we need to test the rendered component with the help of React Testing Library.\n\n\n\nReact testing library is a library that lets us simulate rendered components and assert characteristics of them.\nThe render function is used to simulate the dom to render a component in a test. The screen function is used to get properties from the simulated DOM, eg screen.getByText to assert a particular passage of text appears on the screen.\nThe get functions return an error if the object does not exist. The query functions return null instead. The latter is useful if we want to test when something should NOT be rendered.\nThe getByRole function is useful to pick out specific elements. See available roles here.\nThe userEvent object from React Testing Library simulates user actions like click or hover, so we can test interactive behaviour of components."
  },
  {
    "objectID": "posts/software/react/29_testing/post.html#the-3-as-of-testing",
    "href": "posts/software/react/29_testing/post.html#the-3-as-of-testing",
    "title": "React: Testing",
    "section": "",
    "text": "The general pattern for testing is: Arrange, Act, Assert.\n\nArrange: Render the component.\nAct: Any user events or interaction (if applicable for the test).\nAssert: Check the desired output is in the DOM."
  },
  {
    "objectID": "posts/software/react/29_testing/post.html#test-suites",
    "href": "posts/software/react/29_testing/post.html#test-suites",
    "title": "React: Testing",
    "section": "",
    "text": "We may want to group tests for related features/components together for readability.\nUse the describe function to define a test suite.\nIt takes a test suite description string and anonymous function as argumens. Then each of the tests are defined within it."
  },
  {
    "objectID": "posts/software/react/29_testing/post.html#testing-asynchronous-code",
    "href": "posts/software/react/29_testing/post.html#testing-asynchronous-code",
    "title": "React: Testing",
    "section": "",
    "text": "The get and query functions attempt to retrieve elements from the DOM immediately, as soon as the component is rendered. This is not the behaviour we want for components with asynchronous elements which may take time to fetch data.\nThe findByRole function, and related “find” functions, return a promise which will wait and re-attempt to find the element before failing. This allows us to test async code. You can pass optional args to the find functions to set how long they should wait, when to retry etc.\nWe generally want to avoid sending external requests as part of unit tests, so this is more relevant to integration or end-to-end tests.\nFor unit tests, we can mock the results."
  },
  {
    "objectID": "posts/software/react/29_testing/post.html#using-mocks",
    "href": "posts/software/react/29_testing/post.html#using-mocks",
    "title": "React: Testing",
    "section": "",
    "text": "We want to mock out external calls like fetch when we run them is our unit tests.\nWe can overwrite the window.fetch method:\nwindow.fetch = jest.fn();\nwindow.fetch.mockResolvedValueOnce({\n    json: async() =&gt; {\n        // The mock values\n        [{id: 1, text: 'example text goes here'}] \n    }\n});"
  },
  {
    "objectID": "posts/software/react/29_testing/post.html#references",
    "href": "posts/software/react/29_testing/post.html#references",
    "title": "React: Testing",
    "section": "",
    "text": "Section 29 of “React: The Complete Guide” Udemy course\nJest\nReact Testing Library\nReact Testing Library with custom hooks"
  },
  {
    "objectID": "posts/software/react/7_debugging/post.html",
    "href": "posts/software/react/7_debugging/post.html",
    "title": "React: Debugging",
    "section": "",
    "text": "We’ve done stylin’, now time for profilin’. Woo!\n\n\nDebug react apps using the browser console.\nThis gives the stack trace that raised the error, and the file and line number on which it was raised.\n\n\n\nUse the Sources tab (in chrome) to use the debugger.\nYou’ll see the directory structure of your project. You can click a line number to set a breakpoint and pause execution there, then observe variable values or step through execution.\nYou can also achieve the same by placing a debugger() line in the code.\n\n\n\nStrictMode is a React component which you can wrap any other component in, including the root App component.\nimport { StrictMode } from 'react';\nIt causes React to render every component twice in development mode. This can help surface errors that may not be obvious in normal execution.\n\n\n\nThis is a browser extension. It adds 2 new tabs to the console view in the browser window: profiler and components.\nComponents shows your component tree and highlights these in the browser. It also gives information about that component such as props. You can edit those props values in the browser. You can also see hooks and their state values.\n\n\n\n\nSection 7 of “React: The Complete Guide” Udemy course"
  },
  {
    "objectID": "posts/software/react/7_debugging/post.html#browser-console",
    "href": "posts/software/react/7_debugging/post.html#browser-console",
    "title": "React: Debugging",
    "section": "",
    "text": "Debug react apps using the browser console.\nThis gives the stack trace that raised the error, and the file and line number on which it was raised."
  },
  {
    "objectID": "posts/software/react/7_debugging/post.html#debugger",
    "href": "posts/software/react/7_debugging/post.html#debugger",
    "title": "React: Debugging",
    "section": "",
    "text": "Use the Sources tab (in chrome) to use the debugger.\nYou’ll see the directory structure of your project. You can click a line number to set a breakpoint and pause execution there, then observe variable values or step through execution.\nYou can also achieve the same by placing a debugger() line in the code."
  },
  {
    "objectID": "posts/software/react/7_debugging/post.html#strict-mode",
    "href": "posts/software/react/7_debugging/post.html#strict-mode",
    "title": "React: Debugging",
    "section": "",
    "text": "StrictMode is a React component which you can wrap any other component in, including the root App component.\nimport { StrictMode } from 'react';\nIt causes React to render every component twice in development mode. This can help surface errors that may not be obvious in normal execution."
  },
  {
    "objectID": "posts/software/react/7_debugging/post.html#react-developer-tools",
    "href": "posts/software/react/7_debugging/post.html#react-developer-tools",
    "title": "React: Debugging",
    "section": "",
    "text": "This is a browser extension. It adds 2 new tabs to the console view in the browser window: profiler and components.\nComponents shows your component tree and highlights these in the browser. It also gives information about that component such as props. You can edit those props values in the browser. You can also see hooks and their state values."
  },
  {
    "objectID": "posts/software/react/7_debugging/post.html#references",
    "href": "posts/software/react/7_debugging/post.html#references",
    "title": "React: Debugging",
    "section": "",
    "text": "Section 7 of “React: The Complete Guide” Udemy course"
  },
  {
    "objectID": "posts/software/data_structures_algos/sorting/sorting.html",
    "href": "posts/software/data_structures_algos/sorting/sorting.html",
    "title": "Sorting Algorithms",
    "section": "",
    "text": "Sorting algorithms come up so frequently that they deserve their own section.\nIn general, we want to solve the problem:\n\nGiven an array of unsorted values, how can we sort them in ascending order?\n\n\n\n\nAlgorithm\nBest Case\nAverage Case\nWorst Case\n\n\n\n\nBubble sort\n\\(N^2\\)\n\\(N^2\\)\n\\(N^2\\)\n\n\nSelection sort\n\\(\\frac{N^2}{2}\\)\n\\(\\frac{N^2}{2}\\)\n\\(\\frac{N^2}{2}\\)\n\n\nInsertion sort\n\\(N\\)\n\\(\\frac{N^2}{2}\\)\n\\(N^2\\)\n\n\nQuicksort\n\\(N log N\\)\n\\(N log N\\)\n\\(N^2\\)\n\n\nMerge sort\n\\(N log N\\)\n\\(N log N\\)\n\\(N log N\\)\n\n\nHeap sort\n\\(N log N\\)\n\\(N log N\\)\n\\(N log N\\)\n\n\nCounting sort\n\\(N + K\\)\n\\(N + K\\)\n\\(N + K\\)\n\n\nRadix sort\n\\(N * D\\)\n\\(N * D\\)\n\\(N * D\\)\n\n\n\nSorting is often a pre-requisite for other algorithms, so often as a pre-processing step we want to presort the array. This means the efficiency of the sort is important to the overall efficiency of many other problem types.\n\n\n\n\nWe “bubble up” the next highest unsorted number on each pass-through.\n\nStart with 2 pointers pointing at the first two values in the array.\nCompare the left and right values. If left_value &gt; right_value, swap them. Otherwise, do nothing.\nMove both pointers one cell rightwards.\nRepeat Steps 2-3 until we reach values that have already been sorted. This completes the first “pass-through” and means we have “bubbled up” the biggest number to the end of the array.\nStart over. repeat Steps 1-4 to bubble up the second biggest number into the second to last position. Repeat this until we perform a pass through with no swaps.\n\n\n\n\n\ndef bubble_sort(array):\n    \"\"\"Bubble sort algorithm to sort an array into ascending order.\n\n    Note we ignore edge cases for the sake of clarity.\n    These are left as an exercise for the reader:\n    - array is empty\n    - array has only 1 element\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n\n    Returns\n    -------\n    array: list\n        The input array sorted in-place.\n    \"\"\"\n    # Initially the entire array is unsorted\n    last_unsorted_index = len(array) - 1\n    is_sorted = False\n\n    while not is_sorted:\n        # Set this to True before we pass through the elements, \n        # then if we need to perform a swap the array is not sorted so we set it to False\n        is_sorted = True\n\n        # Perform a pass-through\n        for left_pointer in range(last_unsorted_index):\n            right_pointer = left_pointer + 1\n\n            if array[left_pointer] &gt; array[right_pointer]:\n                # Swap the values\n                array[left_pointer], array[right_pointer] = array[right_pointer], array[left_pointer]\n                is_sorted = False\n\n        # The pass-through is finished so the next highest value has been \"bubbled up\".        \n        last_unsorted_index -= 1\n\n    return array\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\nbubble_sort(input_array)\n\n[0, 1, 3, 5, 7, 8, 8, 9, 12, 23, 69, 420]\n\n\n\n\n\nEach pass through loops through one fewer element:\n\n\n\nPass-through number\nNumber of operations\n\n\n\n\n1\nN-1\n\n\n2\nN-2\n\n\n3\nN-3\n\n\n4\nN-4\n\n\n5\nN-5\n\n\n…\n…\n\n\nk\nN-k\n\n\n\nSo in total, there are \\((N-1) + (N-2) + (N-3) + ... + 1\\) comparisons. This is the sum of an arithmetic progression, which we can calculate as \\(\\frac{N^2}{2}\\).\nAlso worth noting that in the worst case - an input array in descending order - each comparison will also result in a swap. This does not affect the Big-O complexity but would slow down the run time in practice. There are \\(\\frac{N^2}{2}\\) comparisons and up to \\(\\frac{N^2}{2}\\) swaps, resulting in \\(O(N^2)\\) total operations.\nThe complexity is therefore \\(O(N^2)\\).\nIn general, any nested loop should be a hint at quadratic time complexity.\n\n\n\n\n\n\nFind the smallest value from the unsorted part of the array and put it at the beginning.\n\nCheck each cell of the array from left to right to find the lowest value. Store the index of the running minimum value.\nAt the end of pass-through \\(j\\) (starting at 0), swap the minimum value with the one at index \\(j\\).\nRepeat steps 1-2 until we reach a pass-through that would start at the end of the array, i.e. \\(j = N-1\\)\n\n\n\n\n\ndef selection_sort(array):\n    \"\"\"Selection sort algorithm to sort an array into ascending order.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n\n    Returns\n    -------\n    array: list\n        The input array sorted in-place.\n    \"\"\"\n    array_length = len(array)\n\n    # Loop through all array elements\n    for pass_through_number in range(array_length):\n\n        # Find the minimum element in the remaining unsorted array\n        min_index = pass_through_number\n        for idx_unsorted_section in range(pass_through_number + 1, array_length):\n            if array[idx_unsorted_section] &lt; array[min_index]:\n                min_index = idx_unsorted_section\n\n        # Swap the found minimum element with the first element\n        array[pass_through_number], array[min_index] = array[min_index], array[pass_through_number]\n    \n    return array\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\nselection_sort(input_array)\n\n[0, 1, 3, 5, 7, 8, 8, 9, 12, 23, 69, 420]\n\n\n\n\n\nAs with bubble sort, on each pass-through we loop through one fewer element, so there are \\(\\frac{N^2}{2}\\) comparisons. But each pass-through only performs 1 swap, so the total number of operations is \\(\\frac{N^2}{2} + N\\).\nThis is still therefore \\(O(N^2)\\) complexity, but it should be about twice as fast as bubble sort.\n\n\n\n\n\n\nRemove a value to create a gap, shift values along to move the gap leftwards, then fill the gap.\n\nCreate a gap. For the first pass-through, we temporarily remove the second cell (i.e. index 1) and store it as a temporary variable. This leaves a gap at that index.\nShifting phase. Take each value to the left of the gap and compare it to the temporary variable. if left_val &gt; temp_val, move left value to the right. This has the same effect as moving the gap leftwards. As soon as we encounter a value where left_val &lt; temp_val the shifting phase is complete.\nFill the gap. Insert the temporary value into the current gap.\nRepeat pass-throughs. Steps 1-3 constitute a single pass-through. Repeat this until the pass through begins at the final index of the array. At this point the array is sorted.\n\n\n\n\n\ndef insertion_sort(array):\n    \"\"\"Insertion sort algorithm to sort an array into ascending order.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n\n    Returns\n    -------\n    array: list\n        The input array sorted in-place.\n    \"\"\"\n    array_len = len(array)\n\n    for pass_thru_number in range(1, array_len):\n        # 1. Create a gap\n        temp_val = array[pass_thru_number]\n        gap_idx = pass_thru_number\n\n        # 2. Shifting phase\n        # Move leftwards from the gap and keep shifting elements right if they are greater than temp_val\n        while (gap_idx &gt; 0) and (array[gap_idx - 1] &gt; temp_val):\n            array[gap_idx] = array[gap_idx - 1]\n            gap_idx -= 1\n\n        # 3. Fill the gap\n        array[gap_idx] = temp_val\n\n    return array\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\ninsertion_sort(input_array)\n\n[0, 1, 3, 5, 7, 8, 8, 9, 12, 23, 69, 420]\n\n\n\n\n\nThe worst case is when the input array is sorted in descending order.\nThere are 4 types of operations that occur:\n\n\nIf we compare values to the left of the gap on each step, there will be 1 comparison on the first pass-through, 2 on the second, 3 on the third, etc.\nSo there are \\(1 + 2 + ... + N-1 = \\frac{N^2}{2}\\) comparisons in the worst case.\n\n\n\nEach comparison could result in a shift, so there are the same number of shifts as comparisons in the worst case.\n\n\n\nWe remove 1 temp value per pass-through, so there are \\(N-1\\) removals.\nWe insert that value at the end of each pass-through, so there are also \\(N-1\\) insertions.\n\n\n\n\n\n\nOperation\nNumber\n\n\n\n\nRemovals\n\\(N-1\\)\n\n\nComparisons\n\\(\\frac{N^2}{2}\\)\n\n\nShifts\n\\(\\frac{N^2}{2}\\)\n\n\nInsertions\n\\(N-1\\)\n\n\n\nOverall there are \\(N^2 + 2N - 2\\) operations, so the complexity is \\(O(N^2)\\).\n\n\n\n\nThe complexity generally considers the worst case, but insertion sort varies greatly based on the input.\nWe saw in the worst case the number of steps is \\(N^2 + 2N - 2\\).\nIn the best case the data is already sorted, so we do a remove and an insert on each pass-through, for a totla of \\(2N - 2\\) steps.\nIn the average case, let’s assume about half of the data is already sorted. So we’ll still need to do \\(N - 1\\) removes and \\(N - 1\\) inserts in total. We’ll also need to compare about half the data, so \\(\\frac{N^2}{4}\\) comparisons and the same number on swaps. This gives a total of \\(\\frac{N^2}{2} + 2N - 2\\) steps.\nDepending on the state of the input data, the speed can vary considerably. Compare this to selection sort, which will always take \\(\\frac{N^2}{2}\\) steps regardless of the input data.\n\n\n\nAlgorithm\nBest Case\nAverage Case\nWorst Case\n\n\n\n\nSelection sort\n\\(\\frac{N^2}{2}\\)\n\\(\\frac{N^2}{2}\\)\n\\(\\frac{N^2}{2}\\)\n\n\nInsertion sort\n\\(N\\)\n\\(\\frac{N^2}{2}\\)\n\\(N^2\\)\n\n\n\nSo the choice of which algorithm is “best” depends on the state of your input data. Both have the same \\(O(N^2)\\) time complexity.\n\n\n\n\nQuicksort is a sorting algorithm that relies on the concept of partitioning.\n\n\nThe idea behind partitioning is we take a random value from the array which we call the pivot.\nWe then want to ensure any smaller numbers are left of the pivot and any larger numbers to its right.\n\nIf we take the rightmost value as the pivot, we create a left pointer pointing at the leftmost value and a right pointer at the second from the right (i.e. not the pivot but the next rightmost).\nThe left pointer moves rightwards until it reaches a value &gt;= the pivot value.\nThe right pointer moves leftwards until it reaches a value &lt;= the pivot value.\nIf at this pointer the left pointer has crossed past the right pointer, move straight on to the next step. Otherwise swap the values that the left and right pointers are pointing at.\nSwap the pivot value with the left pointer’s value.\n\n\ndef partition(array):\n    \"\"\"Partition an array around the rightmost value.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to partition.\n\n    Returns\n    -------\n    array: list\n        The input array partitioned in-place.\n    \"\"\"\n    # 1. Initialise the pivot and pointers\n    pivot_idx = len(array) - 1\n    left_pointer_idx = 0\n    right_pointer_idx = pivot_idx - 1\n\n    # Loop until the left and right pointers cross \n    while left_pointer_idx &lt; right_pointer_idx:\n        # 2. Move the left pointer\n        while (left_pointer_idx &lt; pivot_idx) and (array[left_pointer_idx] &lt; array[pivot_idx]):\n            left_pointer_idx += 1\n\n        # 3. Move the right pointer\n        while (right_pointer_idx &gt; 0) and (array[right_pointer_idx] &gt; array[pivot_idx]):\n            right_pointer_idx -= 1\n\n        # 4. Swap values if the pointers haven't crossed yet, otherwise end the loop\n        if left_pointer_idx &lt; right_pointer_idx:\n            array[left_pointer_idx], array[right_pointer_idx] = array[right_pointer_idx], array[left_pointer_idx]\n\n    # 5. Swap the pivot and left_pointer values\n    array[left_pointer_idx], array[pivot_idx] = array[pivot_idx], array[left_pointer_idx]\n\n    return array\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\npartition(input_array)\n\n([8, 7, 5, 3, 1, 9, 0, 8, 12, 69, 420, 23], 8)\n\n\nPartitioning isn’t the same as sorting the array, but it’s “sorted-ish”.\n\n\n\nQuicksort is a combination of partitions and recursion.\n\nPartition the array. The pivot is now at the correct position in the sorted array.\nConsider the subarrays to the left and the right of the pivot as their own arrays. We’ll partition these recursively.\nIf a subarray has 0 or 1 elements, that is the base case and we do nothing; it is already sorted.\n\nWe’ll make a few tweaks to the partition function to take a start index and an end index, and to also return the pivot index. These extra parameters will allow us to call it recursively in quicksort.\n\n\n\n\ndef partition(array, start_idx, end_idx):\n    \"\"\"Partition an array around the rightmost value.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to partition.\n    start_idx: int\n        The start index of the array to consider.\n    end_index: int\n        The end index of the array to consider.\n\n    Returns\n    -------\n    array: list\n        The input array partitioned in-place.\n    final_pivot_idx: int\n        The index position of the pivot point in the partitioned array.\n    \"\"\"\n    # 1. Initialise the pivot and pointers\n    pivot_idx = end_idx\n    left_pointer_idx = start_idx\n    right_pointer_idx = pivot_idx - 1\n\n    # Loop until the left and right pointers cross \n    while left_pointer_idx &lt; right_pointer_idx:\n        # 2. Move the left pointer\n        while (left_pointer_idx &lt; pivot_idx) and (array[left_pointer_idx] &lt; array[pivot_idx]):\n            left_pointer_idx += 1\n\n        # 3. Move the right pointer\n        while (right_pointer_idx &gt; 0) and (array[right_pointer_idx] &gt; array[pivot_idx]):\n            right_pointer_idx -= 1\n\n        # 4. Swap values if the pointers haven't crossed yet, otherwise end the loop\n        if left_pointer_idx &lt; right_pointer_idx:\n            array[left_pointer_idx], array[right_pointer_idx] = array[right_pointer_idx], array[left_pointer_idx]\n\n    # 5. Swap the pivot and left_pointer values\n    array[left_pointer_idx], array[pivot_idx] = array[pivot_idx], array[left_pointer_idx]\n    final_pivot_idx = left_pointer_idx\n\n    return array, final_pivot_idx\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\npartition(input_array, 0, 11)\n\n([8, 7, 5, 3, 1, 9, 0, 8, 12, 69, 420, 23], 8)\n\n\n\ndef quicksort(array, start_idx=0, end_idx=None):\n    \"\"\"Quicksort algorithm to sort an array into ascending order.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n    start_idx: int\n        The start index of the array to consider.\n    end_index: int\n        The end index of the array to consider.\n\n    Returns\n    -------\n    array: list\n        The input array sorted in-place.\n    \"\"\"\n    end_idx = end_idx or len(array) - 1\n\n    # 3. Base case - an array of 0 or 1 elements is already sorted\n    if len(array[start_idx: end_idx]) &lt;= 1:\n        return array\n    \n    # 1. Partition the array\n    array, pivot_idx = partition(array, start_idx, end_idx)\n\n    # 2. Recursively partition the left and right subarrays\n    quicksort(array, start_idx=start_idx, end_idx=pivot_idx - 1)\n    quicksort(array, start_idx=pivot_idx + 1, end_idx=end_idx)\n\n    return array\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\nquicksort(input_array)\n\n[0, 1, 3, 5, 7, 8, 9, 8, 12, 23, 69, 420]\n\n\n\n\n\n\n\nPartitioning involves two steps:\n\n\n\nOperation\nNumber\n\n\n\n\nSwaps\n\\(N\\)\n\n\nComparisons\n\\(\\frac{N^2}{2}\\)\n\n\n\nEach element is compared to the pivot point at least N times, because the left and right pointers move until they reach each other. So there are \\(N\\) comparisons.\nEach swap handles two values, so in the worst case if we swapped every value there would be \\(\\frac{N}{2}\\) swaps. On average, there’d be about half that, so \\(\\frac{N}{4}\\).\nEither way, this means a single partition operation is \\(O(N)\\).\n\n\n\nQuicksort performs multiple partitions. How many?\nIn the best case, each partition would place the pivot directly in the middle of the subarray, spltting them clean in half each time. So there would be \\(log_2 N\\) splits.\nIn the average case this is “close enough”. Not every level will split equally so there’ll be a few extra but it will still be broadly symmetric, therefore a logarithmic function of \\(N\\).\nIn the worst case, every partition ends up on the left side, so with each partition we are effectively placing each element one-by-one from the left. This means we will partition \\(N\\) times, and each has \\(O(N)\\) complexity, resulting in a total compleixty of \\(O(N^2)\\).\n\n\n\nAlgorithm\nBest Case\nAverage Case\nWorst Case\n\n\n\n\nQuicksort\n\\(N log N\\)\n\\(N log N\\)\n\\(N^2\\)\n\n\n\n\n\n\n\nQuickselect is an algorithm with a similar approach. It’s a hybrid of Quicksort and a binary search.\n\nGiven an unsorted array, find the fifth-highest value in the array.\n\nOne approach would be to sort the array then pick the fifth element from the right. But sorting is \\(O(N log N)\\) on average for the better algorithms. We can do better without having to sort the whole array.\nRecall that after a partition, the pivot ends up in the correct place in the array. This is crucial. If we want to find the fifth highest value, we want to find the value that ends up in the fifth position from the right. So we can:\n\nPerform a partition and see where our pivot ends up.\nIf our pivot is too far left, partition the right subarray. If the pivot is too dar right, partition the left subarray.\nKeep going until we find the pivot which ends up in our target spot.\n\n\ndef quickselect(array, target_position, start_idx=0, end_idx=None):\n    \"\"\"Quickselect algorithm to find the target position (k-th lowest) element in an unsorted array.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n    target_position: int\n        The kth lowest value that we want to find.\n    start_idx: int\n        The start index of the array to consider.\n    end_index: int\n        The end index of the array to consider.\n\n    Returns\n    -------\n    array: list\n        The input array sorted in-place.\n    \"\"\"\n    end_idx = end_idx or len(array) - 1\n\n    # Base case: If the start index is greater than the end index, or if the target position is out of range\n    if (start_idx &gt; end_idx) or (target_position &lt; 0) or (target_position &gt; end_idx):\n        return None\n\n    # 1. Partition the array\n    array, pivot_idx = partition(array, start_idx, end_idx)\n\n    # 2. Recursively process the subarrays\n    if pivot_idx &lt; target_position:\n        # The pivot is too small, so check the subarray to the right\n        return quickselect(array, target_position, start_idx=pivot_idx + 1, end_idx=end_idx)\n    elif pivot_idx &gt; target_position:\n        # The pivot is too big, so check the subarray to the left\n        return quickselect(array, target_position, start_idx=start_idx, end_idx=pivot_idx - 1)\n    else:\n        # The pivot is at the target position - we have found our target!\n        return array[pivot_idx]\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\nquickselect(input_array, 5)\n\n8\n\n\n\n\n\nIn the average case, we are splitting the subarrays (roughly) in half each time.\nEach split is operating on a subarray of half the size, so in total there are \\(N + \\frac{N}{2} +  \\frac{N}{4} + \\frac{N}{8} + ... + 2 = 2N\\) steps.\nThus, overall the complexity is \\(O(N)\\) in the average case.\n\n\n\n\nMerge sort is like organizing a messy pile of papers. You divide the pile into smaller piles, sort each smaller pile, and then merge them back together in the correct order.\n\n\n\nDivide. Recursively split the array into two halves until each subarray contains only one element (and is therefore sorted).\nSort. Recursively mergesort each half.\nMerge. Recursively merge the sorted subarrays back together in the correct order.\n\n\n\n\n\ndef merge_sort(array):\n    \"\"\"Mergesort sort algorithm to sort an array into ascending order.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n\n    Returns\n    -------\n    array: list\n        The input array sorted in-place.\n    \"\"\"\n    # Base case - the array has 0 or 1 elements so is already sorted\n    if len(array) &lt;= 1:\n        return array\n    \n    # 1. Split the array\n    idx_mid = len(array) // 2\n    left_half = array[:idx_mid]\n    right_half = array[idx_mid:]\n    \n    # 2. Sort each half recursively\n    left_half = merge_sort(left_half)\n    right_half = merge_sort(right_half)\n    \n    # 3. Merge the subarrays\n    return merge(left_half, right_half)\n\ndef merge(left_array, right_array):\n    \"\"\"Merge two subarrays together which are each sorted.\n    \n    Parameters\n    ----------\n    left_array: list\n        The first subarray to merge.\n    right_array: list\n        The second subarray to merge.\n\n    Returns\n    -------\n    result: list\n        The merged, sorted array.\n    \"\"\"\n    # Initialise pointers at the start of each subarray\n    result = []\n    left_idx = 0\n    right_idx = 0\n    \n    # Loop until we reach the end of one of the subarrays\n    while left_idx &lt; len(left_array) and right_idx &lt; len(right_array):\n        # Compare the values of the left and right array, then insert the smaller of the two into the result\n        if left_array[left_idx] &lt; right_array[right_idx]:\n            # The left value is smaller so insert it into the result and increment the pointer\n            result.append(left_array[left_idx])\n            left_idx += 1\n        else:\n            # The right value is smaller so insert it into the result and increment the pointer\n            result.append(right_array[right_idx])\n            right_idx += 1\n    \n    # The array that reaches the end first will be empty, but there will still be elements in the other.\n    # So insert any remaining elements at the end of the result\n    result.extend(left_array[left_idx:])\n    result.extend(right_array[right_idx:])\n    \n    return result\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\nmerge_sort(input_array)\n\n[0, 1, 3, 5, 7, 8, 8, 9, 12, 23, 69, 420]\n\n\n\n\n\nThe time complexity of merge sort is the same for the best case, average case, and worst case.\nIn every case, each divide step splits the array in half, meaning there are \\(log_2(N)\\) splits.\nFor the merge step, the elements subarrays are looped through one-by-one, so all \\(N\\) elements are touched.\nTherefore, the overall complexity is \\(O(NlogN)\\).\nThe space complexity is O(n).\nThis is because merge sort requires additional space to store temporary arrays during the merging process. In the worst case, when merging two halves of the array, an additional array of size equal to the original array is needed to store the merged result temporarily.\n\n\n\n\n\n\nA max-heap is “weakly sorted”; the maximum value is the root of the heap, and nodes are greater than all of their descendants.\nWe can use this property to sort data elements by creating a heap from the data, then popping the maximum value one-by-one to populate an array in order.\n\nBuild a heap: Create a max-heap, which ensures the maximum value is at the root of the heap.\nPop the root: Remove the root and place it at the end of the result array.\nHeapify: Rearrange the remaining blocks to form a new heap.\nRepeat: Iterate through the entire heap until all elements are sorted.\n\n\n\n\n\ndef heapify(array, heap_size, current_idx):\n    \"\"\"Heapify function to maintain the max-heap property.\n    \n    Parameters\n    ----------\n    array: list\n        List of elements\n    heap_size: int\n        Size of heap\n    current_idx: int\n        Index of current node\n\n    Returns\n    -------\n    None\n        The input heap `array` is heapified in-place.\n    \"\"\"\n    # Track the largest node, initially assumed to be the root\n    largest = current_idx  \n    left_child = 2 * current_idx + 1\n    right_child = 2 * current_idx + 2\n\n    # Check if the node's children, if it has any, are larger than the current node\n    if left_child &lt; heap_size and array[left_child] &gt; array[largest]:\n        largest = left_child\n    if right_child &lt; heap_size and array[right_child] &gt; array[largest]:\n        largest = right_child\n\n\n    if largest != current_idx:\n        # If largest is not root, swap it with root to maintain the heap condition\n        array[current_idx], array[largest] = array[largest], array[current_idx]\n        # Recursively heapify the affected sub-tree\n        heapify(array, heap_size, largest)\n\n\ndef heap_sort(array):\n    \"\"\"Heap sort algorithm to sort an array into ascending order.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n\n    Returns\n    -------\n    array: list\n        The sorted array sorted in-place.\n    \"\"\"\n    # 1. Build a max heap.\n    # Start from the last non-leaf node, and heapify each node\n    array_len = len(array)\n    idx_last_non_leaf = array_len // 2 - 1\n    idx_last_node = array_len - 1\n\n    for i in range(idx_last_non_leaf, -1, -1):\n        heapify(array, array_len, i)\n\n    # 2. Extract elements one-by-one starting from the end\n    for i in range(idx_last_node, 0, -1):\n        array[i], array[0] = array[0], array[i]\n        # 3. Maintain the heap condition for the unsorted portion of the array\n        heapify(array, i, 0)\n\n    return array\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\nheap_sort(input_array)\n\n[0, 1, 3, 5, 7, 8, 8, 9, 12, 23, 69, 420]\n\n\n\n\n\nThe pre-processing step is to build the heap. This operation takes O(n) time.\nOnce the heap is created, we perform the following operations:\n\nPop the max value: This is accessible as it is the root, so \\(O(1)\\) time.\nInsert at the end of the result array: This is also \\(O(1)\\) time.\nHeapifying: This takes \\(O(log N)\\) time.\n\nThese are performed for each of the \\(N\\) data elements, so the overall time complexity of heap sort is \\(O(N log N)\\).\nThis is the same regardless of the arrangement of the input, so the time complexity of heap sort is the same in the best case, average case, and worst case.\n\n\n\n\nWe count how many of each element is in the array, i.e. how many 0s, how many 1s, how many 2s.\nFrom this we can then determine what the starting position of each element in the output will be. E.g. if there are three 0s then these will take the first three slots in the sorted output, so 1s will start at the fourth position.\nCounting sort works best when the number of unique items is small, i.e. there are lots of duplicates in the list.\nIt is a stable sorting algorithm, meaning items with identical values in the input will retain their original ordering in the output.\n\n\n\nSay we want to sort the following array:\n[0, 1, 2, 0, 0, 1, 2, 1, 0, 0, 2, 1, 2]\n\nCounting: Count the number of occurrences of each unique element in the input list. How many 0s? How many 1s? How many 2s? Etc.\nCumulative count: Then calculate the cumulative count of the elements. This step determines the starting position of each element in the sorted output.\nPlacement: Finally, you place each element in its correct position in the sorted output based on its cumulative count.\n\n\n\n\n\ndef counting_sort(array):\n    \"\"\"Counting sort algorithm to sort an array into ascending order.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n\n    Returns\n    -------\n    sorted_array: list\n        The sorted array.\n    \"\"\"\n    # 1. Count the elements in the input. \n    max_val = max(array)\n    count_store = [0] * (max_val + 1)\n\n    for num in array:\n        count_store[num] += 1\n    \n    # 2. Cumulative count\n    for i in range(1, len(count_store)):\n        count_store[i] += count_store[i - 1]\n        \n    # 3. Place each element in its correct position in the sorted array\n    sorted_array = [0] * len(array)\n\n    # for num in reversed(array):\n    #     sorted_array[count_store[num] - 1] = num\n    #     count_store[num] -= 1\n\n    for num in array:\n        sorted_array[count_store[num] - 1] = num\n        count_store[num] -= 1\n    \n    return sorted_array\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\ncounting_sort(input_array)\n\n[0, 1, 3, 5, 7, 8, 8, 9, 12, 23, 69, 420]\n\n\n\n\n\nThe complexity of counting sort depends on the number of elements, \\(N\\), and the number of unique elements, \\(K\\).\nFor each each of the steps:\n\nCounting: We loop through all \\(N\\) elements. We also create a count_store array which takes \\(K\\) elements of auxiliary space.\nCumulative count: We loop through the \\(K\\) elements in the count_store array.\nPlacement: We initialise an output_array of size \\(N\\), which therefore takes \\(N\\) elements of auxiliary space. We then loop through the \\(N\\) elements in the original array, using count_store to determine the correct placement.\n\nSo there are \\(N + K + N\\) steps and \\(N + K\\) elements of auxiliary data.\nTime complexity: \\(O(N + K)\\)\nAuxiliary space: \\(O(N + K)\\)\n\n\n\n\nRadix sort is a sorting algorithm that sorts numbers by processing individual digits.\nIt sorts numbers by first grouping the individual digits of the same place value together and sorting them. It starts sorting from the least significant digit (rightmost digit) to the most significant digit (leftmost digit).\n\n\n\nStart from the rightmost digit: Look at the least significant digit of each number. Group the numbers based on this digit.\nSort each group: After grouping, the numbers are rearranged based on their digit value. So, all the numbers with the same rightmost digit are now together, and they are sorted within this group.\nMove to the next digit: Now, look at the second rightmost digit and repeat the process. Group the numbers based on this digit and sort each group.\nContinue until all digits are considered: Keep repeating this process until you’ve looked at all digits, moving towards the leftmost digit.\n\nBy the end of this process, the numbers will be sorted because each time you look at a digit, the numbers are sorted according to that digit.\n\n\n\n\ndef counting_sort(arr, exp):\n    n = len(arr)\n    output = [0] * n\n    count = [0] * 10\n\n    # Count occurrences of digits\n    for i in range(n):\n        index = arr[i] // exp\n        count[index % 10] += 1\n\n    # Cumulative count\n    for i in range(1, 10):\n        count[i] += count[i - 1]\n\n    # Build the output array\n    i = n - 1\n    while i &gt;= 0:\n        index = arr[i] // exp\n        output[count[index % 10] - 1] = arr[i]\n        count[index % 10] -= 1\n        i -= 1\n\n    # Copy the output array to arr, to be ready for the next iteration\n    for i in range(n):\n        arr[i] = output[i]\n\ndef radix_sort(arr):\n    # Find the maximum number to determine the number of digits\n    max_num = max(arr)\n\n    exp = 1\n    while max_num // exp &gt; 0:\n        counting_sort(arr, exp)\n        exp *= 10\n\n    return arr\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\nradix_sort(input_array)\n\n[0, 1, 3, 5, 7, 8, 8, 9, 12, 23, 69, 420]\n\n\n\n\n\nThe time complexity of radix sort depends on the number of digits or the range of the input numbers. Let’s denote:\n\nn as the number of elements in the array to be sorted.\nd as the maximum number of digits in the input numbers.\nb as the base of the number system being used (usually 10 for decimal numbers).\n\nBest Case: The best-case scenario occurs when all the numbers have the same number of digits. In this case, the algorithm still has to iterate through each digit of each number. So, the time complexity in the best case is O(d⋅n)\nAverage Case: The average case time complexity is also O(d⋅n). This is because, on average, each number requires � d passes through the counting and distribution steps of the radix sort. Worst Case: The worst-case scenario happens when the numbers have significantly different numbers of digits, resulting in more passes through the counting and distribution steps. In the worst case, the time complexity is O(d⋅n).\nIn practice, however, the value of d is often considered to be a constant because it is bounded by the word size of the machine (e.g., 32 or 64 bits for integers). Therefore, the time complexity is often simplified to O(n), making radix sort very efficient, especially when the range of input numbers is relatively small.\nIn radix sort, the time complexity is the same in the best case, average case, and worst case. This is because the algorithm always needs to iterate through each digit of each number, regardless of the distribution of digits among the input numbers.\n\n\n\n\n\nCounting sort: https://www.youtube.com/watch?v=OKd534EWcdk\nRadix sort: https://www.youtube.com/watch?v=XiuSW_mEn7g"
  },
  {
    "objectID": "posts/software/data_structures_algos/sorting/sorting.html#bubble-sort",
    "href": "posts/software/data_structures_algos/sorting/sorting.html#bubble-sort",
    "title": "Sorting Algorithms",
    "section": "",
    "text": "We “bubble up” the next highest unsorted number on each pass-through.\n\nStart with 2 pointers pointing at the first two values in the array.\nCompare the left and right values. If left_value &gt; right_value, swap them. Otherwise, do nothing.\nMove both pointers one cell rightwards.\nRepeat Steps 2-3 until we reach values that have already been sorted. This completes the first “pass-through” and means we have “bubbled up” the biggest number to the end of the array.\nStart over. repeat Steps 1-4 to bubble up the second biggest number into the second to last position. Repeat this until we perform a pass through with no swaps.\n\n\n\n\n\ndef bubble_sort(array):\n    \"\"\"Bubble sort algorithm to sort an array into ascending order.\n\n    Note we ignore edge cases for the sake of clarity.\n    These are left as an exercise for the reader:\n    - array is empty\n    - array has only 1 element\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n\n    Returns\n    -------\n    array: list\n        The input array sorted in-place.\n    \"\"\"\n    # Initially the entire array is unsorted\n    last_unsorted_index = len(array) - 1\n    is_sorted = False\n\n    while not is_sorted:\n        # Set this to True before we pass through the elements, \n        # then if we need to perform a swap the array is not sorted so we set it to False\n        is_sorted = True\n\n        # Perform a pass-through\n        for left_pointer in range(last_unsorted_index):\n            right_pointer = left_pointer + 1\n\n            if array[left_pointer] &gt; array[right_pointer]:\n                # Swap the values\n                array[left_pointer], array[right_pointer] = array[right_pointer], array[left_pointer]\n                is_sorted = False\n\n        # The pass-through is finished so the next highest value has been \"bubbled up\".        \n        last_unsorted_index -= 1\n\n    return array\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\nbubble_sort(input_array)\n\n[0, 1, 3, 5, 7, 8, 8, 9, 12, 23, 69, 420]\n\n\n\n\n\nEach pass through loops through one fewer element:\n\n\n\nPass-through number\nNumber of operations\n\n\n\n\n1\nN-1\n\n\n2\nN-2\n\n\n3\nN-3\n\n\n4\nN-4\n\n\n5\nN-5\n\n\n…\n…\n\n\nk\nN-k\n\n\n\nSo in total, there are \\((N-1) + (N-2) + (N-3) + ... + 1\\) comparisons. This is the sum of an arithmetic progression, which we can calculate as \\(\\frac{N^2}{2}\\).\nAlso worth noting that in the worst case - an input array in descending order - each comparison will also result in a swap. This does not affect the Big-O complexity but would slow down the run time in practice. There are \\(\\frac{N^2}{2}\\) comparisons and up to \\(\\frac{N^2}{2}\\) swaps, resulting in \\(O(N^2)\\) total operations.\nThe complexity is therefore \\(O(N^2)\\).\nIn general, any nested loop should be a hint at quadratic time complexity."
  },
  {
    "objectID": "posts/software/data_structures_algos/sorting/sorting.html#selection-sort",
    "href": "posts/software/data_structures_algos/sorting/sorting.html#selection-sort",
    "title": "Sorting Algorithms",
    "section": "",
    "text": "Find the smallest value from the unsorted part of the array and put it at the beginning.\n\nCheck each cell of the array from left to right to find the lowest value. Store the index of the running minimum value.\nAt the end of pass-through \\(j\\) (starting at 0), swap the minimum value with the one at index \\(j\\).\nRepeat steps 1-2 until we reach a pass-through that would start at the end of the array, i.e. \\(j = N-1\\)\n\n\n\n\n\ndef selection_sort(array):\n    \"\"\"Selection sort algorithm to sort an array into ascending order.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n\n    Returns\n    -------\n    array: list\n        The input array sorted in-place.\n    \"\"\"\n    array_length = len(array)\n\n    # Loop through all array elements\n    for pass_through_number in range(array_length):\n\n        # Find the minimum element in the remaining unsorted array\n        min_index = pass_through_number\n        for idx_unsorted_section in range(pass_through_number + 1, array_length):\n            if array[idx_unsorted_section] &lt; array[min_index]:\n                min_index = idx_unsorted_section\n\n        # Swap the found minimum element with the first element\n        array[pass_through_number], array[min_index] = array[min_index], array[pass_through_number]\n    \n    return array\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\nselection_sort(input_array)\n\n[0, 1, 3, 5, 7, 8, 8, 9, 12, 23, 69, 420]\n\n\n\n\n\nAs with bubble sort, on each pass-through we loop through one fewer element, so there are \\(\\frac{N^2}{2}\\) comparisons. But each pass-through only performs 1 swap, so the total number of operations is \\(\\frac{N^2}{2} + N\\).\nThis is still therefore \\(O(N^2)\\) complexity, but it should be about twice as fast as bubble sort."
  },
  {
    "objectID": "posts/software/data_structures_algos/sorting/sorting.html#insertion-sort",
    "href": "posts/software/data_structures_algos/sorting/sorting.html#insertion-sort",
    "title": "Sorting Algorithms",
    "section": "",
    "text": "Remove a value to create a gap, shift values along to move the gap leftwards, then fill the gap.\n\nCreate a gap. For the first pass-through, we temporarily remove the second cell (i.e. index 1) and store it as a temporary variable. This leaves a gap at that index.\nShifting phase. Take each value to the left of the gap and compare it to the temporary variable. if left_val &gt; temp_val, move left value to the right. This has the same effect as moving the gap leftwards. As soon as we encounter a value where left_val &lt; temp_val the shifting phase is complete.\nFill the gap. Insert the temporary value into the current gap.\nRepeat pass-throughs. Steps 1-3 constitute a single pass-through. Repeat this until the pass through begins at the final index of the array. At this point the array is sorted.\n\n\n\n\n\ndef insertion_sort(array):\n    \"\"\"Insertion sort algorithm to sort an array into ascending order.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n\n    Returns\n    -------\n    array: list\n        The input array sorted in-place.\n    \"\"\"\n    array_len = len(array)\n\n    for pass_thru_number in range(1, array_len):\n        # 1. Create a gap\n        temp_val = array[pass_thru_number]\n        gap_idx = pass_thru_number\n\n        # 2. Shifting phase\n        # Move leftwards from the gap and keep shifting elements right if they are greater than temp_val\n        while (gap_idx &gt; 0) and (array[gap_idx - 1] &gt; temp_val):\n            array[gap_idx] = array[gap_idx - 1]\n            gap_idx -= 1\n\n        # 3. Fill the gap\n        array[gap_idx] = temp_val\n\n    return array\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\ninsertion_sort(input_array)\n\n[0, 1, 3, 5, 7, 8, 8, 9, 12, 23, 69, 420]\n\n\n\n\n\nThe worst case is when the input array is sorted in descending order.\nThere are 4 types of operations that occur:\n\n\nIf we compare values to the left of the gap on each step, there will be 1 comparison on the first pass-through, 2 on the second, 3 on the third, etc.\nSo there are \\(1 + 2 + ... + N-1 = \\frac{N^2}{2}\\) comparisons in the worst case.\n\n\n\nEach comparison could result in a shift, so there are the same number of shifts as comparisons in the worst case.\n\n\n\nWe remove 1 temp value per pass-through, so there are \\(N-1\\) removals.\nWe insert that value at the end of each pass-through, so there are also \\(N-1\\) insertions.\n\n\n\n\n\n\nOperation\nNumber\n\n\n\n\nRemovals\n\\(N-1\\)\n\n\nComparisons\n\\(\\frac{N^2}{2}\\)\n\n\nShifts\n\\(\\frac{N^2}{2}\\)\n\n\nInsertions\n\\(N-1\\)\n\n\n\nOverall there are \\(N^2 + 2N - 2\\) operations, so the complexity is \\(O(N^2)\\).\n\n\n\n\nThe complexity generally considers the worst case, but insertion sort varies greatly based on the input.\nWe saw in the worst case the number of steps is \\(N^2 + 2N - 2\\).\nIn the best case the data is already sorted, so we do a remove and an insert on each pass-through, for a totla of \\(2N - 2\\) steps.\nIn the average case, let’s assume about half of the data is already sorted. So we’ll still need to do \\(N - 1\\) removes and \\(N - 1\\) inserts in total. We’ll also need to compare about half the data, so \\(\\frac{N^2}{4}\\) comparisons and the same number on swaps. This gives a total of \\(\\frac{N^2}{2} + 2N - 2\\) steps.\nDepending on the state of the input data, the speed can vary considerably. Compare this to selection sort, which will always take \\(\\frac{N^2}{2}\\) steps regardless of the input data.\n\n\n\nAlgorithm\nBest Case\nAverage Case\nWorst Case\n\n\n\n\nSelection sort\n\\(\\frac{N^2}{2}\\)\n\\(\\frac{N^2}{2}\\)\n\\(\\frac{N^2}{2}\\)\n\n\nInsertion sort\n\\(N\\)\n\\(\\frac{N^2}{2}\\)\n\\(N^2\\)\n\n\n\nSo the choice of which algorithm is “best” depends on the state of your input data. Both have the same \\(O(N^2)\\) time complexity."
  },
  {
    "objectID": "posts/software/data_structures_algos/sorting/sorting.html#quicksort",
    "href": "posts/software/data_structures_algos/sorting/sorting.html#quicksort",
    "title": "Sorting Algorithms",
    "section": "",
    "text": "Quicksort is a sorting algorithm that relies on the concept of partitioning.\n\n\nThe idea behind partitioning is we take a random value from the array which we call the pivot.\nWe then want to ensure any smaller numbers are left of the pivot and any larger numbers to its right.\n\nIf we take the rightmost value as the pivot, we create a left pointer pointing at the leftmost value and a right pointer at the second from the right (i.e. not the pivot but the next rightmost).\nThe left pointer moves rightwards until it reaches a value &gt;= the pivot value.\nThe right pointer moves leftwards until it reaches a value &lt;= the pivot value.\nIf at this pointer the left pointer has crossed past the right pointer, move straight on to the next step. Otherwise swap the values that the left and right pointers are pointing at.\nSwap the pivot value with the left pointer’s value.\n\n\ndef partition(array):\n    \"\"\"Partition an array around the rightmost value.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to partition.\n\n    Returns\n    -------\n    array: list\n        The input array partitioned in-place.\n    \"\"\"\n    # 1. Initialise the pivot and pointers\n    pivot_idx = len(array) - 1\n    left_pointer_idx = 0\n    right_pointer_idx = pivot_idx - 1\n\n    # Loop until the left and right pointers cross \n    while left_pointer_idx &lt; right_pointer_idx:\n        # 2. Move the left pointer\n        while (left_pointer_idx &lt; pivot_idx) and (array[left_pointer_idx] &lt; array[pivot_idx]):\n            left_pointer_idx += 1\n\n        # 3. Move the right pointer\n        while (right_pointer_idx &gt; 0) and (array[right_pointer_idx] &gt; array[pivot_idx]):\n            right_pointer_idx -= 1\n\n        # 4. Swap values if the pointers haven't crossed yet, otherwise end the loop\n        if left_pointer_idx &lt; right_pointer_idx:\n            array[left_pointer_idx], array[right_pointer_idx] = array[right_pointer_idx], array[left_pointer_idx]\n\n    # 5. Swap the pivot and left_pointer values\n    array[left_pointer_idx], array[pivot_idx] = array[pivot_idx], array[left_pointer_idx]\n\n    return array\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\npartition(input_array)\n\n([8, 7, 5, 3, 1, 9, 0, 8, 12, 69, 420, 23], 8)\n\n\nPartitioning isn’t the same as sorting the array, but it’s “sorted-ish”.\n\n\n\nQuicksort is a combination of partitions and recursion.\n\nPartition the array. The pivot is now at the correct position in the sorted array.\nConsider the subarrays to the left and the right of the pivot as their own arrays. We’ll partition these recursively.\nIf a subarray has 0 or 1 elements, that is the base case and we do nothing; it is already sorted.\n\nWe’ll make a few tweaks to the partition function to take a start index and an end index, and to also return the pivot index. These extra parameters will allow us to call it recursively in quicksort.\n\n\n\n\ndef partition(array, start_idx, end_idx):\n    \"\"\"Partition an array around the rightmost value.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to partition.\n    start_idx: int\n        The start index of the array to consider.\n    end_index: int\n        The end index of the array to consider.\n\n    Returns\n    -------\n    array: list\n        The input array partitioned in-place.\n    final_pivot_idx: int\n        The index position of the pivot point in the partitioned array.\n    \"\"\"\n    # 1. Initialise the pivot and pointers\n    pivot_idx = end_idx\n    left_pointer_idx = start_idx\n    right_pointer_idx = pivot_idx - 1\n\n    # Loop until the left and right pointers cross \n    while left_pointer_idx &lt; right_pointer_idx:\n        # 2. Move the left pointer\n        while (left_pointer_idx &lt; pivot_idx) and (array[left_pointer_idx] &lt; array[pivot_idx]):\n            left_pointer_idx += 1\n\n        # 3. Move the right pointer\n        while (right_pointer_idx &gt; 0) and (array[right_pointer_idx] &gt; array[pivot_idx]):\n            right_pointer_idx -= 1\n\n        # 4. Swap values if the pointers haven't crossed yet, otherwise end the loop\n        if left_pointer_idx &lt; right_pointer_idx:\n            array[left_pointer_idx], array[right_pointer_idx] = array[right_pointer_idx], array[left_pointer_idx]\n\n    # 5. Swap the pivot and left_pointer values\n    array[left_pointer_idx], array[pivot_idx] = array[pivot_idx], array[left_pointer_idx]\n    final_pivot_idx = left_pointer_idx\n\n    return array, final_pivot_idx\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\npartition(input_array, 0, 11)\n\n([8, 7, 5, 3, 1, 9, 0, 8, 12, 69, 420, 23], 8)\n\n\n\ndef quicksort(array, start_idx=0, end_idx=None):\n    \"\"\"Quicksort algorithm to sort an array into ascending order.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n    start_idx: int\n        The start index of the array to consider.\n    end_index: int\n        The end index of the array to consider.\n\n    Returns\n    -------\n    array: list\n        The input array sorted in-place.\n    \"\"\"\n    end_idx = end_idx or len(array) - 1\n\n    # 3. Base case - an array of 0 or 1 elements is already sorted\n    if len(array[start_idx: end_idx]) &lt;= 1:\n        return array\n    \n    # 1. Partition the array\n    array, pivot_idx = partition(array, start_idx, end_idx)\n\n    # 2. Recursively partition the left and right subarrays\n    quicksort(array, start_idx=start_idx, end_idx=pivot_idx - 1)\n    quicksort(array, start_idx=pivot_idx + 1, end_idx=end_idx)\n\n    return array\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\nquicksort(input_array)\n\n[0, 1, 3, 5, 7, 8, 9, 8, 12, 23, 69, 420]\n\n\n\n\n\n\n\nPartitioning involves two steps:\n\n\n\nOperation\nNumber\n\n\n\n\nSwaps\n\\(N\\)\n\n\nComparisons\n\\(\\frac{N^2}{2}\\)\n\n\n\nEach element is compared to the pivot point at least N times, because the left and right pointers move until they reach each other. So there are \\(N\\) comparisons.\nEach swap handles two values, so in the worst case if we swapped every value there would be \\(\\frac{N}{2}\\) swaps. On average, there’d be about half that, so \\(\\frac{N}{4}\\).\nEither way, this means a single partition operation is \\(O(N)\\).\n\n\n\nQuicksort performs multiple partitions. How many?\nIn the best case, each partition would place the pivot directly in the middle of the subarray, spltting them clean in half each time. So there would be \\(log_2 N\\) splits.\nIn the average case this is “close enough”. Not every level will split equally so there’ll be a few extra but it will still be broadly symmetric, therefore a logarithmic function of \\(N\\).\nIn the worst case, every partition ends up on the left side, so with each partition we are effectively placing each element one-by-one from the left. This means we will partition \\(N\\) times, and each has \\(O(N)\\) complexity, resulting in a total compleixty of \\(O(N^2)\\).\n\n\n\nAlgorithm\nBest Case\nAverage Case\nWorst Case\n\n\n\n\nQuicksort\n\\(N log N\\)\n\\(N log N\\)\n\\(N^2\\)\n\n\n\n\n\n\n\nQuickselect is an algorithm with a similar approach. It’s a hybrid of Quicksort and a binary search.\n\nGiven an unsorted array, find the fifth-highest value in the array.\n\nOne approach would be to sort the array then pick the fifth element from the right. But sorting is \\(O(N log N)\\) on average for the better algorithms. We can do better without having to sort the whole array.\nRecall that after a partition, the pivot ends up in the correct place in the array. This is crucial. If we want to find the fifth highest value, we want to find the value that ends up in the fifth position from the right. So we can:\n\nPerform a partition and see where our pivot ends up.\nIf our pivot is too far left, partition the right subarray. If the pivot is too dar right, partition the left subarray.\nKeep going until we find the pivot which ends up in our target spot.\n\n\ndef quickselect(array, target_position, start_idx=0, end_idx=None):\n    \"\"\"Quickselect algorithm to find the target position (k-th lowest) element in an unsorted array.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n    target_position: int\n        The kth lowest value that we want to find.\n    start_idx: int\n        The start index of the array to consider.\n    end_index: int\n        The end index of the array to consider.\n\n    Returns\n    -------\n    array: list\n        The input array sorted in-place.\n    \"\"\"\n    end_idx = end_idx or len(array) - 1\n\n    # Base case: If the start index is greater than the end index, or if the target position is out of range\n    if (start_idx &gt; end_idx) or (target_position &lt; 0) or (target_position &gt; end_idx):\n        return None\n\n    # 1. Partition the array\n    array, pivot_idx = partition(array, start_idx, end_idx)\n\n    # 2. Recursively process the subarrays\n    if pivot_idx &lt; target_position:\n        # The pivot is too small, so check the subarray to the right\n        return quickselect(array, target_position, start_idx=pivot_idx + 1, end_idx=end_idx)\n    elif pivot_idx &gt; target_position:\n        # The pivot is too big, so check the subarray to the left\n        return quickselect(array, target_position, start_idx=start_idx, end_idx=pivot_idx - 1)\n    else:\n        # The pivot is at the target position - we have found our target!\n        return array[pivot_idx]\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\nquickselect(input_array, 5)\n\n8\n\n\n\n\n\nIn the average case, we are splitting the subarrays (roughly) in half each time.\nEach split is operating on a subarray of half the size, so in total there are \\(N + \\frac{N}{2} +  \\frac{N}{4} + \\frac{N}{8} + ... + 2 = 2N\\) steps.\nThus, overall the complexity is \\(O(N)\\) in the average case."
  },
  {
    "objectID": "posts/software/data_structures_algos/sorting/sorting.html#merge-sort",
    "href": "posts/software/data_structures_algos/sorting/sorting.html#merge-sort",
    "title": "Sorting Algorithms",
    "section": "",
    "text": "Merge sort is like organizing a messy pile of papers. You divide the pile into smaller piles, sort each smaller pile, and then merge them back together in the correct order.\n\n\n\nDivide. Recursively split the array into two halves until each subarray contains only one element (and is therefore sorted).\nSort. Recursively mergesort each half.\nMerge. Recursively merge the sorted subarrays back together in the correct order.\n\n\n\n\n\ndef merge_sort(array):\n    \"\"\"Mergesort sort algorithm to sort an array into ascending order.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n\n    Returns\n    -------\n    array: list\n        The input array sorted in-place.\n    \"\"\"\n    # Base case - the array has 0 or 1 elements so is already sorted\n    if len(array) &lt;= 1:\n        return array\n    \n    # 1. Split the array\n    idx_mid = len(array) // 2\n    left_half = array[:idx_mid]\n    right_half = array[idx_mid:]\n    \n    # 2. Sort each half recursively\n    left_half = merge_sort(left_half)\n    right_half = merge_sort(right_half)\n    \n    # 3. Merge the subarrays\n    return merge(left_half, right_half)\n\ndef merge(left_array, right_array):\n    \"\"\"Merge two subarrays together which are each sorted.\n    \n    Parameters\n    ----------\n    left_array: list\n        The first subarray to merge.\n    right_array: list\n        The second subarray to merge.\n\n    Returns\n    -------\n    result: list\n        The merged, sorted array.\n    \"\"\"\n    # Initialise pointers at the start of each subarray\n    result = []\n    left_idx = 0\n    right_idx = 0\n    \n    # Loop until we reach the end of one of the subarrays\n    while left_idx &lt; len(left_array) and right_idx &lt; len(right_array):\n        # Compare the values of the left and right array, then insert the smaller of the two into the result\n        if left_array[left_idx] &lt; right_array[right_idx]:\n            # The left value is smaller so insert it into the result and increment the pointer\n            result.append(left_array[left_idx])\n            left_idx += 1\n        else:\n            # The right value is smaller so insert it into the result and increment the pointer\n            result.append(right_array[right_idx])\n            right_idx += 1\n    \n    # The array that reaches the end first will be empty, but there will still be elements in the other.\n    # So insert any remaining elements at the end of the result\n    result.extend(left_array[left_idx:])\n    result.extend(right_array[right_idx:])\n    \n    return result\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\nmerge_sort(input_array)\n\n[0, 1, 3, 5, 7, 8, 8, 9, 12, 23, 69, 420]\n\n\n\n\n\nThe time complexity of merge sort is the same for the best case, average case, and worst case.\nIn every case, each divide step splits the array in half, meaning there are \\(log_2(N)\\) splits.\nFor the merge step, the elements subarrays are looped through one-by-one, so all \\(N\\) elements are touched.\nTherefore, the overall complexity is \\(O(NlogN)\\).\nThe space complexity is O(n).\nThis is because merge sort requires additional space to store temporary arrays during the merging process. In the worst case, when merging two halves of the array, an additional array of size equal to the original array is needed to store the merged result temporarily."
  },
  {
    "objectID": "posts/software/data_structures_algos/sorting/sorting.html#heap-sort",
    "href": "posts/software/data_structures_algos/sorting/sorting.html#heap-sort",
    "title": "Sorting Algorithms",
    "section": "",
    "text": "A max-heap is “weakly sorted”; the maximum value is the root of the heap, and nodes are greater than all of their descendants.\nWe can use this property to sort data elements by creating a heap from the data, then popping the maximum value one-by-one to populate an array in order.\n\nBuild a heap: Create a max-heap, which ensures the maximum value is at the root of the heap.\nPop the root: Remove the root and place it at the end of the result array.\nHeapify: Rearrange the remaining blocks to form a new heap.\nRepeat: Iterate through the entire heap until all elements are sorted.\n\n\n\n\n\ndef heapify(array, heap_size, current_idx):\n    \"\"\"Heapify function to maintain the max-heap property.\n    \n    Parameters\n    ----------\n    array: list\n        List of elements\n    heap_size: int\n        Size of heap\n    current_idx: int\n        Index of current node\n\n    Returns\n    -------\n    None\n        The input heap `array` is heapified in-place.\n    \"\"\"\n    # Track the largest node, initially assumed to be the root\n    largest = current_idx  \n    left_child = 2 * current_idx + 1\n    right_child = 2 * current_idx + 2\n\n    # Check if the node's children, if it has any, are larger than the current node\n    if left_child &lt; heap_size and array[left_child] &gt; array[largest]:\n        largest = left_child\n    if right_child &lt; heap_size and array[right_child] &gt; array[largest]:\n        largest = right_child\n\n\n    if largest != current_idx:\n        # If largest is not root, swap it with root to maintain the heap condition\n        array[current_idx], array[largest] = array[largest], array[current_idx]\n        # Recursively heapify the affected sub-tree\n        heapify(array, heap_size, largest)\n\n\ndef heap_sort(array):\n    \"\"\"Heap sort algorithm to sort an array into ascending order.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n\n    Returns\n    -------\n    array: list\n        The sorted array sorted in-place.\n    \"\"\"\n    # 1. Build a max heap.\n    # Start from the last non-leaf node, and heapify each node\n    array_len = len(array)\n    idx_last_non_leaf = array_len // 2 - 1\n    idx_last_node = array_len - 1\n\n    for i in range(idx_last_non_leaf, -1, -1):\n        heapify(array, array_len, i)\n\n    # 2. Extract elements one-by-one starting from the end\n    for i in range(idx_last_node, 0, -1):\n        array[i], array[0] = array[0], array[i]\n        # 3. Maintain the heap condition for the unsorted portion of the array\n        heapify(array, i, 0)\n\n    return array\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\nheap_sort(input_array)\n\n[0, 1, 3, 5, 7, 8, 8, 9, 12, 23, 69, 420]\n\n\n\n\n\nThe pre-processing step is to build the heap. This operation takes O(n) time.\nOnce the heap is created, we perform the following operations:\n\nPop the max value: This is accessible as it is the root, so \\(O(1)\\) time.\nInsert at the end of the result array: This is also \\(O(1)\\) time.\nHeapifying: This takes \\(O(log N)\\) time.\n\nThese are performed for each of the \\(N\\) data elements, so the overall time complexity of heap sort is \\(O(N log N)\\).\nThis is the same regardless of the arrangement of the input, so the time complexity of heap sort is the same in the best case, average case, and worst case."
  },
  {
    "objectID": "posts/software/data_structures_algos/sorting/sorting.html#counting-sort",
    "href": "posts/software/data_structures_algos/sorting/sorting.html#counting-sort",
    "title": "Sorting Algorithms",
    "section": "",
    "text": "We count how many of each element is in the array, i.e. how many 0s, how many 1s, how many 2s.\nFrom this we can then determine what the starting position of each element in the output will be. E.g. if there are three 0s then these will take the first three slots in the sorted output, so 1s will start at the fourth position.\nCounting sort works best when the number of unique items is small, i.e. there are lots of duplicates in the list.\nIt is a stable sorting algorithm, meaning items with identical values in the input will retain their original ordering in the output.\n\n\n\nSay we want to sort the following array:\n[0, 1, 2, 0, 0, 1, 2, 1, 0, 0, 2, 1, 2]\n\nCounting: Count the number of occurrences of each unique element in the input list. How many 0s? How many 1s? How many 2s? Etc.\nCumulative count: Then calculate the cumulative count of the elements. This step determines the starting position of each element in the sorted output.\nPlacement: Finally, you place each element in its correct position in the sorted output based on its cumulative count.\n\n\n\n\n\ndef counting_sort(array):\n    \"\"\"Counting sort algorithm to sort an array into ascending order.\n    \n    Parameters\n    ----------\n    array: list\n        The array that we wish to sort.\n\n    Returns\n    -------\n    sorted_array: list\n        The sorted array.\n    \"\"\"\n    # 1. Count the elements in the input. \n    max_val = max(array)\n    count_store = [0] * (max_val + 1)\n\n    for num in array:\n        count_store[num] += 1\n    \n    # 2. Cumulative count\n    for i in range(1, len(count_store)):\n        count_store[i] += count_store[i - 1]\n        \n    # 3. Place each element in its correct position in the sorted array\n    sorted_array = [0] * len(array)\n\n    # for num in reversed(array):\n    #     sorted_array[count_store[num] - 1] = num\n    #     count_store[num] -= 1\n\n    for num in array:\n        sorted_array[count_store[num] - 1] = num\n        count_store[num] -= 1\n    \n    return sorted_array\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\ncounting_sort(input_array)\n\n[0, 1, 3, 5, 7, 8, 8, 9, 12, 23, 69, 420]\n\n\n\n\n\nThe complexity of counting sort depends on the number of elements, \\(N\\), and the number of unique elements, \\(K\\).\nFor each each of the steps:\n\nCounting: We loop through all \\(N\\) elements. We also create a count_store array which takes \\(K\\) elements of auxiliary space.\nCumulative count: We loop through the \\(K\\) elements in the count_store array.\nPlacement: We initialise an output_array of size \\(N\\), which therefore takes \\(N\\) elements of auxiliary space. We then loop through the \\(N\\) elements in the original array, using count_store to determine the correct placement.\n\nSo there are \\(N + K + N\\) steps and \\(N + K\\) elements of auxiliary data.\nTime complexity: \\(O(N + K)\\)\nAuxiliary space: \\(O(N + K)\\)"
  },
  {
    "objectID": "posts/software/data_structures_algos/sorting/sorting.html#radix-sort",
    "href": "posts/software/data_structures_algos/sorting/sorting.html#radix-sort",
    "title": "Sorting Algorithms",
    "section": "",
    "text": "Radix sort is a sorting algorithm that sorts numbers by processing individual digits.\nIt sorts numbers by first grouping the individual digits of the same place value together and sorting them. It starts sorting from the least significant digit (rightmost digit) to the most significant digit (leftmost digit).\n\n\n\nStart from the rightmost digit: Look at the least significant digit of each number. Group the numbers based on this digit.\nSort each group: After grouping, the numbers are rearranged based on their digit value. So, all the numbers with the same rightmost digit are now together, and they are sorted within this group.\nMove to the next digit: Now, look at the second rightmost digit and repeat the process. Group the numbers based on this digit and sort each group.\nContinue until all digits are considered: Keep repeating this process until you’ve looked at all digits, moving towards the leftmost digit.\n\nBy the end of this process, the numbers will be sorted because each time you look at a digit, the numbers are sorted according to that digit.\n\n\n\n\ndef counting_sort(arr, exp):\n    n = len(arr)\n    output = [0] * n\n    count = [0] * 10\n\n    # Count occurrences of digits\n    for i in range(n):\n        index = arr[i] // exp\n        count[index % 10] += 1\n\n    # Cumulative count\n    for i in range(1, 10):\n        count[i] += count[i - 1]\n\n    # Build the output array\n    i = n - 1\n    while i &gt;= 0:\n        index = arr[i] // exp\n        output[count[index % 10] - 1] = arr[i]\n        count[index % 10] -= 1\n        i -= 1\n\n    # Copy the output array to arr, to be ready for the next iteration\n    for i in range(n):\n        arr[i] = output[i]\n\ndef radix_sort(arr):\n    # Find the maximum number to determine the number of digits\n    max_num = max(arr)\n\n    exp = 1\n    while max_num // exp &gt; 0:\n        counting_sort(arr, exp)\n        exp *= 10\n\n    return arr\n\n\ninput_array = [8, 7, 5, 3, 1, 9, 0, 8, 23, 69, 420, 12]\nradix_sort(input_array)\n\n[0, 1, 3, 5, 7, 8, 8, 9, 12, 23, 69, 420]\n\n\n\n\n\nThe time complexity of radix sort depends on the number of digits or the range of the input numbers. Let’s denote:\n\nn as the number of elements in the array to be sorted.\nd as the maximum number of digits in the input numbers.\nb as the base of the number system being used (usually 10 for decimal numbers).\n\nBest Case: The best-case scenario occurs when all the numbers have the same number of digits. In this case, the algorithm still has to iterate through each digit of each number. So, the time complexity in the best case is O(d⋅n)\nAverage Case: The average case time complexity is also O(d⋅n). This is because, on average, each number requires � d passes through the counting and distribution steps of the radix sort. Worst Case: The worst-case scenario happens when the numbers have significantly different numbers of digits, resulting in more passes through the counting and distribution steps. In the worst case, the time complexity is O(d⋅n).\nIn practice, however, the value of d is often considered to be a constant because it is bounded by the word size of the machine (e.g., 32 or 64 bits for integers). Therefore, the time complexity is often simplified to O(n), making radix sort very efficient, especially when the range of input numbers is relatively small.\nIn radix sort, the time complexity is the same in the best case, average case, and worst case. This is because the algorithm always needs to iterate through each digit of each number, regardless of the distribution of digits among the input numbers."
  },
  {
    "objectID": "posts/software/data_structures_algos/sorting/sorting.html#references",
    "href": "posts/software/data_structures_algos/sorting/sorting.html#references",
    "title": "Sorting Algorithms",
    "section": "",
    "text": "Counting sort: https://www.youtube.com/watch?v=OKd534EWcdk\nRadix sort: https://www.youtube.com/watch?v=XiuSW_mEn7g"
  },
  {
    "objectID": "posts/software/data_structures_algos/data_structures/data_structures.html",
    "href": "posts/software/data_structures_algos/data_structures/data_structures.html",
    "title": "Data Structures",
    "section": "",
    "text": "There are common ways that we can interact with different data structures.\nIt is useful to frame the appropriateness of a data structure for a given based on the speed of the operations that are required most for that task.\n\nRead: Look up the value at a particular location in the data structure\nSearch: Look for a particular value in the data structure.\nInsert: Add a new value to the data structure.\nDelete: Remove a value from the data structure.\n\nReading is “find by key”, searching is “find by value”.\n\n\n\nAn array is a list of elements.\nIt is stored in memory as a block of contiguous memory addresses. When the array is declared, the head of the array is stored, i.e. the memory address of the first elelment.\nThe size of an array is the number of elements in the array. The index denotes where a particular piece of data resides in that array.\n\n\n\nOperation\nComplexity (Worst)\nComplexity (Best)\n\n\n\n\nRead\nO(1)\nO(1)\n\n\nSearch\nO(N)\nO(1)\n\n\nInsertion\nO(N)\nO(1)\n\n\nDeletion\nO(N)\nO(1)\n\n\n\n\n\nA computer can look up a given memory address in constant time.\nWe know the head of the array and the index to look up. So we can read in O(1) time.\nExample:\n\nHead of the array is memory address 1063.\nWe want to look up index 5.\nRead memory address 1068 (because 1063 + 5 = 1068).\n\n\nIf you were asked to raise your right pinky finger, you wouldn’t need to search through all of your fingers to find it\n\n\n\n\nA computer has immediate access to all of its memory addresses but does not know ahead of time their contents.\nSo to find a particular value, we will potentially have to search through every element.\nSearching an array is therefore O(N).\n\n\n\nThe efficiency of inserting into an array depends on where in the array you are inserting.\nIf inserting an element at the end, we simply place the new value at that memory address (assuming the memory address is empty). This is a constant time operation O(1).\nBut if we insert at any other position, we need to:\n\nShift each of the existing elements to the right of the insert index 1 position rightwards\nInsert the new value in the gap created.\n\nSo to insert at index \\(i\\), there are \\(N - i\\) elements to shift (Step 1), then 1 more operation to insert the new value (Step 2).\nIn the worst case - inserting at the start of an array - insertion is O(N).\n\n\n\nSimilarly, the efficiency of deletion depends on the index being deleted.\nIf deleting the last element, there is simply 1 operation to clear the memory address, so this is a constant time operation O(1).\nBut if we delete an element in any other position, we need to:\n\nDelete the element. This leaves a gap in the middle of the array.\nShift each of the elements to the right of the gap leftwards, to fill the gap.\n\nSo to delete at index \\(i\\), we do 1 operation to delete that element (Step 1), then shift the next \\(N-i\\) elements leftwards (Step 2).\nIn the worst case - deleting the first element of the array - deletion is O(N).\n\n\n\n\nA set is a collection of unique elements, i.e. duplicate values are not allowed.\nThere are different ways of implementing sets: array-based sets and hash-sets are discussed here.\nNote that Python already has sets, but we’ll give outline implementations for clarity.\n\n\nAn array is used to store elements. As with standard arrays, elements are stored in contiguous memory locations, and each element has a unique index.\nExample in Python:\n\nclass ArraySet:\n    \n    def __init__(self):\n        self.elements = []\n        \n    def __repr__(self):\n        return str(self.elements)\n\n    def add(self, element):\n        # Search the array for `element`, then append it if it is not a duplicate.\n        if element not in self.elements:\n            self.elements.append(element)\n\n    def remove(self, element):\n        # Search the array for the value, then remove it.\n        if element in self.elements:\n            self.elements.remove(element)\n\n    def contains(self, element):\n        return element in self.elements\n\n\nset_arr = ArraySet()\nset_arr.add(1)\nset_arr.add(2)\nset_arr.add(3)\nprint(set_arr)\n\n[1, 2, 3]\n\n\nIf we try to add a duplicate value it does not get added to the array:\n\nset_arr.add(2)\nprint(set_arr)\n\n[1, 2, 3]\n\n\nThe read, search and delete operations for an array-based set are identical to the standard array.\nInsert operations are where array-based sets diverge. We always insert at the end of a set, which is constant time. But we need to search the array every time to ensure the new value is not a duplicate.\nSo we always need to do a search of all N elements, and then 1 insert operation at the end.\nThis means even in the best case, insertion into an array-based set is O(N) compared to O(1) when inserting at the end of a standard array.\nThe reason for using a set is because the use case requires no duplicates, not because it is inherently “quicker” than a standard array.\n\n\n\nOperation\nComplexity (Worst)\nComplexity (Best)\n\n\n\n\nRead\nO(1)\nO(1)\n\n\nSearch\nO(N)\nO(1)\n\n\nInsertion\nO(N)\nO(N)\n\n\nDeletion\nO(N)\nO(1)\n\n\n\n\n\n\nA hash-based set computes the hash of each element and uses this to store elements.\nAn example implementation implements the set as key-value pairs where keys are the hash of the elements and values are a placeholder value like True, or an array to handle hash collisions.\nWhen there is a hash collision between mutliple elements, a typical approach is to insert all of these elements as an array under the same hash key.\nThe worst case scenario is caused by the extreme edge case where hash collisions are so prominent that every element has the same hash, essentially reducing the hash set to an array. This is generally avoided as long as the hash algorithm is decent.\nFor this reason, the average complexity is more meaningful in the table below. (Note that best has been replace with average in the table headings.)\nHash-based sets do not support reading by index, unlike array-based sets. But all other operations are typically constant time.\n\n\n\nOperation\nComplexity (Worst)\nComplexity (Average)\n\n\n\n\nRead\nN/A\nN/A\n\n\nSearch\nO(N)\nO(1)\n\n\nInsertion\nO(N)\nO(1)\n\n\nDeletion\nO(N)\nO(1)\n\n\n\nExample in Python:\n\nclass HashSet:\n    def __init__(self):\n        # Use a dict to represent the hash set.\n        self.elements = {}\n        \n    def __repr__(self):\n        return str(self.elements)\n\n    def add(self, element):\n        # The key is the element and the value is arbitrary.\n        # There are two extensions we could add here:\n        #   1. The key should really be the *hash* of the element, not just the element itself.\n        #      Essentially, this is using an implicit hash function which is just a pass-through:\n        #      hash_func = lambda x: x\n        #   2. Handle hash collisions by making the value an array which is appended to in the case of collisions.\n        self.elements[element] = True\n\n    def remove(self, element):\n        if element in self.elements:\n            del self.elements[element]\n\n    def contains(self, element):\n        return element in self.elements\n\n\nset_hash = HashSet()\nset_hash.add(1)\nset_hash.add(2)\nset_hash.add(3)\nprint(set_hash)\n\n{1: True, 2: True, 3: True}\n\n\nIf we try to add a duplicate value it simply overwrites the previous value:\n\nset_hash.add(2)\nprint(set_hash)\n\n{1: True, 2: True, 3: True}\n\n\n\n\n\n\nThese are identical to regular arrays with the additional condition that elements are always ordered.\nThis obviously relies heavily on efficient sorting. This is a topic unto itself; see notes on sorting for more info.\nWhen inserting into an ordered array, we need to:\n\nSearch for the correct position - Look at each element in turn and compare if the insert element is greater than it\nInsert into the array\n\nThese two terms increase in opposite directions depending on the insert position. The further into the array we need to search (Step 1), the fewer elements we need to shift for the insertion (Step 2).\n\n\nIn a typical (unordered) array, the only option for searching is a linear search: we loop through each element in turn until we find our target.\nFor an ordered array, we can improve on this using a binary search.\n\nPick the middle element.\nIf the target value is greater than this, search the right half, otherwise search the left half.\nRepeat this recursively until we find our target.\n\nThis approach splits the search region in half for every constant time comparison operation.\nOr put another way, if we doubled the number of elements in the array, the binary search would only have to perform 1 extra step. For \\(N\\) elements we need \\(log_2(N)\\) binary splits.\nTherefore, the time complexity is O(log(N)).\n\ndef binary_search(ordered_array, target):\n    \"\"\"Perform a binary search for the target value on the given ordered array.\n\n    Parameters\n    ----------\n    ordered_array: list\n        The array to search in.\n    target: int\n        The target value we are searching for.\n\n    Returns\n    -------\n    target_index: int\n        The index of the target value.\n        Returns None if the value does not exist in the array.\n    \"\"\"\n    # Establish the lower and upper bounds of our search.\n    # Initially, this is the entire array\n    idx_lower = 0\n    idx_upper = len(ordered_array) - 1\n\n    while idx_lower &lt;= idx_upper:\n        # Find the midpoint between our bounds\n        idx_midpoint = (idx_upper + idx_lower) // 2\n        value_at_midpoint = ordered_array[idx_midpoint]\n\n        # Compare to our target value and narrow the upper or lower bound accordingly\n        if value_at_midpoint == target:\n            # We have found the target!\n            return idx_midpoint\n        elif value_at_midpoint &lt; target:\n            # The target is bigger so must be to the right side\n            idx_lower = idx_midpoint + 1\n        elif value_at_midpoint &gt; target:\n            # The target is smaller so must be on the left side\n            idx_upper = idx_midpoint - 1\n\n    # If the lower and upper bounds meet we have exhausted the whole array, so the target is not in the array\n    return None\n\nLet’s try this on a few examples.\n\nordered_array = [1, 2, 4, 5, 7, 8, 9, 10, 13, 14]\n\n\nbinary_search(ordered_array, 7)\n\n4\n\n\n\nbinary_search(ordered_array, 14)\n\n9\n\n\nNow a value that’s not in the array:\n\nbinary_search(ordered_array, 14)\n\n9\n\n\nCompare this with a linear search\n\ndef linear_search(array, target):\n    \"\"\"Perform a linear search for the target value on the given array.\n\n    Parameters\n    ----------\n    array: list\n        The array to search in.\n    target: int\n        The target value we are searching for.\n\n    Returns\n    -------\n    target_index: int\n        The index of the target value.\n        Returns None if the value does not exist in the array.\n    \"\"\"\n    # Loop through every element in the array.\n    # Note: we should really use enumerate() rather than range(len()) but I wanted to keep this generic \n    # without too many python-specific helpers\n    for idx in range(len(array)):\n        if array[idx] == target:\n            return idx\n    \n    # If we reach the end of the array without returning a value, then the target does not exist in the array.\n    return None\n\nLet’s compare how they perform for a reasonably big array with 1 million elements.\n\narray = [k for k in range(1000000)]\n\n\n%%timeit\nbinary_search(array, 987654)\n\n1.13 µs ± 56.9 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\n\n%%timeit\nlinear_search(array, 987654)\n\n15.9 ms ± 320 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nThe binary search is ~14000x faster than the linear search!\n\n\n\n\nHash tables are key:value pairs. We can look up a the value for a given key in \\(O(1)\\) time.\nAlso known as hash maps, dictionaries, maps, associative arrays.\n\n\nThe process of taking characters and converting them to numbers is called hashing.\nThe code that performs this conversion is the hash function.\nA hash function requires one condition:\n\nConsistency: A hash function must convert the same string to the same number every time it’s applied.\n\nIn practice, for the hash function to be useful, it should also be collision resistant:\n\nCollision resistant: Different inputs should hash to different outputs.\n\nAs an extreme example, the following hash function is consistent but not collision resistant:\ndef crappy_hash(input_str: str) -&gt; int:\n    \"\"\"This is the world's worst hash function.\"\"\"\n    return 1\n\n\n\nWe want to insert the following key:value pair into our hash table:\nkey = \"Name\"\nvalue = \"Gurp\"\nLet’s say we have a hash function that is actually good, and in this particular case hash_function(key) returns 12.\nThe hash table will then insert value at memory address 12 (or more specifically, the memory address of the head of the dictionary + 12).\nThis means if we ever want to look up the key \"Name\", we hash it and immediately know to access memory address 12 and return the value \"Gurp\".\nSo hash table lookups are \\(O(1)\\).\nMore specifically, looking up by key is \\(O(1)\\). Searching by value is essentially searching through an array, so is \\(O(N)\\).\n\n\n\nA collision occurs when we try to add data to a cell that is already occupied.\nOne approach to handling this is called separate chaining.\nInstead of placing a single value in a cell, we place a pointer to an array. This array contains length-2 subarrays where the first element is the key and the second element is the value.\nIf there are no collisions, a hash table look up is \\(O(1)\\). In the worst case, ALL keys collide and so we essentially have to search through an array which is \\(O(N)\\).\n\n\n\nA hash tables efficiency depends on:\n\nHow much data we’re storing in it\nHow many cells are available\nWhich hash function we use\n\nA good hash function (3) should distribute the data (1) evenly across all cells (2).\nThis ensures the memory is used efficiently while avoiding collisions.\nThe load factor is the ratio of data to cells, and ideally should be around 0.7, i.e. 7 elements for every 10 cells.\n\n\n\nOperation\nComplexity (Worst)\nComplexity (Average)\n\n\n\n\nRead\nO(N)\nO(1)\n\n\nSearch\nO(N)\nKeys: O(1) Values: O(N)\n\n\nInsertion\nO(N)\nO(1)\n\n\nDeletion\nO(N)\nO(1)\n\n\n\nThe worst case corresponds to when all keys collide, reducing the hash table to an array effectively.\n\n\n\n\nA stack is stored in memory the same as an array, but it has 3 constraints:\n\nData can only be inserted at the end (push)\nData can only be deleted from the end (pop)\nOnly the last element can be read (peek)\n\n\nRead, insert and delete can only happen at the end.\n\nThis makes them useful as Last-In First-Out (LIFO) data stores: the last item pushed the the stack is the first to be popped.\nExample in Python:\n\nclass Stack:\n\n    def __init__(self, initial_elements: list = []):\n        # We can pass a list to initialise the Stack\n        self.data = initial_elements\n\n    def __repr__(self):\n        return str(self.data)\n\n    def push(self, element):\n        self.data.append(element)\n\n    def pop(self):\n        return self.data.pop()\n\n    def peek(self):\n        if self.data:\n            return self.data[-1]\n\n\nstack = Stack([1, 2, 3, 4])\nstack\n\n[1, 2, 3, 4]\n\n\n\nstack.push(5)\nprint(stack)\n\n[1, 2, 3, 4, 5]\n\n\n\nstack.peek()\n\n5\n\n\n\nprint(stack)\n\n[1, 2, 3, 4, 5]\n\n\n\nstack.pop()\n\n5\n\n\n\nprint(stack)\n\n[1, 2, 3, 4]\n\n\nThe benefits of stacks, and other constrained data structures, are:\n\nPrevent potential bugs when using certain algorithms. For example, an algorithm that relies on stacks may break if removing elements from the middle of the array, so using a standard array is more error-prone.\nA new mental model for tackling problems. In the case of stacks, this is the LIFO approach.\n\nThe concept of stacks is a useful precursor to recursion, as we push to and pop from the end of a stack.\n\n\n\nA queue is conceptually similar to a stack - it is a constrained array. This time, it is First-In First-Out (FIFO) like a queue of people; the first person to arrive is the first to leave.\nQueue restrictions:\n\nData can only be inserted at the end (enqueue)\nData can only be deleted from the front (dequeue)\nData can only be read from the front (peek)\n\nPoints (2) and (3) are the opposite of the stack.\n\nclass Queue:\n\n    def __init__(self, initial_elements: list = []):\n        # We can pass a list to initialise the Stack\n        self.data = initial_elements\n\n    def __repr__(self):\n        return str(self.data)\n\n    def enqueue(self, element):\n        self.data.append(element)\n\n    def dequeue(self):\n        return self.data.pop(0)\n\n    def peek(self):\n        if self.data:\n            return self.data[0]\n\n\nq = Queue([1, 2, 3, 4])\nq\n\n[1, 2, 3, 4]\n\n\n\nq.enqueue(5)\nprint(q)\n\n[1, 2, 3, 4, 5]\n\n\n\nq.dequeue()\n\n1\n\n\n\nprint(q)\n\n[2, 3, 4, 5]\n\n\n\nq.peek()\n\n2\n\n\n\nprint(q)\n\n[2, 3, 4, 5]\n\n\n\n\n\nA linked list represents a list of items as non-contiguous blocks of memory.\nIt is a list of items, similar to an array. But an array occupies a continuous block of memory.\n\n\n\nOperation\nComplexity (Worst)\nComplexity (Best)*\n\n\n\n\nRead\nO(N)\nO(1)\n\n\nSearch\nO(N)\nO(1)\n\n\nInsertion\nO(N)\nO(1)\n\n\nDeletion\nO(N)\nO(1)\n\n\n\nThe best case corresponds to operating on the head node.\nIn a linked list, each element is contained in a node that can be in scattered positions in memory. The node contains the data element and a “link” which is a pointer to the memory address of the next element.\nBenefits of a linked list over an array:\n\nMemory efficient: we don’t need a continuous block of memory\n\\(O(1)\\) inserts and deletes from the beginning of the list\nUseful when we want to traverse through a data structure while making inserts and deletes, because we do not have to shift the entire data structure each time as we would have to with an array\n\nA node contains two pieces of information:\n\n\n\nData\nLink\n\n\n\n\n“a”\n1666\n\n\n\nThese nodes can then be linked together in a list… a linked list!\n\n\n\n\n\nflowchart LR\n\n  A(\"'a'|1666\") --&gt; B(\"'b'|1984\") --&gt; C(\"'c'|1066\") --&gt; D(\"...\") --&gt; E(\"'z'|null\")\n\n\n\n\n\n\n\nThe link of the last node is null to indicate the end of the list.\n\n\nWe first need a node data structure, which will hold our data and a link to the next node.\nWe’ll point to the next node itself, rather than its memory address. This still has the same effect as nodes are scattered throughout different memory locations.\n\nclass Node:\n\n    def __init__(self, data, link=None):\n        self.data = data\n        self.link = link\n    \n    def __repr__(self) -&gt; str:\n        return f\"Data: {self.data}\\tLink: \\n{self.link}\"\n\nCreate some nodes and link them together\n\nnode1 = Node(\"a\")\nnode2 = Node(\"b\")\nnode3 = Node(\"c\")\nnode4 = Node(\"d\")\n\nThis is what a single node looks like:\n\nprint(node1)\n\nData: a Link: \nNone\n\n\nNow we link them\n\nnode1.link = node2\nnode2.link = node3\nnode3.link = node4\n\n\nnode1\n\nData: a Link: \nData: b Link: \nData: c Link: \nData: d Link: \nNone\n\n\n\n\n\nThe linked list simply keeps track of the head, i.e. the first node in the list.\nWhen using linked lists, we only have immediate access to this first node. For any other values, we need to start at the head node and traverse the list.\n\nclass LinkedList:\n\n    def __init__(self, head):\n        self.head = head\n\n    def __repr__(self) -&gt; str:\n        return str(self.head)\n\n\nll = LinkedList(node1)\nll\n\nData: a Link: \nData: b Link: \nData: c Link: \nData: d Link: \nNone\n\n\n\n\n\nWe start at the head an traverse the list until we reach the desired index.\nThis means they ar \\(O(N)\\) in the worst case.\n\nclass LinkedList:\n\n    def __init__(self, head):\n        self.head = head\n\n    def __repr__(self) -&gt; str:\n        return str(self.head)\n    \n    def read(self, index):\n        \"\"\"Read the node at the given index.\"\"\"\n        current_idx = 0\n        current_node = self.head\n\n        while (index &gt; current_idx):\n            if current_node.link is None:\n                # The index does not exist in the linked list, we have reached the end\n                return None\n            \n            current_node = current_node.link\n            current_idx += 1          \n\n        return current_node\n\n\nll = LinkedList(node1)\nll.read(2)\n\nData: c Link: \nData: d Link: \nNone\n\n\n\nll.read(10)\n\n\n\n\nTo search for a value, again we have to traverse the whole list.\nThis means the worst case complexity is \\(O(N)\\).\nThe mechanics of searching are the same as reading - we traverse the graph. The difference is we keep going until we find the value or reach the end of the list, rather than stopping at a predetermined index with read.\n\nclass LinkedList:\n\n    def __init__(self, head):\n        self.head = head\n\n    def __repr__(self) -&gt; str:\n        return str(self.head)\n    \n    def read(self, index):\n        \"\"\"Read the node at the given index.\"\"\"\n        # Start at the head\n        current_idx = 0\n        current_node = self.head\n\n        # Traverse the list until we find the desired index\n        while (index &gt; current_idx):\n            if current_node.link is None:\n                # The index does not exist in the linked list, we have reached the end\n                return None\n            \n            current_node = current_node.link\n            current_idx += 1          \n\n        return current_node\n    \n    def search(self, value):\n        \"\"\"Find the index of the given value.\"\"\"\n        # Start at the head\n        current_idx = 0\n        current_node = self.head\n\n        # Loop until we reach the None value which denotes the end of the list\n        while current_node:\n            if current_node.data == value:\n                # We've found our target value\n                return current_idx\n            # Try the next node\n            current_node = current_node.link\n            current_idx += 1\n\n        # We have traversed the whole list without finding a matching value\n        return None\n\n\nll = LinkedList(node1)\nll.search('c')\n\n2\n\n\n\n\n\nInserting a node into a linked list where we already have the current node is an \\(O(1)\\) operation.\n\nPoint to the next node. new_node.link = current_node.link\nLink from the previous node. current_node.link = new_node\n\nWith a linked list, we only have the head node, so we can insert at the start in \\(O(1)\\) time.\nBut to insert at any other point, we have to traverse there first (an \\(O(N)\\) operation) and then do the insert.\nThis is the key point of linked lists: insertion at the beginning is \\(O(1)\\) but at the end is \\(O(N)\\). This is the opposite of arrays, meaning linked lists are useful in cases where insertions are mostly at the beginning.\n\nclass LinkedList:\n\n    def __init__(self, head):\n        self.head = head\n\n    def __repr__(self) -&gt; str:\n        return str(self.head)\n    \n    def read(self, index):\n        \"\"\"Read the node at the given index.\"\"\"\n        # Start at the head\n        current_idx = 0\n        current_node = self.head\n\n        # Traverse the list until we find the desired index\n        while (index &gt; current_idx):\n            if current_node.link is None:\n                # The index does not exist in the linked list, we have reached the end\n                return None\n            \n            current_node = current_node.link\n            current_idx += 1          \n\n        return current_node\n    \n    def search(self, value):\n        \"\"\"Find the index of the given value.\"\"\"\n        # Start at the head\n        current_idx = 0\n        current_node = self.head\n\n        # Loop until we reach the None value which denotes the end of the list\n        while current_node:\n            if current_node.data == value:\n                # We've found our target value\n                return current_idx\n            # Try the next node\n            current_node = current_node.link\n            current_idx += 1\n\n        # We have traversed the whole list without finding a matching value\n        return None\n    \n\n    def insert(self, value, index):\n        \"\"\"Insert the value at the given index.\"\"\"\n        new_node = Node(value)\n\n        if index == 0:\n            # Link to the old head and update the linked lists head\n            new_node.link = self.head\n            self.head = new_node\n            return\n        \n        # Traverse the linked list until we find our node\n        current_node = self.head\n        current_idx = 0\n        while current_idx &lt; index - 1:\n            current_node = current_node.link\n            current_idx += 1\n\n        # Update the links to insert the new node\n        new_node.link = current_node.link\n        current_node.link = new_node\n        return \n\n\n        \n\nInsert a new head of our linked list\n\nll = LinkedList(node1)\nll\n\nData: a Link: \nData: b Link: \nData: c Link: \nData: d Link: \nNone\n\n\n\nll.insert('new_head', 0)\n\n\nll\n\nData: new_head  Link: \nData: a Link: \nData: b Link: \nData: c Link: \nData: d Link: \nNone\n\n\nInsert in the middle\n\nll.insert(\"I'm new here\", 3)\n\n\nll\n\nData: new_head  Link: \nData: a Link: \nData: b Link: \nData: I'm new here  Link: \nData: c Link: \nData: d Link: \nNone\n\n\n\n\n\nIt is quick to delete from the beginning of a linked list for the same reasons as insertion.\n\nMake the previous node point to the next next node\n\n\nclass LinkedList:\n\n    def __init__(self, head):\n        self.head = head\n\n    def __repr__(self) -&gt; str:\n        return str(self.head)\n    \n    def read(self, index):\n        \"\"\"Read the node at the given index.\"\"\"\n        # Start at the head\n        current_idx = 0\n        current_node = self.head\n\n        # Traverse the list until we find the desired index\n        while (index &gt; current_idx):\n            if current_node.link is None:\n                # The index does not exist in the linked list, we have reached the end\n                return None\n            \n            current_node = current_node.link\n            current_idx += 1          \n\n        return current_node\n    \n    def search(self, value):\n        \"\"\"Find the index of the given value.\"\"\"\n        # Start at the head\n        current_idx = 0\n        current_node = self.head\n\n        # Loop until we reach the None value which denotes the end of the list\n        while current_node:\n            if current_node.data == value:\n                # We've found our target value\n                return current_idx\n            # Try the next node\n            current_node = current_node.link\n            current_idx += 1\n\n        # We have traversed the whole list without finding a matching value\n        return None\n    \n\n    def insert(self, value, index):\n        \"\"\"Insert the value at the given index.\"\"\"\n        new_node = Node(value)\n\n        if index == 0:\n            # Link to the old head and update the linked lists head\n            new_node.link = self.head\n            self.head = new_node\n            return\n        \n        # Traverse the linked list until we find our node\n        current_node = self.head\n        current_idx = 0\n        while current_idx &lt; index - 1:\n            current_node = current_node.link\n            current_idx += 1\n\n        # Update the links to insert the new node\n        new_node.link = current_node.link\n        current_node.link = new_node\n        return \n    \n    def delete(self, index):\n        \"\"\"Delete the value at the given index.\"\"\"\n        if index == 0:\n            # We are deleting the head node, so point at the second node instead\n            self.head = self.head.link\n            return\n    \n        # Traverse the linked list until we find our node\n        current_node = self.head\n        current_idx = 0\n        while current_idx &lt; index - 1:\n            current_node = current_node.link\n            current_idx += 1\n\n        # Skip the next node (which we are deleting) and point ot its link instead\n        current_node.link = current_node.link.link\n        return       \n\n\nll = LinkedList(node1)\nll\n\nData: a Link: \nData: b Link: \nData: I'm new here  Link: \nData: c Link: \nData: d Link: \nNone\n\n\nDelete the head node\n\nll.delete(0)\nprint(ll)\n\nData: b Link: \nData: I'm new here  Link: \nData: c Link: \nData: d Link: \nNone\n\n\nDelete a middle node\n\nll.delete(1)\nprint(ll)\n\nData: b Link: \nData: c Link: \nData: d Link: \nNone\n\n\n\n\n\nA doubly linked list is a variant where each node contains pointers to the previous node and the next node.\n\n\n\nData\nPrevious\nNext\n\n\n\n\n“a”\nnull\n1666\n\n\n\nThe linked list tracks the head and tail.\nThis makes it quicker to read/insert/delete from either the beginning or end. We can also traverse backwards or forwards through the list.\n\n\n\n\n\nflowchart LR\n\n  A(\"'a'|null|1666\") &lt;---&gt; B(\"'b'|1234|1984\") &lt;--&gt; C(\"'c'|1666|1066\") &lt;--&gt; D(\"...\") &lt;--&gt; E(\"'z'|1993|null\")\n\n\n\n\n\n\n\nDoubly linked lists are a good data structure to use for queues, since we can insert/delete at either end.\n\n\n\n\nWe can have some use cases where we want to keep our data sorted.\nSorting is expensive, \\(O(N log N)\\) at the best of times, so we want to avoid sorting often. Ideally we would keep our data sorted at all times. An ordered array could do the job, but insertions and deletions are slow as we have to shift a chunk over the array every time.\nWe want a data structure that:\n\nMaintains order\nHas fast inserts, deletes and search\n\nThis is where a binary search tree comes in.\n\n\n\nOperation\nComplexity (Worst)*\nComplexity (Best)\n\n\n\n\nRead\nN/A\nN/A\n\n\nSearch\nO(N)\nO(log N)\n\n\nInsertion\nO(N)\nO(log N)\n\n\nDeletion\nO(N)\nO(log N)\n\n\n\n*The worst case corresponds to an imbalanced tree that is essentailly a linked list (a straight line). The best case is a perfectly balanced tree.\n\n\nTrees are another node-based data structure when each node can point to multiple other nodes.\n\n\n\n\n\nflowchart TD\n\n\n  A(a) --&gt; B(b)\n  A(a) --&gt; C(c)\n\n  B(b) --&gt; D(d)\n  B(b) --&gt; E(e)\n\n  C(c) --&gt; F(f)\n\n\n\n\n\n\n\nThe root is the uppermost node.\na is the parent of b and c; b and c are children of a.\nThe descendants of a node are all of its children and its children’s children’s children etc. The ancestors of anode are its parents and its parent’s parent’s parents etc.\nEach horizontal layer ofthe tree is a level.\nA tree is balanced if all of its subtrees have the same number of nodes.\n\n\n\n\nA binary tree is one in which each node can have at most 2 children.\nA binary search treemust abide by the following rules:\n\nEach node can have at most one “left” child and one “right” child\nA node’s left descendants are all smaller than the node. It’s right descendants are all larger.\n\n\n\n\n\nclass TreeNode:\n\n    def __init__(self, value, left=None, right=None):\n        self.value = value\n        self.left = left\n        self.right = right\n\n    def __repr__(self):\n        return f\"TreeNode with value: {self.value}\"\n\n\nclass Tree: \n\n    def __init__(self, root_node):\n        self.root_node = root_node\n\n    def __repr__(self) -&gt; str:\n        return f\"Tree with root: {self.root_node}\"\n\n\ntree_node2 = TreeNode('b')\ntree_node3 = TreeNode('c')\ntree_node1 = TreeNode('a', tree_node2, tree_node3)\n\ntree = Tree(tree_node1)\n\n\ntree_node1\n\nTreeNode with value: a\n\n\n\ntree_node1.left\n\nTreeNode with value: b\n\n\n\ntree_node1.right\n\nTreeNode with value: c\n\n\n\ntree\n\nTree with root: TreeNode with value: a\n\n\n\n\n\n\nDesignate a current node (start with the root) and inspect its value.\nIf the current node is our target, success! Stop here.\nIf the current node is smaller than our target, search the left subtree. If it’s larger, search the right subtree.\nRepeat until we find our target. If we reach the end of the subtree without finding the target, then the target is not in the tree.\n\n\n\n\nSearching a binary search tree is \\(O(log N)\\) in the best case (a perfectly balanced tree) since we narrow our search area by half on each step.\nThe worst case is a horribly imbalanced tree. Imagine a tree that only has left descendants. This is essentially a linked list, so searching through it means inspecting every element. Therefore the worst case complexity is \\(O(N)\\).\nSo searching a binary search tree is the same complexity as a binary search performed on an ordered array. Where trees differentiate themselves is on insertions and deletes.\n\n\n\n\nCompare our new_node to each value in the tree starting from the root. Like with search, follow the path to the left if new_node is lower or right if higher.\nTraverse until we find a node where the appropriate child (left or right) does not exist yet. Make new_node the child.\n\nThe order of insertion is important to maintain balance. Binary search trees work best when seeded with randomly sorted data, as this will typically end up quite well-balanced. Seeding the tree with sorted data leads to imbalanced trees.\nInsertion is \\(O(log N)\\) for balanced trees because we search for the correct place to insert and then perform an \\(O(1)\\) operation to insert it.\n\n\n\nDeleting a node is more complicated because it depends if the target node has 0, 1 or 2 children.\n\n\n\nNumber of Children\nAction\n\n\n\n\n0\nSimply delete the node\n\n\n1\nReplace the deleted node with its child\n\n\n2\nReplace the deleted node with the successor node\n\n\n\nThe successor node is the next largest descendant of the deleted node. More formally: the child node whose value is the least of all values that are greater than the deleted node.\nFinding the successor:\n\nVisit the right child of the deleted node.\nKeep visiting left children until there are no more elft children. This is the successor node.\nIf the successor node has a right child, that child takes the original place of the successor node. The successor node gets moved to replace the deleted node.\n\nDeletion is \\(O(log N)\\) for balanced trees because we search for the correct place to insert and then perform (potentially several) \\(O(1)\\) operations to delete the node and replace it with a child / successor.\n\n\n\nThe point of a binary search tree is to maintain order. We can traverse the elements in order with the following recursive algorithm.\n\nCall itself recursively on the node’s left child. This will repeat until we hit a node without a left child.\nVisit this node.\nCall itself recursviely on the node’s right child.\n\nSince we are visiting every element of the tree, this is necessarily \\(O(N)\\).\n\ndef traverse(tree_node):\n    \"\"\"Inorder traversal of a binary search tree.\"\"\"\n    if tree_node is None:\n        return\n    traverse(tree_node.left)\n    print(tree_node.value)\n    traverse(tree_node.right)\n\n\n\n\n\n\n\nA queue is a FIFO list that means data is inserted at the end but accessed/removed from the front.\nA priority queue is a list where deletions and access are the same as a regular queue, but insertions are like an ordered array. So the priority queue is always ordered.\nAn example use case is a triage system in a hospital: patients are seen based on severity, not just when they arrived.\nThis is an abstract data types that we could implement in multiple ways using different fundamental data types. E.g. we could implement using an ordered array, but insertions would be O(N).\nHeaps are another data structure that fit this use case well.\n\n\n\nThere a multiple kinds of heaps. As a starting point, we consider the binary max-heap, which is a special kind of binary tree.\nBinary max-heap conditions:\n\nHeap condition: Each node is greater than its children.\nThe tree must be complete.\n\nBecause the heap condition is true for each node, we can recursively reason that a node is greater than its children, and they are too, then a node is greater than all of its descendants.\nThe complete condition essentially means values are filled left-to-right, top-down with no holes. There are no empty positions anywhere except the bottom row, and the bottom row is filled from left to right. The last node is the rightmost node at its bottom level.\nThere is also a min-heap variant. The difference is trivial, aside from the heap condition inequality being reversed, the logic is the same.\n\n\n\n\nHeaps are weakly ordered.\nThe root node is the maximum value.\n\nWith a binary search tree, we know precisely whether a value is the left or right descendant of a node. E.g. 3 will be a left child of 100.\nBut in a max-heap, we don’t know whether 3 is to the left or right, only that it is a descendant rather than an ancestor.\nSearching would require inspecting every node, \\(O(N)\\). In practice, searching a heap is not typically done in the use cases it is used for.\nReading a heap typically refers to accessing the root node, which is \\(O(1)\\).\n\n\n\n\nLast node: Insert the new node as the heap’s last node\nTrickle up the new node: Compare the new node to its parent. If it’s greater than its parent, swap them.\n\nThe number of steps to trickle is proportional to the depth of the tree. Therefore, insertion is a \\(O(log N)\\) operation.\n\n\n\nWe only ever delete the root node from a heap.\n\nNew root: The last node becomes the new root node.\nTrickle down the root node: Trickle the root down to its proper place.\n\nTrickling down is more complicated than trickling up because there are two possible directions to swap in. We compare to both children and swap with the larger of the two. This ensures the largest value ends up as the root node, which is the key property we want to preserve.\nDeletion is also \\(O(log N)\\) since the number of steps to trickle is again proprtional to the depth of the tree.\n\n\n\nAn alternative way of implementing a priority queue would be using an ordered array rather than a heap.\n\n\n\nOperation\nHeap Complexity\nArray Complexity\n\n\n\n\nRead*\n\\(O(1)\\)\n\\(O(1)\\)\n\n\nSearch^\n\\(O(N)\\)\n\\(O(N)\\)\n\n\nInsertion\n\\(O(log N)\\)\n\\(O(N)\\)\n\n\nDeletion\n\\(O(log N)\\)\n\\(O(1)\\)\n\n\n\n* Reading the root node\n^ Searching is not typically done on a heap\nComparing the complexities, heaps are fast, \\(O(log N)\\), for both insertions and deletions, wheres ordered arrays are faster for deletions but slower for insertions.\nAs this is in log space and priority queues typically perform similar numbers of inserts and deletes, on average this makes heap much faster.\n\n\n\nMany operations with heaps, such as insertion, rely on knowing where the last node is.\nThis is important because inserting/deleting the last node ensures the heap is always complete, and completeness is important to ensure the graph remains well-balanced.\nBeing well-balanced ensures the heap remains efficient. As in the case of binary search tree, if a tree is severely unbalanced it effectively becomes a linked list, so actions that should search through the depth of the tree in \\(O(log N)\\) time take \\(O(N)\\) in the worst case.\n\n\nHeaps are often implemented as arrays because the problem of the last node is so crucial, and accessing the last element of an array is \\(O(1)\\).\nThe tree below shows the values and their index in the corresponding array as value|index:\n\n\n\n\n\nflowchart TD\n\n    A[\"100|0\"] ---&gt; B[\"88|1\"]\n    A[\"100|0\"] ---&gt; C[\"25|2\"]\n\n    B[\"88|1\"] ---&gt; B1[\"87|3\"]\n    B[\"88|1\"] ---&gt; B2[\"16|4\"]\n\n    C[\"25|2\"] ---&gt; C1[\"8|5\"]\n    C[\"25|2\"] ---&gt; C2[\"12|6\"]\n\n\n\n\n\n\nThe array representation is then:\n\n\n\n0\n1\n2\n3\n4\n5\n6\n\n\n\n\n100\n88\n25\n87\n16\n8\n12\n\n\n\nThe indices are then deterministic. We can find the child index of any given node as:\nleft_child_idx = (parent_index * 2) + 1\nright_child_idx = (parent_index * 2) + 2\nTo find a given node’s parent index:\nparent_idx = (child_idx - 1) // 2\n\n\n\nIt is also possible to implement heaps as linked nodes.\nIn this case, a different trick is required to solve the problem of the last node using binary numbers.\n\nAssign Binary Numbers: Each level of the heap is assigned a unique binary number. The root is 0, then each left child is concatenates the parent’s number with 0 at the end, and each right child concatenates the parent’s index with 1 at the end.\nInsertion using Binary Representation: To insert a new node into the heap, you convert the index of the node into binary form. Starting from the most significant bit (MSB), traverse the heap according to the binary digits:\n\nIf the bit is 0, move to the left child.\nIf the bit is 1, move to the right child.\n\nInsertion at the Last Available Position: As you traverse the heap, you’ll eventually reach a node that doesn’t exist yet. This node represents the last available position in the heap, where the new node can be inserted while maintaining the complete binary tree property.\n\n\n\n\n\n\nflowchart TD\n\nA[\"100|0\"] ---&gt; B[\"88|00\"]\nA[\"100|0\"] ---&gt; C[\"25|01\"]\n\nB[\"88|00\"] ---&gt; B1[\"87|000\"]\nB[\"88|00\"] ---&gt; B2[\"16|001\"]\n\nC[\"25|01\"] ---&gt; C1[\"8|010\"]\nC[\"25|01\"] ---&gt; C2[\"12|011\"]\n\n\n\n\n\n\n\n\n\n\n\nThis is a kind of tree which is particularly useful for text-based features.\nEach trie node can have any number of children. Each word is split into characters which are stored as a series of nested child nodes. A terminal character is used to denote the end of a word.\n\n\n\n\n\nflowchart TD\n\nG(G) ---&gt; U(U) ---&gt; R(R) ---&gt; P(P) ---&gt; X(*)\n\n\n\n\n\n\n\n\nThere are two flavours of search: (1) checking if a substring is a valid prefix, and (2) checking if a substring is a valid whole word.\nThe algorithm below is for the more general case of searching for a prefix. Searching for a whole word then becomes trivial because we have a terminal character, so we can search for “Implement*” to find the whole word.\nIterate through the trie one character at a time:\n\nInitialise current_node at the root.\nIterate through each character of search_string and see if current_node has a child with that key.\nRepeat Step 3 for the whole search_string. If the character is not a valid key, the word does not exist.\n\nThe efficiency of a trie search depends on the length of the search term, \\(K\\), not the number of elements stored in the trie, \\(N\\). Therefore, it has complexity \\(O(K)\\).\n\n\n\nInserting a new word into a trie follows similar steps to searching for an existing word: we traverse the trie and insert new nodes where they do not already exist.\n\nInitialise current_node at the root.\nIterate through each character of search_string and see if current_node has a child with that key.\n\nIf the key exists, update current_node to move to that next node.\nOtherwise, create a new child node and update current_node to move to it.\n\nInsert the terminal character of the word at the end.\n\nThis is again \\(O(K)\\) since it depends on the length of the input, not the data stored in the trie.\n\n\n\nWe can implement a trie as dictionaries nested within dictionaries.\n\nclass TrieNode:\n    \n    def __init__(self):\n        self.children = {}\n        \n        \nclass Trie:\n    \n    def __init__(self):\n        self.root = TrieNode()\n        self._TERMINAL_CHAR = \"*\"\n        \n    def search(self, word):\n        \"\"\"Search for a given word in the Trie.\"\"\"\n        current_node = self.root\n        \n        for char in word:\n            if char in current_node.keys():\n                # Keep traversing as long as we find valid children\n                current_node = current_node[char]\n            else: \n                # The search_string is not a valid prefix, so return None\n                return None\n        \n        return current_node\n    \n    def insert(self, new_word):\n        \"\"\"Insert a new word into the Trie.\"\"\"\n        current_node = self.root\n\n        # Traverse the word + terminal character\n        for char in new_word + self._TERMINAL_CHAR:\n            if char in current_node.keys():\n                # Keep traversing as long as we find valid children\n                current_node = current_node[char]\n            else:\n                current_node[char] = {}\n                current_node = current_node[char]\n\n\n\n\nA good use case of tries is for autocomplete.\nWe use a trie to store our dictionary of possible words.\nThen for a given user input, we can recursively list all of the words with that prefix.\nWe can improve autocomplete further by storing an integer popularity value as the terminal value rather than an empty dictionary or null value. This can be a 1-10 score of how commonly used that word is, so that we can prioritise showing more common words as autocompletion suggestions."
  },
  {
    "objectID": "posts/software/data_structures_algos/data_structures/data_structures.html#data-structure-operations",
    "href": "posts/software/data_structures_algos/data_structures/data_structures.html#data-structure-operations",
    "title": "Data Structures",
    "section": "",
    "text": "There are common ways that we can interact with different data structures.\nIt is useful to frame the appropriateness of a data structure for a given based on the speed of the operations that are required most for that task.\n\nRead: Look up the value at a particular location in the data structure\nSearch: Look for a particular value in the data structure.\nInsert: Add a new value to the data structure.\nDelete: Remove a value from the data structure.\n\nReading is “find by key”, searching is “find by value”."
  },
  {
    "objectID": "posts/software/data_structures_algos/data_structures/data_structures.html#arrays",
    "href": "posts/software/data_structures_algos/data_structures/data_structures.html#arrays",
    "title": "Data Structures",
    "section": "",
    "text": "An array is a list of elements.\nIt is stored in memory as a block of contiguous memory addresses. When the array is declared, the head of the array is stored, i.e. the memory address of the first elelment.\nThe size of an array is the number of elements in the array. The index denotes where a particular piece of data resides in that array.\n\n\n\nOperation\nComplexity (Worst)\nComplexity (Best)\n\n\n\n\nRead\nO(1)\nO(1)\n\n\nSearch\nO(N)\nO(1)\n\n\nInsertion\nO(N)\nO(1)\n\n\nDeletion\nO(N)\nO(1)\n\n\n\n\n\nA computer can look up a given memory address in constant time.\nWe know the head of the array and the index to look up. So we can read in O(1) time.\nExample:\n\nHead of the array is memory address 1063.\nWe want to look up index 5.\nRead memory address 1068 (because 1063 + 5 = 1068).\n\n\nIf you were asked to raise your right pinky finger, you wouldn’t need to search through all of your fingers to find it\n\n\n\n\nA computer has immediate access to all of its memory addresses but does not know ahead of time their contents.\nSo to find a particular value, we will potentially have to search through every element.\nSearching an array is therefore O(N).\n\n\n\nThe efficiency of inserting into an array depends on where in the array you are inserting.\nIf inserting an element at the end, we simply place the new value at that memory address (assuming the memory address is empty). This is a constant time operation O(1).\nBut if we insert at any other position, we need to:\n\nShift each of the existing elements to the right of the insert index 1 position rightwards\nInsert the new value in the gap created.\n\nSo to insert at index \\(i\\), there are \\(N - i\\) elements to shift (Step 1), then 1 more operation to insert the new value (Step 2).\nIn the worst case - inserting at the start of an array - insertion is O(N).\n\n\n\nSimilarly, the efficiency of deletion depends on the index being deleted.\nIf deleting the last element, there is simply 1 operation to clear the memory address, so this is a constant time operation O(1).\nBut if we delete an element in any other position, we need to:\n\nDelete the element. This leaves a gap in the middle of the array.\nShift each of the elements to the right of the gap leftwards, to fill the gap.\n\nSo to delete at index \\(i\\), we do 1 operation to delete that element (Step 1), then shift the next \\(N-i\\) elements leftwards (Step 2).\nIn the worst case - deleting the first element of the array - deletion is O(N)."
  },
  {
    "objectID": "posts/software/data_structures_algos/data_structures/data_structures.html#sets",
    "href": "posts/software/data_structures_algos/data_structures/data_structures.html#sets",
    "title": "Data Structures",
    "section": "",
    "text": "A set is a collection of unique elements, i.e. duplicate values are not allowed.\nThere are different ways of implementing sets: array-based sets and hash-sets are discussed here.\nNote that Python already has sets, but we’ll give outline implementations for clarity.\n\n\nAn array is used to store elements. As with standard arrays, elements are stored in contiguous memory locations, and each element has a unique index.\nExample in Python:\n\nclass ArraySet:\n    \n    def __init__(self):\n        self.elements = []\n        \n    def __repr__(self):\n        return str(self.elements)\n\n    def add(self, element):\n        # Search the array for `element`, then append it if it is not a duplicate.\n        if element not in self.elements:\n            self.elements.append(element)\n\n    def remove(self, element):\n        # Search the array for the value, then remove it.\n        if element in self.elements:\n            self.elements.remove(element)\n\n    def contains(self, element):\n        return element in self.elements\n\n\nset_arr = ArraySet()\nset_arr.add(1)\nset_arr.add(2)\nset_arr.add(3)\nprint(set_arr)\n\n[1, 2, 3]\n\n\nIf we try to add a duplicate value it does not get added to the array:\n\nset_arr.add(2)\nprint(set_arr)\n\n[1, 2, 3]\n\n\nThe read, search and delete operations for an array-based set are identical to the standard array.\nInsert operations are where array-based sets diverge. We always insert at the end of a set, which is constant time. But we need to search the array every time to ensure the new value is not a duplicate.\nSo we always need to do a search of all N elements, and then 1 insert operation at the end.\nThis means even in the best case, insertion into an array-based set is O(N) compared to O(1) when inserting at the end of a standard array.\nThe reason for using a set is because the use case requires no duplicates, not because it is inherently “quicker” than a standard array.\n\n\n\nOperation\nComplexity (Worst)\nComplexity (Best)\n\n\n\n\nRead\nO(1)\nO(1)\n\n\nSearch\nO(N)\nO(1)\n\n\nInsertion\nO(N)\nO(N)\n\n\nDeletion\nO(N)\nO(1)\n\n\n\n\n\n\nA hash-based set computes the hash of each element and uses this to store elements.\nAn example implementation implements the set as key-value pairs where keys are the hash of the elements and values are a placeholder value like True, or an array to handle hash collisions.\nWhen there is a hash collision between mutliple elements, a typical approach is to insert all of these elements as an array under the same hash key.\nThe worst case scenario is caused by the extreme edge case where hash collisions are so prominent that every element has the same hash, essentially reducing the hash set to an array. This is generally avoided as long as the hash algorithm is decent.\nFor this reason, the average complexity is more meaningful in the table below. (Note that best has been replace with average in the table headings.)\nHash-based sets do not support reading by index, unlike array-based sets. But all other operations are typically constant time.\n\n\n\nOperation\nComplexity (Worst)\nComplexity (Average)\n\n\n\n\nRead\nN/A\nN/A\n\n\nSearch\nO(N)\nO(1)\n\n\nInsertion\nO(N)\nO(1)\n\n\nDeletion\nO(N)\nO(1)\n\n\n\nExample in Python:\n\nclass HashSet:\n    def __init__(self):\n        # Use a dict to represent the hash set.\n        self.elements = {}\n        \n    def __repr__(self):\n        return str(self.elements)\n\n    def add(self, element):\n        # The key is the element and the value is arbitrary.\n        # There are two extensions we could add here:\n        #   1. The key should really be the *hash* of the element, not just the element itself.\n        #      Essentially, this is using an implicit hash function which is just a pass-through:\n        #      hash_func = lambda x: x\n        #   2. Handle hash collisions by making the value an array which is appended to in the case of collisions.\n        self.elements[element] = True\n\n    def remove(self, element):\n        if element in self.elements:\n            del self.elements[element]\n\n    def contains(self, element):\n        return element in self.elements\n\n\nset_hash = HashSet()\nset_hash.add(1)\nset_hash.add(2)\nset_hash.add(3)\nprint(set_hash)\n\n{1: True, 2: True, 3: True}\n\n\nIf we try to add a duplicate value it simply overwrites the previous value:\n\nset_hash.add(2)\nprint(set_hash)\n\n{1: True, 2: True, 3: True}"
  },
  {
    "objectID": "posts/software/data_structures_algos/data_structures/data_structures.html#ordered-arrays",
    "href": "posts/software/data_structures_algos/data_structures/data_structures.html#ordered-arrays",
    "title": "Data Structures",
    "section": "",
    "text": "These are identical to regular arrays with the additional condition that elements are always ordered.\nThis obviously relies heavily on efficient sorting. This is a topic unto itself; see notes on sorting for more info.\nWhen inserting into an ordered array, we need to:\n\nSearch for the correct position - Look at each element in turn and compare if the insert element is greater than it\nInsert into the array\n\nThese two terms increase in opposite directions depending on the insert position. The further into the array we need to search (Step 1), the fewer elements we need to shift for the insertion (Step 2).\n\n\nIn a typical (unordered) array, the only option for searching is a linear search: we loop through each element in turn until we find our target.\nFor an ordered array, we can improve on this using a binary search.\n\nPick the middle element.\nIf the target value is greater than this, search the right half, otherwise search the left half.\nRepeat this recursively until we find our target.\n\nThis approach splits the search region in half for every constant time comparison operation.\nOr put another way, if we doubled the number of elements in the array, the binary search would only have to perform 1 extra step. For \\(N\\) elements we need \\(log_2(N)\\) binary splits.\nTherefore, the time complexity is O(log(N)).\n\ndef binary_search(ordered_array, target):\n    \"\"\"Perform a binary search for the target value on the given ordered array.\n\n    Parameters\n    ----------\n    ordered_array: list\n        The array to search in.\n    target: int\n        The target value we are searching for.\n\n    Returns\n    -------\n    target_index: int\n        The index of the target value.\n        Returns None if the value does not exist in the array.\n    \"\"\"\n    # Establish the lower and upper bounds of our search.\n    # Initially, this is the entire array\n    idx_lower = 0\n    idx_upper = len(ordered_array) - 1\n\n    while idx_lower &lt;= idx_upper:\n        # Find the midpoint between our bounds\n        idx_midpoint = (idx_upper + idx_lower) // 2\n        value_at_midpoint = ordered_array[idx_midpoint]\n\n        # Compare to our target value and narrow the upper or lower bound accordingly\n        if value_at_midpoint == target:\n            # We have found the target!\n            return idx_midpoint\n        elif value_at_midpoint &lt; target:\n            # The target is bigger so must be to the right side\n            idx_lower = idx_midpoint + 1\n        elif value_at_midpoint &gt; target:\n            # The target is smaller so must be on the left side\n            idx_upper = idx_midpoint - 1\n\n    # If the lower and upper bounds meet we have exhausted the whole array, so the target is not in the array\n    return None\n\nLet’s try this on a few examples.\n\nordered_array = [1, 2, 4, 5, 7, 8, 9, 10, 13, 14]\n\n\nbinary_search(ordered_array, 7)\n\n4\n\n\n\nbinary_search(ordered_array, 14)\n\n9\n\n\nNow a value that’s not in the array:\n\nbinary_search(ordered_array, 14)\n\n9\n\n\nCompare this with a linear search\n\ndef linear_search(array, target):\n    \"\"\"Perform a linear search for the target value on the given array.\n\n    Parameters\n    ----------\n    array: list\n        The array to search in.\n    target: int\n        The target value we are searching for.\n\n    Returns\n    -------\n    target_index: int\n        The index of the target value.\n        Returns None if the value does not exist in the array.\n    \"\"\"\n    # Loop through every element in the array.\n    # Note: we should really use enumerate() rather than range(len()) but I wanted to keep this generic \n    # without too many python-specific helpers\n    for idx in range(len(array)):\n        if array[idx] == target:\n            return idx\n    \n    # If we reach the end of the array without returning a value, then the target does not exist in the array.\n    return None\n\nLet’s compare how they perform for a reasonably big array with 1 million elements.\n\narray = [k for k in range(1000000)]\n\n\n%%timeit\nbinary_search(array, 987654)\n\n1.13 µs ± 56.9 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\n\n%%timeit\nlinear_search(array, 987654)\n\n15.9 ms ± 320 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nThe binary search is ~14000x faster than the linear search!"
  },
  {
    "objectID": "posts/software/data_structures_algos/data_structures/data_structures.html#hash-tables",
    "href": "posts/software/data_structures_algos/data_structures/data_structures.html#hash-tables",
    "title": "Data Structures",
    "section": "",
    "text": "Hash tables are key:value pairs. We can look up a the value for a given key in \\(O(1)\\) time.\nAlso known as hash maps, dictionaries, maps, associative arrays.\n\n\nThe process of taking characters and converting them to numbers is called hashing.\nThe code that performs this conversion is the hash function.\nA hash function requires one condition:\n\nConsistency: A hash function must convert the same string to the same number every time it’s applied.\n\nIn practice, for the hash function to be useful, it should also be collision resistant:\n\nCollision resistant: Different inputs should hash to different outputs.\n\nAs an extreme example, the following hash function is consistent but not collision resistant:\ndef crappy_hash(input_str: str) -&gt; int:\n    \"\"\"This is the world's worst hash function.\"\"\"\n    return 1\n\n\n\nWe want to insert the following key:value pair into our hash table:\nkey = \"Name\"\nvalue = \"Gurp\"\nLet’s say we have a hash function that is actually good, and in this particular case hash_function(key) returns 12.\nThe hash table will then insert value at memory address 12 (or more specifically, the memory address of the head of the dictionary + 12).\nThis means if we ever want to look up the key \"Name\", we hash it and immediately know to access memory address 12 and return the value \"Gurp\".\nSo hash table lookups are \\(O(1)\\).\nMore specifically, looking up by key is \\(O(1)\\). Searching by value is essentially searching through an array, so is \\(O(N)\\).\n\n\n\nA collision occurs when we try to add data to a cell that is already occupied.\nOne approach to handling this is called separate chaining.\nInstead of placing a single value in a cell, we place a pointer to an array. This array contains length-2 subarrays where the first element is the key and the second element is the value.\nIf there are no collisions, a hash table look up is \\(O(1)\\). In the worst case, ALL keys collide and so we essentially have to search through an array which is \\(O(N)\\).\n\n\n\nA hash tables efficiency depends on:\n\nHow much data we’re storing in it\nHow many cells are available\nWhich hash function we use\n\nA good hash function (3) should distribute the data (1) evenly across all cells (2).\nThis ensures the memory is used efficiently while avoiding collisions.\nThe load factor is the ratio of data to cells, and ideally should be around 0.7, i.e. 7 elements for every 10 cells.\n\n\n\nOperation\nComplexity (Worst)\nComplexity (Average)\n\n\n\n\nRead\nO(N)\nO(1)\n\n\nSearch\nO(N)\nKeys: O(1) Values: O(N)\n\n\nInsertion\nO(N)\nO(1)\n\n\nDeletion\nO(N)\nO(1)\n\n\n\nThe worst case corresponds to when all keys collide, reducing the hash table to an array effectively."
  },
  {
    "objectID": "posts/software/data_structures_algos/data_structures/data_structures.html#stacks",
    "href": "posts/software/data_structures_algos/data_structures/data_structures.html#stacks",
    "title": "Data Structures",
    "section": "",
    "text": "A stack is stored in memory the same as an array, but it has 3 constraints:\n\nData can only be inserted at the end (push)\nData can only be deleted from the end (pop)\nOnly the last element can be read (peek)\n\n\nRead, insert and delete can only happen at the end.\n\nThis makes them useful as Last-In First-Out (LIFO) data stores: the last item pushed the the stack is the first to be popped.\nExample in Python:\n\nclass Stack:\n\n    def __init__(self, initial_elements: list = []):\n        # We can pass a list to initialise the Stack\n        self.data = initial_elements\n\n    def __repr__(self):\n        return str(self.data)\n\n    def push(self, element):\n        self.data.append(element)\n\n    def pop(self):\n        return self.data.pop()\n\n    def peek(self):\n        if self.data:\n            return self.data[-1]\n\n\nstack = Stack([1, 2, 3, 4])\nstack\n\n[1, 2, 3, 4]\n\n\n\nstack.push(5)\nprint(stack)\n\n[1, 2, 3, 4, 5]\n\n\n\nstack.peek()\n\n5\n\n\n\nprint(stack)\n\n[1, 2, 3, 4, 5]\n\n\n\nstack.pop()\n\n5\n\n\n\nprint(stack)\n\n[1, 2, 3, 4]\n\n\nThe benefits of stacks, and other constrained data structures, are:\n\nPrevent potential bugs when using certain algorithms. For example, an algorithm that relies on stacks may break if removing elements from the middle of the array, so using a standard array is more error-prone.\nA new mental model for tackling problems. In the case of stacks, this is the LIFO approach.\n\nThe concept of stacks is a useful precursor to recursion, as we push to and pop from the end of a stack."
  },
  {
    "objectID": "posts/software/data_structures_algos/data_structures/data_structures.html#queues",
    "href": "posts/software/data_structures_algos/data_structures/data_structures.html#queues",
    "title": "Data Structures",
    "section": "",
    "text": "A queue is conceptually similar to a stack - it is a constrained array. This time, it is First-In First-Out (FIFO) like a queue of people; the first person to arrive is the first to leave.\nQueue restrictions:\n\nData can only be inserted at the end (enqueue)\nData can only be deleted from the front (dequeue)\nData can only be read from the front (peek)\n\nPoints (2) and (3) are the opposite of the stack.\n\nclass Queue:\n\n    def __init__(self, initial_elements: list = []):\n        # We can pass a list to initialise the Stack\n        self.data = initial_elements\n\n    def __repr__(self):\n        return str(self.data)\n\n    def enqueue(self, element):\n        self.data.append(element)\n\n    def dequeue(self):\n        return self.data.pop(0)\n\n    def peek(self):\n        if self.data:\n            return self.data[0]\n\n\nq = Queue([1, 2, 3, 4])\nq\n\n[1, 2, 3, 4]\n\n\n\nq.enqueue(5)\nprint(q)\n\n[1, 2, 3, 4, 5]\n\n\n\nq.dequeue()\n\n1\n\n\n\nprint(q)\n\n[2, 3, 4, 5]\n\n\n\nq.peek()\n\n2\n\n\n\nprint(q)\n\n[2, 3, 4, 5]"
  },
  {
    "objectID": "posts/software/data_structures_algos/data_structures/data_structures.html#linked-lists",
    "href": "posts/software/data_structures_algos/data_structures/data_structures.html#linked-lists",
    "title": "Data Structures",
    "section": "",
    "text": "A linked list represents a list of items as non-contiguous blocks of memory.\nIt is a list of items, similar to an array. But an array occupies a continuous block of memory.\n\n\n\nOperation\nComplexity (Worst)\nComplexity (Best)*\n\n\n\n\nRead\nO(N)\nO(1)\n\n\nSearch\nO(N)\nO(1)\n\n\nInsertion\nO(N)\nO(1)\n\n\nDeletion\nO(N)\nO(1)\n\n\n\nThe best case corresponds to operating on the head node.\nIn a linked list, each element is contained in a node that can be in scattered positions in memory. The node contains the data element and a “link” which is a pointer to the memory address of the next element.\nBenefits of a linked list over an array:\n\nMemory efficient: we don’t need a continuous block of memory\n\\(O(1)\\) inserts and deletes from the beginning of the list\nUseful when we want to traverse through a data structure while making inserts and deletes, because we do not have to shift the entire data structure each time as we would have to with an array\n\nA node contains two pieces of information:\n\n\n\nData\nLink\n\n\n\n\n“a”\n1666\n\n\n\nThese nodes can then be linked together in a list… a linked list!\n\n\n\n\n\nflowchart LR\n\n  A(\"'a'|1666\") --&gt; B(\"'b'|1984\") --&gt; C(\"'c'|1066\") --&gt; D(\"...\") --&gt; E(\"'z'|null\")\n\n\n\n\n\n\n\nThe link of the last node is null to indicate the end of the list.\n\n\nWe first need a node data structure, which will hold our data and a link to the next node.\nWe’ll point to the next node itself, rather than its memory address. This still has the same effect as nodes are scattered throughout different memory locations.\n\nclass Node:\n\n    def __init__(self, data, link=None):\n        self.data = data\n        self.link = link\n    \n    def __repr__(self) -&gt; str:\n        return f\"Data: {self.data}\\tLink: \\n{self.link}\"\n\nCreate some nodes and link them together\n\nnode1 = Node(\"a\")\nnode2 = Node(\"b\")\nnode3 = Node(\"c\")\nnode4 = Node(\"d\")\n\nThis is what a single node looks like:\n\nprint(node1)\n\nData: a Link: \nNone\n\n\nNow we link them\n\nnode1.link = node2\nnode2.link = node3\nnode3.link = node4\n\n\nnode1\n\nData: a Link: \nData: b Link: \nData: c Link: \nData: d Link: \nNone\n\n\n\n\n\nThe linked list simply keeps track of the head, i.e. the first node in the list.\nWhen using linked lists, we only have immediate access to this first node. For any other values, we need to start at the head node and traverse the list.\n\nclass LinkedList:\n\n    def __init__(self, head):\n        self.head = head\n\n    def __repr__(self) -&gt; str:\n        return str(self.head)\n\n\nll = LinkedList(node1)\nll\n\nData: a Link: \nData: b Link: \nData: c Link: \nData: d Link: \nNone\n\n\n\n\n\nWe start at the head an traverse the list until we reach the desired index.\nThis means they ar \\(O(N)\\) in the worst case.\n\nclass LinkedList:\n\n    def __init__(self, head):\n        self.head = head\n\n    def __repr__(self) -&gt; str:\n        return str(self.head)\n    \n    def read(self, index):\n        \"\"\"Read the node at the given index.\"\"\"\n        current_idx = 0\n        current_node = self.head\n\n        while (index &gt; current_idx):\n            if current_node.link is None:\n                # The index does not exist in the linked list, we have reached the end\n                return None\n            \n            current_node = current_node.link\n            current_idx += 1          \n\n        return current_node\n\n\nll = LinkedList(node1)\nll.read(2)\n\nData: c Link: \nData: d Link: \nNone\n\n\n\nll.read(10)\n\n\n\n\nTo search for a value, again we have to traverse the whole list.\nThis means the worst case complexity is \\(O(N)\\).\nThe mechanics of searching are the same as reading - we traverse the graph. The difference is we keep going until we find the value or reach the end of the list, rather than stopping at a predetermined index with read.\n\nclass LinkedList:\n\n    def __init__(self, head):\n        self.head = head\n\n    def __repr__(self) -&gt; str:\n        return str(self.head)\n    \n    def read(self, index):\n        \"\"\"Read the node at the given index.\"\"\"\n        # Start at the head\n        current_idx = 0\n        current_node = self.head\n\n        # Traverse the list until we find the desired index\n        while (index &gt; current_idx):\n            if current_node.link is None:\n                # The index does not exist in the linked list, we have reached the end\n                return None\n            \n            current_node = current_node.link\n            current_idx += 1          \n\n        return current_node\n    \n    def search(self, value):\n        \"\"\"Find the index of the given value.\"\"\"\n        # Start at the head\n        current_idx = 0\n        current_node = self.head\n\n        # Loop until we reach the None value which denotes the end of the list\n        while current_node:\n            if current_node.data == value:\n                # We've found our target value\n                return current_idx\n            # Try the next node\n            current_node = current_node.link\n            current_idx += 1\n\n        # We have traversed the whole list without finding a matching value\n        return None\n\n\nll = LinkedList(node1)\nll.search('c')\n\n2\n\n\n\n\n\nInserting a node into a linked list where we already have the current node is an \\(O(1)\\) operation.\n\nPoint to the next node. new_node.link = current_node.link\nLink from the previous node. current_node.link = new_node\n\nWith a linked list, we only have the head node, so we can insert at the start in \\(O(1)\\) time.\nBut to insert at any other point, we have to traverse there first (an \\(O(N)\\) operation) and then do the insert.\nThis is the key point of linked lists: insertion at the beginning is \\(O(1)\\) but at the end is \\(O(N)\\). This is the opposite of arrays, meaning linked lists are useful in cases where insertions are mostly at the beginning.\n\nclass LinkedList:\n\n    def __init__(self, head):\n        self.head = head\n\n    def __repr__(self) -&gt; str:\n        return str(self.head)\n    \n    def read(self, index):\n        \"\"\"Read the node at the given index.\"\"\"\n        # Start at the head\n        current_idx = 0\n        current_node = self.head\n\n        # Traverse the list until we find the desired index\n        while (index &gt; current_idx):\n            if current_node.link is None:\n                # The index does not exist in the linked list, we have reached the end\n                return None\n            \n            current_node = current_node.link\n            current_idx += 1          \n\n        return current_node\n    \n    def search(self, value):\n        \"\"\"Find the index of the given value.\"\"\"\n        # Start at the head\n        current_idx = 0\n        current_node = self.head\n\n        # Loop until we reach the None value which denotes the end of the list\n        while current_node:\n            if current_node.data == value:\n                # We've found our target value\n                return current_idx\n            # Try the next node\n            current_node = current_node.link\n            current_idx += 1\n\n        # We have traversed the whole list without finding a matching value\n        return None\n    \n\n    def insert(self, value, index):\n        \"\"\"Insert the value at the given index.\"\"\"\n        new_node = Node(value)\n\n        if index == 0:\n            # Link to the old head and update the linked lists head\n            new_node.link = self.head\n            self.head = new_node\n            return\n        \n        # Traverse the linked list until we find our node\n        current_node = self.head\n        current_idx = 0\n        while current_idx &lt; index - 1:\n            current_node = current_node.link\n            current_idx += 1\n\n        # Update the links to insert the new node\n        new_node.link = current_node.link\n        current_node.link = new_node\n        return \n\n\n        \n\nInsert a new head of our linked list\n\nll = LinkedList(node1)\nll\n\nData: a Link: \nData: b Link: \nData: c Link: \nData: d Link: \nNone\n\n\n\nll.insert('new_head', 0)\n\n\nll\n\nData: new_head  Link: \nData: a Link: \nData: b Link: \nData: c Link: \nData: d Link: \nNone\n\n\nInsert in the middle\n\nll.insert(\"I'm new here\", 3)\n\n\nll\n\nData: new_head  Link: \nData: a Link: \nData: b Link: \nData: I'm new here  Link: \nData: c Link: \nData: d Link: \nNone\n\n\n\n\n\nIt is quick to delete from the beginning of a linked list for the same reasons as insertion.\n\nMake the previous node point to the next next node\n\n\nclass LinkedList:\n\n    def __init__(self, head):\n        self.head = head\n\n    def __repr__(self) -&gt; str:\n        return str(self.head)\n    \n    def read(self, index):\n        \"\"\"Read the node at the given index.\"\"\"\n        # Start at the head\n        current_idx = 0\n        current_node = self.head\n\n        # Traverse the list until we find the desired index\n        while (index &gt; current_idx):\n            if current_node.link is None:\n                # The index does not exist in the linked list, we have reached the end\n                return None\n            \n            current_node = current_node.link\n            current_idx += 1          \n\n        return current_node\n    \n    def search(self, value):\n        \"\"\"Find the index of the given value.\"\"\"\n        # Start at the head\n        current_idx = 0\n        current_node = self.head\n\n        # Loop until we reach the None value which denotes the end of the list\n        while current_node:\n            if current_node.data == value:\n                # We've found our target value\n                return current_idx\n            # Try the next node\n            current_node = current_node.link\n            current_idx += 1\n\n        # We have traversed the whole list without finding a matching value\n        return None\n    \n\n    def insert(self, value, index):\n        \"\"\"Insert the value at the given index.\"\"\"\n        new_node = Node(value)\n\n        if index == 0:\n            # Link to the old head and update the linked lists head\n            new_node.link = self.head\n            self.head = new_node\n            return\n        \n        # Traverse the linked list until we find our node\n        current_node = self.head\n        current_idx = 0\n        while current_idx &lt; index - 1:\n            current_node = current_node.link\n            current_idx += 1\n\n        # Update the links to insert the new node\n        new_node.link = current_node.link\n        current_node.link = new_node\n        return \n    \n    def delete(self, index):\n        \"\"\"Delete the value at the given index.\"\"\"\n        if index == 0:\n            # We are deleting the head node, so point at the second node instead\n            self.head = self.head.link\n            return\n    \n        # Traverse the linked list until we find our node\n        current_node = self.head\n        current_idx = 0\n        while current_idx &lt; index - 1:\n            current_node = current_node.link\n            current_idx += 1\n\n        # Skip the next node (which we are deleting) and point ot its link instead\n        current_node.link = current_node.link.link\n        return       \n\n\nll = LinkedList(node1)\nll\n\nData: a Link: \nData: b Link: \nData: I'm new here  Link: \nData: c Link: \nData: d Link: \nNone\n\n\nDelete the head node\n\nll.delete(0)\nprint(ll)\n\nData: b Link: \nData: I'm new here  Link: \nData: c Link: \nData: d Link: \nNone\n\n\nDelete a middle node\n\nll.delete(1)\nprint(ll)\n\nData: b Link: \nData: c Link: \nData: d Link: \nNone\n\n\n\n\n\nA doubly linked list is a variant where each node contains pointers to the previous node and the next node.\n\n\n\nData\nPrevious\nNext\n\n\n\n\n“a”\nnull\n1666\n\n\n\nThe linked list tracks the head and tail.\nThis makes it quicker to read/insert/delete from either the beginning or end. We can also traverse backwards or forwards through the list.\n\n\n\n\n\nflowchart LR\n\n  A(\"'a'|null|1666\") &lt;---&gt; B(\"'b'|1234|1984\") &lt;--&gt; C(\"'c'|1666|1066\") &lt;--&gt; D(\"...\") &lt;--&gt; E(\"'z'|1993|null\")\n\n\n\n\n\n\n\nDoubly linked lists are a good data structure to use for queues, since we can insert/delete at either end."
  },
  {
    "objectID": "posts/software/data_structures_algos/data_structures/data_structures.html#binary-search-trees",
    "href": "posts/software/data_structures_algos/data_structures/data_structures.html#binary-search-trees",
    "title": "Data Structures",
    "section": "",
    "text": "We can have some use cases where we want to keep our data sorted.\nSorting is expensive, \\(O(N log N)\\) at the best of times, so we want to avoid sorting often. Ideally we would keep our data sorted at all times. An ordered array could do the job, but insertions and deletions are slow as we have to shift a chunk over the array every time.\nWe want a data structure that:\n\nMaintains order\nHas fast inserts, deletes and search\n\nThis is where a binary search tree comes in.\n\n\n\nOperation\nComplexity (Worst)*\nComplexity (Best)\n\n\n\n\nRead\nN/A\nN/A\n\n\nSearch\nO(N)\nO(log N)\n\n\nInsertion\nO(N)\nO(log N)\n\n\nDeletion\nO(N)\nO(log N)\n\n\n\n*The worst case corresponds to an imbalanced tree that is essentailly a linked list (a straight line). The best case is a perfectly balanced tree.\n\n\nTrees are another node-based data structure when each node can point to multiple other nodes.\n\n\n\n\n\nflowchart TD\n\n\n  A(a) --&gt; B(b)\n  A(a) --&gt; C(c)\n\n  B(b) --&gt; D(d)\n  B(b) --&gt; E(e)\n\n  C(c) --&gt; F(f)\n\n\n\n\n\n\n\nThe root is the uppermost node.\na is the parent of b and c; b and c are children of a.\nThe descendants of a node are all of its children and its children’s children’s children etc. The ancestors of anode are its parents and its parent’s parent’s parents etc.\nEach horizontal layer ofthe tree is a level.\nA tree is balanced if all of its subtrees have the same number of nodes.\n\n\n\n\nA binary tree is one in which each node can have at most 2 children.\nA binary search treemust abide by the following rules:\n\nEach node can have at most one “left” child and one “right” child\nA node’s left descendants are all smaller than the node. It’s right descendants are all larger.\n\n\n\n\n\nclass TreeNode:\n\n    def __init__(self, value, left=None, right=None):\n        self.value = value\n        self.left = left\n        self.right = right\n\n    def __repr__(self):\n        return f\"TreeNode with value: {self.value}\"\n\n\nclass Tree: \n\n    def __init__(self, root_node):\n        self.root_node = root_node\n\n    def __repr__(self) -&gt; str:\n        return f\"Tree with root: {self.root_node}\"\n\n\ntree_node2 = TreeNode('b')\ntree_node3 = TreeNode('c')\ntree_node1 = TreeNode('a', tree_node2, tree_node3)\n\ntree = Tree(tree_node1)\n\n\ntree_node1\n\nTreeNode with value: a\n\n\n\ntree_node1.left\n\nTreeNode with value: b\n\n\n\ntree_node1.right\n\nTreeNode with value: c\n\n\n\ntree\n\nTree with root: TreeNode with value: a\n\n\n\n\n\n\nDesignate a current node (start with the root) and inspect its value.\nIf the current node is our target, success! Stop here.\nIf the current node is smaller than our target, search the left subtree. If it’s larger, search the right subtree.\nRepeat until we find our target. If we reach the end of the subtree without finding the target, then the target is not in the tree.\n\n\n\n\nSearching a binary search tree is \\(O(log N)\\) in the best case (a perfectly balanced tree) since we narrow our search area by half on each step.\nThe worst case is a horribly imbalanced tree. Imagine a tree that only has left descendants. This is essentially a linked list, so searching through it means inspecting every element. Therefore the worst case complexity is \\(O(N)\\).\nSo searching a binary search tree is the same complexity as a binary search performed on an ordered array. Where trees differentiate themselves is on insertions and deletes.\n\n\n\n\nCompare our new_node to each value in the tree starting from the root. Like with search, follow the path to the left if new_node is lower or right if higher.\nTraverse until we find a node where the appropriate child (left or right) does not exist yet. Make new_node the child.\n\nThe order of insertion is important to maintain balance. Binary search trees work best when seeded with randomly sorted data, as this will typically end up quite well-balanced. Seeding the tree with sorted data leads to imbalanced trees.\nInsertion is \\(O(log N)\\) for balanced trees because we search for the correct place to insert and then perform an \\(O(1)\\) operation to insert it.\n\n\n\nDeleting a node is more complicated because it depends if the target node has 0, 1 or 2 children.\n\n\n\nNumber of Children\nAction\n\n\n\n\n0\nSimply delete the node\n\n\n1\nReplace the deleted node with its child\n\n\n2\nReplace the deleted node with the successor node\n\n\n\nThe successor node is the next largest descendant of the deleted node. More formally: the child node whose value is the least of all values that are greater than the deleted node.\nFinding the successor:\n\nVisit the right child of the deleted node.\nKeep visiting left children until there are no more elft children. This is the successor node.\nIf the successor node has a right child, that child takes the original place of the successor node. The successor node gets moved to replace the deleted node.\n\nDeletion is \\(O(log N)\\) for balanced trees because we search for the correct place to insert and then perform (potentially several) \\(O(1)\\) operations to delete the node and replace it with a child / successor.\n\n\n\nThe point of a binary search tree is to maintain order. We can traverse the elements in order with the following recursive algorithm.\n\nCall itself recursively on the node’s left child. This will repeat until we hit a node without a left child.\nVisit this node.\nCall itself recursviely on the node’s right child.\n\nSince we are visiting every element of the tree, this is necessarily \\(O(N)\\).\n\ndef traverse(tree_node):\n    \"\"\"Inorder traversal of a binary search tree.\"\"\"\n    if tree_node is None:\n        return\n    traverse(tree_node.left)\n    print(tree_node.value)\n    traverse(tree_node.right)"
  },
  {
    "objectID": "posts/software/data_structures_algos/data_structures/data_structures.html#heaps",
    "href": "posts/software/data_structures_algos/data_structures/data_structures.html#heaps",
    "title": "Data Structures",
    "section": "",
    "text": "A queue is a FIFO list that means data is inserted at the end but accessed/removed from the front.\nA priority queue is a list where deletions and access are the same as a regular queue, but insertions are like an ordered array. So the priority queue is always ordered.\nAn example use case is a triage system in a hospital: patients are seen based on severity, not just when they arrived.\nThis is an abstract data types that we could implement in multiple ways using different fundamental data types. E.g. we could implement using an ordered array, but insertions would be O(N).\nHeaps are another data structure that fit this use case well.\n\n\n\nThere a multiple kinds of heaps. As a starting point, we consider the binary max-heap, which is a special kind of binary tree.\nBinary max-heap conditions:\n\nHeap condition: Each node is greater than its children.\nThe tree must be complete.\n\nBecause the heap condition is true for each node, we can recursively reason that a node is greater than its children, and they are too, then a node is greater than all of its descendants.\nThe complete condition essentially means values are filled left-to-right, top-down with no holes. There are no empty positions anywhere except the bottom row, and the bottom row is filled from left to right. The last node is the rightmost node at its bottom level.\nThere is also a min-heap variant. The difference is trivial, aside from the heap condition inequality being reversed, the logic is the same.\n\n\n\n\nHeaps are weakly ordered.\nThe root node is the maximum value.\n\nWith a binary search tree, we know precisely whether a value is the left or right descendant of a node. E.g. 3 will be a left child of 100.\nBut in a max-heap, we don’t know whether 3 is to the left or right, only that it is a descendant rather than an ancestor.\nSearching would require inspecting every node, \\(O(N)\\). In practice, searching a heap is not typically done in the use cases it is used for.\nReading a heap typically refers to accessing the root node, which is \\(O(1)\\).\n\n\n\n\nLast node: Insert the new node as the heap’s last node\nTrickle up the new node: Compare the new node to its parent. If it’s greater than its parent, swap them.\n\nThe number of steps to trickle is proportional to the depth of the tree. Therefore, insertion is a \\(O(log N)\\) operation.\n\n\n\nWe only ever delete the root node from a heap.\n\nNew root: The last node becomes the new root node.\nTrickle down the root node: Trickle the root down to its proper place.\n\nTrickling down is more complicated than trickling up because there are two possible directions to swap in. We compare to both children and swap with the larger of the two. This ensures the largest value ends up as the root node, which is the key property we want to preserve.\nDeletion is also \\(O(log N)\\) since the number of steps to trickle is again proprtional to the depth of the tree.\n\n\n\nAn alternative way of implementing a priority queue would be using an ordered array rather than a heap.\n\n\n\nOperation\nHeap Complexity\nArray Complexity\n\n\n\n\nRead*\n\\(O(1)\\)\n\\(O(1)\\)\n\n\nSearch^\n\\(O(N)\\)\n\\(O(N)\\)\n\n\nInsertion\n\\(O(log N)\\)\n\\(O(N)\\)\n\n\nDeletion\n\\(O(log N)\\)\n\\(O(1)\\)\n\n\n\n* Reading the root node\n^ Searching is not typically done on a heap\nComparing the complexities, heaps are fast, \\(O(log N)\\), for both insertions and deletions, wheres ordered arrays are faster for deletions but slower for insertions.\nAs this is in log space and priority queues typically perform similar numbers of inserts and deletes, on average this makes heap much faster.\n\n\n\nMany operations with heaps, such as insertion, rely on knowing where the last node is.\nThis is important because inserting/deleting the last node ensures the heap is always complete, and completeness is important to ensure the graph remains well-balanced.\nBeing well-balanced ensures the heap remains efficient. As in the case of binary search tree, if a tree is severely unbalanced it effectively becomes a linked list, so actions that should search through the depth of the tree in \\(O(log N)\\) time take \\(O(N)\\) in the worst case.\n\n\nHeaps are often implemented as arrays because the problem of the last node is so crucial, and accessing the last element of an array is \\(O(1)\\).\nThe tree below shows the values and their index in the corresponding array as value|index:\n\n\n\n\n\nflowchart TD\n\n    A[\"100|0\"] ---&gt; B[\"88|1\"]\n    A[\"100|0\"] ---&gt; C[\"25|2\"]\n\n    B[\"88|1\"] ---&gt; B1[\"87|3\"]\n    B[\"88|1\"] ---&gt; B2[\"16|4\"]\n\n    C[\"25|2\"] ---&gt; C1[\"8|5\"]\n    C[\"25|2\"] ---&gt; C2[\"12|6\"]\n\n\n\n\n\n\nThe array representation is then:\n\n\n\n0\n1\n2\n3\n4\n5\n6\n\n\n\n\n100\n88\n25\n87\n16\n8\n12\n\n\n\nThe indices are then deterministic. We can find the child index of any given node as:\nleft_child_idx = (parent_index * 2) + 1\nright_child_idx = (parent_index * 2) + 2\nTo find a given node’s parent index:\nparent_idx = (child_idx - 1) // 2\n\n\n\nIt is also possible to implement heaps as linked nodes.\nIn this case, a different trick is required to solve the problem of the last node using binary numbers.\n\nAssign Binary Numbers: Each level of the heap is assigned a unique binary number. The root is 0, then each left child is concatenates the parent’s number with 0 at the end, and each right child concatenates the parent’s index with 1 at the end.\nInsertion using Binary Representation: To insert a new node into the heap, you convert the index of the node into binary form. Starting from the most significant bit (MSB), traverse the heap according to the binary digits:\n\nIf the bit is 0, move to the left child.\nIf the bit is 1, move to the right child.\n\nInsertion at the Last Available Position: As you traverse the heap, you’ll eventually reach a node that doesn’t exist yet. This node represents the last available position in the heap, where the new node can be inserted while maintaining the complete binary tree property.\n\n\n\n\n\n\nflowchart TD\n\nA[\"100|0\"] ---&gt; B[\"88|00\"]\nA[\"100|0\"] ---&gt; C[\"25|01\"]\n\nB[\"88|00\"] ---&gt; B1[\"87|000\"]\nB[\"88|00\"] ---&gt; B2[\"16|001\"]\n\nC[\"25|01\"] ---&gt; C1[\"8|010\"]\nC[\"25|01\"] ---&gt; C2[\"12|011\"]"
  },
  {
    "objectID": "posts/software/data_structures_algos/data_structures/data_structures.html#tries",
    "href": "posts/software/data_structures_algos/data_structures/data_structures.html#tries",
    "title": "Data Structures",
    "section": "",
    "text": "This is a kind of tree which is particularly useful for text-based features.\nEach trie node can have any number of children. Each word is split into characters which are stored as a series of nested child nodes. A terminal character is used to denote the end of a word.\n\n\n\n\n\nflowchart TD\n\nG(G) ---&gt; U(U) ---&gt; R(R) ---&gt; P(P) ---&gt; X(*)\n\n\n\n\n\n\n\n\nThere are two flavours of search: (1) checking if a substring is a valid prefix, and (2) checking if a substring is a valid whole word.\nThe algorithm below is for the more general case of searching for a prefix. Searching for a whole word then becomes trivial because we have a terminal character, so we can search for “Implement*” to find the whole word.\nIterate through the trie one character at a time:\n\nInitialise current_node at the root.\nIterate through each character of search_string and see if current_node has a child with that key.\nRepeat Step 3 for the whole search_string. If the character is not a valid key, the word does not exist.\n\nThe efficiency of a trie search depends on the length of the search term, \\(K\\), not the number of elements stored in the trie, \\(N\\). Therefore, it has complexity \\(O(K)\\).\n\n\n\nInserting a new word into a trie follows similar steps to searching for an existing word: we traverse the trie and insert new nodes where they do not already exist.\n\nInitialise current_node at the root.\nIterate through each character of search_string and see if current_node has a child with that key.\n\nIf the key exists, update current_node to move to that next node.\nOtherwise, create a new child node and update current_node to move to it.\n\nInsert the terminal character of the word at the end.\n\nThis is again \\(O(K)\\) since it depends on the length of the input, not the data stored in the trie.\n\n\n\nWe can implement a trie as dictionaries nested within dictionaries.\n\nclass TrieNode:\n    \n    def __init__(self):\n        self.children = {}\n        \n        \nclass Trie:\n    \n    def __init__(self):\n        self.root = TrieNode()\n        self._TERMINAL_CHAR = \"*\"\n        \n    def search(self, word):\n        \"\"\"Search for a given word in the Trie.\"\"\"\n        current_node = self.root\n        \n        for char in word:\n            if char in current_node.keys():\n                # Keep traversing as long as we find valid children\n                current_node = current_node[char]\n            else: \n                # The search_string is not a valid prefix, so return None\n                return None\n        \n        return current_node\n    \n    def insert(self, new_word):\n        \"\"\"Insert a new word into the Trie.\"\"\"\n        current_node = self.root\n\n        # Traverse the word + terminal character\n        for char in new_word + self._TERMINAL_CHAR:\n            if char in current_node.keys():\n                # Keep traversing as long as we find valid children\n                current_node = current_node[char]\n            else:\n                current_node[char] = {}\n                current_node = current_node[char]\n\n\n\n\nA good use case of tries is for autocomplete.\nWe use a trie to store our dictionary of possible words.\nThen for a given user input, we can recursively list all of the words with that prefix.\nWe can improve autocomplete further by storing an integer popularity value as the terminal value rather than an empty dictionary or null value. This can be a 1-10 score of how commonly used that word is, so that we can prioritise showing more common words as autocompletion suggestions."
  },
  {
    "objectID": "posts/software/ddia/chapter1/lesson.html#describe-load",
    "href": "posts/software/ddia/chapter1/lesson.html#describe-load",
    "title": "Designing Data Intensive Applications: Part 1",
    "section": "3.1. Describe Load",
    "text": "3.1. Describe Load\nWe first need to describe the load. This means summarising the system’s load with load parameters which depend on the system itself. This may be requests per second for a web server, read/write ratio for a database, number of concurrent users, cache hit rate etc.\n\nTwitter Example\nScaling can be made difficult by fan out. For exmaple, Twitter’s scaling challenge is not primarily due to tweet volume, but due to fan-out, whereby each user follows many people, and each user is followed by many people. When a tweet is published it needs to be updated on the tweeter’s timeline and update on each of their follower’s home screen.\nApproach (1) would be to store all data in a relational database and join the follows, tweets and users table on each tweet and timeline update.\nApproach (2) is to keep a cache for each user’s timeline, and insert new tweets into the relevant users’ caches when someone they follow tweets. This generally works well because there are far more reads than writes - more lurkers than tweeters.\nTwitter started with approach (1) then moved to (2). But for celebrities who have millions of followers, the fan out of approach (2) is prohibitively slow, so they use a hybrid of both where users with lots of followers use approach 1."
  },
  {
    "objectID": "posts/software/ddia/chapter1/lesson.html#describe-performance",
    "href": "posts/software/ddia/chapter1/lesson.html#describe-performance",
    "title": "Designing Data Intensive Applications: Part 1",
    "section": "3.2. Describe Performance",
    "text": "3.2. Describe Performance"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html",
    "href": "posts/software/system_design/system_design_notes.html",
    "title": "System Design Notes",
    "section": "",
    "text": "Steps:\n\nRequirements engineering\nCapacity estimation\nData modeling\nAPI design\nSystem design\nDesign discussion\n\n\n\nFunctional and non-functional requirements. Scale of the system.\n\n\nWhat the system should do.\n\nWhich problem does the system solve\nWhich features are essential to solve these problems\n\nCore features: bare minimum required to solve the user’s problem. Support features: features that help the user solve their problem more conveniently.\n\n\n\nWhat the system should handle.\n\n\n\n\n\n\nInterview Tips\n\n\n\n\nIdentify the non-functional requirements the interviewer cares most about\nShow awareness of system attributes, trade-offs and user experience\n\n\n\nSystem analysis:\n\nRead or write heavy?\n\nImpacts requirements for database scaling, redundancy of services, effectiveness of caching.\nSystem priorities - is it worse if the system fails to read or to write?\n\nMonolithic or distributed architecture?\n\nScale is the key consideration. Large scale applications must be distributed, smaller scale can be single server.\n\nData availability vs consistency\n\nCAP trilemma - A system can have only two of: Consistency, Availability, Partition tolerance\nChoice determines the impact of a network failure\n\n\nNon-functional requirements:\n\nAvailability - How long is the system up and running per year?\n\nAvailability of a system is the product of its components’ availability\nReliability, redundancy, fault tolerance\n\nConsistency - Data appears the same on all nodes regardless of the user and location.\n\nLinearizability\n\nScalability - Ability of a system to handle fast-growing user base and temporary load spikes\n\nVertical scaling - single server that we add more CPU/RAM to\n\nPros: fast inter-process communication, data consistency\nCons: single point of failure, hardware limits on maximum scale\n\nHorizontal scaling - cluster of servers in parallel\n\nPros: redundancy improves availability, linear cost of scaling\nCons: complex architecture, data inconsistency\n\n\nLatency - Amount of time taken for a single message to be delivered\n\nExperienced latency = network latency + system latency\nDatabase and data model choices affect latency\nCaching can be effective\n\nCompatibility - Ability of a system to operate seamlessly with other software, hardware and systems\nSecurity\n\nBasic security measures: TLS encrypted network traffic, API keys for rate limiting\n\n\nNon-functional requirements depend heavily on expected scale. The following help quantify expected scale, and relate to capacity estimation in the next step.\n\nDaily active users (DAU)\nPeak active users\nInteractions per user - including read/write ratio\nRequest size\nReplication factor - determines the storage requirement (typically 3x)\n\n\n\n\n\n\n\n\n\n\n\nInterview Tips\n\n\n\n\nSimplify wherever you can - simple assumptions and round heavily\nConvert all numbers to scientific notation\nKnow powers of 2\n\n\n\n\n\nRequests per second (RPS) is the key measure.\nRequests per day = Daily active users * Activities per user\nRequests per second = RPD / 10^5\nPeak load = RPS * Peak load factor\nRead/write ratio is important to get the total requests. Usually, information on either reads or writes is missing and must be inferred using the other.\nTotal RPS = Read RPS + Write RPS\n\n\n\nThe amount of data that can be transmitted between two nodes in a fixed period of time. Bits per second.\nThroughput, bandwidth and latency are all related and can be thought of with the motorway analogy.\n\nThroughput is the total number of vehicles that must pass through that road per day.\nBandwidth is the number of lanes.\nLatency is the time taken to get from one end of the road to another.\n\nThe bandwidth must be large enough to support the required throughput.\nBandwidth = Total RPS * Request size\nEven high bandwidth can result in low latency if there is an unexpectedly high peak throughput. Rush hour traffic jam.\nRequest size can be varied according to bandwidth, e.g. YouTube lowers the resolution if the connection is slow.\n\n\n\nMeasured in bits per second using the Write RPS. Long term storage (assuming the same user base) is also helpful.\nStorage capacity per second = Write RPS * Request size * Replication factor\nStorage capacity in 5 years = Storage capacity per second * 2*10^8 \n\n\n\n\nKey concepts are: entities, attributes, relations.\nOptimisation discussions are anchored by the data model.\nRelational databases: denormalization, SQL tuning, sharding, federation\nNon-relational databases: indexing, caching\nThe data model is not a full-blown data schema (which only applies to relational databases) but a more informal, high-level version.\nMust haves:\n\nDerive entities and attributes.\nDraw connections between entities. Then:\nSelect a database based on the requirements\nExtend data model to a proper data schema\nIdentify bottlenecks and apply quick fixes Later:\nDefer any detailed discussions to the design discussion section.\n\n\n\nEntities are “things” with a distinct and independent existence, e.g. users, files, tweets, posts, channels. Create a unique table for each entity.\nAttributes describe properties of an entity, e.g. a user’s name, date of birth. These are the fields in the table of that entity.\n\n\n\n\n\n\nInterview Tips\n\n\n\n\nDo not focus on irrelevant details\nDo not miss any critical functionality\nDouble check all tables after you extract all info from requirements\n\n\n\n\n\n\nExtract connections between entities from the requirements. Relationships can be one-to-one, one-to-many or many-to-many.\n\n\n\n\nSpecify the API endpoints with:\n\nFunction signature\nParameters\nResponse and status codes\n\nThis specifies a binding contract between the client and the application server. There can be APIs between every system component, but just focus on the client/server interface.\nProcess:\n\nRevisit the functional requirements\nDerive the goal of each endpoint\nDerive the signature from the goal and the inputs from the data model\nDefine the response outputs\n\nNaming conventions. Use HTTP request types in the call signature where appropriate: GET, POST, PUT, PATCH, DELETE\n\n\n\n\n\n\nInterview Tips\n\n\n\n\nAvoid vague inputs\nDon’t add extra parameters that are out of scope\nAvoid redundant parameters\nDon’t miss any parameters\nConfirm the output repsonse satisfies the requirements\nIdentify inefficiencies of the data structure\n\n\n\n\n\n\nSystem components:\n\nRepresentation layer\n\nWeb app\nMobile app\nConsole\n\nService (and load balancer)\nData store\n\nRelational database\n\nPros: linked tables reduce duplicate data; flexible queries\n\nKey-value store\n\nUseful for lots of small continuous reads and writes\nPros: Fast access, lightweight, highly scalable\nCons: Cannot query by value, no standard query language so data access in written in the application layer no data normalisation\n\n\n\nScaling:\n\nScale services by having multiple instances of the service with a load balancer in front\nScale database with federation (functional partitioning) - split up data by function\n\nSystem diagrams: arrows point in direction of user flow (not data flow)\n\n\n\nTypes of questions:\n\nNFR questions\n\nHow to achieve scalability\n\nIdentify each bottleneck and remedy it\n\nHow to improve resiliency\n\nJustification question\n\nChoice of components, e.g. what would change if you changed block storage for object storage or relational database\nAdditional components, e.g. is there a use case for a message queue and where\n\nExtension questions\n\nAdditional functional requirements not in the original scope\n\n\n\n\n\n\n\nThere are two parameters we can vary:\n\nLength of the key - should be limited so the URL is short enough\nRange of allowed characters - should be URL-safe\n\nUsing Base-64 encoding as the character range, then a 6-digit key length gives enough unique URLs; 64^6 = 68 billion\nEncoding options: 1. MD5 hash: not collision resistant and too long 2. Encoded counter, each URL is just an index int, and its value is the base-64 encoding of that int: length is not fixed and increases over time 3. Key range, generate all unique keys beforehand: no collisions, but storage intensive 4. Key range modified, generate 5-digit keys beforehand, then add the 6th digit on the fly.\n\n\n\n\n\n4 main operations (CRUD): Create, Read, Update, Delete.\nTransactions - group several operations together\n\nEither the entire transaction succeeds or it fails and rolls back to the initial state. Commit or abort on error.\nWithout transactions, if an individual operation fails, it can be complex to unwind the related transactions to return to the initial state.\nError handling is simpler as manual rollback of operations is not required.\nTransactions provide guarantees so we can reason about the database state before and after the transaction.\n\nTransactions enforce “ACID” guarantees:\n\nAtomicity - Transactions cannot be broken down into smaller parts.\nConsistency - All data points within the database must align to be properly read and accepted. Raise a consistency error if this is not the case. Database consistency is different from system consistency which states the data should be the same across all nodes.\nIsolation - Concurrently executing transactions are isolated from each other. Avoids race conditions if multiple clients are accessing the same database record.\nDurability - Once a transaction is committed to the database, it is permanently preserved. Backups and transaction logs can restore committed transactions in the case of a failure.\n\nRelational databases are a good fit when:\n\nData is well-structured\nUse case requires complex querying\nData consistency is important\n\nLimitations of relational databases:\n\nHorizontal scaling is complex\nDistributed databases are more complex to keep transaction guarantees\n\nTwo phase commit protocol (2PC):\n\nPrepare - Ask each node if it;s able to promise to carry out the transaction\nCommit - Block the nodes and do the commit\n\nBlocking causes problems on distributed systems where it may cause unexpected consequences if the database is unavailable for this short time.\n\n\n\n\n\nExamples of non-relational databases:\n\nKey-value store\nDocument store\nWide-column store\nGraph store\n\nThese vary a lot, and in general NoSQL simply means “no ACID guarantees”.\nTransactions were seen as the enemy of scalability, so needed to be abandoned completely for performance and scalability. ACID enforces consistency for relational databases; for non-relational databases there is eventual consistency.\nBASE:\n\nBasically Available - Always possible to read and write data even though it may not be consistent. E.g. reads may not reflect latest changes, writes may not be persisted.\nSoft state - Lack of consistency guarantees mean data state may change without any interactions with the application as the database reaches eventual consistency.\nEventual consistency - Data will eventually become consistent once inputs stops.\n\nBenefits:\n\nWithout atomicity constraint, overheads like 2PC are not required\nWithout consistency constraint, horizontal scaling is trivial\nWithout isolation constraint, no blocking is required which improves availability.\n\nNon-relational databases are a good fit when:\n\nLarge data volume that isn’t tabular\nHigh availability requirement\nLack of consistency across nodes is acceptable\n\nLimitations:\n\nConsistency is necessary for some use cases\nLack of standardisation\n\n\n\n\n\nProblem: if we have large files with small changes, we don’t want to have to re-upload and re-download the whole file every time something changes.\nSolution: identify the changes and only push/pull these. Similar to git.\nThe rsync algorithm makes it easier to compare changes by:\n\nSplit the file into blocks\nCalculate a checksum for each block (using MD5 hashing algorithm)\nTo compare files between the client and the server, we only need to send the hashes back and forth\nFor the mismatching hashes, transfer the corresponding blocks\n\n\n\n\nOne-way communication that broadcasts file updates one-to-many.\n\n\nThese are synchronous.\nPolling is an example of a pull API. We periodically query the server to see if there are any updates. If not, the server responds immediately saying there is no new data. This may result in delayed updates and can overwhelm the server with too many requests.\n\n\n\n\n\n\nFigure 1: Polling\n\n\n\n\n\n\nThese are asynchronous.\nLong-polling. The client connects to the server and makes a request. Instead of replying immediately, the server waits until there is an update and then sends the response. If there is no update for a long time, the connection times out and the client must reconnect. Server resources are tied up until the connection is closed, even if there is no update.\n\n\n\n\n\n\nFigure 2: LongPolling\n\n\n\nWebsockets. The client establishes a connection using an HTTP request and response. This establishes a TCP/IP connection for 2-way communication. This allows for real-time applications without long-polling.\n\n\n\n\n\n\nFigure 3: Web Sockets\n\n\n\nServer sent events. Event-based approach. EventSource API supported by all browsers. The connection is 1-directional; the client can only pass data to the server at the point of connection. After that, only the server can send data to the client. The server doesn’t know if the client loses connection.\n\n\n\n\n\n\nFigure 4: SSE\n\n\n\n\n\n\n\nA system component that implements asynchronous communication between services, decoupling them. An independent server which producers and consumers connect to. AKA message queues.\nMessage brokers persist the messages until there are consumers ready to receive them.\nMessage brokers are similar in principle to databases since they persist data. The differences are they: automatically delete messages after delivery, do not support queries, typically have a small working set, and notify clients about changes.\nRabbitMQ, Kafka and Redis are open source message brokers. Amazon, GCP and Azure have managed service implementations.\n\n\nA one-to-one relationship between sender and receiver. Each message is consumed by exactly one receiver.\nThe message broker serves as an abstraction layer. The producer only sends to the broker. The receiver only receives from the broker. The services do not need to know about one another. The broker also guarantees delivery, so the message does not get lost if the consumer is unavailable, it will retry.\n\n\n\n\n\n\nFigure 5: Point-to-Point\n\n\n\n\n\n\nA one-to-many relationship between sender and receiver. The publisher publishes to a certain topic. Any consumer who is interested can then subscribe to receive that message.\n\n\n\n\n\n\nFigure 6: Pub-Sub\n\n\n\n\n\n\n\n\n\nFiles are stored in folders, with metadata about creation time and modification time.\nBenefits:\n\nSimplicity - simple and well-known pattern\nCompatibility - works with most applications and OSes\nLimitations:\nPerformance degradation - as size increases, the resource demands increase.\nExpensive - although cheap in principle, they can get expensive when trying to work around the performance issues.\n\nUse cases: data protection, local archiving, data retrieval done by users.\n\n\n\nA system component that manages data as objects in a flat structure. An object contains the file and its metadata.\nBenefits:\n\nHorizontal scalability\n\nLimitations:\n\nObjects must be edited as a unit - degrades performance if only a small part of the file needs to be updated.\n\nUse cases: large amounts of unstructured data.\n\n\n\nA system component that breaks up and then stores data as fixed-size blocks, each with a unique identifier.\nBenefits:\n\nHighly structured - easy to index, search and retrieve blocks\nLow data transfer overheads\nSupports frequent data writes without performance degradation\nLow latency\n\nLimitations:\n\nNo metadata, so metadata must be handled manually\nExpensive\n\n\n\n\nA subclass of key-value stores, which hold computer-readable documents, like XML or JSON objects.\n\n\n\n\nIssues:\n\nMassive files: 4TB for 4 hours of 4K video\n\nHow can we reliably upload very large files?\n\nLots of different end-user clients/devices/connection speeds\n\nHow can we process and store a range of versions/resolutions?\n\n\nProcessing pipeline:\n\n\n\n\n\n\nFigure 7: Video Processing Pipeline\n\n\n\n\nFile chunker\n\nSame principle as the file-sharing problem.\nSplit the file into chunks, then use checksums to determine that files are present and correct.\nIf there is a network outage, we only need to send the remaining chunks.\n\nContent filter\n\nCheck if the video complies with the platform’s content policy wrt copyright, piracy, NSFW\n\nTranscoder\n\nData is decoded to an uncompressed format\nThis will fan out in the next step to create multiple optimised versions of the file\n\nQuality conversion\n\nConvert the uncompressed file into multiple different resolution options.\n\n\nArchitecture considerations:\n\nWait for the full file to upload before processing? Or process each chunk as it is uploaded?\nAn upload service persists chunks to a database. Object store is a good choice as in the file-sharing example.\nOnce a chunk is ready to be processed, it can be read and placed in the message queue for the processing pipeline.\nThe processing pipeline handles fixed-size chunks, so the hardware requirements are known ahead of time. A chunk can be processed as soon as a hardware unit becomes available.\n\n\n\n\n\n\n\nFigure 8: Video Upload Architecture\n\n\n\n\n\n\nThe challenges are similar to that of file-sharing, as this is essentially large files being transferred. Latency and processing power of the end-user device.\nTwo main transfer protocols:\n\nUDP\nTCP\n\n\n\nA connection-less protocol. Nodes do NOT establish a stable end-to-end connection before sending data. Instead, each packet has its destination address attached so the network can route it to the correct place.\nAdvantages:\n\nFast\nPackets are routed independently so if some are lost the others can still reach their destination\n\nDisadvantages:\n\nNo guarantee that data will arrive intact.\n\nUse cases:\n\nLow-latency applications like video conferencing or gaming.\nNot suitable for movie or audio streaming, as missing bits cannot be tolerated.\n\n\n\n\nA connection is established between 2 nodes. The nodes agree on certain parameters before any data is transferred. Parameters: IP addresses of source and destination, port numbers of source and destination.\nThree-way handshake establishes the connection:\n\nSender transfers their sequence number to the Receiver.\nReceiver acknowledges and sends its own sequence number.\nSender acknowledges.\n\nAdvantages:\n\nReliability; guarantees delivery and receives acknowledgements before further packets are sent.\nReceiver acknowledgements ensure the receiver is able to process/buffer the data in time before more packets are sent.\n\nDisadvantages:\n\nSlower due to error checking and resending lost packets.\nRequires three-way handshake to establish connection which is slower.\n\nUse cases:\n\nMedia streaming (HTTP live-streaming or MPEG-DASH)\n\nAdaptive bitrate streaming\n\nTCP adjusts transmission speed in response to network conditions\nWhen packet loss is detected, smaller chunks are sent at a lower transmission speed. The lower resolution file can be sent in this case.\n\n\n\n\n\nPerceived latency = Network latency + System latency\n\n\nCaching is the process of storing copies of frequently accessed data in a temporary storage location.\nCaches utilise the difference in read performance between memory and storage. Times to fetch 1MB from:\n\nHDD: 3 ms\nSSD: 0.2 ms\nRAM: 0.01 ms\n\nWhen a user requests data it first requests from the cache\n\nCache HIT: If the data is in the cache, return it\nCache MISS: If the data is not in the cache, pass the request to the database, update the cache and return the data\n\nA cache can grow stale over time. There are 2 common approaches to mitigate this:\n\nSet up a time-to-live (TTL) policy within the cache\n\n\nTrade off between short TTL (fresh data but worse performance) vs long TTL (data can be stale)\n\n\nImplement active cache invalidation mechanism\n\n\nComplicated to implement. Requires an “invalidation service” component to monitor and read from the database, then update the cache when necessary.\n\nApplication-level caching. Insert caching logic into the application’s source code to temporarily store data in memory.\nTwo approaches to application-level caching:\n\nQuery-based implementation\n\nHash the query as a key and store the value against the key in a key-value store.\nLimitations: hard to delete cached result with a complex query; if a cell changes then all cached query which might include it need updating.\n\nObject-based implementation\n\nThe result of a query is stored as an object\nBenefits: only one serialisation/deserialisation overhead when reading/writing; complexity of object doesn’t matter, serialized objects in cache can be used by multiple applications.\n\n\nPopular implementations: Redis, Memcache, Hazlecast\n\n\n\nA network of caching layer in different locations (Points of Presence, PoPs). Full data is stored on SSD and most frequently accessed data is stored in RAM.\nDon’t cache dynamic or time-sensitive data.\nInvalidating the cache. You can manually update the CDN, but there may be additional caches at the ISP- and browser-level which would still serve stale data.\n\n\n\n\n\n\nFigure 9: CDN Invalidation\n\n\n\nApproaches to invalidating cache:\n\nCaching headers - set a time when an object should be cleared, e.g. max-age=86400 caches for a day.\nCache busting - change the link to all files and assets, so the CDN treats them like new files.\n\n\n\n\n\nThese are specialised NoSQL databases with the following benefits:\n\nCan match even with typos or non-exact matches\nFull-text search means you can suggest autocomplete results and related queries as the user types\nIndexing allows faster search performance on big data.\n\nThe database is structures as a hashmap where the inverted index points at documents.\n\nDocument - Represents a specific entity, like a row in a relational database\nInverted index - Maps from content to the documents that include it. The inverted index splits each document into individual search terms and makes each point to the document itself.\nRanking algorithm - Produces a list of possibly relevant documents. Additional factors may impact this, like a user’s previous search history.\n\nThe database is a dictionary of {search_term_id: [document_ids, …]}\n\n\n\n\n\n\nFigure 10: Inverted Index\n\n\n\nPopular implementations: Lucene, Elastic search, Apache solr, Atas search (MongoDB), Redis search.\nBenefits of search engine databases:\n\nScalable - NoSQL so scale horizontally\nSchemaless\nWorks out of the box - just need ot decide upfront what attributes should be searchable.\n\nLimitations:\n\nNo ACID guarantees.\nNot efficient at reading/writing data, so requires another database to manage state.\n\nMaintaining consistency between the main database and the search engine database can be tricky.\n\nSome SQL databases have full-text search so may not require a dedicated search engine database.\n\n\n\n\n\n\n\nAn app that lets a user add and delete items from a todo list.\n\nRepresentation layer is a web app\nMicroservices handle the todo CRUD operations and the user details separately\n\nEach service is scaled horizontally, so a load balancer is placed in front of it\n\nRelational database stores data\n\nThere are two databases, one per service, which is an example of functional partitioning. This means todo service I/O does not interfere with user service I/O and vice versa.\n\n\n\n\n\n\n\n\nFigure 11: To-do App System Design\n\n\n\n\n\n\nTake an input URL and return a unique, shortened URL that redirects to the original.\nPlanned system architecture:\n\nPre-generate all 5 digit keys (1 billion entries rather than 64 billion for 6 digits)\nSystem retrieves 5-difit keys from database\nSystem appends 1 out of 64 characters\nThe system is extendable because if we run out of keys we can append 2 more characters rather than 1\n\n\n\nSystem analysis:\n\nSystem is read heavy - Once a short URL is created it will be read multiple times\nDistributed system as this has to scale\nAvailability &gt; Consistency - not so much of a concern if a link isn’t available to all users at the same time, but they must be unique and lead to their destination.\n\nRequirements engineering:-\nCore feature:\n\nA user can input a URL of arbitrary length and receive a unique short URL of fixed size.\nA user can navigate to a short link and be redirected to the original URL.\n\nSupport features:\n\nA user can see their link history of all created short URLs.\nLifecycle policy - links should expire after a default time span.\n\nNon-functional requirements:\n\nAvailability - the system should be available 99% of the time.\nScalability - the system should support billions of short URLs, and thousands of concurrent users.\nLatency - the system should return redirect from a short URL to the original in under 1 second.\n\nQuestions to capture scale:\n\nDaily active users, and how often do users interact per day\n\n100 million, 1 interaction per day\n\nPeak active users - are there events that lead to traffic spikes?\n\nNo spikes\n\nRead/write ratio\n\n10 to 1\n\nRequest size - how long are the originals URLs typically\n\n200 characters =&gt; 200 Bytes\n\nReplication factor\n\n1x - ignore replication\n\n\nCapacity estimation:-\n\nRequests per second\n\nReads per second = DAU * interactions per day / seconds in day = 10^8 * 1 / 10^5 = 1000 reads/s\nWrites per second = Reads per second / read write ratio = 1000 / 10 = 100 writes/s\nRequests per second = Reads per second + Writes per second = 1000 + 100 = 1100 requests/s\nNo peak loads to consider\n\nBandwidth\n\nBandwidth = Requests per second * Message size\nRead bandwidth = 1000 * 200 Bytes = 200kB/s\nWrite bandwidth = 100 * 200 Bytes = 20 kB/s\n\nStorage\n\nStorage per year = Write bandwidth * seconds per year * Replication factor = 20000 Bytes * (360024365) * 1 = 630 GB\n\n\nData model:-\nIdentify entities, attributes and relationships.\nEntities and attributes:\n\nLinks: Key (used to create short URL), Original URL, Expiry date\nUsers: UserID, Links\nKey ranges: Key range, In use (bool)\n\nRelationships:\n\nUsers own Links\nLinks belong to Key ranges\n\nData stores:\n\nUsers\n\nUser data is typically relational and we rarely want it all returned at once.\nConsistency is important as we want the user to have the same experience regardless of which server handles their log in, and don’t want userIds to clash.\nRelational database.\n\nLinks\n\nNon-functional requirements of low latency and high availability.\nData and relationships are not complex.\nKey-value store.\n\nKey ranges\n\nFavour data consistency as we don’t want to accidentally reuse the same key range.\nFiltering keys by status would help to find available key ranges.\nRelational database.\n\n\nAPI design:-\nEndpoints:\n\ncreateUrl(originalUrl: str) -&gt; shortUrl\n\nresponse code 200, {shortURL: str}\n\ngetLinkHistory(userId: str, sorting: {‘asc’, ‘desc’})\n\nresponse code 200, {links: [str], order: ‘asc’}\n\nredirectURL(shortUrl: str) -&gt; originalUrl\n\nresponse code 200, {originalUrl: str}\n\n\nSystem design:-\nUser flows for each of the required functional requirements.\n\n\n\n\n\n\nFigure 12: URL Shortener System Design\n\n\n\n\n\n\n\nRequirements engineering\nRequirements gathering:\n\nRead-heavy\nDistributed\nData consistency is more important than availability - files should be consistent for all users\n\nCore functional requirements:\n\nA user can upload a file to the server\nA user can download their files from the server\n\nSecondary functional requirements:\n\nA user can see a history of files uploaded and downloaded\nA user can see who has downloaded a specific file and when\n\nNon-functional requirements:\n\nConsistency - data should be consistent across users and devices\nResilience - customer data should never be lost\nMinimal latency\nCompatibility across different devices\nSecurity - multi-tenancy; customer data should be separate of one another\n\nQuestions to determine scale:\n\nDaily active users\nPeak active users - what events lead to a spike?\nInteractions per user - how many file syncs, how many files does a user store\nRequest size - how large are files?\nRead/write ratio\nReplication factor\n\nCapacity estimation\nThroughput\n\nPeak write RPS = 2 peak load ratio * 10^8 users * 2 files/day / 10^5 seconds = 4000 RPS\nPeak read RPS = 10 read/write ration * 4000 write RPS = 40000 RPS\nPeak total RPS = 44000 RPS\n\nBandwidth\n\nWrite bandwidth = 4000 Write RPS * 100 kB Request size = 400 MB/s\nRead bandwidth = 40000 Read RPS * 100 kB request size = 4 GB/s\nTotal bandwidth = Write bandwidth + Read bandwidth = 4.4 GB/s\n\nStorage\n\nStorage capacity = 5 * 10^8 Total users * 100 files per user * 1MB File Size * 3 Replication factor = 15*10^10 MB = 15000 TB\n\nData model\nUsers: Store Ids for the files that belong to them\nFiles: Metadata on the owner, file edit history, filename, size, chunks that comprise the file, version history\nAPI Design\nEndpoints:\n\ncompareHashes(fileId: str, chunkHashes: list)\n\nTakes a file ID and a list of chunk hashes and returns a list of the hashes that need resyncing .\nresponse code 200, {syncChunks: [Hash,…]}\n\nuploadChange(fileId: str, chunks: list)\n\nUpload the chunks to resync the file\nresponse code 200, {message: “Chunks uploaded successfully”}\n\nrequestUpdate(fileId: str, chunkHashes: list)\n\nUpdate the local version of the file to match that on the server.\nresponse code 200, {chunks: [Chunk,…]}\n\n\nSystem Design\n\n\n\n\n\n\nFigure 13: Dropbox System Design\n\n\n\nDesign Discussion\n\nWhat can we do to achieve a scalable design?\n\nIdentify bottlenecks and remedy each.\n\nWhat can we do to achieve resiliency?\n\n\n\n\n\n\nUdemy course\nExcalidraw session\nCapacity estimation cheat sheet\n“Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems” by Martin Kleppmann\nRsync algorithm\nSearch Engines Information Retrieval in Practice book\n\n\n\n\nFigure 1: Polling\nFigure 2: LongPolling\nFigure 3: Web Sockets\nFigure 4: SSE\nFigure 5: Point-to-Point\nFigure 6: Pub-Sub\nFigure 7: Video Processing Pipeline\nFigure 8: Video Upload Architecture\nFigure 9: CDN Invalidation\nFigure 10: Inverted Index\nFigure 11: To-do App System Design\nFigure 12: URL Shortener System Design\nFigure 13: Dropbox System Design"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#requirements-engineering",
    "href": "posts/software/system_design/system_design_notes.html#requirements-engineering",
    "title": "System Design Notes",
    "section": "",
    "text": "Functional and non-functional requirements. Scale of the system.\n\n\nWhat the system should do.\n\nWhich problem does the system solve\nWhich features are essential to solve these problems\n\nCore features: bare minimum required to solve the user’s problem. Support features: features that help the user solve their problem more conveniently.\n\n\n\nWhat the system should handle.\n\n\n\n\n\n\nInterview Tips\n\n\n\n\nIdentify the non-functional requirements the interviewer cares most about\nShow awareness of system attributes, trade-offs and user experience\n\n\n\nSystem analysis:\n\nRead or write heavy?\n\nImpacts requirements for database scaling, redundancy of services, effectiveness of caching.\nSystem priorities - is it worse if the system fails to read or to write?\n\nMonolithic or distributed architecture?\n\nScale is the key consideration. Large scale applications must be distributed, smaller scale can be single server.\n\nData availability vs consistency\n\nCAP trilemma - A system can have only two of: Consistency, Availability, Partition tolerance\nChoice determines the impact of a network failure\n\n\nNon-functional requirements:\n\nAvailability - How long is the system up and running per year?\n\nAvailability of a system is the product of its components’ availability\nReliability, redundancy, fault tolerance\n\nConsistency - Data appears the same on all nodes regardless of the user and location.\n\nLinearizability\n\nScalability - Ability of a system to handle fast-growing user base and temporary load spikes\n\nVertical scaling - single server that we add more CPU/RAM to\n\nPros: fast inter-process communication, data consistency\nCons: single point of failure, hardware limits on maximum scale\n\nHorizontal scaling - cluster of servers in parallel\n\nPros: redundancy improves availability, linear cost of scaling\nCons: complex architecture, data inconsistency\n\n\nLatency - Amount of time taken for a single message to be delivered\n\nExperienced latency = network latency + system latency\nDatabase and data model choices affect latency\nCaching can be effective\n\nCompatibility - Ability of a system to operate seamlessly with other software, hardware and systems\nSecurity\n\nBasic security measures: TLS encrypted network traffic, API keys for rate limiting\n\n\nNon-functional requirements depend heavily on expected scale. The following help quantify expected scale, and relate to capacity estimation in the next step.\n\nDaily active users (DAU)\nPeak active users\nInteractions per user - including read/write ratio\nRequest size\nReplication factor - determines the storage requirement (typically 3x)"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#capacity-estimation",
    "href": "posts/software/system_design/system_design_notes.html#capacity-estimation",
    "title": "System Design Notes",
    "section": "",
    "text": "Interview Tips\n\n\n\n\nSimplify wherever you can - simple assumptions and round heavily\nConvert all numbers to scientific notation\nKnow powers of 2\n\n\n\n\n\nRequests per second (RPS) is the key measure.\nRequests per day = Daily active users * Activities per user\nRequests per second = RPD / 10^5\nPeak load = RPS * Peak load factor\nRead/write ratio is important to get the total requests. Usually, information on either reads or writes is missing and must be inferred using the other.\nTotal RPS = Read RPS + Write RPS\n\n\n\nThe amount of data that can be transmitted between two nodes in a fixed period of time. Bits per second.\nThroughput, bandwidth and latency are all related and can be thought of with the motorway analogy.\n\nThroughput is the total number of vehicles that must pass through that road per day.\nBandwidth is the number of lanes.\nLatency is the time taken to get from one end of the road to another.\n\nThe bandwidth must be large enough to support the required throughput.\nBandwidth = Total RPS * Request size\nEven high bandwidth can result in low latency if there is an unexpectedly high peak throughput. Rush hour traffic jam.\nRequest size can be varied according to bandwidth, e.g. YouTube lowers the resolution if the connection is slow.\n\n\n\nMeasured in bits per second using the Write RPS. Long term storage (assuming the same user base) is also helpful.\nStorage capacity per second = Write RPS * Request size * Replication factor\nStorage capacity in 5 years = Storage capacity per second * 2*10^8"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#data-modeling",
    "href": "posts/software/system_design/system_design_notes.html#data-modeling",
    "title": "System Design Notes",
    "section": "",
    "text": "Key concepts are: entities, attributes, relations.\nOptimisation discussions are anchored by the data model.\nRelational databases: denormalization, SQL tuning, sharding, federation\nNon-relational databases: indexing, caching\nThe data model is not a full-blown data schema (which only applies to relational databases) but a more informal, high-level version.\nMust haves:\n\nDerive entities and attributes.\nDraw connections between entities. Then:\nSelect a database based on the requirements\nExtend data model to a proper data schema\nIdentify bottlenecks and apply quick fixes Later:\nDefer any detailed discussions to the design discussion section.\n\n\n\nEntities are “things” with a distinct and independent existence, e.g. users, files, tweets, posts, channels. Create a unique table for each entity.\nAttributes describe properties of an entity, e.g. a user’s name, date of birth. These are the fields in the table of that entity.\n\n\n\n\n\n\nInterview Tips\n\n\n\n\nDo not focus on irrelevant details\nDo not miss any critical functionality\nDouble check all tables after you extract all info from requirements\n\n\n\n\n\n\nExtract connections between entities from the requirements. Relationships can be one-to-one, one-to-many or many-to-many."
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#api-design",
    "href": "posts/software/system_design/system_design_notes.html#api-design",
    "title": "System Design Notes",
    "section": "",
    "text": "Specify the API endpoints with:\n\nFunction signature\nParameters\nResponse and status codes\n\nThis specifies a binding contract between the client and the application server. There can be APIs between every system component, but just focus on the client/server interface.\nProcess:\n\nRevisit the functional requirements\nDerive the goal of each endpoint\nDerive the signature from the goal and the inputs from the data model\nDefine the response outputs\n\nNaming conventions. Use HTTP request types in the call signature where appropriate: GET, POST, PUT, PATCH, DELETE\n\n\n\n\n\n\nInterview Tips\n\n\n\n\nAvoid vague inputs\nDon’t add extra parameters that are out of scope\nAvoid redundant parameters\nDon’t miss any parameters\nConfirm the output repsonse satisfies the requirements\nIdentify inefficiencies of the data structure"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#system-design-1",
    "href": "posts/software/system_design/system_design_notes.html#system-design-1",
    "title": "System Design Notes",
    "section": "",
    "text": "System components:\n\nRepresentation layer\n\nWeb app\nMobile app\nConsole\n\nService (and load balancer)\nData store\n\nRelational database\n\nPros: linked tables reduce duplicate data; flexible queries\n\nKey-value store\n\nUseful for lots of small continuous reads and writes\nPros: Fast access, lightweight, highly scalable\nCons: Cannot query by value, no standard query language so data access in written in the application layer no data normalisation\n\n\n\nScaling:\n\nScale services by having multiple instances of the service with a load balancer in front\nScale database with federation (functional partitioning) - split up data by function\n\nSystem diagrams: arrows point in direction of user flow (not data flow)"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#design-discussion",
    "href": "posts/software/system_design/system_design_notes.html#design-discussion",
    "title": "System Design Notes",
    "section": "",
    "text": "Types of questions:\n\nNFR questions\n\nHow to achieve scalability\n\nIdentify each bottleneck and remedy it\n\nHow to improve resiliency\n\nJustification question\n\nChoice of components, e.g. what would change if you changed block storage for object storage or relational database\nAdditional components, e.g. is there a use case for a message queue and where\n\nExtension questions\n\nAdditional functional requirements not in the original scope"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#system-components-deep-dive",
    "href": "posts/software/system_design/system_design_notes.html#system-components-deep-dive",
    "title": "System Design Notes",
    "section": "",
    "text": "There are two parameters we can vary:\n\nLength of the key - should be limited so the URL is short enough\nRange of allowed characters - should be URL-safe\n\nUsing Base-64 encoding as the character range, then a 6-digit key length gives enough unique URLs; 64^6 = 68 billion\nEncoding options: 1. MD5 hash: not collision resistant and too long 2. Encoded counter, each URL is just an index int, and its value is the base-64 encoding of that int: length is not fixed and increases over time 3. Key range, generate all unique keys beforehand: no collisions, but storage intensive 4. Key range modified, generate 5-digit keys beforehand, then add the 6th digit on the fly.\n\n\n\n\n\n4 main operations (CRUD): Create, Read, Update, Delete.\nTransactions - group several operations together\n\nEither the entire transaction succeeds or it fails and rolls back to the initial state. Commit or abort on error.\nWithout transactions, if an individual operation fails, it can be complex to unwind the related transactions to return to the initial state.\nError handling is simpler as manual rollback of operations is not required.\nTransactions provide guarantees so we can reason about the database state before and after the transaction.\n\nTransactions enforce “ACID” guarantees:\n\nAtomicity - Transactions cannot be broken down into smaller parts.\nConsistency - All data points within the database must align to be properly read and accepted. Raise a consistency error if this is not the case. Database consistency is different from system consistency which states the data should be the same across all nodes.\nIsolation - Concurrently executing transactions are isolated from each other. Avoids race conditions if multiple clients are accessing the same database record.\nDurability - Once a transaction is committed to the database, it is permanently preserved. Backups and transaction logs can restore committed transactions in the case of a failure.\n\nRelational databases are a good fit when:\n\nData is well-structured\nUse case requires complex querying\nData consistency is important\n\nLimitations of relational databases:\n\nHorizontal scaling is complex\nDistributed databases are more complex to keep transaction guarantees\n\nTwo phase commit protocol (2PC):\n\nPrepare - Ask each node if it;s able to promise to carry out the transaction\nCommit - Block the nodes and do the commit\n\nBlocking causes problems on distributed systems where it may cause unexpected consequences if the database is unavailable for this short time.\n\n\n\n\n\nExamples of non-relational databases:\n\nKey-value store\nDocument store\nWide-column store\nGraph store\n\nThese vary a lot, and in general NoSQL simply means “no ACID guarantees”.\nTransactions were seen as the enemy of scalability, so needed to be abandoned completely for performance and scalability. ACID enforces consistency for relational databases; for non-relational databases there is eventual consistency.\nBASE:\n\nBasically Available - Always possible to read and write data even though it may not be consistent. E.g. reads may not reflect latest changes, writes may not be persisted.\nSoft state - Lack of consistency guarantees mean data state may change without any interactions with the application as the database reaches eventual consistency.\nEventual consistency - Data will eventually become consistent once inputs stops.\n\nBenefits:\n\nWithout atomicity constraint, overheads like 2PC are not required\nWithout consistency constraint, horizontal scaling is trivial\nWithout isolation constraint, no blocking is required which improves availability.\n\nNon-relational databases are a good fit when:\n\nLarge data volume that isn’t tabular\nHigh availability requirement\nLack of consistency across nodes is acceptable\n\nLimitations:\n\nConsistency is necessary for some use cases\nLack of standardisation\n\n\n\n\n\nProblem: if we have large files with small changes, we don’t want to have to re-upload and re-download the whole file every time something changes.\nSolution: identify the changes and only push/pull these. Similar to git.\nThe rsync algorithm makes it easier to compare changes by:\n\nSplit the file into blocks\nCalculate a checksum for each block (using MD5 hashing algorithm)\nTo compare files between the client and the server, we only need to send the hashes back and forth\nFor the mismatching hashes, transfer the corresponding blocks\n\n\n\n\nOne-way communication that broadcasts file updates one-to-many.\n\n\nThese are synchronous.\nPolling is an example of a pull API. We periodically query the server to see if there are any updates. If not, the server responds immediately saying there is no new data. This may result in delayed updates and can overwhelm the server with too many requests.\n\n\n\n\n\n\nFigure 1: Polling\n\n\n\n\n\n\nThese are asynchronous.\nLong-polling. The client connects to the server and makes a request. Instead of replying immediately, the server waits until there is an update and then sends the response. If there is no update for a long time, the connection times out and the client must reconnect. Server resources are tied up until the connection is closed, even if there is no update.\n\n\n\n\n\n\nFigure 2: LongPolling\n\n\n\nWebsockets. The client establishes a connection using an HTTP request and response. This establishes a TCP/IP connection for 2-way communication. This allows for real-time applications without long-polling.\n\n\n\n\n\n\nFigure 3: Web Sockets\n\n\n\nServer sent events. Event-based approach. EventSource API supported by all browsers. The connection is 1-directional; the client can only pass data to the server at the point of connection. After that, only the server can send data to the client. The server doesn’t know if the client loses connection.\n\n\n\n\n\n\nFigure 4: SSE\n\n\n\n\n\n\n\nA system component that implements asynchronous communication between services, decoupling them. An independent server which producers and consumers connect to. AKA message queues.\nMessage brokers persist the messages until there are consumers ready to receive them.\nMessage brokers are similar in principle to databases since they persist data. The differences are they: automatically delete messages after delivery, do not support queries, typically have a small working set, and notify clients about changes.\nRabbitMQ, Kafka and Redis are open source message brokers. Amazon, GCP and Azure have managed service implementations.\n\n\nA one-to-one relationship between sender and receiver. Each message is consumed by exactly one receiver.\nThe message broker serves as an abstraction layer. The producer only sends to the broker. The receiver only receives from the broker. The services do not need to know about one another. The broker also guarantees delivery, so the message does not get lost if the consumer is unavailable, it will retry.\n\n\n\n\n\n\nFigure 5: Point-to-Point\n\n\n\n\n\n\nA one-to-many relationship between sender and receiver. The publisher publishes to a certain topic. Any consumer who is interested can then subscribe to receive that message.\n\n\n\n\n\n\nFigure 6: Pub-Sub\n\n\n\n\n\n\n\n\n\nFiles are stored in folders, with metadata about creation time and modification time.\nBenefits:\n\nSimplicity - simple and well-known pattern\nCompatibility - works with most applications and OSes\nLimitations:\nPerformance degradation - as size increases, the resource demands increase.\nExpensive - although cheap in principle, they can get expensive when trying to work around the performance issues.\n\nUse cases: data protection, local archiving, data retrieval done by users.\n\n\n\nA system component that manages data as objects in a flat structure. An object contains the file and its metadata.\nBenefits:\n\nHorizontal scalability\n\nLimitations:\n\nObjects must be edited as a unit - degrades performance if only a small part of the file needs to be updated.\n\nUse cases: large amounts of unstructured data.\n\n\n\nA system component that breaks up and then stores data as fixed-size blocks, each with a unique identifier.\nBenefits:\n\nHighly structured - easy to index, search and retrieve blocks\nLow data transfer overheads\nSupports frequent data writes without performance degradation\nLow latency\n\nLimitations:\n\nNo metadata, so metadata must be handled manually\nExpensive\n\n\n\n\nA subclass of key-value stores, which hold computer-readable documents, like XML or JSON objects.\n\n\n\n\nIssues:\n\nMassive files: 4TB for 4 hours of 4K video\n\nHow can we reliably upload very large files?\n\nLots of different end-user clients/devices/connection speeds\n\nHow can we process and store a range of versions/resolutions?\n\n\nProcessing pipeline:\n\n\n\n\n\n\nFigure 7: Video Processing Pipeline\n\n\n\n\nFile chunker\n\nSame principle as the file-sharing problem.\nSplit the file into chunks, then use checksums to determine that files are present and correct.\nIf there is a network outage, we only need to send the remaining chunks.\n\nContent filter\n\nCheck if the video complies with the platform’s content policy wrt copyright, piracy, NSFW\n\nTranscoder\n\nData is decoded to an uncompressed format\nThis will fan out in the next step to create multiple optimised versions of the file\n\nQuality conversion\n\nConvert the uncompressed file into multiple different resolution options.\n\n\nArchitecture considerations:\n\nWait for the full file to upload before processing? Or process each chunk as it is uploaded?\nAn upload service persists chunks to a database. Object store is a good choice as in the file-sharing example.\nOnce a chunk is ready to be processed, it can be read and placed in the message queue for the processing pipeline.\nThe processing pipeline handles fixed-size chunks, so the hardware requirements are known ahead of time. A chunk can be processed as soon as a hardware unit becomes available.\n\n\n\n\n\n\n\nFigure 8: Video Upload Architecture\n\n\n\n\n\n\nThe challenges are similar to that of file-sharing, as this is essentially large files being transferred. Latency and processing power of the end-user device.\nTwo main transfer protocols:\n\nUDP\nTCP\n\n\n\nA connection-less protocol. Nodes do NOT establish a stable end-to-end connection before sending data. Instead, each packet has its destination address attached so the network can route it to the correct place.\nAdvantages:\n\nFast\nPackets are routed independently so if some are lost the others can still reach their destination\n\nDisadvantages:\n\nNo guarantee that data will arrive intact.\n\nUse cases:\n\nLow-latency applications like video conferencing or gaming.\nNot suitable for movie or audio streaming, as missing bits cannot be tolerated.\n\n\n\n\nA connection is established between 2 nodes. The nodes agree on certain parameters before any data is transferred. Parameters: IP addresses of source and destination, port numbers of source and destination.\nThree-way handshake establishes the connection:\n\nSender transfers their sequence number to the Receiver.\nReceiver acknowledges and sends its own sequence number.\nSender acknowledges.\n\nAdvantages:\n\nReliability; guarantees delivery and receives acknowledgements before further packets are sent.\nReceiver acknowledgements ensure the receiver is able to process/buffer the data in time before more packets are sent.\n\nDisadvantages:\n\nSlower due to error checking and resending lost packets.\nRequires three-way handshake to establish connection which is slower.\n\nUse cases:\n\nMedia streaming (HTTP live-streaming or MPEG-DASH)\n\nAdaptive bitrate streaming\n\nTCP adjusts transmission speed in response to network conditions\nWhen packet loss is detected, smaller chunks are sent at a lower transmission speed. The lower resolution file can be sent in this case.\n\n\n\n\n\nPerceived latency = Network latency + System latency\n\n\nCaching is the process of storing copies of frequently accessed data in a temporary storage location.\nCaches utilise the difference in read performance between memory and storage. Times to fetch 1MB from:\n\nHDD: 3 ms\nSSD: 0.2 ms\nRAM: 0.01 ms\n\nWhen a user requests data it first requests from the cache\n\nCache HIT: If the data is in the cache, return it\nCache MISS: If the data is not in the cache, pass the request to the database, update the cache and return the data\n\nA cache can grow stale over time. There are 2 common approaches to mitigate this:\n\nSet up a time-to-live (TTL) policy within the cache\n\n\nTrade off between short TTL (fresh data but worse performance) vs long TTL (data can be stale)\n\n\nImplement active cache invalidation mechanism\n\n\nComplicated to implement. Requires an “invalidation service” component to monitor and read from the database, then update the cache when necessary.\n\nApplication-level caching. Insert caching logic into the application’s source code to temporarily store data in memory.\nTwo approaches to application-level caching:\n\nQuery-based implementation\n\nHash the query as a key and store the value against the key in a key-value store.\nLimitations: hard to delete cached result with a complex query; if a cell changes then all cached query which might include it need updating.\n\nObject-based implementation\n\nThe result of a query is stored as an object\nBenefits: only one serialisation/deserialisation overhead when reading/writing; complexity of object doesn’t matter, serialized objects in cache can be used by multiple applications.\n\n\nPopular implementations: Redis, Memcache, Hazlecast\n\n\n\nA network of caching layer in different locations (Points of Presence, PoPs). Full data is stored on SSD and most frequently accessed data is stored in RAM.\nDon’t cache dynamic or time-sensitive data.\nInvalidating the cache. You can manually update the CDN, but there may be additional caches at the ISP- and browser-level which would still serve stale data.\n\n\n\n\n\n\nFigure 9: CDN Invalidation\n\n\n\nApproaches to invalidating cache:\n\nCaching headers - set a time when an object should be cleared, e.g. max-age=86400 caches for a day.\nCache busting - change the link to all files and assets, so the CDN treats them like new files.\n\n\n\n\n\nThese are specialised NoSQL databases with the following benefits:\n\nCan match even with typos or non-exact matches\nFull-text search means you can suggest autocomplete results and related queries as the user types\nIndexing allows faster search performance on big data.\n\nThe database is structures as a hashmap where the inverted index points at documents.\n\nDocument - Represents a specific entity, like a row in a relational database\nInverted index - Maps from content to the documents that include it. The inverted index splits each document into individual search terms and makes each point to the document itself.\nRanking algorithm - Produces a list of possibly relevant documents. Additional factors may impact this, like a user’s previous search history.\n\nThe database is a dictionary of {search_term_id: [document_ids, …]}\n\n\n\n\n\n\nFigure 10: Inverted Index\n\n\n\nPopular implementations: Lucene, Elastic search, Apache solr, Atas search (MongoDB), Redis search.\nBenefits of search engine databases:\n\nScalable - NoSQL so scale horizontally\nSchemaless\nWorks out of the box - just need ot decide upfront what attributes should be searchable.\n\nLimitations:\n\nNo ACID guarantees.\nNot efficient at reading/writing data, so requires another database to manage state.\n\nMaintaining consistency between the main database and the search engine database can be tricky.\n\nSome SQL databases have full-text search so may not require a dedicated search engine database."
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#system-design-examples-and-discussion",
    "href": "posts/software/system_design/system_design_notes.html#system-design-examples-and-discussion",
    "title": "System Design Notes",
    "section": "",
    "text": "An app that lets a user add and delete items from a todo list.\n\nRepresentation layer is a web app\nMicroservices handle the todo CRUD operations and the user details separately\n\nEach service is scaled horizontally, so a load balancer is placed in front of it\n\nRelational database stores data\n\nThere are two databases, one per service, which is an example of functional partitioning. This means todo service I/O does not interfere with user service I/O and vice versa.\n\n\n\n\n\n\n\n\nFigure 11: To-do App System Design\n\n\n\n\n\n\nTake an input URL and return a unique, shortened URL that redirects to the original.\nPlanned system architecture:\n\nPre-generate all 5 digit keys (1 billion entries rather than 64 billion for 6 digits)\nSystem retrieves 5-difit keys from database\nSystem appends 1 out of 64 characters\nThe system is extendable because if we run out of keys we can append 2 more characters rather than 1\n\n\n\nSystem analysis:\n\nSystem is read heavy - Once a short URL is created it will be read multiple times\nDistributed system as this has to scale\nAvailability &gt; Consistency - not so much of a concern if a link isn’t available to all users at the same time, but they must be unique and lead to their destination.\n\nRequirements engineering:-\nCore feature:\n\nA user can input a URL of arbitrary length and receive a unique short URL of fixed size.\nA user can navigate to a short link and be redirected to the original URL.\n\nSupport features:\n\nA user can see their link history of all created short URLs.\nLifecycle policy - links should expire after a default time span.\n\nNon-functional requirements:\n\nAvailability - the system should be available 99% of the time.\nScalability - the system should support billions of short URLs, and thousands of concurrent users.\nLatency - the system should return redirect from a short URL to the original in under 1 second.\n\nQuestions to capture scale:\n\nDaily active users, and how often do users interact per day\n\n100 million, 1 interaction per day\n\nPeak active users - are there events that lead to traffic spikes?\n\nNo spikes\n\nRead/write ratio\n\n10 to 1\n\nRequest size - how long are the originals URLs typically\n\n200 characters =&gt; 200 Bytes\n\nReplication factor\n\n1x - ignore replication\n\n\nCapacity estimation:-\n\nRequests per second\n\nReads per second = DAU * interactions per day / seconds in day = 10^8 * 1 / 10^5 = 1000 reads/s\nWrites per second = Reads per second / read write ratio = 1000 / 10 = 100 writes/s\nRequests per second = Reads per second + Writes per second = 1000 + 100 = 1100 requests/s\nNo peak loads to consider\n\nBandwidth\n\nBandwidth = Requests per second * Message size\nRead bandwidth = 1000 * 200 Bytes = 200kB/s\nWrite bandwidth = 100 * 200 Bytes = 20 kB/s\n\nStorage\n\nStorage per year = Write bandwidth * seconds per year * Replication factor = 20000 Bytes * (360024365) * 1 = 630 GB\n\n\nData model:-\nIdentify entities, attributes and relationships.\nEntities and attributes:\n\nLinks: Key (used to create short URL), Original URL, Expiry date\nUsers: UserID, Links\nKey ranges: Key range, In use (bool)\n\nRelationships:\n\nUsers own Links\nLinks belong to Key ranges\n\nData stores:\n\nUsers\n\nUser data is typically relational and we rarely want it all returned at once.\nConsistency is important as we want the user to have the same experience regardless of which server handles their log in, and don’t want userIds to clash.\nRelational database.\n\nLinks\n\nNon-functional requirements of low latency and high availability.\nData and relationships are not complex.\nKey-value store.\n\nKey ranges\n\nFavour data consistency as we don’t want to accidentally reuse the same key range.\nFiltering keys by status would help to find available key ranges.\nRelational database.\n\n\nAPI design:-\nEndpoints:\n\ncreateUrl(originalUrl: str) -&gt; shortUrl\n\nresponse code 200, {shortURL: str}\n\ngetLinkHistory(userId: str, sorting: {‘asc’, ‘desc’})\n\nresponse code 200, {links: [str], order: ‘asc’}\n\nredirectURL(shortUrl: str) -&gt; originalUrl\n\nresponse code 200, {originalUrl: str}\n\n\nSystem design:-\nUser flows for each of the required functional requirements.\n\n\n\n\n\n\nFigure 12: URL Shortener System Design\n\n\n\n\n\n\n\nRequirements engineering\nRequirements gathering:\n\nRead-heavy\nDistributed\nData consistency is more important than availability - files should be consistent for all users\n\nCore functional requirements:\n\nA user can upload a file to the server\nA user can download their files from the server\n\nSecondary functional requirements:\n\nA user can see a history of files uploaded and downloaded\nA user can see who has downloaded a specific file and when\n\nNon-functional requirements:\n\nConsistency - data should be consistent across users and devices\nResilience - customer data should never be lost\nMinimal latency\nCompatibility across different devices\nSecurity - multi-tenancy; customer data should be separate of one another\n\nQuestions to determine scale:\n\nDaily active users\nPeak active users - what events lead to a spike?\nInteractions per user - how many file syncs, how many files does a user store\nRequest size - how large are files?\nRead/write ratio\nReplication factor\n\nCapacity estimation\nThroughput\n\nPeak write RPS = 2 peak load ratio * 10^8 users * 2 files/day / 10^5 seconds = 4000 RPS\nPeak read RPS = 10 read/write ration * 4000 write RPS = 40000 RPS\nPeak total RPS = 44000 RPS\n\nBandwidth\n\nWrite bandwidth = 4000 Write RPS * 100 kB Request size = 400 MB/s\nRead bandwidth = 40000 Read RPS * 100 kB request size = 4 GB/s\nTotal bandwidth = Write bandwidth + Read bandwidth = 4.4 GB/s\n\nStorage\n\nStorage capacity = 5 * 10^8 Total users * 100 files per user * 1MB File Size * 3 Replication factor = 15*10^10 MB = 15000 TB\n\nData model\nUsers: Store Ids for the files that belong to them\nFiles: Metadata on the owner, file edit history, filename, size, chunks that comprise the file, version history\nAPI Design\nEndpoints:\n\ncompareHashes(fileId: str, chunkHashes: list)\n\nTakes a file ID and a list of chunk hashes and returns a list of the hashes that need resyncing .\nresponse code 200, {syncChunks: [Hash,…]}\n\nuploadChange(fileId: str, chunks: list)\n\nUpload the chunks to resync the file\nresponse code 200, {message: “Chunks uploaded successfully”}\n\nrequestUpdate(fileId: str, chunkHashes: list)\n\nUpdate the local version of the file to match that on the server.\nresponse code 200, {chunks: [Chunk,…]}\n\n\nSystem Design\n\n\n\n\n\n\nFigure 13: Dropbox System Design\n\n\n\nDesign Discussion\n\nWhat can we do to achieve a scalable design?\n\nIdentify bottlenecks and remedy each.\n\nWhat can we do to achieve resiliency?"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#references",
    "href": "posts/software/system_design/system_design_notes.html#references",
    "title": "System Design Notes",
    "section": "",
    "text": "Udemy course\nExcalidraw session\nCapacity estimation cheat sheet\n“Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems” by Martin Kleppmann\nRsync algorithm\nSearch Engines Information Retrieval in Practice book\n\n\n\n\nFigure 1: Polling\nFigure 2: LongPolling\nFigure 3: Web Sockets\nFigure 4: SSE\nFigure 5: Point-to-Point\nFigure 6: Pub-Sub\nFigure 7: Video Processing Pipeline\nFigure 8: Video Upload Architecture\nFigure 9: CDN Invalidation\nFigure 10: Inverted Index\nFigure 11: To-do App System Design\nFigure 12: URL Shortener System Design\nFigure 13: Dropbox System Design"
  },
  {
    "objectID": "posts/business/conflict_resolution/notes.html",
    "href": "posts/business/conflict_resolution/notes.html",
    "title": "Conflict Resolution",
    "section": "",
    "text": "There is a distinction between healthy confilct (e.g. a disagreement) and unhealthy conflict. Disagreement isn’t inherently a bad thing!\nRelationships break down when people see conflicts as win/lose (“I’m so assertive and everyone needs to do what’s best for me”) or lose/win (“I’m so scared of conflict that I’ll people please to avoid it”). Eventually, both lead to lose/lose situations as parties retaliate in their own ways.\nConflicts should be seen as opportunities for win/win.\n\n\n\nHarmony. Things are peaceful. Differences of opinion may happen, but they’re handled quickly and effectively.\nDiscomfort. You’re becoming aware in differences of values, ideas, needs, roles, goals, personalities, etc.\nDisagreement. Disagreements begin to feel more emotional. You feel vulnerable and threatened when there are differences of opinion.\nDiscord. You have tried unsuccessfully to work through disagreements, and now tensions are rising. You gather evidence to prove why you are right and they are wrong. Talking does not help as you are both trying to prove the other wrong.\nPolarisation. Emotions are high and objectivity is lost. This is unhealthy conflict. A change of tactics is required to resolve conflict, otherwise the party with more power/authority will impose their will on others.\nDisintegration. Point of no return. The relationship has broken down and is beyond repair.\n\n\n\n\nTrust is like a barometer of the health of a relationship.\nTrust is a necessary but not sufficient condition of a healthy relationship. Having trust doesn’t mean conflicts will never occur, but they are less likely to turn toxic and more likely to be handled in good faith.\n\nAct from personal values.\nMake and keep commitments.\nSeek to improve.\nAccept accountability. Apologise.\nBe curious and interested in others.\nPraise in public.\nListen to others and seek their input. Ensure all people have airtime.\nSeek solutions that are important to others, not just yourself.\n\n\n“We go fast when we walk alone. We go far when we walk together.”"
  },
  {
    "objectID": "posts/business/conflict_resolution/notes.html#phases-of-escalation",
    "href": "posts/business/conflict_resolution/notes.html#phases-of-escalation",
    "title": "Conflict Resolution",
    "section": "",
    "text": "Harmony. Things are peaceful. Differences of opinion may happen, but they’re handled quickly and effectively.\nDiscomfort. You’re becoming aware in differences of values, ideas, needs, roles, goals, personalities, etc.\nDisagreement. Disagreements begin to feel more emotional. You feel vulnerable and threatened when there are differences of opinion.\nDiscord. You have tried unsuccessfully to work through disagreements, and now tensions are rising. You gather evidence to prove why you are right and they are wrong. Talking does not help as you are both trying to prove the other wrong.\nPolarisation. Emotions are high and objectivity is lost. This is unhealthy conflict. A change of tactics is required to resolve conflict, otherwise the party with more power/authority will impose their will on others.\nDisintegration. Point of no return. The relationship has broken down and is beyond repair."
  },
  {
    "objectID": "posts/business/conflict_resolution/notes.html#building-trust",
    "href": "posts/business/conflict_resolution/notes.html#building-trust",
    "title": "Conflict Resolution",
    "section": "",
    "text": "Trust is like a barometer of the health of a relationship.\nTrust is a necessary but not sufficient condition of a healthy relationship. Having trust doesn’t mean conflicts will never occur, but they are less likely to turn toxic and more likely to be handled in good faith.\n\nAct from personal values.\nMake and keep commitments.\nSeek to improve.\nAccept accountability. Apologise.\nBe curious and interested in others.\nPraise in public.\nListen to others and seek their input. Ensure all people have airtime.\nSeek solutions that are important to others, not just yourself.\n\n\n“We go fast when we walk alone. We go far when we walk together.”"
  },
  {
    "objectID": "posts/business/conflict_resolution/notes.html#the-four-styles",
    "href": "posts/business/conflict_resolution/notes.html#the-four-styles",
    "title": "Conflict Resolution",
    "section": "2.1. The Four Styles",
    "text": "2.1. The Four Styles\nWe can categorise communication styles with respect to empathy (concern for others) vs assertiveness (concern for self).\n\n\n\nFour styles of communication\n\n\nThis gives rise to four communication styles:\n\nDominators. High assertiveness, low empathy. During conflict they handle their emotions by defending, arguing, lecturing, blaming, or attacking. Dominators don’t like to be wrong and they don’t like to lose. Their strategy is to convince, control or coerce other people.\nAccommodators. Low assertiveness, high empathy. Accommodators tend to put the opinions, needs and feelings of others ahead of their own; they are generally polite, easy to get along with, non-judgmental, and more self-aware than dominators or avoiders. During conflict, they defer to others and even give in. Over time, accommodators can grow resentful that their needs are unmet and act out in passive-aggressive ways.\nAvoiders. Low assertivenss, low empathy. They are usually easy going, independent, rational and emotionally detached. If things get tense, they try to pretend that everything is okay; their strategy is to leave issues alone and hope that they will go away. They suppress their feelings, use humor, rationalize and minimize. Because they have a hard time dealing with emotional issues, their relationships aren’t as deep; they don’t disclose their true selves to other people but seek to play it safe.\nCollaborators. High assertiveness, high empathy. They are able to share their own point of view, needs and so on but are also able to show empathy for the views and needs of others, responding in ways that allow all parties to “win.”\n\nCollaboration is what we aim for. In a healthy relationship, people are able to assert their own needs and opinions. Yet they are also sensitive to the needs and feelings of others. Understanding this balance is the essence of good communication, a healthy relationship, and the ability to resolve conflict productively.\nAn additional thought on this: does the “self” in “concern for self” expand and contract depending on the situation? If somebody acts more or less assertively whne around others, is that because the sense of self expands to include others in the group? Like a mama bear being more assertive to protect her baby, because her sense of self now includes the baby; she will be more assertive for “mama + baby” than she would have been for “mama” alone. Similarly when people behave differently when their spouse is around."
  },
  {
    "objectID": "posts/business/conflict_resolution/notes.html#common-combinations-of-commincation-styles-and-handling-conflicts",
    "href": "posts/business/conflict_resolution/notes.html#common-combinations-of-commincation-styles-and-handling-conflicts",
    "title": "Conflict Resolution",
    "section": "2.2. Common Combinations of Commincation Styles and Handling Conflicts",
    "text": "2.2. Common Combinations of Commincation Styles and Handling Conflicts\n\nDominate / Accommodate. Can work where one party is indifferent. The risk is the accommodator becomes resentful.\nAvoid / Avoid. Can be temporarily harmonious but by avoiding the lows they avoid the highs.\nDominate / Dominate. Lots of arguments, neither person listening or responding.\nDominate / Avoid. One person in control, the other indifferent.\nAccommodate / Accommodate. Both parties defer to each other, more concerned about the other’s needs, not concerned about or used to getting what they personally want."
  },
  {
    "objectID": "posts/business/conflict_resolution/notes.html#when-is-each-style-desirable",
    "href": "posts/business/conflict_resolution/notes.html#when-is-each-style-desirable",
    "title": "Conflict Resolution",
    "section": "2.3. When is Each Style Desirable?",
    "text": "2.3. When is Each Style Desirable?\nNobody is “always” one style. It depends on the situation at hand and the other party involved.\nGenerally collaboration is ideal, but in certain scenarios we may fall into other styles:\n\nDominate: when there is an emergency requiring decisive action, when protecting ourselves from others taking advantage, when our values are being violated.\nAccommodate: when preserving the relationship is more important than the issue at hand, when you need to buy time to respond, when you may be wrong or others may have better ideas, you want to give the other person a “win”.\nAvoid: when an issue seems trivial or unimportant, when the timing isn’t right, when the issue is outside of your control, when relationship damage outweighs the benefits of action."
  },
  {
    "objectID": "posts/business/conflict_resolution/notes.html#dialogue",
    "href": "posts/business/conflict_resolution/notes.html#dialogue",
    "title": "Conflict Resolution",
    "section": "3.1. Dialogue",
    "text": "3.1. Dialogue\nCollaboration becomes crucial as the topic of communication becomes sensitive or important, or as conflict is escalating.\nCollaboration is the only style that can address high stakes issues effectively. We succeed together or we fail together.\nDialogue is crucial for collaboration. Dialogue is defined as:\n\n“A participative process of communication in which people listen to understand each other’s point of view and then agree upon options to solve problems or resolve their disagreements”\n\nThere a key distinctions between arguments and dialogue.\n\n\n\nArgument\nDialogue\n\n\n\n\nConcerned with self\nConcerned with self and others\n\n\nAdversarial\nUnity\n\n\nIntent to win\nIntent to learn and explore\n\n\nListen to respond\nListen to understand\n\n\nPolarised positions\nMany sides\n\n\nOversimplify issues\nView nuances and complexities\n\n\nRight vs wrong\nDiscovery\n\n\nLook for confirming data\nLook for enlightenment"
  },
  {
    "objectID": "posts/business/conflict_resolution/notes.html#conflict-resolution-model",
    "href": "posts/business/conflict_resolution/notes.html#conflict-resolution-model",
    "title": "Conflict Resolution",
    "section": "3.2. Conflict Resolution Model",
    "text": "3.2. Conflict Resolution Model\nThere is usually a trigger for a conflict.\nThe intensity of the trigger depends on:\n\nRelationship context and culture - is there trust and goodwill?\nThe phases of escalation\n\n\n\n\nConflict resolution model"
  },
  {
    "objectID": "posts/business/conflict_resolution/notes.html#collusion-the-opposite-of-conflict-management",
    "href": "posts/business/conflict_resolution/notes.html#collusion-the-opposite-of-conflict-management",
    "title": "Conflict Resolution",
    "section": "4.1. Collusion: The Opposite of Conflict Management",
    "text": "4.1. Collusion: The Opposite of Conflict Management\nConflict can be thought of as “a circular and mutually reinforcing negative interaction”.\nYou are not responding to the behaviour of the other person, but your interpretation of their behaviour. We judge ourselves by our intentions and others by their actions.\n\n\n\nCollusion model\n\n\nYou need to break out of the collusive cycle to resolve conflict. Be curious about the other person’s interpretation: “the third story” in between yours and theirs.\nDialogue is key to breaking collusion, but you first need to be willing to challenge your interpretation."
  },
  {
    "objectID": "posts/business/conflict_resolution/notes.html#adopting-the-right-mindset",
    "href": "posts/business/conflict_resolution/notes.html#adopting-the-right-mindset",
    "title": "Conflict Resolution",
    "section": "4.2. Adopting the Right Mindset",
    "text": "4.2. Adopting the Right Mindset\n\n4.2.1. Responsibility\n\nLook at your contribution during collusion. It takes two to collude, and even if you’re not fully at fault, it’s rare that you’re 0% at fault. Can you take accountability for the negative parts you are responsible for?\nTake responsibility for yourself. See the collusion diagram - You can’t control the other person’s interpretations, feelings or behaviours. But you can control your own. The other person doesn’t cause you to behave a certain way; they may be a trigger, but they cannot cause it, that takes away your personal power.\nThink through three stories: yours, theirs, and the “third story” of a neutral observer.\n\n\n\n4.2.2. Dealing with Feelings\n“Avoiding feelings” doesn’t work because they will always leak in to discussions through tone of voice, facial expressions etc. It also makes it difficult to listen to others when focusing so hard on repressing emotions.\nFeelings are a symptom of conflict. Unfulfilled needs are the cause. You wouldn’t ignore an obvious physical symptom like a tumour, so don’t ignore feelings.\nLabel your own feelings. All are valid, don’t judge them as good or bad. Then you can understand your needs and which of these are not being met.\nThere are four possible ways that one could respond to negative messages:\n\nTake the message personally and accept it\nDefend yourself and retaliate\nBecome more aware of your own feelings and needs\nUnderstand the other person’s feelings and needs\n\nOption 4 is the best approach. Option 3 can be useful too. Options 1 and 2 are harmful.\nWhen sharing feelings, it is important to do so without judging or blaming. Start statements with “I” rather than “you”.\n\n\n4.2.3. Committing to Outcomes\nExamine your trust paradigm. Do you see people as adversaries or allies? The point isn’t to assume bad people don’t exist; some people are untrustworthy. But do you start from an assumption of mistrust or trust? Guilty until proven innocent or innocent until proven guilty?\nThe majority of disputes you have aren’t down to malice.\nWe have a tendency to assign negative emotions to others and positive to ourselves. Trust begets trust.\nCommit to collaboration. Disagreements should focus on issues, not relationships.\nBe clear about the outcomes you desire during negotiations. This doesn’t mean have a stubborn outcome in mind that you aren’t willing to budge from. Rather, the outcome is to collaborate on a win-win result."
  },
  {
    "objectID": "posts/business/conflict_resolution/notes.html#who-should-initiate-dialogue",
    "href": "posts/business/conflict_resolution/notes.html#who-should-initiate-dialogue",
    "title": "Conflict Resolution",
    "section": "4.3. Who Should Initiate Dialogue",
    "text": "4.3. Who Should Initiate Dialogue\nA common misconception is that it should be the person with greater authority.\nIt can be anyone with a vested interest in the outcome and some ability to influence the outcome.\nYou may also want to weigh the risk vs reward. Some conversations might not be worth having.\nTimes when it may be best to not initiate:\n\nYou’re upset and want to vent\nThe true conflict is internal - you’re actually angry about something else\nThe other person has no desire to resolve the conflict, and you don’t have the power to change their mind\nThere is a power/authority imbalance\n\nFour considerations when deciding whether to enter into dialogue:\n\nImpact. Short-term and long-term consequences\nRisk. Are you putting the relationship at risk by bringing up a topic? What could you potentially lose? What are the best, worst and most likely outcomes?\nReward. What do you have to gain by resolving this conflict?\nConfidence. How confident are you in initiating and facilitating dialogue?\n\nNot every conflict can be resolved:\n\nSome issues are protracted and the people involved may not see the value in dialogue.\nSome problems have no solutions."
  },
  {
    "objectID": "posts/business/conflict_resolution/notes.html#preparation",
    "href": "posts/business/conflict_resolution/notes.html#preparation",
    "title": "Conflict Resolution",
    "section": "5.1. Preparation",
    "text": "5.1. Preparation\nPutting yourself in the right frame of mind and deciding how to initiate dialogue.\nStep back from conflict, even if only briefly.\n\nReview these course materials rationally: nature of conflict, four communication styles, phases of escalation, collusion diagram and conflict resolution steps.\nWork through your thoughts and feelings.\nEvaluate your commitment to collaboration by answering the questions below.\n\nEvaluating your commitment to collaboration:\n\nWhat is your intent going in to this conversation? Is it to solve a problem, or to win, or to punish someone, or to survive an encounter?\nAm I willing to show respect (in language, tone, demeanour)?\nAm I committed to collaboration and win-win outcomes?\nAm I willing to take responsibility rather than blame others?\nAm I seeking shared understanding rather than trying to build my own case?\nAm I willing to be curious about the other party and understand their point of view?\nCan I listen and draw the other out?\nAm I concerned for us rather than just me?\nAm I willing to take the time necessary to arrive at a good outcome?"
  },
  {
    "objectID": "posts/business/conflict_resolution/notes.html#invitation",
    "href": "posts/business/conflict_resolution/notes.html#invitation",
    "title": "Conflict Resolution",
    "section": "5.2. Invitation",
    "text": "5.2. Invitation\nInviting the other person to problem solve together.\nInitiating the dialogue in a way that makes the other party feel safe sharing, thereby increasing the chances of a positive outcome. Safety, respect and openness. Allies not adversaries.\n96% of the time, if a conversation starts poorly, it will end poorly.\n\nUse a soft start up. This doesn’t mean you should beat around the bush, but it means avoiding criticism, contempt and accusations.\nShow respect. You can disagree but still be respectful.\nAvoid blame. Start with “I” rather than “you”; you own your feelings but you’re not responsible for the actions of others.\nUse repair attempts. This is throughout the conversation, not just at the start. Try to de-escalate and defuse tension. Listen more if the other is getting defensive, acknowledge or apologise for mistakes, restate your intent to collaborate, recognise something good about the other or concede a point they’re making, light humour. Don’t get so caught up in this issue that your ignore HOW you’re communicating.\n\nThe following sub-sections discuss some important skills for invitation.\n\n5.2.1. Soft Start Up\nSome examples of a soft start up to invite dialogue:\n\n“There’s something important that I’d like to get your thoughts on”\n“I’d like to share something that I’ve been thinking about lately”\n“I’d like to know your point of view about…”\n\n\n\n5.2.2. Leveling\nCreating a climate for dialogue in which everybody knows they will be treated in a fair way.\nLeveling statements are usually short and should include:\n\nState a concern without blame or hostility\nExpress a desire to understand the other person’s point of view\nLet them know you want to work things out together\n\n\n\n5.2.3. Clarifying Your Intent\nPoint out what you do and do not intend. Start with the don’t.\n\n“I don’t intend to place blame. I do intend to get back to a good working relationship.”\n\n\n\n5.2.4. Clarifying Your Concerns\nHighlight any concerns about entering into dialogue at the beginning.\n\n“I don’t want you to think our relationship will be harmed by bringing this up.”\n\n\n\n5.2.5. Stating Your Commitment to Collaboration\n\n“My intent is to be helpful and not divisive”."
  },
  {
    "objectID": "posts/business/conflict_resolution/notes.html#exploration",
    "href": "posts/business/conflict_resolution/notes.html#exploration",
    "title": "Conflict Resolution",
    "section": "5.3. Exploration",
    "text": "5.3. Exploration\nDevelop a shared understanding of everybody’s perspective before we try to solve the problem. Problem solving happens in the next step, this step is purely informational.\nExploration is a feedback loop of inquiry and advocacy.\nGoals:\n\nIdentify and understand the assumptions and perceptions of all parties\nTo search for complete view of reality\n\nTwo heads are better than one. No single person is smarter than everybody. Research shows groups come to better decisions than individuals if they draw everybody’s opinions out and don’t fall victim to groupthink.\nPotential mistakes in this phase:\n\nTrying to persuade or coerce others to your point of view\nMoving on to problem solving too quickly\n\n\n5.3.1. Inquiry\nInquiry is “encouraging others to disclose their point of view then listening with empathy”.\nStart with inquiry to demonstrate your intentions to listen and build trust.\nExamples:\n\n“What’s the situation as you see it?”\n“How do you see things?”\n“What are your thoughts?”\n\nThis is active not passive. Ask follow-on questions to draw them out. Acknowledge their point of view which is not the same as conceding or agreeing to it.\n\n“Here’s how it looks from your point of view”\n\nKeys to inquiry:\n\nNon-judgement\nSuspend your own point of view\nBe open and willing to learn\nKeep the responsibility on the speaker\n\nFour skills of inquiry:\n\nInviting. A statement that shows your willingness to listen.\n\n“What’s your POV?”\n“Could you tell me more?”\n“How do you see this differently?”\n\nClarifying. Asking questions that invite the speaker to expand. Don’t overdo it or it begins to feel like an interrogation.\n\n“What was your interpretation of that comment?”\n“Can you share more what you mean by that?”\n“What did you feel when that occurred?”.\n\nEmpathy. Understand the deeper meaning, not just the words.\nPriming. Making statements that represent your best guess when the other person shuts down or is struggling to articulate themselves.\n\n“I would guess that you’re opposed to what we’ve been discussing”\n\n\n\n\n5.3.2. Advocacy\nAdvocacy is “disclosing your own point of view”.\nDo this clearly but not dogmatically; show a willingness to be influenced.\n\n“This is the situation as I see it”\n“These are my thoughts”\n\nIt can be help to do this in two parts:\n\n“This is my point of view and this is how I arrived at it”\n\n\nGet right to the point. Don’t be vague or ramble.\nBe willing to share your feelings.\nAdvocate non-dogmatically. Share your opinion and be aware that there may be other valid opinions.\nEncourage commentary.\n\nUsually, inquiry is the more important skill over advocacy because people need to be drawn out and made to feel safe. The exception is when dealing with dominators. Here, it is important to acknowledge their point but focus on assertively advocating your own point. You may even need to say “Can you let me know you understand what I’m saying?” or “Can you repeat back to me what I’m saying?”\n\n\n5.3.3. Immediacy\nImmediacy is commenting on the process of communicating.\nThis is often necessary when you have done your best to understand the other’s point of view but they are unwilling to reciprocate.\n\n“Can we get back to the issue? It feels like the conversation has become hostile”\n\n\n\n5.3.4. Ground Rules\nParticularly helpful with groups.\n\nStay with the topic at hand and have one conversation at a time\nSeek what is right not who is right\nSeek involvement from all members\nShare opinions honestly with the intent to be constructive\nShare feedback with the intent of helpfulness, even when it is difficult to give\nBe open-minded to receive feedback, even when it is difficult to hear\nIf you disagree with the majority of the group, you have a responsibility to:\n\nMake sure you understand others’ point of view\nExplain your own point of view\nSuggest alternative ideas that will work as well or better\n\nKeep disagreements in the group\nWhat happens in the room stays in the room, aside from follow up actions and decisions\nBe patient with the process\nTry to have fun, keep a sense of humour"
  },
  {
    "objectID": "posts/business/conflict_resolution/notes.html#collaboration",
    "href": "posts/business/conflict_resolution/notes.html#collaboration",
    "title": "Conflict Resolution",
    "section": "5.4. Collaboration",
    "text": "5.4. Collaboration\nWorking together to find a win-win solution that solves everybody’s needs.\nIn some cases we don’t even need the collaboration step. The shared understanding built in the exploration phase is enough to “clear the air” and no more action is needed.\nThree steps to achieve solutions during collaboration:\n\nIdentify what is important to each party. Unmet needs are at the heart of conflict. People don’t fight over solutions, they fight over their needs. These needs may have been identified in the exploration phase, but it’s worth getting each party to explicitly state their needs in their own words so they can be confident that these will be considered in any proposed solution. “What is most important to you as we search for a solution?”, “What needs do you have that need to be met in order to know that we’ve solved the problem”\nBrainstorm alternatives. Just list the options, you’re not settling on any single option yet. See rules for brainstorming below.\nCome up with actions or solutions. Consensus is often helpful at this point. This is not the same as unanimity. Consensus means that everyone has contributed to the process of brainstorming solutions. Everyone can support the final solution even if it wasn’t their idea or their first choice. The decisions made by the group may well be different than those made if one person had been in complete control, and this is ok (and actively encouraged).\n\nRules for brainstorming:\n\nMake sure everyone understand the desired outcome.\nEach person shares ideas. Can be round robin or spontaneously.\nOne thought expressed at a time.\nNo criticism of any ideas.\nOutrageous ideas encouraged.\nNo discussion of the relative merits of ideas, only to clarify its meaning.\nBuild on each other’s ideas."
  },
  {
    "objectID": "posts/business/executive_presence/presence.html",
    "href": "posts/business/executive_presence/presence.html",
    "title": "Executive Presence",
    "section": "",
    "text": "Identify personal blocks.\n\n\nSelf-reflection is important, just beware of “the judgement zone”.\nYou are not the same person you were at 9 years old, 20 years old, 21 years old. You are always changing. Be compassionate towards yourself. Compassionate, curious and committed.\nIdentify your strengths and “stretches”; where you can grow. But these “stretch” areas change over time because you will improve and change.\n\n\n\nWhat is your habitual mind state? Do you see yourself as “less than”, shy, or some other negative trait?\n“The chains of habit are too light to be felt until they’re too heavy to be broken.”\n\n\n\nThere are several “errors in thinking” that cause us to be depressed:\n\nMinimise positives and maximise negatives.\nOvergeneralise.\nFuture telling. Assume you’re going to mess something up, which makes you more likely to mess it up.\nMind reading. You assume everyone thinks you’re terrible.\nYou “should” yourself. “I should be doing better”, “I should make no mistakes”, “I should be fluent in this”, etc.\n\nThis is a vicious cycle; once a few negative thoughts creep in, we might make a mistake and reinforce this view so more negative thoughts flood in.\nThe way to correct this is to think objectively. It can be helpful to do this in writing. Being objective doesn’t mean being blindly positive. Write down a negative thought and ask a close friend what they think about it - do they agree?\n\n\n\nThings that are making us seem less confident:\n\nRising intonation when things aren’t questions. Address this by imagining a big full stop at the end of your sentence.\nFiller words - “umm, err, like”. Stop and pause instead of using filler words.\nHigh pitched voice.\n\n\n\nRecord yourself speaking and identify which of the following vocal checklist applies to you:\n\nAiry or breathy\nSoft-apoken\nWhimsical\nFast\nSlurred or tired\nOver-articulated\nWarm\nRelaxed and comfortable\nHoarse\nHonky\nHyper-nasal\nHarsh\nStrained\nRaspy\nWobbly\nLoud\nChesty\nChild-like\nSultry\n\n\n\n\nWarm up your breathing:\n\nConnect to your breath. Notice your breath and let it drop down to your diaphragm.\nRelax your belly. Rub your belly with your hands.\nSend your breath down to your toes.\nBreathe in through nose and sigh out through mouth.\n\nWarm up your face:\n\nBring your voice forward to the front of your mouth. Roll your tongue forcefully behind your lips.\nBlow air through lips on a sigh. This should flap your lips.\n\nWarm up your lips:\n\nHum on a sigh. Feel the buzzing on your lips.\nSend the humming vibrations to a spot in front of you.\n\nWarm up your nasal resonators:\n\nCreate a bright “me” sound and push it out of your nose and the middle of your face.\nSend the sound through your cheekbones. Use your hands to touch your cheeks in rhythm with the “me” sound to help engage them.\n\nWarm up your chest resonators:\n\nConnect to your chest while making a “maaah” and “haah” sounds.\nThump on your chest as you do it to engage it.\n\nWarm up your articulators:\n\n“Topeka” x3 and “bodega” x3.\nAlternate the sounds. Play with pitch and rhythm.\n\nIntegrate your voice:\n\nSlide sound from head (nasal resonators) down to chest resonators, then out through mouth .\n“Me me me me me me my my my my ha hum ma”.\n\n\n\n\n\nExercise more, you’ll naturally stand up straighter.\nPilates and intentional breathing is helpful. Two useful Pilates exercises are “hundreds” and the bridge.\nThink of the core not just as the abs, but everything from the pelvic floor up to the neck."
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#assessing-yourself",
    "href": "posts/business/executive_presence/presence.html#assessing-yourself",
    "title": "Executive Presence",
    "section": "",
    "text": "Self-reflection is important, just beware of “the judgement zone”.\nYou are not the same person you were at 9 years old, 20 years old, 21 years old. You are always changing. Be compassionate towards yourself. Compassionate, curious and committed.\nIdentify your strengths and “stretches”; where you can grow. But these “stretch” areas change over time because you will improve and change."
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#your-mind",
    "href": "posts/business/executive_presence/presence.html#your-mind",
    "title": "Executive Presence",
    "section": "",
    "text": "What is your habitual mind state? Do you see yourself as “less than”, shy, or some other negative trait?\n“The chains of habit are too light to be felt until they’re too heavy to be broken.”"
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#cognitive-corrections",
    "href": "posts/business/executive_presence/presence.html#cognitive-corrections",
    "title": "Executive Presence",
    "section": "",
    "text": "There are several “errors in thinking” that cause us to be depressed:\n\nMinimise positives and maximise negatives.\nOvergeneralise.\nFuture telling. Assume you’re going to mess something up, which makes you more likely to mess it up.\nMind reading. You assume everyone thinks you’re terrible.\nYou “should” yourself. “I should be doing better”, “I should make no mistakes”, “I should be fluent in this”, etc.\n\nThis is a vicious cycle; once a few negative thoughts creep in, we might make a mistake and reinforce this view so more negative thoughts flood in.\nThe way to correct this is to think objectively. It can be helpful to do this in writing. Being objective doesn’t mean being blindly positive. Write down a negative thought and ask a close friend what they think about it - do they agree?"
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#your-voice",
    "href": "posts/business/executive_presence/presence.html#your-voice",
    "title": "Executive Presence",
    "section": "",
    "text": "Things that are making us seem less confident:\n\nRising intonation when things aren’t questions. Address this by imagining a big full stop at the end of your sentence.\nFiller words - “umm, err, like”. Stop and pause instead of using filler words.\nHigh pitched voice.\n\n\n\nRecord yourself speaking and identify which of the following vocal checklist applies to you:\n\nAiry or breathy\nSoft-apoken\nWhimsical\nFast\nSlurred or tired\nOver-articulated\nWarm\nRelaxed and comfortable\nHoarse\nHonky\nHyper-nasal\nHarsh\nStrained\nRaspy\nWobbly\nLoud\nChesty\nChild-like\nSultry\n\n\n\n\nWarm up your breathing:\n\nConnect to your breath. Notice your breath and let it drop down to your diaphragm.\nRelax your belly. Rub your belly with your hands.\nSend your breath down to your toes.\nBreathe in through nose and sigh out through mouth.\n\nWarm up your face:\n\nBring your voice forward to the front of your mouth. Roll your tongue forcefully behind your lips.\nBlow air through lips on a sigh. This should flap your lips.\n\nWarm up your lips:\n\nHum on a sigh. Feel the buzzing on your lips.\nSend the humming vibrations to a spot in front of you.\n\nWarm up your nasal resonators:\n\nCreate a bright “me” sound and push it out of your nose and the middle of your face.\nSend the sound through your cheekbones. Use your hands to touch your cheeks in rhythm with the “me” sound to help engage them.\n\nWarm up your chest resonators:\n\nConnect to your chest while making a “maaah” and “haah” sounds.\nThump on your chest as you do it to engage it.\n\nWarm up your articulators:\n\n“Topeka” x3 and “bodega” x3.\nAlternate the sounds. Play with pitch and rhythm.\n\nIntegrate your voice:\n\nSlide sound from head (nasal resonators) down to chest resonators, then out through mouth .\n“Me me me me me me my my my my ha hum ma”."
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#your-body",
    "href": "posts/business/executive_presence/presence.html#your-body",
    "title": "Executive Presence",
    "section": "",
    "text": "Exercise more, you’ll naturally stand up straighter.\nPilates and intentional breathing is helpful. Two useful Pilates exercises are “hundreds” and the bridge.\nThink of the core not just as the abs, but everything from the pelvic floor up to the neck."
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#everything-is-sales",
    "href": "posts/business/executive_presence/presence.html#everything-is-sales",
    "title": "Executive Presence",
    "section": "4.1. Everything is Sales",
    "text": "4.1. Everything is Sales\nEverything is selling even if you don’t realise it. If you are recommending a place to eat, or talking about yourself or selling a product, it’s the same idea.\nReframe selling as being in service to the other person. You’re doing them a favour.\nRemind yourself:\n\nYou have amazing gifts to share with the world\nPeople love to be inspired\nYour work is your service to other people"
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#find-your-spark",
    "href": "posts/business/executive_presence/presence.html#find-your-spark",
    "title": "Executive Presence",
    "section": "4.2. Find Your Spark",
    "text": "4.2. Find Your Spark\nWhat is an activity that gets you “in the zone”, in a state of flow? What is it about that activity that does it for you? What are the things that light you up?\nWhen you are in the zone, you are not self-doubting, overly critical, etc. Try to mimic that confidence when you’re not in a flow state; you know you are capable of it.\nI am in the zone when…\n\nIn the zone I am…\n\nAt work, I love to…\nWhen you’re communicating, e.g. giving an elevator pitch about yourself, it’s not just a list of facts you’re communicating. Think about what gets you in a flow state and get that across to the other person."
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#the-elevator-pitch",
    "href": "posts/business/executive_presence/presence.html#the-elevator-pitch",
    "title": "Executive Presence",
    "section": "4.3. The Elevator Pitch",
    "text": "4.3. The Elevator Pitch\nThe goal isn’t to information dump, it’s to spark curiosity so that the other person wants to carry on talking to you after the elevator ride is over.\n\nThe purpose of an elevator pitch isn’t to close the sale. The goal isn’t even to give a short, accurate, Wikipedia-standard description of you or your project. And the idea of using vacuous, vague words to craft a bland mission statement is dumb. No, the purpose of an elevator pitch is to describe a situation or solution so compelling that the person you’re with wants to hear more even after the elevator ride is over.\n\n- Seth Godin\nHow do you typically describe what you do?\nThink about what your spark is and then rewrite your elevator pitch with that at the centre. Then practice the new pitch.\nThe sign of a successful elevator pitch is that they ask you questions. You’ve hooked them and they want to know more."
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#listening",
    "href": "posts/business/executive_presence/presence.html#listening",
    "title": "Executive Presence",
    "section": "4.4. Listening",
    "text": "4.4. Listening\nHow to listen your way to more friends/sales:\n\nShift your attention from yourself (and what you plan to say next) to the other person\nBe present\nAsk questions - “what brings you here?” is open-ended enough to get to the heart of what they need not just what they’re doing.\nLet the conversation flow, don’t force it back to your topic / product\nBe generous - “Who can I help?”. Help others and they’ll want to help you."
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#how-to-identify-your-ideal-client",
    "href": "posts/business/executive_presence/presence.html#how-to-identify-your-ideal-client",
    "title": "Executive Presence",
    "section": "4.5. How to Identify Your Ideal Client",
    "text": "4.5. How to Identify Your Ideal Client\nLots of sales approaches try to cast too wide a net - “anybody is a potential client”. Adopt an abundance mindset - “there are so many potential clients out there that I can be picky”.\nAsk yourself the following questions to figure out your ideal client:\n\nWhich clients have you enjoyed working with the most? How would you describe them? What was your experience working together?\nWhat aspect of your work truly lights you up?\nWhat problems did they have that you solved? How did you impact their lives?\n\nThen write a client avatar for that ideal client:\n\nName\nAge\nProfession\nChallenge/Need/Frustration\nWhat are their fears?\nWhat are their desires and dreams?\nWhy do they need you?\nHow do they feel working with you?"
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#dealing-with-rejection",
    "href": "posts/business/executive_presence/presence.html#dealing-with-rejection",
    "title": "Executive Presence",
    "section": "4.6. Dealing with Rejection",
    "text": "4.6. Dealing with Rejection\nHow do you deal with rejection? You can’t control being rejected but you can control your response to it.\n\nDon’t take it personally.\nFeelings are not facts. Just because you feel like you failed, you performed badly, etc, doesn’t mean you are bad at what you do.\nPause before reacting.\n\nProcessing rejection:\n\nWrite. Brain dump your feelings.\nShower.\nFlick. If you have the urge to beat yourself up, put an elastic band on your wrist and flick yourself whenever you feel bad and move on."
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#picture-the-stage",
    "href": "posts/business/executive_presence/presence.html#picture-the-stage",
    "title": "Executive Presence",
    "section": "5.1. Picture the stage",
    "text": "5.1. Picture the stage\nAllow yourself to dream big. What is your ideal stage you would want to be on?\nOften the same limiting thoughts that tell us we aren’t good at presenting or public speaking are the same root cause as those stopping us from speaking our mind in meetings or asking for a promotion."
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#how-to-interview",
    "href": "posts/business/executive_presence/presence.html#how-to-interview",
    "title": "Executive Presence",
    "section": "5.2. How to Interview",
    "text": "5.2. How to Interview\nThink about what you love, your “why”. You don’t need a script, but you need to be able to articulate this.\nCommunicate how you do it. Think about a nice sound bite, especially if it’s a media interview. Short and sweet message.\nPay attention to what the other person is saying. Don’t be so focused on yourself that you don’t engage with the other person.\nPractice interviewing in lower stakes situations."
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#powerpoint-presentations",
    "href": "posts/business/executive_presence/presence.html#powerpoint-presentations",
    "title": "Executive Presence",
    "section": "5.3. PowerPoint Presentations",
    "text": "5.3. PowerPoint Presentations\nLess is more.\nWe remember stories, not information. Your job isn’t to dump information, it’s to tell a story.\n\nWhat is your story?\nThe targeted takeaway. What do you want the audience to remember? What action should they take?\nToss the script. Create thought bubbles around high level ideas that you can talk around them spontaneously. Instead of a script, it’s a journey through thoughts, like acts in a story. One thought bubble per slide keeps the message clear.\n\nPreparing your presentation:\n\nIdentify your goal. What should the audience feel or do?\nWhat stories illustrate your point?\nWhat images are absolutely necessary to support your point?\nWhat thought bubbles walk your audience through your message?\nWhat are the transitions to get from one thought bubble to the next?\n\nSuggested TED Talks:\n\nSusan Cain: The Power of Introverts\nBrene Brown: The Power of Vulnerability\nAmy Cuddy: Your Body Language Shapes Who You Are"
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#preparation-exercises",
    "href": "posts/business/executive_presence/presence.html#preparation-exercises",
    "title": "Executive Presence",
    "section": "5.4. Preparation exercises",
    "text": "5.4. Preparation exercises\n\nSmile\nBreathe\nEngage. Look at the audience. Look at the back row. Pick a friendly face; some may look disinterested but find the ones that are keen.\nPrepare. Use notes as necessary.\nVisualise. Imagine the presentation going well and the audience responding well.\nLet go. You may not see immediately whether people have taken your message on board and you may never see the impact directly. Trust that your message will have an impact. You can control your contribution but not the outcome."
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#dealing-with-anxiety",
    "href": "posts/business/executive_presence/presence.html#dealing-with-anxiety",
    "title": "Executive Presence",
    "section": "5.5. Dealing with Anxiety",
    "text": "5.5. Dealing with Anxiety\n\nDeep breathing. Physiological sighs.\nVisualisation. Picture the presentation and the audience’s positive reaction. Notice the details of the room.\nListen to relaxing sounds and music."
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#holding-the-neuro",
    "href": "posts/business/executive_presence/presence.html#holding-the-neuro",
    "title": "Executive Presence",
    "section": "5.6. Holding the Neuro",
    "text": "5.6. Holding the Neuro\nThe neurovascular points are the ridges above the eyes. They regulate the flow of blood in the body and bring blood to the thinking brain.\nWhen you are under stress, less blood goes to the frontal area of your brain. Rubbing the neurovascular points and temples can stimulate more blood flow. Do this while breathing deeply.\nA hand on the forehead and the other hand on the back of the neck can also help.\nThis can also help when processing trauma."
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#being-comfortable-on-camera",
    "href": "posts/business/executive_presence/presence.html#being-comfortable-on-camera",
    "title": "Executive Presence",
    "section": "6.1. Being Comfortable on Camera",
    "text": "6.1. Being Comfortable on Camera\nPractice in low stakes situations. FaceTime calls are the same basic idea as being on camera, so get used to it. Practice looking at the camera not the person; looking at a camera lens is weird and unusual so practice it."
  },
  {
    "objectID": "posts/business/executive_presence/presence.html#types-of-videos",
    "href": "posts/business/executive_presence/presence.html#types-of-videos",
    "title": "Executive Presence",
    "section": "6.2. Types of Videos",
    "text": "6.2. Types of Videos\nSuggestions of business video content:\n\nProduct demonstrations\nAbout me\nNew product and service launches\nHomepage\nMeet the team\nBehind the scenes\nWeekly video blogs\nFrequently asked questions\nMarket updates\nReviews\n\nThink about the videos you like and dislike.\nTips:\n\nKnow your viewer\nKeep it short and sweet\nLove the lens\nUse calls to action\nPractice"
  },
  {
    "objectID": "posts/business/pitching/pitching.html",
    "href": "posts/business/pitching/pitching.html",
    "title": "Pitching Notes",
    "section": "",
    "text": "Notes from “Pitch Anything” by Oren Klaff.\nSome of the book comes across as a bit incel sigma vibes, talking about alphas and betas. I don’t agree with it, but summarising it here for the parts that are interesting.\n\n\nFrames, AKA perspective AKA point of view, describe a person’s mental model of controlling the world. The person who owns the frame owns the conversation.\nInformation must penetrate through the following layers of the brain in order.\n\nSurvival - “crocodile brain”. Ignore anything that isn’t new, exciting, dangerous, unexpected. Needs big picture, high contrast points.\nSocial\nReason\n\nLogic won’t reach the “reason” part of the brain until the croc brain and social brain are satiated. Data is perceived as a threat by layer 1. the croc brain needs unexpected information that is concrete not abstract.\nSTRONG process:\n\nSet the frame\nTell the story\nReveal the intrigue\nOffer the prize\nNail the hook point\nGet the deal\n\n\n\n\nFrames are the mental structure that shape how you see the world. When frames collide, the stronger frame absorbs the weaker frame. If you have to explain your authority, power, position, leverage then you have the weaker frame.\nOpposing frames and how to counter them.\n\nPower frame - fuelled by status and ego\n\nDon’t react to our strengthen their status\nActs of denial/defiance - establish your own rituals of power rather than abiding by theirs.\nMildly shocking but not unfriendly. Humour is key.\n\nPrize frame\n\nMake yourself the prize. Make them qualify themselves to you. The money has to earn you, not the other way around.\nMake statements not “trial close” questions\n\nTime frame - occurs later in the interaction\n\nStop as soon as you are done OR have lost their attention. Continuing further signals desperation.\n\nAnalyst frame - stats, technical details\n\nIntrigue frame counters this\nThe brain can’t perform cold cognitions (analysis) at the same time as hot cognitions (excitement)\nTell a brief, relevant, intriguing story with tension involving you at the centre of it to break the cold cognition.\n\n\n\n\n\nGlobal status is a function of wealth, power and status.\nBeta traps enforce your lower status. E.g. making you wait, big dogs not showing up to meetings, small talk before meetings.\nSituational status is the temporary shift of status based on the situation. Create local star power to increase your situational status.\n\nIgnore power rituals, avoid beta traps, ignore their global status\nFrame control - small, friendly acts of defiance\nLocal star power - shift conversation to a topic where you are the domain expert\nPrize frame - position yourself as the reward, e.g. leave after a hard cut off time\nConfirm your status - make them say you’re the alpha\n\nShock -&gt; frame control -&gt; local star power -&gt; hook point -&gt; leave\n\n\n\nEvery pitch should tell a story.\nFour sections of the pitch:\n\nIntroduce yourself and the big idea (5 mins)\nExplain the budget and secret sauce (10 mins)\nOffer the deal (2 mins)\nStack hot cognition frames (3 mins)\n\n\n\nKeep the pitch short - 20 mins.\n\nAnnounce the time constraint at the start to appease the croc brain. Otherwise people don’t know how long they are trapped for and panic.\nDNA presentation by Crick and Watson was 5 mins\n\nIntro should be highlights of successes NOT a life story.\n\nThe impression you leave is based the average not the sum, so don’t dilute it.\n\nThe “why now?” frame. 3 market forces pattern:\n\nEconomic\nSocial\nTechnology\n\nTell the story of how your idea came to be.\n\nMovement is key to overcoming change blindness. Don’t just show 2 states, describe the transition between them.\nTrends, impacts of those trends, how have these opened a (temporary) market window (time frame).\nThis story places you as the hero at the centre of solving this problem (prize frame).\n\nIdea introduction pattern:\nFor [target customers]\n\nWho are dissatisfied with [current offerings in the market]\n \nMy idea/product is a [new idea or product category]\n \nThat provides [key problem/solution features]\n \nUnlike [the competing product]\n \nMy idea/product is [describe key features]\n\n\n\nTune the message to the croc brain of the target.\n\nSimplicity can come across as naive, but focus on describing concrete things like relationships between people. This is high-level enough that it will not engage cold cognitions or threaten the croc brain.\n\nAttention = Desire (anticipation of reward) + Tension (fear of loss, introduce consequences)\n\nAttention is high when novelty is high.\nPush/pull patterns to increase tension and desire\nThe pitch narrative is a series of tension loops.\nWhen the tension is released and attention is lost the pitch is over.\nDesire eventually becomes fear, so keep the pitch short.\n\nMust haves:\n\nNumbers and projections\n\nThis should demonstrate your skill at budgeting (a difficult, precise skill), not your skill at projecting (a simple, inaccurate skill).\n\nCompetition\n\nHow easy is it for new competitors to enter?\nHow easy is it for customers to switch TO you and AWAY from you?\n\nSecret sauce\n\nYour competitive advantage.\nPhrasing it this way frames you as the prize.\n\n\n\n\n\nDescribe what they get if they do business with you\n\nWhat you will deliver, when, and how.\nWhat are their roles, responsibilities and next steps.\n\n\n\n\n\nPropose something concrete and actionable in such a compelling way that the target wants to chase it. Don’t wait for the target to evaluate you on the spot at the end of the pitch as that triggers cold cognitions. Pile up the hot cognitions instead.\nA hot cognition is emotional rather than analytical.\n\nYou decide that you like something before you understand it.\nData is used to justify decisions after the fact.\nHot cognitions are generally unavoidable; you may be able to control the expression of emotion, but the initial experience is still there.\nHot cognitions tend to be instant and enduring.\nHot vs cold cognitions can be characterised by spinach vs chocolate - you know one is good for you but you want the other anyway.\nHot cognitions are “knowing” something through feeling it, cold cognitions are “knowing something through evaluating it.”\n\nStacking frames.\n\nWe want to stack frames to create “wanting”, an emotional response\nThis is not the same as getting the target to “like” us, as that is a slow, intellectual cold cognition.\nAppealing to the analytical brain can only trigger a “like”; we want to trigger a “want” in the croc brain.\n\nThe 4 frame stack of hot cognitions:\n\nThe intrigue frame\n\nA story where most important things are who it happened to and how the characters reacted to their situation.\nThe events don’t need to be extreme but the characters’ reactions should be.\nA narrative that feels correct in time with convey a strong sense of truth and accuracy.\n“Man in the jungle” formula - Put a man in the jungle; have beasts attack him; will he get to safety?; get the man to the edge of the jungle but don’t get him out of it (cliffhanger)\n\nThe prize frame\n\nPosition yourself as the most important party in the deal. Conviction is key.\n“I am the prize. You are trying to impress me. You are trying to win my approval.”\n\nThe time frame\n\nThis creates a scarcity bias that triggers fear that triggers action.\nExtreme time pressure feels forced and cutrate. There is a balance between pressure and fairness; there should be a real-world time constraint.\n\nThe moral authority frame\n\nOnce the frame stacking is complete, you have the target’s attention for about 30 seconds.\n\n\n\nValidation-seeking behaviour (i.e. neediness) is the number one deal killer.\n\nNeediness is a threat signal, it triggers fear and uncertainty in the target which triggers self-protection.\n\nAvoid “soft closes” aiming to seek validation, e.g. “So what do you think so far?”, “Do you still think this is a good deal?”\nCauses of neediness:\n\nAnxiety and insecurity turn to fear, so you resort to acceptance-seeking behaviour to confirm that you’re still “part of the pack”\nResponse to disappointment is to seek validation\nWe want something only the target can give us\nWe need cooperation from the target, and the lack of cooperation causes anxiety\nWe believe the target can make us feel good by accepting our offer\n\nCounteracting validation-seeking behaviours:\n\nWant nothing\nFocus only on things you do well\nAnnounce your intention to leave - time frame. When you finish the pitch, withdraw and deny your audience; this creates a prize frame.\n\nMantra: \"I don't need these people, they need me. I am the prize\".\nExample:\n\nTime frame - “The deal will be fully subscribed in the next 10 days”.\nPower frame - “We don’t need VC money but want a big name on cap sheet for later IPO”\nPrize frame - “Are you really the right investor for this? We need to know more about the relationships and value you can bring.”\n\n\n\n\nYou are not pushing, you are interacting with people using basic rules of social dynamics.\nCroc brain:\n\nBoring: Ignore it\nDangerous? Fight it or run\nComplicated? Radically summarise\n\nThe presenter’s problem boils down to deciding which information to include. It is not like an engineering problem where more information is better.\nThree key insights:\n\nAppeal to the croc brain\nControl frames\nUse humour - this is not to relieve tension but to signal that you are so confident that you’re able to play around a little\n\nIf the pitch stops being fun, you’re doing it wrong.\nSteps to learning the method:\n\nRecognise beta traps; anything designed to control your behaviour\nStep around beta traps\nIdentify social frames\nInitiate frame collisions with safe targets - always with good humour and “a twinkle in your eye”\nPractice the push/pull of small acts of defiance and denial\n\nThe only rule is that you make the rules that others follow."
  },
  {
    "objectID": "posts/business/pitching/pitching.html#the-method",
    "href": "posts/business/pitching/pitching.html#the-method",
    "title": "Pitching Notes",
    "section": "",
    "text": "Frames, AKA perspective AKA point of view, describe a person’s mental model of controlling the world. The person who owns the frame owns the conversation.\nInformation must penetrate through the following layers of the brain in order.\n\nSurvival - “crocodile brain”. Ignore anything that isn’t new, exciting, dangerous, unexpected. Needs big picture, high contrast points.\nSocial\nReason\n\nLogic won’t reach the “reason” part of the brain until the croc brain and social brain are satiated. Data is perceived as a threat by layer 1. the croc brain needs unexpected information that is concrete not abstract.\nSTRONG process:\n\nSet the frame\nTell the story\nReveal the intrigue\nOffer the prize\nNail the hook point\nGet the deal"
  },
  {
    "objectID": "posts/business/pitching/pitching.html#frame-control",
    "href": "posts/business/pitching/pitching.html#frame-control",
    "title": "Pitching Notes",
    "section": "",
    "text": "Frames are the mental structure that shape how you see the world. When frames collide, the stronger frame absorbs the weaker frame. If you have to explain your authority, power, position, leverage then you have the weaker frame.\nOpposing frames and how to counter them.\n\nPower frame - fuelled by status and ego\n\nDon’t react to our strengthen their status\nActs of denial/defiance - establish your own rituals of power rather than abiding by theirs.\nMildly shocking but not unfriendly. Humour is key.\n\nPrize frame\n\nMake yourself the prize. Make them qualify themselves to you. The money has to earn you, not the other way around.\nMake statements not “trial close” questions\n\nTime frame - occurs later in the interaction\n\nStop as soon as you are done OR have lost their attention. Continuing further signals desperation.\n\nAnalyst frame - stats, technical details\n\nIntrigue frame counters this\nThe brain can’t perform cold cognitions (analysis) at the same time as hot cognitions (excitement)\nTell a brief, relevant, intriguing story with tension involving you at the centre of it to break the cold cognition."
  },
  {
    "objectID": "posts/business/pitching/pitching.html#status",
    "href": "posts/business/pitching/pitching.html#status",
    "title": "Pitching Notes",
    "section": "",
    "text": "Global status is a function of wealth, power and status.\nBeta traps enforce your lower status. E.g. making you wait, big dogs not showing up to meetings, small talk before meetings.\nSituational status is the temporary shift of status based on the situation. Create local star power to increase your situational status.\n\nIgnore power rituals, avoid beta traps, ignore their global status\nFrame control - small, friendly acts of defiance\nLocal star power - shift conversation to a topic where you are the domain expert\nPrize frame - position yourself as the reward, e.g. leave after a hard cut off time\nConfirm your status - make them say you’re the alpha\n\nShock -&gt; frame control -&gt; local star power -&gt; hook point -&gt; leave"
  },
  {
    "objectID": "posts/business/pitching/pitching.html#pitching-the-big-idea",
    "href": "posts/business/pitching/pitching.html#pitching-the-big-idea",
    "title": "Pitching Notes",
    "section": "",
    "text": "Every pitch should tell a story.\nFour sections of the pitch:\n\nIntroduce yourself and the big idea (5 mins)\nExplain the budget and secret sauce (10 mins)\nOffer the deal (2 mins)\nStack hot cognition frames (3 mins)\n\n\n\nKeep the pitch short - 20 mins.\n\nAnnounce the time constraint at the start to appease the croc brain. Otherwise people don’t know how long they are trapped for and panic.\nDNA presentation by Crick and Watson was 5 mins\n\nIntro should be highlights of successes NOT a life story.\n\nThe impression you leave is based the average not the sum, so don’t dilute it.\n\nThe “why now?” frame. 3 market forces pattern:\n\nEconomic\nSocial\nTechnology\n\nTell the story of how your idea came to be.\n\nMovement is key to overcoming change blindness. Don’t just show 2 states, describe the transition between them.\nTrends, impacts of those trends, how have these opened a (temporary) market window (time frame).\nThis story places you as the hero at the centre of solving this problem (prize frame).\n\nIdea introduction pattern:\nFor [target customers]\n\nWho are dissatisfied with [current offerings in the market]\n \nMy idea/product is a [new idea or product category]\n \nThat provides [key problem/solution features]\n \nUnlike [the competing product]\n \nMy idea/product is [describe key features]\n\n\n\nTune the message to the croc brain of the target.\n\nSimplicity can come across as naive, but focus on describing concrete things like relationships between people. This is high-level enough that it will not engage cold cognitions or threaten the croc brain.\n\nAttention = Desire (anticipation of reward) + Tension (fear of loss, introduce consequences)\n\nAttention is high when novelty is high.\nPush/pull patterns to increase tension and desire\nThe pitch narrative is a series of tension loops.\nWhen the tension is released and attention is lost the pitch is over.\nDesire eventually becomes fear, so keep the pitch short.\n\nMust haves:\n\nNumbers and projections\n\nThis should demonstrate your skill at budgeting (a difficult, precise skill), not your skill at projecting (a simple, inaccurate skill).\n\nCompetition\n\nHow easy is it for new competitors to enter?\nHow easy is it for customers to switch TO you and AWAY from you?\n\nSecret sauce\n\nYour competitive advantage.\nPhrasing it this way frames you as the prize.\n\n\n\n\n\nDescribe what they get if they do business with you\n\nWhat you will deliver, when, and how.\nWhat are their roles, responsibilities and next steps."
  },
  {
    "objectID": "posts/business/pitching/pitching.html#stack-hot-cognition-frames-3-mins",
    "href": "posts/business/pitching/pitching.html#stack-hot-cognition-frames-3-mins",
    "title": "Pitching Notes",
    "section": "",
    "text": "Propose something concrete and actionable in such a compelling way that the target wants to chase it. Don’t wait for the target to evaluate you on the spot at the end of the pitch as that triggers cold cognitions. Pile up the hot cognitions instead.\nA hot cognition is emotional rather than analytical.\n\nYou decide that you like something before you understand it.\nData is used to justify decisions after the fact.\nHot cognitions are generally unavoidable; you may be able to control the expression of emotion, but the initial experience is still there.\nHot cognitions tend to be instant and enduring.\nHot vs cold cognitions can be characterised by spinach vs chocolate - you know one is good for you but you want the other anyway.\nHot cognitions are “knowing” something through feeling it, cold cognitions are “knowing something through evaluating it.”\n\nStacking frames.\n\nWe want to stack frames to create “wanting”, an emotional response\nThis is not the same as getting the target to “like” us, as that is a slow, intellectual cold cognition.\nAppealing to the analytical brain can only trigger a “like”; we want to trigger a “want” in the croc brain.\n\nThe 4 frame stack of hot cognitions:\n\nThe intrigue frame\n\nA story where most important things are who it happened to and how the characters reacted to their situation.\nThe events don’t need to be extreme but the characters’ reactions should be.\nA narrative that feels correct in time with convey a strong sense of truth and accuracy.\n“Man in the jungle” formula - Put a man in the jungle; have beasts attack him; will he get to safety?; get the man to the edge of the jungle but don’t get him out of it (cliffhanger)\n\nThe prize frame\n\nPosition yourself as the most important party in the deal. Conviction is key.\n“I am the prize. You are trying to impress me. You are trying to win my approval.”\n\nThe time frame\n\nThis creates a scarcity bias that triggers fear that triggers action.\nExtreme time pressure feels forced and cutrate. There is a balance between pressure and fairness; there should be a real-world time constraint.\n\nThe moral authority frame\n\nOnce the frame stacking is complete, you have the target’s attention for about 30 seconds."
  },
  {
    "objectID": "posts/business/pitching/pitching.html#eradicating-neediness",
    "href": "posts/business/pitching/pitching.html#eradicating-neediness",
    "title": "Pitching Notes",
    "section": "",
    "text": "Validation-seeking behaviour (i.e. neediness) is the number one deal killer.\n\nNeediness is a threat signal, it triggers fear and uncertainty in the target which triggers self-protection.\n\nAvoid “soft closes” aiming to seek validation, e.g. “So what do you think so far?”, “Do you still think this is a good deal?”\nCauses of neediness:\n\nAnxiety and insecurity turn to fear, so you resort to acceptance-seeking behaviour to confirm that you’re still “part of the pack”\nResponse to disappointment is to seek validation\nWe want something only the target can give us\nWe need cooperation from the target, and the lack of cooperation causes anxiety\nWe believe the target can make us feel good by accepting our offer\n\nCounteracting validation-seeking behaviours:\n\nWant nothing\nFocus only on things you do well\nAnnounce your intention to leave - time frame. When you finish the pitch, withdraw and deny your audience; this creates a prize frame.\n\nMantra: \"I don't need these people, they need me. I am the prize\".\nExample:\n\nTime frame - “The deal will be fully subscribed in the next 10 days”.\nPower frame - “We don’t need VC money but want a big name on cap sheet for later IPO”\nPrize frame - “Are you really the right investor for this? We need to know more about the relationships and value you can bring.”"
  },
  {
    "objectID": "posts/business/pitching/pitching.html#closing-thoughts",
    "href": "posts/business/pitching/pitching.html#closing-thoughts",
    "title": "Pitching Notes",
    "section": "",
    "text": "You are not pushing, you are interacting with people using basic rules of social dynamics.\nCroc brain:\n\nBoring: Ignore it\nDangerous? Fight it or run\nComplicated? Radically summarise\n\nThe presenter’s problem boils down to deciding which information to include. It is not like an engineering problem where more information is better.\nThree key insights:\n\nAppeal to the croc brain\nControl frames\nUse humour - this is not to relieve tension but to signal that you are so confident that you’re able to play around a little\n\nIf the pitch stops being fun, you’re doing it wrong.\nSteps to learning the method:\n\nRecognise beta traps; anything designed to control your behaviour\nStep around beta traps\nIdentify social frames\nInitiate frame collisions with safe targets - always with good humour and “a twinkle in your eye”\nPractice the push/pull of small acts of defiance and denial\n\nThe only rule is that you make the rules that others follow."
  },
  {
    "objectID": "projects_section/xai_research/xai.html",
    "href": "projects_section/xai_research/xai.html",
    "title": "Explainable AI in Healthcare",
    "section": "",
    "text": "This is a research project I completed which aimed to quantify the confidence we should have in a trained model applied ot our data set.\nThe full paper is available here.\nSlides from a presentation I gave at the Nuffield Department of Orthopaedics, Rheumatology and Musculoskeletal Sciences (NDORMS) at the University of Oxford are available here.\n\n\n\n Back to top"
  },
  {
    "objectID": "projects_section/tradeintel/tradeintel.html",
    "href": "projects_section/tradeintel/tradeintel.html",
    "title": "TradeIntel",
    "section": "",
    "text": "This is a robo-advisor app to give tailored stock portfolio recommendations.\nThe user can input high-level preferences like their risk tolerance, industry preferences, and how closely they would like to follow the broader market. We then perform a portfolio optimisation process to recommend a robust portfolio based on those criteria.\nThis is available on the App Store\n\n\n\n\n Back to top"
  },
  {
    "objectID": "projects_section/bjj/bjj.html",
    "href": "projects_section/bjj/bjj.html",
    "title": "Brazilian Jiu Jitsu Taxonomy",
    "section": "",
    "text": "This is a (work-in-progress) interactive React app I made to:\n\nKeep track of my BJJ notes\nPractice some web development\n\nCheck it out here.\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html",
    "href": "posts/business/public_speaking/public_speaking.html",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "Notes from reading “Ted Talks: The offical TED guide to public speaking” by Chris Anderson.\n\n\n\n\nPresentation literacy is its own skill. Your goal is to be you, not a version of someone you like.\n\n\n\nIdeas are all that matter in the talk. This can be a novel idea, or a novel story or presentation of an existing idea.\nYour goal is to recreate the core idea inside your audience’s mind. The language you use must be shared by the speaker and listener for it to have this effect. Focus on what you can give to the audience.\nVisualise the talk as a journey that you guide the user through. Start where the audience starts. Don’t make any impossible leaps or unexpected changes of direction.\n\nWhat issues matter the most\nHow are they related?\nHow can they be easily explained?\nWhat are the key open questions? Controversies?\n\n\n\n\nFour talk styles to AVOID.\n\nSales Pitch\n\nYou should give not take.\nGenerosity evokes a response.\n\nRamble\n\nInsults the audience; suggests that their time doesn’t matter to you.\n\nOrganisation bore\n\nYour company’s structure is boring.\nFocus on the work and ideas, not the company, products or individuals.\n\nInspiration tryhard\n\nAll style and no substance.\nComes across as manipulative.\nInspiration has to be earned. “Like love you don’t get it by pursuing it”.\nInspiration is the response to authenticity, courage, wisdom, selflessness.\n\n\n\n\n\nThe throughline is a connecting theme that ties the narrative elements\n\nA focused, precise, unexpected idea\nNot the same as a theme\nYou should be able to encapsulate it in &lt;15 words\n\nTrace the path the journey will take. If the audience knows where you’re going, it will be easier to follow.\nCut down the talk to keep the throughline focused.\n\nDON’T just keep all the content but say it in less detail. Overstuffed = underexplained\nCut back the range of topics. Say less with more impact.\n“The power of the deleted word”.\nAn 18 minute time limit is short enough to keep an audience’s attention, long enough to cover enough ground and precise enough to be taken seriously.\n\nThe overall structure can be thought of in 3 parts: What? So what? What now? In more detail, this is:\n\nIntro - what will be covered\nContext - why this issue matters\nMain concepts\nPractical implications\n\nTalk about ideas not issues. Write for an audience of one.\nChecklist:\n\nIs this a topic I’m passionate about? Do I know enough to be worth the audience’s time? Do I have credibility?\nDoes it inspire curiosity?\nWill knowing this make a difference to the audience?\nIs the talk a gift or an ask?\nIs the information fresh?\nCan the topic be covered in enough detail with examples in the time slot?\nWhat are the 15 words that encapsulate the talk?\nWould those 15 words persuade someone they’re interested in hearing the talk?\n\n\n\n\n\n\n\nA human connection is required before you can inform, persuade or inspire. Knowledge has to be pulled in, not pushed in. Be AUTHENTIC.\nDos\n\nEye contact.\nVulnerability, but authentic.\nHumour signals to the group that you’ve all connected.\n\nDon’ts\n\nEgo breeds contempt.\nAvoid tribal topics/language, e.g. politics.\n\n\n\n\nStorytelling helps people to imagine and dream -&gt; empathise -&gt; care\nKey elements:\n\nCharacter\nTension\nRight level of detail\nSatisfying resolution\n\nIf you’re telling a story, know why you’re telling it. The goal is to give, not boost your ego.\nConnect the dots enough, but don’t force-feed the conclusion.\n\n\n\nSteps:\n\nStart where the audience is. No assumed knowledge or jargon.\nSpark curiosity. Create a knowledge gap that the listener wants to fill.\nIntroduce new concepts one-by-one. Name each concept.\nMetaphors and analogies. Reveal the “shape” of new concepts.\nExamples confirm understanding.\n\nKnowledge is built hierarchically. Explain the structure and where each concept fits. Convert a web of ideas into a string of words.\nCurse of knowledge. Revisit assumed knowledge.\nMake clear what the concept isn’t. An explanation is a small mental model in a large space of possibilities. Reduce the size of that space.\n\n\n\nDemolish the listener’s existing knowledge and rebuild it; replace it with something better. Genius is something that comes to you, not something you are.\nAn “intuition pump” is an example or metaphor that makes the conclusion more plausible. Not a rigorous argument, but nudges the listener in the right direction. Prime with emotion or story, then persuade with reason.\nTurn the audience into detectives. Tell a story. Start with a big mystery, traverse the possible solutions until only one plausible conclusion remains.\n\n\n\nThree approaches to revelatory talks:\n\nThe wonder walk\n\nReveal a succession of inspiration images\nObvious throughline with accessible language and silence\nThe message should be “how intriguing is this” not “look what I achieved”\n\nDemo\n\nInitial tease, background context, demo, implications\n\nDreamscape\n\nCreate a world that doesn’t exist but someday might\nPaint a bold picture of an alternative future\nMake others desire that future\nEmphasise human values not just clever tech, otherwise the audience might freak out at the possible negative implications of the technology\n\n\n\n\n\n\n\n\nSlides can distract. If you do use them, make sure they add something.\nVisuals should:\n\nReveal - set context, prime, BAM!\nExplain - one idea per slide with a clickbait headline not a summary. Highlight the point you’re making.\nDelight - show impressive things and let them speak for themselves. Don’t need to explain every image/slide.\n\nDon’t let the slides be spoilers for what you’re about to say. The message loses its impact, it’s not news anymore.\n\n\n\nThe goal is to have a set structure that you speak naturally and authentically about.\nBoth approaches have the same result:\n\nScript and memorise until natural, or\nFreestyle around bullet points and rehearse until structure is set\n\nBeing read to and being talked to are very different experiences\n\nDictating the speech rather than writing it can help make sure it comes across how you would speak rather than how you would write.\nDon’t end up in the uncanny valley between reading and speaking\nIn some cases, reading is powerful if it’s clear that this is a poetic, written piece. Abandon the script for an impactful finale.\n\nWhat you are saying matters more than the exact word choice.\nRehearse your impromptu remarks.\n\n\n\nPractice speaking by speaking!\nRehearse in phases:\n\nEditing and cutting\nPacing and timing\nDelivery\n\nAim to use &lt;90% of the time limit. results in tighter writing, and time to riff and enjoy the reaction.\n\n\n\nYou have the audience’s attention at the start, then it’s yours to lose. Opener has to capture attention and keep it. 2 stages:\n\n10 seconds to capture attention\n1 minute to hold it\n\nOpening techniques:\n\nDrama\nCuriosity - more specific questions are more intriguing\nCompelling slide/image - “the next image changed my life”\nTense but don’t give away - show where you’re going but save the reveal\n\nThe closer dictates how the whole talk will be remembered.\nClosing techniques:\n\nCamera pullback - big picture and implications\nCall to action\nPersonal commitment\nInspiring values/vision\nEncapsulation - neatly reframe the main idea\nNarrative symmetry - callback to opener\nLyrical inspiration - poetic conclusion\n\n\n\n\n\n\n\n\nChoose an outfit early\nDress like the audience but smarter\nDress for the people in the back row\n\n\n\n\n\nEmbrace the nerves and draw attention to it. Vulnerability humanises you.\nPick friendly faces in the audience and speak to them.\nBackup plan - have a story ready to fill in during unexpected technical issues\nFocus on the message - “THIS MATTERS”\n\n\n\n\nVisual barriers between the speaker and audience can create a sense of authority but at the expense of a human connection. If they can see you, you’re vulnerable. If you’re vulnerable, they can connect.\nReferring to notes is fine if done sparingly and unambiguously. It humanises you if done honestly. It appears sneaky and dishonest if you try to do it secretly.\n\n\n\nSpeaking style adds another stream of input parallel to the words themselves. It tells the listener how they should interpret the words.\nSix voice tools. Vary each of them depending on the meaning and emotion.\n\nVolume\nPitch\nPace\nTimbre\nTone\nProsody (singsong rise and fall)\n\nSpeed should be a conversational pace, approx 130-170 words per minute.\n\nPeople often worry about speaking too fast\nSpeaking too slow is a more common problem (overcorrection?)\nUnderstanding outpaces articulation, the listener is “dying of word starvation”\n\nSpeak, don’t orate.\nBody language\n\nStand and move intentionally.\nStop to make a point then walk to the next point.\n\n\n\n\nAdd too many ingredients and you risk losing attention. What you say can just be signposts for what you show - set it up, show it, shut up.\n\n\n\n\n\n\nKnowledge of specific facts is becoming more specialised and commoditised. Understanding how everything fits together is becoming broader, more unified.\nConcepts of specialised knowledge are outdated, stemming from industrial age thinknig. Modern thinking values:\n\nContextual knowledge\nCreating knowledge\nUnderstanding of humanity\n\nAs people become more interconnected, innovation becomes crowd-accelerated.\nHappiness is finding something bigger than you and dedicating your life to it. Nudge the world - give it questions to spark conversations. “The future is not yet written, we are all collectively in the process of writing it”"
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html#foundation",
    "href": "posts/business/public_speaking/public_speaking.html#foundation",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "Presentation literacy is its own skill. Your goal is to be you, not a version of someone you like.\n\n\n\nIdeas are all that matter in the talk. This can be a novel idea, or a novel story or presentation of an existing idea.\nYour goal is to recreate the core idea inside your audience’s mind. The language you use must be shared by the speaker and listener for it to have this effect. Focus on what you can give to the audience.\nVisualise the talk as a journey that you guide the user through. Start where the audience starts. Don’t make any impossible leaps or unexpected changes of direction.\n\nWhat issues matter the most\nHow are they related?\nHow can they be easily explained?\nWhat are the key open questions? Controversies?\n\n\n\n\nFour talk styles to AVOID.\n\nSales Pitch\n\nYou should give not take.\nGenerosity evokes a response.\n\nRamble\n\nInsults the audience; suggests that their time doesn’t matter to you.\n\nOrganisation bore\n\nYour company’s structure is boring.\nFocus on the work and ideas, not the company, products or individuals.\n\nInspiration tryhard\n\nAll style and no substance.\nComes across as manipulative.\nInspiration has to be earned. “Like love you don’t get it by pursuing it”.\nInspiration is the response to authenticity, courage, wisdom, selflessness.\n\n\n\n\n\nThe throughline is a connecting theme that ties the narrative elements\n\nA focused, precise, unexpected idea\nNot the same as a theme\nYou should be able to encapsulate it in &lt;15 words\n\nTrace the path the journey will take. If the audience knows where you’re going, it will be easier to follow.\nCut down the talk to keep the throughline focused.\n\nDON’T just keep all the content but say it in less detail. Overstuffed = underexplained\nCut back the range of topics. Say less with more impact.\n“The power of the deleted word”.\nAn 18 minute time limit is short enough to keep an audience’s attention, long enough to cover enough ground and precise enough to be taken seriously.\n\nThe overall structure can be thought of in 3 parts: What? So what? What now? In more detail, this is:\n\nIntro - what will be covered\nContext - why this issue matters\nMain concepts\nPractical implications\n\nTalk about ideas not issues. Write for an audience of one.\nChecklist:\n\nIs this a topic I’m passionate about? Do I know enough to be worth the audience’s time? Do I have credibility?\nDoes it inspire curiosity?\nWill knowing this make a difference to the audience?\nIs the talk a gift or an ask?\nIs the information fresh?\nCan the topic be covered in enough detail with examples in the time slot?\nWhat are the 15 words that encapsulate the talk?\nWould those 15 words persuade someone they’re interested in hearing the talk?"
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html#talk-tools",
    "href": "posts/business/public_speaking/public_speaking.html#talk-tools",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "A human connection is required before you can inform, persuade or inspire. Knowledge has to be pulled in, not pushed in. Be AUTHENTIC.\nDos\n\nEye contact.\nVulnerability, but authentic.\nHumour signals to the group that you’ve all connected.\n\nDon’ts\n\nEgo breeds contempt.\nAvoid tribal topics/language, e.g. politics.\n\n\n\n\nStorytelling helps people to imagine and dream -&gt; empathise -&gt; care\nKey elements:\n\nCharacter\nTension\nRight level of detail\nSatisfying resolution\n\nIf you’re telling a story, know why you’re telling it. The goal is to give, not boost your ego.\nConnect the dots enough, but don’t force-feed the conclusion.\n\n\n\nSteps:\n\nStart where the audience is. No assumed knowledge or jargon.\nSpark curiosity. Create a knowledge gap that the listener wants to fill.\nIntroduce new concepts one-by-one. Name each concept.\nMetaphors and analogies. Reveal the “shape” of new concepts.\nExamples confirm understanding.\n\nKnowledge is built hierarchically. Explain the structure and where each concept fits. Convert a web of ideas into a string of words.\nCurse of knowledge. Revisit assumed knowledge.\nMake clear what the concept isn’t. An explanation is a small mental model in a large space of possibilities. Reduce the size of that space.\n\n\n\nDemolish the listener’s existing knowledge and rebuild it; replace it with something better. Genius is something that comes to you, not something you are.\nAn “intuition pump” is an example or metaphor that makes the conclusion more plausible. Not a rigorous argument, but nudges the listener in the right direction. Prime with emotion or story, then persuade with reason.\nTurn the audience into detectives. Tell a story. Start with a big mystery, traverse the possible solutions until only one plausible conclusion remains.\n\n\n\nThree approaches to revelatory talks:\n\nThe wonder walk\n\nReveal a succession of inspiration images\nObvious throughline with accessible language and silence\nThe message should be “how intriguing is this” not “look what I achieved”\n\nDemo\n\nInitial tease, background context, demo, implications\n\nDreamscape\n\nCreate a world that doesn’t exist but someday might\nPaint a bold picture of an alternative future\nMake others desire that future\nEmphasise human values not just clever tech, otherwise the audience might freak out at the possible negative implications of the technology"
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html#preparation-process",
    "href": "posts/business/public_speaking/public_speaking.html#preparation-process",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "Slides can distract. If you do use them, make sure they add something.\nVisuals should:\n\nReveal - set context, prime, BAM!\nExplain - one idea per slide with a clickbait headline not a summary. Highlight the point you’re making.\nDelight - show impressive things and let them speak for themselves. Don’t need to explain every image/slide.\n\nDon’t let the slides be spoilers for what you’re about to say. The message loses its impact, it’s not news anymore.\n\n\n\nThe goal is to have a set structure that you speak naturally and authentically about.\nBoth approaches have the same result:\n\nScript and memorise until natural, or\nFreestyle around bullet points and rehearse until structure is set\n\nBeing read to and being talked to are very different experiences\n\nDictating the speech rather than writing it can help make sure it comes across how you would speak rather than how you would write.\nDon’t end up in the uncanny valley between reading and speaking\nIn some cases, reading is powerful if it’s clear that this is a poetic, written piece. Abandon the script for an impactful finale.\n\nWhat you are saying matters more than the exact word choice.\nRehearse your impromptu remarks.\n\n\n\nPractice speaking by speaking!\nRehearse in phases:\n\nEditing and cutting\nPacing and timing\nDelivery\n\nAim to use &lt;90% of the time limit. results in tighter writing, and time to riff and enjoy the reaction.\n\n\n\nYou have the audience’s attention at the start, then it’s yours to lose. Opener has to capture attention and keep it. 2 stages:\n\n10 seconds to capture attention\n1 minute to hold it\n\nOpening techniques:\n\nDrama\nCuriosity - more specific questions are more intriguing\nCompelling slide/image - “the next image changed my life”\nTense but don’t give away - show where you’re going but save the reveal\n\nThe closer dictates how the whole talk will be remembered.\nClosing techniques:\n\nCamera pullback - big picture and implications\nCall to action\nPersonal commitment\nInspiring values/vision\nEncapsulation - neatly reframe the main idea\nNarrative symmetry - callback to opener\nLyrical inspiration - poetic conclusion"
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html#on-stage",
    "href": "posts/business/public_speaking/public_speaking.html#on-stage",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "Choose an outfit early\nDress like the audience but smarter\nDress for the people in the back row\n\n\n\n\n\nEmbrace the nerves and draw attention to it. Vulnerability humanises you.\nPick friendly faces in the audience and speak to them.\nBackup plan - have a story ready to fill in during unexpected technical issues\nFocus on the message - “THIS MATTERS”\n\n\n\n\nVisual barriers between the speaker and audience can create a sense of authority but at the expense of a human connection. If they can see you, you’re vulnerable. If you’re vulnerable, they can connect.\nReferring to notes is fine if done sparingly and unambiguously. It humanises you if done honestly. It appears sneaky and dishonest if you try to do it secretly.\n\n\n\nSpeaking style adds another stream of input parallel to the words themselves. It tells the listener how they should interpret the words.\nSix voice tools. Vary each of them depending on the meaning and emotion.\n\nVolume\nPitch\nPace\nTimbre\nTone\nProsody (singsong rise and fall)\n\nSpeed should be a conversational pace, approx 130-170 words per minute.\n\nPeople often worry about speaking too fast\nSpeaking too slow is a more common problem (overcorrection?)\nUnderstanding outpaces articulation, the listener is “dying of word starvation”\n\nSpeak, don’t orate.\nBody language\n\nStand and move intentionally.\nStop to make a point then walk to the next point.\n\n\n\n\nAdd too many ingredients and you risk losing attention. What you say can just be signposts for what you show - set it up, show it, shut up."
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html#reflection",
    "href": "posts/business/public_speaking/public_speaking.html#reflection",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "Knowledge of specific facts is becoming more specialised and commoditised. Understanding how everything fits together is becoming broader, more unified.\nConcepts of specialised knowledge are outdated, stemming from industrial age thinknig. Modern thinking values:\n\nContextual knowledge\nCreating knowledge\nUnderstanding of humanity\n\nAs people become more interconnected, innovation becomes crowd-accelerated.\nHappiness is finding something bigger than you and dedicating your life to it. Nudge the world - give it questions to spark conversations. “The future is not yet written, we are all collectively in the process of writing it”"
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html",
    "href": "posts/business/science_of_leadership/leadership.html",
    "title": "Science of Leadership",
    "section": "",
    "text": "There are four “parts” of the brain:\n\nReptilian brain\nPaleolimbic brain\nNeolimbic brain\nPrefrontal brain\n\n\n\nAlso called the “primitive brain”. Responsible for survival instincts; only concerned with individual self, not others. When the reptilian brain is active, we feel stressed.\nName comes from the (erroneous) belief that this comes from reptilian ancestors, whereas it actually goes back even further likely to a common ancestor of all vertebrates.\nControls the three possible responses to a threat:\n\nFlight\nFight\nFreeze\n\nIn the modern world, this survival instinct kicks in when we experience stress or anxiety, even if it is not life threatening. As a response to stress, these manifest as:\n\nFlight - Anxiety and fidgeting.\nFight - Aggression and anger.\nFreeze - Helplessness.\n\nWe can recognise the reptilian brain being active when people behave in one of the three stress responses.\n\n\n\nResponsible for the survival of the group. Stems from the herd behaviour of early mammals; living together maximises survival but this social set up requires regulating.\nThe paleolimbic brain defines the position of the individual within the wider group. This manifests as self-confidence and trust.\n\n\n\nTrust vs Self-Confidence\n\n\n\nDominance isn’t just being brash; dominant people can be charming and nice, but this is a manipulation tactic and not genuine. Others are evaluated in terms of their usefulness to the dominant person. “Too much” self-confidence becomes a belief that one is entitled to more than others. In the extreme case, this leads to Narcissistic Personality Disorder.\nSubmissive people believe all of their successes are due to luck and anything that goes wrong is their fault. This is in contrast to dominant people, who will take credit for all success and blame others for all failures. In its most extreme form, submissiveness leads to melancholic depression.\nMarginal people have no trust in anything or anyone. Conspiracy nuts.\nAxial people have too much trust in others to the point of gullibility. Becomes mystical delirium in its most extreme form.\n\nDominant behaviour is the most problematic. But the paleolimbic brain is cowardly. It is rare in nature that power struggles are actually fights to the death. After encountering some resistance, the dominant type will back off.\nThe paleolimbic brain can change but only very slowly. The exception is traumatic events, which can quickly shift the paleolimbic brain “downwards”, but there are never quick shifts “upwards”.\nWe can recognise the paleolimbic brain being active when people behave territorially.\n\n\n\nResponsible for our intrinsic and extrinsic motivations; our likes and dislikes.\nWhere our memory resides. Usually “in charge” for routine tasks and uses the minimum attention span required to achieve them; our “standby mode”.\nThree layers of motivation:\n\nIntrinsic motivation. This is fixed, and is a result of genes and very early experiences. This remains fixed from 3 months old for the rest of our lives. Intrinsic motivations give us energy and joy.\nExtrinsic motivations. These continuously evolve for our whole lives. Likes and dislikes. They cost us energy and will fade if we don’t succeed. Extrinsic motivations push us to do what others expect of us, adhere to social pressures. Irritations and prejudices stem from here. Intrinsic motivations will last, but extrinsic motivations will fade away.\nObsessions. A passion that has gone beyond a tipping point. We are often blind to our obsessions.\n\n\n\n\nThis is unique to humans; ours is far bigger than any other animal. It is suited for adaptation and creativity in unknown and complex situations.\nNeolimbic brain handles the well-known and simple tasks. It is not able to see further than our previous experiences. Prefrontal brain handles the new, unknown and complex tasks.\nOr at least it should kick in for these new situations. Often creativity is limited because the neolimbic brain kicks in, meaning we can’t see any possibilities which we haven’t already experienced."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#reptilian-brain",
    "href": "posts/business/science_of_leadership/leadership.html#reptilian-brain",
    "title": "Science of Leadership",
    "section": "",
    "text": "Also called the “primitive brain”. Responsible for survival instincts; only concerned with individual self, not others. When the reptilian brain is active, we feel stressed.\nName comes from the (erroneous) belief that this comes from reptilian ancestors, whereas it actually goes back even further likely to a common ancestor of all vertebrates.\nControls the three possible responses to a threat:\n\nFlight\nFight\nFreeze\n\nIn the modern world, this survival instinct kicks in when we experience stress or anxiety, even if it is not life threatening. As a response to stress, these manifest as:\n\nFlight - Anxiety and fidgeting.\nFight - Aggression and anger.\nFreeze - Helplessness.\n\nWe can recognise the reptilian brain being active when people behave in one of the three stress responses."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#paleolimbic-brain",
    "href": "posts/business/science_of_leadership/leadership.html#paleolimbic-brain",
    "title": "Science of Leadership",
    "section": "",
    "text": "Responsible for the survival of the group. Stems from the herd behaviour of early mammals; living together maximises survival but this social set up requires regulating.\nThe paleolimbic brain defines the position of the individual within the wider group. This manifests as self-confidence and trust.\n\n\n\nTrust vs Self-Confidence\n\n\n\nDominance isn’t just being brash; dominant people can be charming and nice, but this is a manipulation tactic and not genuine. Others are evaluated in terms of their usefulness to the dominant person. “Too much” self-confidence becomes a belief that one is entitled to more than others. In the extreme case, this leads to Narcissistic Personality Disorder.\nSubmissive people believe all of their successes are due to luck and anything that goes wrong is their fault. This is in contrast to dominant people, who will take credit for all success and blame others for all failures. In its most extreme form, submissiveness leads to melancholic depression.\nMarginal people have no trust in anything or anyone. Conspiracy nuts.\nAxial people have too much trust in others to the point of gullibility. Becomes mystical delirium in its most extreme form.\n\nDominant behaviour is the most problematic. But the paleolimbic brain is cowardly. It is rare in nature that power struggles are actually fights to the death. After encountering some resistance, the dominant type will back off.\nThe paleolimbic brain can change but only very slowly. The exception is traumatic events, which can quickly shift the paleolimbic brain “downwards”, but there are never quick shifts “upwards”.\nWe can recognise the paleolimbic brain being active when people behave territorially."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#neolimbic-brain",
    "href": "posts/business/science_of_leadership/leadership.html#neolimbic-brain",
    "title": "Science of Leadership",
    "section": "",
    "text": "Responsible for our intrinsic and extrinsic motivations; our likes and dislikes.\nWhere our memory resides. Usually “in charge” for routine tasks and uses the minimum attention span required to achieve them; our “standby mode”.\nThree layers of motivation:\n\nIntrinsic motivation. This is fixed, and is a result of genes and very early experiences. This remains fixed from 3 months old for the rest of our lives. Intrinsic motivations give us energy and joy.\nExtrinsic motivations. These continuously evolve for our whole lives. Likes and dislikes. They cost us energy and will fade if we don’t succeed. Extrinsic motivations push us to do what others expect of us, adhere to social pressures. Irritations and prejudices stem from here. Intrinsic motivations will last, but extrinsic motivations will fade away.\nObsessions. A passion that has gone beyond a tipping point. We are often blind to our obsessions."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#prefrontal-brain",
    "href": "posts/business/science_of_leadership/leadership.html#prefrontal-brain",
    "title": "Science of Leadership",
    "section": "",
    "text": "This is unique to humans; ours is far bigger than any other animal. It is suited for adaptation and creativity in unknown and complex situations.\nNeolimbic brain handles the well-known and simple tasks. It is not able to see further than our previous experiences. Prefrontal brain handles the new, unknown and complex tasks.\nOr at least it should kick in for these new situations. Often creativity is limited because the neolimbic brain kicks in, meaning we can’t see any possibilities which we haven’t already experienced."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#reptilian-leader",
    "href": "posts/business/science_of_leadership/leadership.html#reptilian-leader",
    "title": "Science of Leadership",
    "section": "2.1. Reptilian Leader",
    "text": "2.1. Reptilian Leader\nLed by stress. Reactive, constantly in survival mode.\nWe can think of 3 sub-categories of reptilian leader based on their typical threat response:\n\nFight - Aggressive\nFlight - Chaotic.\nFreeze - Terrified, never making decisions."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#paleolimbic-leader",
    "href": "posts/business/science_of_leadership/leadership.html#paleolimbic-leader",
    "title": "Science of Leadership",
    "section": "2.2. Paleolimbic Leader",
    "text": "2.2. Paleolimbic Leader\nThey are territorial. Manipulative, deceitful, power games.\nWe can think of 4 sub-categories of paleolimbic leader based on trust vs assertiveness:\n\nDominant. Rule through fear, initiates aggression, shouts, intimidates and publicly humiliates.\nMarginal. Paranoid. Don’t trust anyone so micromanage.\nAxial. No boundaries. Wants to be everybody’s friend.\nSubmissive. Exchanges favours to bargain for things. Takes responsibility for every failure. Often does the work of his team, not because of a lack of trust like a marginal leader, but because they lack the skills to rally the team or impose their views.\n\nNobody is always one single type. A common combination is for a leader to be dominant with their team but submissive to their boss. Scheme and manipulate."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#neolimbic-leader",
    "href": "posts/business/science_of_leadership/leadership.html#neolimbic-leader",
    "title": "Science of Leadership",
    "section": "2.3. Neolimbic Leader",
    "text": "2.3. Neolimbic Leader\nAuthentic.\nSub-categories:\n\nPhilosopher. Optimistic, curious, seek consent. Performs well in stressful environments where they have a calming influence on the team.\nInnovator. Innovation, abstract, theoretical, interdisciplinary. Respectful and empowering for their team. Perform well in research environments.\nAnimator. Change, strong sensations. High energy and playful. Perform well in consulting where the constant change of environment is a draw.\nAdministrator. Admin and safety, planning. Love procedures and process.\nStrategist. People management. Mediate and actively solve problems. Support their team. Excel in HR, middle management, logistics and planning.\nCompetitor. Fighting and conquest. Hands on. Status driven. Perform well in sales and short-term projects.\nParticipative. Sharing and caring. Leadership with kindness, where everyone is important. Perform well in HR, coaching.\nSupportive. Selfless. Demanding towards themselves and tolerant towards others. Perform well in back office or customer service.\n\nIn practice, we are all some combination of 2-4 of these archetypes."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#prefrontal-leader",
    "href": "posts/business/science_of_leadership/leadership.html#prefrontal-leader",
    "title": "Science of Leadership",
    "section": "2.4. Prefrontal Leader",
    "text": "2.4. Prefrontal Leader\nCalm, serene, curious.\nAble to adapt and handle complex situations.\nFavours reflection over rushing decisions. Listens to the team without caving to peer pressure."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#survival-instincts",
    "href": "posts/business/science_of_leadership/leadership.html#survival-instincts",
    "title": "Science of Leadership",
    "section": "3.1. Survival Instincts",
    "text": "3.1. Survival Instincts\nThere are only really two scenarios from a survival point of view: facing danger and safe.\n\n3.1.1. Facing Danger\nWhen facing danger, there are the three familiar stress responses: fight, flight or freeze.\nStress is jokingly referred to as a “visually transmitted disease”. Research shows that seeing someone stressed is enough to make us stressed. From an evolutionary standpoint, if another person in the tribe has identified a danger then we should also be more vigilant.\nWe should be extra conscious as leaders that we are reducing the stress around us, not just picking it up and amplifying it.\nSometimes we are unknowingly the threat, causing stress among the team, particularly as a dominant boss. An example of this is setting deadlines; naively this is used to give the team some momentum. Brain research shows it actually limits our thinking and leads to worse decision making.\nA tight deadline increases stress levels causing the reptilian brain to engage, when ideally we would be using the prefrontal cortex. The more stressful the deadline, the less open we are to alternative approaches. Putting extra pressure on a team is counterproductive; the three stress responses are fight (aggression), flight (confusion) or freeze (demoralisation).\n\n\n3.1.2. Safe\nWhen safe, there are two considerations: avoid danger and avoid starvation.\n\nAvoid danger\nWe are hard-wired to avoid danger. People with “reckless genes” tend to remove themselves from the gene pool so there is an evolutionary bias towards avoiding danger. “Successful people are simply those with successful habits”.\nThis manifests as:\n\nResistance to change. To get people to embrace change you have to deactivate their reptilian brain, take away the perceived danger. Activate their limbic system by making them think for themselves; give them info and let them reach the conclusion.\nLack of initiative. Address this by taking away the danger, assure them that no one will be in trouble for bad ideas. A “flop meeting” where everybody shares things that have gone wrong. Reward initiative taking with money, attention, etc.\nNeed for repetition. Repetition tells our brain that something is true, not just an outlier. “It takes 5 (repetitions) to stay alive”.\nHerding. A good survival strategy is to eat things that we’ve seen others eat so that we know it’s safe. This is still true in “non-survival” settings. Social proof is powerful. Start with willing early adopters, then point to their success when getting others on board.\n\nHow to handle change as a manager:\n\nBe on their side\nReward initiative taking\nRepeat the message\nCreate momentum through early adopters\n\n\n\nAvoid starvation\nWe have an in-built need to feel safe from the fluctuations in food supply from a bad hunt or bad harvest. Studies show the modern day equivalent is with money.\nMoney is a double edged sword. Not enough salary means people are forced into submissiveness, activating the paleolimbic brain and resulting in frustration and resentment. Too much money has the opposite effect; still activating the paleolimbic brain but forcing them into dominance, creating greedy, entitled people."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#personality",
    "href": "posts/business/science_of_leadership/leadership.html#personality",
    "title": "Science of Leadership",
    "section": "3.2. Personality",
    "text": "3.2. Personality\nPersonality profiles are an outdated practice based on Jung’s work.\nPersonality depends on many factors: genetics determine some part of it, but which genes get activated depend on our environment, how much attention we received as a child, etc. Nature and nurture.\nIntrinsic/primary motivations. Doing the activity gives us energy; we get joy from the act of doing it regardless of results. These are lifelong activities that we enjoy.\nRefer back to the 8 neolimbic leader archetypes."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#relational-stress-management",
    "href": "posts/business/science_of_leadership/leadership.html#relational-stress-management",
    "title": "Science of Leadership",
    "section": "3.3. Relational Stress Management",
    "text": "3.3. Relational Stress Management\nA person may move between these stress modes over time when in the “facing danger” frame of mind, but it is helpful to recognise which mode they are in and address it accordingly.\n\n3.3.1. Flight / Flee\nThe signs:\n\nHands and feet - Contained movements: Fidgeting, nail biting, leg shaking, tapping, fiddling.\nEyes - Erratic eye movement and avoiding eye contact. Looking for an escape route.\nFast breathing and sweating.\n\nHow not to handle it:\n\nTelling them to calm down doesn’t help. This will be interpreted as a further restriction of their options, causing them to panic more.\nDon’t ask closed questions (“yes” or “no”). Their answers will most likely be evasive (“I don’t know”).\nDo not shout or threaten. Reminding them of the negative consequences or deadlines makes things worse.\nJudging and moralising.\n\nHow to handle it:\n\nAsk open questions. This helps remind them that they are free to choose and do have options. If they are having trouble organising their thoughts you can suggest some options.\nDe-dramatise and use humour. Put the situation into perspective.\nInvite them to take a walk with you. Their instinct is to flee and this gives them the option of doing that.\n\n\n\n3.3.2. Fight\nThe signs:\n\nStance - Standing tall\nVoice - Raising their voice\nEyes - Squinting to focus on “the enemy”\nNeck and jaw clench\n\nHow not to handle it:\n\nDon’t overpower them by yelling harder. The fight will escalate.\nDon’t undermine their authority. Don’t interrupt or laugh at them.\nDon’t annoy them. They will be impatient so being slow will anger them.\n\nHow to handle it:\n\nListen without interruption or resistance. It takes two to fight so if you don’t engage the fight won’t happen.\nTake responsibility, don’t make excuses. If you weren’t to blame you can say “I’m sorry if what I did made you feel that way. That wasn’t my intention”. “If I were you I’d be angry too”.\nOffer solutions\n\n\n“You don’t have to attend every argument you’re invited to”.\n\nPick your battles. Some fights aren’t worth having.\n\n\n3.3.3. Freeze\nThe signs:\n\nMovement - They literally stop moving; they drop their jaw or put hands in front of their mouth. This stops the arrival of oxygen to the lungs in an attempt to slow the heart rate.\nShoulders and head hang\nEyes - Looking down\nCover face with hands\nVoice - Lower their voice\nSpeech - Talk slower and use smaller sentences and words.\nSighs and maybe even tears.\n\nHow not to handle it:\n\nDon’t shake them up, tell them to get a grip etc. The frozen person doesn’t want to be autonomous.\n\nHow to handle it:\n\nUnderstand their POV\nMirror their pose\nRespect their silence and give them space\nDefine small objectives together\n\nTo get them out of their reptilian brain mode we need to take away the perceived danger. In freeze mode, this means offering them shelter."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#socratic-questioning",
    "href": "posts/business/science_of_leadership/leadership.html#socratic-questioning",
    "title": "Science of Leadership",
    "section": "4.1. Socratic Questioning",
    "text": "4.1. Socratic Questioning\nIf we encounter unacceptable behaviour, we can address it head on and tell someone not to do that. But it doesn’t change their behaviour or perceptions. It just means that they will continue with their belief system, but be more careful about whom they share their comments with and when; they will learn to hide it better but they won’t learn to be a better person.\nThe Socratic questioning technique can help challenge people’s belief systems. This needs to be one-on-one; others can’t be listening as this needs to be open and honest. Invite the person somewhere quiet. It is important to stay neutral and not express judgement. Explain that you overheard XYZ comment and you want to address it. From then on, just act as a mirror to their responses, question what they say. Let them reach the conclusion themselves and eventually apologise.\nMaking crass remarks is usually the paleolimbic brain in charge. The Socratic method forces the prefrontal brain to kick in."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#extrinsic-motivations",
    "href": "posts/business/science_of_leadership/leadership.html#extrinsic-motivations",
    "title": "Science of Leadership",
    "section": "4.2. Extrinsic Motivations",
    "text": "4.2. Extrinsic Motivations\nEmotional feelings of hurt generate the same brain response as physical pain. We have evolved to be deeply social animals and this gives rise to our extrinsic or secondary motivations. This kicks in at the neolimbic level to help us come together as a group and adapt to each other.\nThe more alike we are, the more likely we were to be accepted into the group. This is why rejection hurts so much - it’s a warning mechanism that we are in physical danger, isolated from the safety of the group.\nThe opinions of others mattered and we adapted our behaviour to the norms of the group.\nCharacteristics of extrinsic motivations:\n\nVariable. They change over time or depending on the group.\nCost energy. They don’t come naturally and require intentional work.\nResults-driven. If the behaviour is successful, the motivation is reinforced, otherwise it fades away.\n\nWhen hiring people, it is helpful to determine whether the role fits a person’s intrinsic or extrinsic motivations. A successful way to do this is to match the job description to a personality type (or two) and ask questions to determine whether a person shows traits of that personality type.\nAnother case is when we change somebody’s role to include more tasks which are not aligned with their intrinsic motivations. Research shows that only 30% of the job needs to be aligned with our intrinsic motivations in order for their overall job to feel enjoyable. (We have a high tolerance for bitch work.) But obviously we should aim for higher than 30%."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#employee-engagement",
    "href": "posts/business/science_of_leadership/leadership.html#employee-engagement",
    "title": "Science of Leadership",
    "section": "4.3. Employee Engagement",
    "text": "4.3. Employee Engagement\nThere are two ingredients to achieve employee engagement:\n\nOxytocin makes sure the team bonds.\nDopamine makes sure each individual enjoys their work.\n\n\n4.3.1. Oxytocin\nOxytocin is the love hormone and regulates our relaxation, trust and psychological stability. It can neutralise the negative effects of stress by lowering cortisol levels. It causes our team to be more cohesive, less stressed, help each other more.\nCaring and sharing is the key to releasing oxytocin. Collaborative projects, create interdependencies, promote knowledge sharing. Give them shared responsibility and shared rewards.\n\n\n4.3.2. Dopamine\nHormone responsible for reward and pleasure.\nIt fades away and we become used to a stimulus - if an activity (or drug) gives us a dopamine hit now, then next time it will give slightly less. Or from another perspective, we will need more of the stimulus to get the same dopamine hit next time. This is the key behind addiction.\nAnticipation of a stimulus triggers the dopamine hit, not just the stimulus itself, so anticipation is a key part. Hire based on intrinsic motivations so that people anticipate a dopamine hit from doing their job.\nThe job needs to evolve to keep things interesting and keep releasing dopamine. Adapt objectives. Set goals over shorter time frames to give a steady stream of dopamine hits. Celebrate achievements."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#red-flags",
    "href": "posts/business/science_of_leadership/leadership.html#red-flags",
    "title": "Science of Leadership",
    "section": "4.4.Red Flags",
    "text": "4.4.Red Flags\n\n4.4.1. Instant Gratification\nThe ability to delay gratification is a strong predictor of success in life.\nDelaying gratification is about more than just sheer willpower. It depends on our upbringing and our level of trust. If you believe the world is stable and trustworthy, you will feel more comfortable sacrificing now and be confident that you will see a reward later. If you see the world as hostile, you will prioritise instant, certain rewards.\nWe need to answer the following questions about our team:\n\nHow many of your team members understand why they are working? Do they know the vision?\nHow important is that vision to them?\nDo they trust your capabilities to achieve that vision?\n\nRepetition is key. We can’t just say it once and expect them to believe it. What the brain hears repeated becomes true. Repetition is key. Repetition is key.\nThe manager is part of the team, not a separate entity.\n\n\n4.4.2. Self Evaluation\nThe Dunning-Kruger effect: ignorance begets confidence, not knowledge.\nIncompetent people overestimate their skills. Competent people underestimate their skills.\nHiring and promotions are susceptible to being gamed by overconfident incompetent people. Micromanagement is another effect if a clueless manager thinks he knows more than his team.\nThose more susceptible to overconfidence:\n\nAt a paleolimbic level this is dominant and axial behaviour\nAt a neolimbic level this is extroverted personalities: philosopher, animator, strategist, participative\n\nThe opposite is true for those susceptible to underestimating their abilities.\nPeer review and 360 feedback helps combat the Dunning-Kruger effect. Measurable standards help too. Check references when hiring.\n\n\n4.4.3. Workaholism\nObsessions vs passions.\nPassion is about trying to repeat a pleasant experience. Obsession is about trying to avoid an unpleasant experience.\nWe overcompensate by behaving in an obsessive way. This leads to workaholism. It is easy to think this is desirable in the workplace, but we should seek to avoid it. It is unsustainable, leads to resentment in that employee and others in the team, and leads to burnout. It is an indication of more deep-rooted problems in that person. They are fleeing something else.\nAn obsession is different from an intrinsic motivation because:\n\nThere is no sense of reward when achieving something, just the immediate need to do even more. Filling a bottomless void.\nGenerates strong emotional reactions\nEnergy draining\n\nHow to handle workaholism in an employee?\n\nTake away the object of obsession, i.e. tell them to take a holiday. If they object, they likely are addicted.\n“Blow hot and cold” - we value and appreciate you, but we also prioritise balance and want to place a clear limit."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#the-hawthorne-experiment",
    "href": "posts/business/science_of_leadership/leadership.html#the-hawthorne-experiment",
    "title": "Science of Leadership",
    "section": "5.1. The Hawthorne Experiment",
    "text": "5.1. The Hawthorne Experiment\nA team is not the sum of individuals.\nPrevious neuroscience research on productivity focused on the individual. The Hawthorne experiments studied productivity in a social setting. The conclusions were that the environment can have as strong an impact on individual performance as innate individual ability.\n\n\n\n\n\n\nThe Hawthorne Effect\n\n\n\nIndividuals alter their behavior or performance when they know they are being observed. This is independent of any actual intervention being applied.\n\n\nWorking in a group leads all members to work to that standard, which is oftentimes higher than any externally imposed target standard set by management. Let people work in teams and rely on each other.\nWe work differently in a group than we would as individuals. Sometimes this means 1+1=3 but other times it means 1+1=1."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#the-conformity-experiment",
    "href": "posts/business/science_of_leadership/leadership.html#the-conformity-experiment",
    "title": "Science of Leadership",
    "section": "5.2. The Conformity Experiment",
    "text": "5.2. The Conformity Experiment\nIn a group setting when asked to say which of 3 lines is the same length as the example line, 5 stooges and 1 real person answered in sequence. The stooges all gave the same wrong answer and 37% of the time the real person agreed despite it being clearly wrong.\n\n\n\n\n\n\nThe Conformity Effect\n\n\n\nIndividuals may trust the group over their own knowledge, even when the group is clearly wrong.\n\n\nImplications:\n\nDon’t assume “yes” just because no one says “no”.\nDon’t ask “any questions?“. Ask “who has the first question?”.\nDon’t use meetings to generate ideas, ask feedback or get buy-in.\nInteract on both individual-level and group-level.\nHave everyone prepare BEFORE meetings.\nFear reinforces the conformity effect."
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#fear-of-authority",
    "href": "posts/business/science_of_leadership/leadership.html#fear-of-authority",
    "title": "Science of Leadership",
    "section": "5.3. Fear of Authority",
    "text": "5.3. Fear of Authority\nFear of authority is reinforced from an early age, and we are conditioned to blindly comply.\nThe Milgram electric shock experiment is a good example of conformity to authority. Participants delivered painful electric shocks to unseen participants just because somebody in a white coat told them to proceed.\nIn the Stanford prison experiment, participants were split into guards and prisoners. Without prompting, the guards adopted heavy-handed tactics and abused the prisoners. The 2 week experiment was cut short early after 6 days due to fears around the prisoners’ safety.\nPut normal people in an abnormal context and we can make them do almost anything.\n\n\n\n\n\n\nAuthority vs Leadership\n\n\n\nAuthority is less effective than leadership. It triggers the fear response which encourages conformity; safety in numbers.\nLeadership is about empowering people. Authority is the opposite.\n\n\nImplications:\n\nAuthority forces people to comply, but at the expense of any critical thinking.\nAuthority negatively effects autonomy, responsibility, creativity, proactivity\nAuthority generates resentment, passive aggression, high levels of stress\nPower corrupts"
  },
  {
    "objectID": "posts/business/science_of_leadership/leadership.html#the-pygmalion-effect",
    "href": "posts/business/science_of_leadership/leadership.html#the-pygmalion-effect",
    "title": "Science of Leadership",
    "section": "5.4. The Pygmalion Effect",
    "text": "5.4. The Pygmalion Effect\nThe Pygmalion effect is also known as the self-fulfilling prophecy. It is a psychological phenomenon where high expectations lead to improved performance, and low expectations lead to diminished performance.\n\n\n\n\n\n\nThe Pygmalion Effect\n\n\n\nHow we think about others influences how they perform.\n\n\nImplications:\n\nManager expectations determine performance and career progress\nGood managers create high performance expectations\nWe do what we believe we’re expected to do\n\nConditions:\n\nYou have to mean it. The high expectations need to be genuine.\nExpectations should be realistic (not just high). If someone repeatedly tried and fails to meet expectations then motivation drops. Motivation (y axis) vs probability of success (x-axis). Upside down parabola, motivation peaks when p(success)=0.5 - too easy or too hard are both demotivating.\nExpectations are linked to how managers see themselves.\nYoung people are most impacted.\n\nMotivation peaks when P(success) = 0.5 - too easy or too hard are both demotivating.\n\n\n\nStrength of Motivation vs Probability of Success\n\n\nConclusions:\n\nHire intrinsically motivated people.\nCommunicate our high expectations repeatedly.\nAdapt their goals along the way.\nDevelop our own training skills."
  },
  {
    "objectID": "posts/business/marketing/marketing.html",
    "href": "posts/business/marketing/marketing.html",
    "title": "Marketing Notes",
    "section": "",
    "text": "Notes from “The 1-Page Marketing Plan” by Allan Dib.\n\n\n\n\nThe overall phases of the direct response marketing strategy are:\n\n\n\nPhase\nStatus\nGoal\n\n\n\n\nBefore\nProspect\nGet them to know you\n\n\nDuring\nLead\nGet them to like you\n\n\nAfter\nCustomer\nGet them to trust you\n\n\n\nDirect response marketing should be:\n\nSpecific\nTrackable\nInclude an offer\nFollow up\n\nA marketing plan encompasses:\n\n\n\n\n\n\n\nStage\nExample\n\n\n\n\nAdvertising\nSign for the circus\n\n\nPromotion\nPut the sign on an elephant and walk it into town\n\n\nPublicity\nElephant tramples the mayor’s flowers and it makes the news\n\n\nPR\nYou get the mayor to laugh it off\n\n\nSales\nPeople come to the circus and spend money\n\n\n\nStrategy changes with scale\n\nDon’t blindly copy large companies. “Branding” to “get your name out there” is expensive and ineffective.\nThe one and only objective of marketing for a small company is “make a profit” For large companies it may be: appease shareholders, board, managers, existing clients, win advertising awards. Making a profit is a low priority for them.\nStrategy without tactics = paralysis by analysis\nTactics without strategy = bright shiny object syndrome\n\nProduct quality is a customer retention tool, not a customer acquisition tool.\n\nPeople don’t know how good the product is until they’ve already bought it\n“If you build it, they will come” doesn’t work\n\nThe 80/20 rule (Pareto principle) states that 80% of the outputs come from 20% of the inputs. Applying the 80/20 rule to the 80/20 rule gives the 64/4 rule: 64% of the outputs come from 4% of the inputs. Identify this 4% and focus on it. Often this is marketing.\nSpend money to make time, not vice versa. Money is a renewable resource, time is finite.\n\n\n\nThe niche should be as specific as possible - an inch wide and a mile deep.\n\nToo broad a target market is useless as the message gets diluted\nMass marketing/branding can work for larger companies but not small. Large companies have enough firepower to fire arrows in all directions. Small companies need to fire in a specific direction.\n\nBecome a specialist in your field to make price irrelevant\n\nBecome an educator and build trust\nYou don’t want to be a commodity shopped on price. A customer won on price will be lost on price.\n\nUse the PVP index to identify the ideal niche. Assign values to each possible segment for:\n\nPersonal fulfillment\nValue to the marketplace\nProfitability\n\nQuestions for the target prospect:\n\nWhat keeps them up at night?\nWhat are they afraid of?\nWhat are they angry about? Who are they angry at?\nWhat trends affect them?\nWhat do they secretly desire most? What one thing do they crave above all else?\nHow do they make decisions? Analytical, emotional, social proof\nWhat language/jargon do they use?\nWhat magazines do they read? What websites do they visit?\nWhat is a typical day like?\nWhat is the single most dominant emotion this market feels?\n\nCreate an avatar for each prospect. Put a name and photo against each market segment and describe a specific person in detail. Describe their life, job, activities, relationships. Answer each of the questions above from their POV.\n\n\n\nThere are two key elements of the message:\n\nPurpose of the ad\nWhat it focuses on\n\nKeep the message focused: 1 ad = 1 objective. “Marketing on purpose” rather than “marketing by accident”. There should be a clear call to action. You should know the exact action you want the prospect to take next.\nThe following two questions help refine the USP:\n\nWhy should they buy?\nWhy should they buy from me?\n\nKey takeaways for USP:\n\nWhy should they buy from you rather than competitor\nIf you remove the company name and logo, would people still know it’s you?\nQuality and service are not USPs. The customer only finds these out after they buy from you. USPs should attract customers before they buy.\nLowest price is not a USP. There’s always someone willing to go out of business quicker than you.\n\nPeople rarely want what you’re selling, they want the result of what you’re selling. Sell the problem. People don’t know what they want until they’re presented with it. Purchasing is done with emotions and justified with logic after the fact. “If I had asked people what they wanted, they’d have said faster horses” - Henry Ford.\nWhen you confuse them, you lose them.\n\nExplain the product and its benefit in one sentence.\nThe biggest threat isn’t that someone buys from a competitor, it’s that they do nothing.\nElevator pitch of the form: “You know ? What we do is ? In fact .”\n\nBe remarkable\n\nTransform something ordinary into something that makes the customer smile.\nUnique is dangerous, you just need to be “different enough”.\n\nCraft the offer\n\nIf you don’t give a compelling reason to buy, customers will default to price, and you become a commodity.\nDon’t just offer a discount, think about which product will definitely solve their biggest problem.\nAdd value rather than discounting, through bundles and customisation.\nCreate “apples to oranges” comparisons to your competitors to get out of the commmodity game.\nTarget pain points. Sell the problem, not a feature list. Customers in pain are price insensitive.\n\nCreate the offer:\n\nValue to customer\nLanguage and jargon they use\nReason why this offer is here. People are skeptical if it is too good to be true.\nValue stacking. Add in high-margin bonuses.\nUpsells\nOutrageous guarantee\nCreate scarcity. People react more to fear of loss than prospect of gain.\n\nWriting copy:\n\nBe yourself and be authentic. “Professional” = boring.\n“The enemy in common”. People buy with emotions: fear, love, greed, guilt, pride.\nCopy should address the bad parts of the product, not just the good. Who is this not for?\n\nNaming:\n\nTitle=content\nChoose clarity over cleverness\n\n\n\n\n\n“Half the money I spend on advertising is wasted; the trouble is I don’t know which half” - John Wanamaker\n\nMeasure ROI of ad campaign\n\nIf it made more money that it lost, it was a success. The only metric that matters.\nCustomer acquisition cost vs lifetime value of customer\nOther metrics like response rate are only useful to the extent that they help you calculate the above.\nSpecific target market is key. The goal of the ad is to make they prospect say “Hey, that’s for me”.\nWhat gets measured gets managed. Conversely, what you don’t know will hurt you.\n\nLifetime value of customer = Frontend (initial purchase) + Backend (subsequent purchases)\n\nIdeally the frontend alone exceeds the customer acquisition cost to be sustainable. Then the cost of the ad is immediately recouped and the ad campaign can be reupped for another cycle.\n\nSuccessful marketing has three components:\n\nMarket\nMessage\nMedia\n\nSocial media is a media not a strategy and it’s not free. It’s only free if your time is worthless.\nEmail is preferable to social media because you own the mailing list. The customer database which is a valuable asset in its own right.\n\nEmail regularly - weekly at least\nGive value - 3 value-building emails for each offer email\nSnail mail complements email - less cluttered and less competitive because fewer people still use it.\n\nMarketing budget\n\nThe only time for a budget is during the testing phase when your finding what works (what delivers a positive ROI)\nOnce you have a campaign that works, go all in. It makes money, it’s not an expense so don’t limit it.\nIf you could buy £10 notes for £5 you’d buy as many as possible, you wouldn’t limit it with a budget. The same applies for a marketing campaign with a £5 cost of acquisition and £10 lifetime value of customer.\n\nAvoid single points of failure\n\nOne customer, supplier, ad medium, lead generation method.\nPaid media is reliable and allows ROI to be tracked and measured.\n\n\n\n\n\n\n\nTreat marketing like farming not hunting. Avoid “random acts of marketing” that hunters do. Build a CRM system and infrastructure for new leads, follow-ups, conversions, etc.\nThe first goal is to generate leads - find people who are interested.\n\nAvoid trying to sell from the ad immediately\nCapture all who could be interested, not just those ready to buy immediately\nBuild a customer database, then sell to them later\n\nThe market for your product might break down something like:\n\n3% - ready to buy now\n7% - open to buying\n30% - interested but not right now\n60% - not interested\n\nBy selling directly from the ad you limit yourself just to the 3%. Capture leads first and nurture them to appeal to the 40% who are interested.\nEducate customers to offer value, then you become a trusted advisor rather than a pest salesman.\n\n\n\nThe process of taking people from being vaguely interested to desiring it. The nurturing process means they should be predisposed to buying from you before you ever try to sell to them.\nStay in contact.\n\nMost stop after 1 or 2 follow-ups. The most effective salespeople do 10+ follow-ups.\nThe money is in the follow-up\n\nBuild trust and add value. Don’t pester to close.\n\nWhen the customer is ready to buy, you will be top of mind and they are predisposed to buy. It should be when they are ready to buy, not when you are ready to sell.\nDo not pressure prospects to close. Don’t be a pest, be a welcome guest.\nContact regularly - “market until they buy or die”.\nMost contact should be value add, with occasional pitch/offer emails. 3:1 ratio of value add:pitch.\n\nShock and awe package.\n\nPhysical parcel mailed to high probability prospects.\n“Lumpy” mail grabs attention - snail mail with something physical inside.\nNeed numbers on conversion probabilities and lifetime value of customer for this approach to be viable. Good marketing can’t beat bad math.\nHigh value contents should:\n\nProvide value\nPosition you as a trusted expert\nMove prospect further down the buying cycle.\n\n\nMake lots of offers. A/B test with small segments. Take risks, write compelling copy and make outrageous guarantees.\nThree major types of people:\n\n\n\nType\nRole\nResponsibility\n\n\n\n\nEntrepreneur\n“Make it up”\nVision\n\n\nSpecialist\n“Make it real”\nImplement\n\n\nManager\n“Make it recur”\nDaily BAU\n\n\n\nMarketing calendar.\n\nWhat needs to be done and when\nDaily, weekly, monthly, quarterly, annually\nEvent-triggers tasks, e.g. on signup, on complaint\n\nDelegate.\n\nIf someone else can do the job 80% as good as you, delegate it.\nSeparate majors and minors. Focus on the majors, dump or delegate the minors.\nDon’t mistake movement for achievement.\nDays are expensive. You can get more money but not more time.\n\n\n\n\nOnce you reach a certain level of expertise, the real profit is in how you market yourself. A concert violinist makes more money performing concerts than busking.\nTrust is key in sales.\n\nDifficult for unknown businesses.\nPeople are wary because of dodgy sellers. Assume every dog bites.\nEducate -&gt; Position as expert -&gt; Trust\nDelay the sale to show:\n\nYou are willing to give before you take\nYou are an educator, so can be trusted\n\nStop selling and start educating, consulting, advising.\nEntrepreneur is someone who solves people’s problems for profit.\n\nOutrageous guarantees.\n\nReverse the risk from the customer to you.\nGuarantee needs to be specific and address the prospect’s reservations directly. Not just a generic money-back guarantee.\nIf you’re an ethical operator, you’re already implicitly providing a guarantee, just not marketing it. Use it!\n\nPricing.\n\nThe marketing potential of price is often underutilised.\nDon’t just naively undercut the market leader or use cost-plus pricing.\nSmall number of options/tiers\n\nMore choice means you are more likely to pique interest but less likely to keep interest.\nFear of a suboptimal choice; the paradox of choice.\nStandard and premium tiers and a good rule of thumb.\n\n“Unlimited” option can often be profitable because customers overestimate how much they will use, so overpay for unlimited use.\nUltra high ticket item\n\nMakes other tiers seem more reasonably priced\n10% of customers would pay 10x more. 1% of customers would pay 100x more.\n\n\nOther price considerations:\n\nDon’t discount, unless specifically using a loss-leader strategy.\nTry before you buy (the puppy dog close)\n\nThe sale feels less permanent so more palatable for the customer.\nThe onus is on the customer to return - shifts the power balance.\n\nDon’t segregate departments - everyone can warm up a prospect.\nPayment plans - people are less attached to future money than present money.\n\n\n\n\n\n\n\nThe goal is to turn customers into a tribe of raving fans. Most companies stop marketing at the sell, but this is where the process begins. Most of the money is in the backend not the frontend.\nSell them what they want, give them what they need.\n\nThese don’t always overlap\nIf they buy a treadmill and don’t use it, they’ll blame you when they don’t lose weight. You need to make sure they use your product correctly after they buy it.\nSplit the process up for them to make it less daunting.\n\nCreate a sense of theatre\n\nPeople don’t just want ot be serviced, they want to be entertained.\nInnovate on: pricing, financing, packaging, support, delivery etc, not just product. For example, “will it blend” YouTube videos to sell ordinary blenders.\n\nUse technology to reduce friction. Treat tech like an employee. Why am I hiring it? What are its KPIs?\nBecome a voice of value to your tribe.\n\nEducation-based marketing leads to trust and becoming an expert in the field.\nTell your audience the effort that goes into the product. For example, shampoo bottle that says how many mint leaves went into it.\n\nCreate replicable systems\n\nSystems ensure you have a business, rather than you are the business\nOften overlooked because it is “back office” or “not urgent”\nBenefits: build valuable asset; leverage and scalability; consistency; lower labour costs\nAreas:\n\nMarketing (leads)\nSales (follow-up)\nFulfillment\nAdmin\n\nCreate an operations manual:\n\nImagine the business is 10x the size\nIdentify all roles\nIdentify all tasks - what does each role do\nChecklist for each task\n\n\nExit strategy. Start with the end in mind. If the goal is to sell for $50 million, any decision can be framed as “will this help me get to $50 million?”. Who will buy the company? Why? Customer base, revenue or IP?\n\n\n\nPeople are 21x more likely to buy from a company they’ve previously bought from. Increase revenue by selling more to existing customers.\n\nRaise prices\n\n\nPeople are less price sensitive than you’d expect.\nSome may leave but these were the lowest-value customers - this may be polluted revenue you want to fire anyway. A customer won on price can be lost on price.\nGive a reason why prices increased. Explain the benefits they’ve received and future innovations.\nGrandfather existing customers on old price as a loyalty bonus.\n\n\nUpselling\n\n\n“Contrast principle” - people are less price sensitive to add-ons. If they’ve already bought the suit then the shirt seems cheap.\n“Customers who bought X also bought Y” - people want to fit social norms.\nSell more when the client is “hot and heavy” in the buying mood. Some people naively think you should overwhelm them with more selling.\n\n\nAscension\n\n\nMove customers up to a higher-priced tier.\nProduct mix - standard, premium, ultra-high.\n\n\nFrequency of purchases\n\n\nReminders, subscriptions\nReasons to come back; voucher for next purchase.\n\n\nReactivation\n\n\nLure back lapsed customers with strong offer and call to action.\nAsk why they haven’t returned.\nApologise and make corrections if necessary. Make the feel special if they do return.\n\nKey numbers - what gets measured gets managed\n\nLeads\nConversion rate\nAverage transaction value\nBreak even point\n\nFor subscription models measure:\n\nMonthly recurring revenue\nChurn rate\nCustomer lifetime value\n\nPolluted revenue - dollars from toxic customers != dollars from raving fans. Types of customers:\n\nTribe\nChurners - can’t afford you so drop you on price or intro offer ending\nVampires - suck up all your time, you can’t afford them\nSnow leopard - big customer but not replicable\n\nNet promoter score\n\n“How likely are you to recommend us to a friend?”\nDetractors 0-6; Passive 7-8; Promoters 9-10\nNPS=+100 if everyone is a promoter, -100 if everyone is a detractor\n\nFire problem customers\n\n“The customer is always right” is naive. It should be “the right customer is always right”.\nGenuine customer complaints are valuable and may help retain other customers who had the same issue but didn’t speak up.\nLow-value, price-sensitive customers complain the most. They take up your time and energy at the expense of your tribe.\nFiring detractors creates scarcity and frees your time to focus on the tribe. With limited supply, customers have to play and pay by your rules.\n\n\n\n\nOrchestrating referrals is an active task, not needy or desperate.\n\nDon’t just deliver a good product and hope for the best\nPeople refer because it makes them feel good, helpful or knowledgeable, not to do you a favour.\nAppeal to their ego, offer them something valuable, give them a reason to refer.\n\nAsk (for a referral) and ye shall receive.\n\n“Law of 250” - most people know 250 people well enough to invite to a wedding or funeral. This is 250 potential referrals per customer.\nSet expectation of referrals beforehand - “We’re going to do a great job and we need your help… Most people refer 3 others.”\n\nBe specific about who you ask and what kind of referral you want. Conquer the bystander effect.\nMake referral profiles for each customer group in your database.\n\nWho do they know?\nHow can you make them remember you?\nWhat will make them look good?\nHow will they provide value to their referral target\n\nCustomer buying behaviour\n\nWho has your customers before you?\nJoint venture for referrals\nSell/exchange leads, resell complementary products, affiliate referral partner\n\nBrand = personality of the business. Think of the business as a person:\n\nName\nDesign (what they wear)\nPositioning (how they communicate)\nBrand promise (core values)\nTarget market (who they associate with)\nBrand awareness (how well known / popular)\n\nBest form of brand building is selling\n\nBranding is what you do after someone has bought from you.\nBrand equity - customers crossing the road to buy from you and walk past competitors.\n\n\n\n\n\nImplementation is crucial\n\nKnowing and not doing is the same as not knowing.\nCommon pitfalls:\n\nParalysis by analysis - 80% out the door is better than 100% in the drawer; money loves speed.\nFail to delegate\n“My business is different” - Spend your effort figuring out how to make it work rather than why it won’t work.\n\n\nTime is not money\n\nEntrepreneurs get paid for the value they create, not the time or effort they put in.\nMaking money is a side effect of creating value.\n\nThe value of a business is the eyeballs it has access to.\nQuestions to ponder:\n\nWhat business should I be in?\nWhat technologies will disrupt it?\nHow can I take advantage of tech rather than fight it?"
  },
  {
    "objectID": "posts/business/marketing/marketing.html#the-before-phase",
    "href": "posts/business/marketing/marketing.html#the-before-phase",
    "title": "Marketing Notes",
    "section": "",
    "text": "The overall phases of the direct response marketing strategy are:\n\n\n\nPhase\nStatus\nGoal\n\n\n\n\nBefore\nProspect\nGet them to know you\n\n\nDuring\nLead\nGet them to like you\n\n\nAfter\nCustomer\nGet them to trust you\n\n\n\nDirect response marketing should be:\n\nSpecific\nTrackable\nInclude an offer\nFollow up\n\nA marketing plan encompasses:\n\n\n\n\n\n\n\nStage\nExample\n\n\n\n\nAdvertising\nSign for the circus\n\n\nPromotion\nPut the sign on an elephant and walk it into town\n\n\nPublicity\nElephant tramples the mayor’s flowers and it makes the news\n\n\nPR\nYou get the mayor to laugh it off\n\n\nSales\nPeople come to the circus and spend money\n\n\n\nStrategy changes with scale\n\nDon’t blindly copy large companies. “Branding” to “get your name out there” is expensive and ineffective.\nThe one and only objective of marketing for a small company is “make a profit” For large companies it may be: appease shareholders, board, managers, existing clients, win advertising awards. Making a profit is a low priority for them.\nStrategy without tactics = paralysis by analysis\nTactics without strategy = bright shiny object syndrome\n\nProduct quality is a customer retention tool, not a customer acquisition tool.\n\nPeople don’t know how good the product is until they’ve already bought it\n“If you build it, they will come” doesn’t work\n\nThe 80/20 rule (Pareto principle) states that 80% of the outputs come from 20% of the inputs. Applying the 80/20 rule to the 80/20 rule gives the 64/4 rule: 64% of the outputs come from 4% of the inputs. Identify this 4% and focus on it. Often this is marketing.\nSpend money to make time, not vice versa. Money is a renewable resource, time is finite.\n\n\n\nThe niche should be as specific as possible - an inch wide and a mile deep.\n\nToo broad a target market is useless as the message gets diluted\nMass marketing/branding can work for larger companies but not small. Large companies have enough firepower to fire arrows in all directions. Small companies need to fire in a specific direction.\n\nBecome a specialist in your field to make price irrelevant\n\nBecome an educator and build trust\nYou don’t want to be a commodity shopped on price. A customer won on price will be lost on price.\n\nUse the PVP index to identify the ideal niche. Assign values to each possible segment for:\n\nPersonal fulfillment\nValue to the marketplace\nProfitability\n\nQuestions for the target prospect:\n\nWhat keeps them up at night?\nWhat are they afraid of?\nWhat are they angry about? Who are they angry at?\nWhat trends affect them?\nWhat do they secretly desire most? What one thing do they crave above all else?\nHow do they make decisions? Analytical, emotional, social proof\nWhat language/jargon do they use?\nWhat magazines do they read? What websites do they visit?\nWhat is a typical day like?\nWhat is the single most dominant emotion this market feels?\n\nCreate an avatar for each prospect. Put a name and photo against each market segment and describe a specific person in detail. Describe their life, job, activities, relationships. Answer each of the questions above from their POV.\n\n\n\nThere are two key elements of the message:\n\nPurpose of the ad\nWhat it focuses on\n\nKeep the message focused: 1 ad = 1 objective. “Marketing on purpose” rather than “marketing by accident”. There should be a clear call to action. You should know the exact action you want the prospect to take next.\nThe following two questions help refine the USP:\n\nWhy should they buy?\nWhy should they buy from me?\n\nKey takeaways for USP:\n\nWhy should they buy from you rather than competitor\nIf you remove the company name and logo, would people still know it’s you?\nQuality and service are not USPs. The customer only finds these out after they buy from you. USPs should attract customers before they buy.\nLowest price is not a USP. There’s always someone willing to go out of business quicker than you.\n\nPeople rarely want what you’re selling, they want the result of what you’re selling. Sell the problem. People don’t know what they want until they’re presented with it. Purchasing is done with emotions and justified with logic after the fact. “If I had asked people what they wanted, they’d have said faster horses” - Henry Ford.\nWhen you confuse them, you lose them.\n\nExplain the product and its benefit in one sentence.\nThe biggest threat isn’t that someone buys from a competitor, it’s that they do nothing.\nElevator pitch of the form: “You know ? What we do is ? In fact .”\n\nBe remarkable\n\nTransform something ordinary into something that makes the customer smile.\nUnique is dangerous, you just need to be “different enough”.\n\nCraft the offer\n\nIf you don’t give a compelling reason to buy, customers will default to price, and you become a commodity.\nDon’t just offer a discount, think about which product will definitely solve their biggest problem.\nAdd value rather than discounting, through bundles and customisation.\nCreate “apples to oranges” comparisons to your competitors to get out of the commmodity game.\nTarget pain points. Sell the problem, not a feature list. Customers in pain are price insensitive.\n\nCreate the offer:\n\nValue to customer\nLanguage and jargon they use\nReason why this offer is here. People are skeptical if it is too good to be true.\nValue stacking. Add in high-margin bonuses.\nUpsells\nOutrageous guarantee\nCreate scarcity. People react more to fear of loss than prospect of gain.\n\nWriting copy:\n\nBe yourself and be authentic. “Professional” = boring.\n“The enemy in common”. People buy with emotions: fear, love, greed, guilt, pride.\nCopy should address the bad parts of the product, not just the good. Who is this not for?\n\nNaming:\n\nTitle=content\nChoose clarity over cleverness\n\n\n\n\n\n“Half the money I spend on advertising is wasted; the trouble is I don’t know which half” - John Wanamaker\n\nMeasure ROI of ad campaign\n\nIf it made more money that it lost, it was a success. The only metric that matters.\nCustomer acquisition cost vs lifetime value of customer\nOther metrics like response rate are only useful to the extent that they help you calculate the above.\nSpecific target market is key. The goal of the ad is to make they prospect say “Hey, that’s for me”.\nWhat gets measured gets managed. Conversely, what you don’t know will hurt you.\n\nLifetime value of customer = Frontend (initial purchase) + Backend (subsequent purchases)\n\nIdeally the frontend alone exceeds the customer acquisition cost to be sustainable. Then the cost of the ad is immediately recouped and the ad campaign can be reupped for another cycle.\n\nSuccessful marketing has three components:\n\nMarket\nMessage\nMedia\n\nSocial media is a media not a strategy and it’s not free. It’s only free if your time is worthless.\nEmail is preferable to social media because you own the mailing list. The customer database which is a valuable asset in its own right.\n\nEmail regularly - weekly at least\nGive value - 3 value-building emails for each offer email\nSnail mail complements email - less cluttered and less competitive because fewer people still use it.\n\nMarketing budget\n\nThe only time for a budget is during the testing phase when your finding what works (what delivers a positive ROI)\nOnce you have a campaign that works, go all in. It makes money, it’s not an expense so don’t limit it.\nIf you could buy £10 notes for £5 you’d buy as many as possible, you wouldn’t limit it with a budget. The same applies for a marketing campaign with a £5 cost of acquisition and £10 lifetime value of customer.\n\nAvoid single points of failure\n\nOne customer, supplier, ad medium, lead generation method.\nPaid media is reliable and allows ROI to be tracked and measured."
  },
  {
    "objectID": "posts/business/marketing/marketing.html#the-during-phase",
    "href": "posts/business/marketing/marketing.html#the-during-phase",
    "title": "Marketing Notes",
    "section": "",
    "text": "Treat marketing like farming not hunting. Avoid “random acts of marketing” that hunters do. Build a CRM system and infrastructure for new leads, follow-ups, conversions, etc.\nThe first goal is to generate leads - find people who are interested.\n\nAvoid trying to sell from the ad immediately\nCapture all who could be interested, not just those ready to buy immediately\nBuild a customer database, then sell to them later\n\nThe market for your product might break down something like:\n\n3% - ready to buy now\n7% - open to buying\n30% - interested but not right now\n60% - not interested\n\nBy selling directly from the ad you limit yourself just to the 3%. Capture leads first and nurture them to appeal to the 40% who are interested.\nEducate customers to offer value, then you become a trusted advisor rather than a pest salesman.\n\n\n\nThe process of taking people from being vaguely interested to desiring it. The nurturing process means they should be predisposed to buying from you before you ever try to sell to them.\nStay in contact.\n\nMost stop after 1 or 2 follow-ups. The most effective salespeople do 10+ follow-ups.\nThe money is in the follow-up\n\nBuild trust and add value. Don’t pester to close.\n\nWhen the customer is ready to buy, you will be top of mind and they are predisposed to buy. It should be when they are ready to buy, not when you are ready to sell.\nDo not pressure prospects to close. Don’t be a pest, be a welcome guest.\nContact regularly - “market until they buy or die”.\nMost contact should be value add, with occasional pitch/offer emails. 3:1 ratio of value add:pitch.\n\nShock and awe package.\n\nPhysical parcel mailed to high probability prospects.\n“Lumpy” mail grabs attention - snail mail with something physical inside.\nNeed numbers on conversion probabilities and lifetime value of customer for this approach to be viable. Good marketing can’t beat bad math.\nHigh value contents should:\n\nProvide value\nPosition you as a trusted expert\nMove prospect further down the buying cycle.\n\n\nMake lots of offers. A/B test with small segments. Take risks, write compelling copy and make outrageous guarantees.\nThree major types of people:\n\n\n\nType\nRole\nResponsibility\n\n\n\n\nEntrepreneur\n“Make it up”\nVision\n\n\nSpecialist\n“Make it real”\nImplement\n\n\nManager\n“Make it recur”\nDaily BAU\n\n\n\nMarketing calendar.\n\nWhat needs to be done and when\nDaily, weekly, monthly, quarterly, annually\nEvent-triggers tasks, e.g. on signup, on complaint\n\nDelegate.\n\nIf someone else can do the job 80% as good as you, delegate it.\nSeparate majors and minors. Focus on the majors, dump or delegate the minors.\nDon’t mistake movement for achievement.\nDays are expensive. You can get more money but not more time.\n\n\n\n\nOnce you reach a certain level of expertise, the real profit is in how you market yourself. A concert violinist makes more money performing concerts than busking.\nTrust is key in sales.\n\nDifficult for unknown businesses.\nPeople are wary because of dodgy sellers. Assume every dog bites.\nEducate -&gt; Position as expert -&gt; Trust\nDelay the sale to show:\n\nYou are willing to give before you take\nYou are an educator, so can be trusted\n\nStop selling and start educating, consulting, advising.\nEntrepreneur is someone who solves people’s problems for profit.\n\nOutrageous guarantees.\n\nReverse the risk from the customer to you.\nGuarantee needs to be specific and address the prospect’s reservations directly. Not just a generic money-back guarantee.\nIf you’re an ethical operator, you’re already implicitly providing a guarantee, just not marketing it. Use it!\n\nPricing.\n\nThe marketing potential of price is often underutilised.\nDon’t just naively undercut the market leader or use cost-plus pricing.\nSmall number of options/tiers\n\nMore choice means you are more likely to pique interest but less likely to keep interest.\nFear of a suboptimal choice; the paradox of choice.\nStandard and premium tiers and a good rule of thumb.\n\n“Unlimited” option can often be profitable because customers overestimate how much they will use, so overpay for unlimited use.\nUltra high ticket item\n\nMakes other tiers seem more reasonably priced\n10% of customers would pay 10x more. 1% of customers would pay 100x more.\n\n\nOther price considerations:\n\nDon’t discount, unless specifically using a loss-leader strategy.\nTry before you buy (the puppy dog close)\n\nThe sale feels less permanent so more palatable for the customer.\nThe onus is on the customer to return - shifts the power balance.\n\nDon’t segregate departments - everyone can warm up a prospect.\nPayment plans - people are less attached to future money than present money."
  },
  {
    "objectID": "posts/business/marketing/marketing.html#the-after-phase",
    "href": "posts/business/marketing/marketing.html#the-after-phase",
    "title": "Marketing Notes",
    "section": "",
    "text": "The goal is to turn customers into a tribe of raving fans. Most companies stop marketing at the sell, but this is where the process begins. Most of the money is in the backend not the frontend.\nSell them what they want, give them what they need.\n\nThese don’t always overlap\nIf they buy a treadmill and don’t use it, they’ll blame you when they don’t lose weight. You need to make sure they use your product correctly after they buy it.\nSplit the process up for them to make it less daunting.\n\nCreate a sense of theatre\n\nPeople don’t just want ot be serviced, they want to be entertained.\nInnovate on: pricing, financing, packaging, support, delivery etc, not just product. For example, “will it blend” YouTube videos to sell ordinary blenders.\n\nUse technology to reduce friction. Treat tech like an employee. Why am I hiring it? What are its KPIs?\nBecome a voice of value to your tribe.\n\nEducation-based marketing leads to trust and becoming an expert in the field.\nTell your audience the effort that goes into the product. For example, shampoo bottle that says how many mint leaves went into it.\n\nCreate replicable systems\n\nSystems ensure you have a business, rather than you are the business\nOften overlooked because it is “back office” or “not urgent”\nBenefits: build valuable asset; leverage and scalability; consistency; lower labour costs\nAreas:\n\nMarketing (leads)\nSales (follow-up)\nFulfillment\nAdmin\n\nCreate an operations manual:\n\nImagine the business is 10x the size\nIdentify all roles\nIdentify all tasks - what does each role do\nChecklist for each task\n\n\nExit strategy. Start with the end in mind. If the goal is to sell for $50 million, any decision can be framed as “will this help me get to $50 million?”. Who will buy the company? Why? Customer base, revenue or IP?\n\n\n\nPeople are 21x more likely to buy from a company they’ve previously bought from. Increase revenue by selling more to existing customers.\n\nRaise prices\n\n\nPeople are less price sensitive than you’d expect.\nSome may leave but these were the lowest-value customers - this may be polluted revenue you want to fire anyway. A customer won on price can be lost on price.\nGive a reason why prices increased. Explain the benefits they’ve received and future innovations.\nGrandfather existing customers on old price as a loyalty bonus.\n\n\nUpselling\n\n\n“Contrast principle” - people are less price sensitive to add-ons. If they’ve already bought the suit then the shirt seems cheap.\n“Customers who bought X also bought Y” - people want to fit social norms.\nSell more when the client is “hot and heavy” in the buying mood. Some people naively think you should overwhelm them with more selling.\n\n\nAscension\n\n\nMove customers up to a higher-priced tier.\nProduct mix - standard, premium, ultra-high.\n\n\nFrequency of purchases\n\n\nReminders, subscriptions\nReasons to come back; voucher for next purchase.\n\n\nReactivation\n\n\nLure back lapsed customers with strong offer and call to action.\nAsk why they haven’t returned.\nApologise and make corrections if necessary. Make the feel special if they do return.\n\nKey numbers - what gets measured gets managed\n\nLeads\nConversion rate\nAverage transaction value\nBreak even point\n\nFor subscription models measure:\n\nMonthly recurring revenue\nChurn rate\nCustomer lifetime value\n\nPolluted revenue - dollars from toxic customers != dollars from raving fans. Types of customers:\n\nTribe\nChurners - can’t afford you so drop you on price or intro offer ending\nVampires - suck up all your time, you can’t afford them\nSnow leopard - big customer but not replicable\n\nNet promoter score\n\n“How likely are you to recommend us to a friend?”\nDetractors 0-6; Passive 7-8; Promoters 9-10\nNPS=+100 if everyone is a promoter, -100 if everyone is a detractor\n\nFire problem customers\n\n“The customer is always right” is naive. It should be “the right customer is always right”.\nGenuine customer complaints are valuable and may help retain other customers who had the same issue but didn’t speak up.\nLow-value, price-sensitive customers complain the most. They take up your time and energy at the expense of your tribe.\nFiring detractors creates scarcity and frees your time to focus on the tribe. With limited supply, customers have to play and pay by your rules.\n\n\n\n\nOrchestrating referrals is an active task, not needy or desperate.\n\nDon’t just deliver a good product and hope for the best\nPeople refer because it makes them feel good, helpful or knowledgeable, not to do you a favour.\nAppeal to their ego, offer them something valuable, give them a reason to refer.\n\nAsk (for a referral) and ye shall receive.\n\n“Law of 250” - most people know 250 people well enough to invite to a wedding or funeral. This is 250 potential referrals per customer.\nSet expectation of referrals beforehand - “We’re going to do a great job and we need your help… Most people refer 3 others.”\n\nBe specific about who you ask and what kind of referral you want. Conquer the bystander effect.\nMake referral profiles for each customer group in your database.\n\nWho do they know?\nHow can you make them remember you?\nWhat will make them look good?\nHow will they provide value to their referral target\n\nCustomer buying behaviour\n\nWho has your customers before you?\nJoint venture for referrals\nSell/exchange leads, resell complementary products, affiliate referral partner\n\nBrand = personality of the business. Think of the business as a person:\n\nName\nDesign (what they wear)\nPositioning (how they communicate)\nBrand promise (core values)\nTarget market (who they associate with)\nBrand awareness (how well known / popular)\n\nBest form of brand building is selling\n\nBranding is what you do after someone has bought from you.\nBrand equity - customers crossing the road to buy from you and walk past competitors."
  },
  {
    "objectID": "posts/business/marketing/marketing.html#conclusion",
    "href": "posts/business/marketing/marketing.html#conclusion",
    "title": "Marketing Notes",
    "section": "",
    "text": "Implementation is crucial\n\nKnowing and not doing is the same as not knowing.\nCommon pitfalls:\n\nParalysis by analysis - 80% out the door is better than 100% in the drawer; money loves speed.\nFail to delegate\n“My business is different” - Spend your effort figuring out how to make it work rather than why it won’t work.\n\n\nTime is not money\n\nEntrepreneurs get paid for the value they create, not the time or effort they put in.\nMaking money is a side effect of creating value.\n\nThe value of a business is the eyeballs it has access to.\nQuestions to ponder:\n\nWhat business should I be in?\nWhat technologies will disrupt it?\nHow can I take advantage of tech rather than fight it?"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html",
    "href": "posts/software/snowflake/snowpro_notes.html",
    "title": "Snowflake: SnowPro Core",
    "section": "",
    "text": "The Snowsight interface is the GUI through which we interact with Snowflake.\nWhen querying a Snowflake table, a fully qualified table name means database_name + schema_name + table_name. For example, “DERIVED_DB.PUBLIC.TRADES_DATA”\nWorksheets are associated with a role.\nA warehouse is needed for compute to execute a query.\nSnowflake is a “self-managed cloud data platform”. It is cloud only. No on-premise option.\n“Self-managed” service means:\n\nNo hardware\nNo software\nNo maintenance\n\n“Data platform” means it can function as:\n\nData warehouse\nData lake - mix of structured and semi structured data\nData science - use your preferred language via Snowpark"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#multi-cluster-shared-disk",
    "href": "posts/software/snowflake/snowpro_notes.html#multi-cluster-shared-disk",
    "title": "Snowflake: SnowPro Core",
    "section": "2.1. Multi-Cluster Shared Disk",
    "text": "2.1. Multi-Cluster Shared Disk\nIn general, there are two approaches to designing a dsitributed data / compute platform: shared-disk and shared-nothing.\nShared-disk uses central data storage connected to multiple compute nodes.\n\nPros: simple, easy data management since their is only one database/disk\nCons: limited scalability (bottleneck of the central disk), single point of failure\n\nShared-nothing keeps each node independent. Each node is a separate processor, memory and disk.\n\nPros: scalability, availability\nCons: complicated, expensive\n\nSnowflake uses a hybrid approach: “multi-cluster shared-data”.\n\nThere is a single data repository like shared-disk.\nThere are multiple clusters or nodes that store a portion of the data locally, like shared-nothing.\n\nThis combines the pros of both: simplicity and scalability."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#layers-of-snowflake",
    "href": "posts/software/snowflake/snowpro_notes.html#layers-of-snowflake",
    "title": "Snowflake: SnowPro Core",
    "section": "2.2. Layers of Snowflake",
    "text": "2.2. Layers of Snowflake\nThere are three distinct layers of Snowflake:\n\nDatabase storage\n\nCompressed columnar storage.\nThis is stored as blobs in AWS, Azure, GCP etc.\nSnowflake abstracts this away so we just interact with it like a table.\nThis is optimised for OLAP (analytical purposes) which is read-heavy, rather than OLTP which is write-heavy.\n\nCompute\n\n“The muscle of the system”.\nQuery processing.\nQueries are processed using “virtual warehouses”. These are massive parallel processing compute clusters, e.g. EC2 on AWS.\n\nCloud services\n\n“The brain of the system”.\nCollection of services to manage and coordinate components, e.g. the S3 and EC2 instances used in the other two layers.\nThe cloud services layer also runs on a compute instance of the cloud provider and is completely handled by Snowflake.\nThis layer handles: authentication, access control, metadata management, infrastructure management, query parsing and optimisation. The query execution happens in the compute layer."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#loading-data-into-snowflake",
    "href": "posts/software/snowflake/snowpro_notes.html#loading-data-into-snowflake",
    "title": "Snowflake: SnowPro Core",
    "section": "2.3. Loading Data into Snowflake",
    "text": "2.3. Loading Data into Snowflake\nThis is covered more extensively in its own section, but this sub-section serves as a brief introduction.\nThe usual SQL commands can be used to create databases and tables.\nCREATE DATABASE myfirstdb\nALTER DATABASE myfirstdb RENAME firstdb\nCREATE TABLE loan_payments (\n    col1 string,\n    col2 string,\n);\nWe can specify a database to use with the USE DATABASE command to switch the active database. This avoids having to use the fully qualified table name everywhere.\nUSE DATABASE firstdb\n\nCOPY INTO loan_payments\nFROM s3/… -- The URL to copy from\nfile_format = (delimiter = “,”,\n               skip rows=1,\n               type=csv);"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#snowflake-editions",
    "href": "posts/software/snowflake/snowpro_notes.html#snowflake-editions",
    "title": "Snowflake: SnowPro Core",
    "section": "2.4. Snowflake Editions",
    "text": "2.4. Snowflake Editions\nThe different Snowflake editions vary by features and pricing. The feature matrix is available on the Snowflake docs.\n\nStandard\n\nComplete DWH, automatic data encryption, support for standard and special data types, time travel 1 day, disaster recovery for 7 days beyond time travel, network policies, federated auth and SSO, 24/7 support\n\nEnterprise\n\nMulti cluster warehouse, time travel 90 days, materialised views, search optimisation, column-level security, 24 hour early access to new releases\n\nBusiness critical\n\nAdditional security features such as customer managed encryption, support for data specific regulation, database failover and fallback\n\nVirtual private\n\nDedicated virtual servers and warehouse, dedicated metadata store. Isolated from all other snowflake accounts."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#compute-costs",
    "href": "posts/software/snowflake/snowpro_notes.html#compute-costs",
    "title": "Snowflake: SnowPro Core",
    "section": "2.5. Compute Costs",
    "text": "2.5. Compute Costs\n\n2.5.1. Overview of Cost Categories\nCompute costs and storage costs are decoupled and can be scaled separately. “Pay for what you need”.\n\nActive warehouses\n\nUsed for standard query processing.\nBilled per second (minimum 1 minute).\nDepends on size of warehouse, time and number of warehouses.\n\nCloud services\n\nBehind-the-scenes cloud service tasks.\nOnly charged if &gt;10% of warehouse consumption, which is not the case for most customers.\n\nServerless\n\nUsed for search optimisation and Snowpipe.\nThis is compute that is managed by snowflake, e.g. event-based processing.\n\n\nThese are charged in Snowflake credits.\n\n\n2.5.2. Calculating Number of Credits Consumed-\nThe warehouses consume the following number of credits per hour:\n\n\n\nWarehouse Size\nNumber of Credits\n\n\n\n\nXS\n1\n\n\nS\n2\n\n\nM\n4\n\n\nL\n8\n\n\nXL\n16\n\n\n4XL\n128\n\n\n\nCredits cost different amounts per edition. It also depends on the cloud provider (AWS) and region (US-East-1). Indicative costs for AWS US-East-1 are:\n\n\n\nEdition\n$ / Credit\n\n\n\n\nStandard\n2\n\n\nEnterprise\n3\n\n\nBusiness Critical\n4"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#storage-and-data-costs",
    "href": "posts/software/snowflake/snowpro_notes.html#storage-and-data-costs",
    "title": "Snowflake: SnowPro Core",
    "section": "2.6 Storage and Data Costs",
    "text": "2.6 Storage and Data Costs\n\n2.6.1. Storage Types and Costs\nMonthly storage costs are based on average storage used per month. Also depends on cloud provider and region. Cost is calculated AFTER Snowflake’s data compression.\nThere are two options for storage pricing:\n\nOn demand storage: Pay for what you use.\nCapacity storage: Pay upfront for defined capacity.\n\nTypically start with on demand until we understand our actual usage, then shift to capacity storage once this is stable.\n\n\n2.6.2. Transfer Costs\nThis depends on data ingress vs egress.\n\nData IN is free\n\nSnowflake wants to remove friction to getting your data in.\n\nData OUT is charged\n\nSnowflake wants to add friction to leaving.\nDepends on cloud provider and region. In-region transfers are free. Cross-region or cross-providers are charged."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#storage-monitoring",
    "href": "posts/software/snowflake/snowpro_notes.html#storage-monitoring",
    "title": "Snowflake: SnowPro Core",
    "section": "2.7. Storage Monitoring",
    "text": "2.7. Storage Monitoring\nWe can monitor storage for individual tables.\nSHOW TABLES gives general table storage stats and properties.\nWe get more detailed views with TABLE_STORAGE_METRICS. We can run this against the information schema or the account storage. These split the sizes into active bytes, time travel bytes and failsafe bytes.\nFor the information schema metrics:\nSELECT * FROM DB_NAME.INFORMATION_SCHEMA.TABLE_STORAGE_METRICS;\nFor the account admin metrics, this needs to use the correct account admin role USE ROLE ACCOUNTADMIN.\nSELECT * FROM SNOWFLAKE.ACCOUNT_USAGE.TABLE_STORAGE_METRICS;\nWe can also look at the Admin -&gt; Usage screen in the Snowflake GUI."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#resource-monitors",
    "href": "posts/software/snowflake/snowpro_notes.html#resource-monitors",
    "title": "Snowflake: SnowPro Core",
    "section": "2.8. Resource Monitors",
    "text": "2.8. Resource Monitors\nResource monitors help us control and monitor credit usage of individual warehouses and the entire account.\nWe can set a credit quota which limit the credits used per period. For example, the maximim number of credits that can be spent per month.\nWe can set actions based on when a percentage of the credit limit is reached. These percentages can be &gt;100%. There are three options for the choice of action:\n\nNotify\nSuspend and notify (but continue running tasks that have already started)\nSuspend immediately (aborting any running queries) and notify.\n\nWe set this using the Usage tab in the ACCOUNTADMIN role in the snowsight UI under Admin -&gt; Usage. Other roles can be granted MONITOR and MODIFY privileges.\nWe can select a warehouse then filter on different dimensions, for example, distinguishing storage vs compute vs data transfer costs.\nTo set up a new resource monitor, we give it:\n\nName\nCredit quota: how many credits to limit to\nMonitor type: specific warehouse, group of warehouses, or overall account\nSchedule\nActions"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#warehouses-and-multi-clustering",
    "href": "posts/software/snowflake/snowpro_notes.html#warehouses-and-multi-clustering",
    "title": "Snowflake: SnowPro Core",
    "section": "2.9. Warehouses and Multi Clustering",
    "text": "2.9. Warehouses and Multi Clustering\n\n2.9.1. Warehouse Properties\nThere are different types and sizes of warehouse and they can be multi-clustered.\nTypes: standard and snowpark-optimised (for memeory-intensive tasks like ML)\nSize: XS to XXL. Snowpark type is only M or bigger and consumes 50% more credits\nMulti-clustering is good for more queries, i.e. more concurrent users. We scale horizontally so there are multiple small warehouses rather than one big one. They can be in maximised mode (set size) or autoscaled mode (number of nodes scales between predefined min and max)\nThe autoscaler decides to add warehouses based on the queue, according to the scaling policy.\n\nStandard\n\nFavours starting extra clusters.\nStarts a new cluster as soon as there is a query queued.\nCluster shuts down after 2 to 3 successful checks. A “check” is when the load on the least used node could be redistributed to other nodes.\n\nEconomy\n\nFavours conserving credits.\nStarts a new cluster once the workload for the cluster would keep it running for &gt; 6 mins.\nCluster shuts down after 5-6 successful checks.\n\n\n\n\n2.9.2. Creating a Warehouse\nTo create a warehouse, we need to use the ACCOUNTADMIN, SECURTIYADMIN or SYSADMIN role.\nWarehouses can either be created through UI or SQL.\nCREATE WAREHOUSE my_wh\nWITH\nWAREHOUSE_SIZE = XSMALL\nMIN_CLUSTER_COUNT = 1\nMAX_CLUSTER_COUNT = 3\nAUTO_RESUME = TRUE\nAUTO_SUSPEND = 300\nCOMMENT = 'This is the first warehouse'\nWe can also ALTER or DROP a warehouse in SQL, just like we normally would with DROP TABLE.\nDROP WAREHOUSE my_wh;"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#snowflake-objects",
    "href": "posts/software/snowflake/snowpro_notes.html#snowflake-objects",
    "title": "Snowflake: SnowPro Core",
    "section": "2.10. Snowflake Objects",
    "text": "2.10. Snowflake Objects\nThere is a hierarchy of objects in Snowflake.\n\n\n\n\n\nflowchart TD\n\n\n  A(Organisation) --&gt; B1(Account 1)\n  A(Organisation) --&gt; B2(Account 2)\n\n\n  B1 --&gt; C1(Users)\n  B1 --&gt; C2(Roles)\n  B1 --&gt; C3(Databases)\n  B1 --&gt; C4(Warehouses)\n  B1 --&gt; C5(Other account objects)\n  \n  C3 --&gt; D1(Schemas)\n\n  D1 --&gt; E1(UDFs)\n  D1 --&gt; E2(Views)\n  D1 --&gt; E3(Tables)\n  D1 --&gt; E4(Stages)\n  D1 --&gt; E5(Other database objects)\n\n\n\n\n\n\nAn organisation (managed by ORGADMIN) can have multiple accounts (each managed by am ACCOUNTADMIN). These accounts might be by cloud region or department.\nWithin each account we have multiple account objects: users, roles, databases, warehouses, other objects.\nDatabases can have multiple schemas.\nSchemas can have multiples UDFs, views, tables, stages, other objects."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#snowsql",
    "href": "posts/software/snowflake/snowpro_notes.html#snowsql",
    "title": "Snowflake: SnowPro Core",
    "section": "2.11. SnowSQL",
    "text": "2.11. SnowSQL\nSnowSQL is used to connect to Snowflake via the command line. It needs to be installed on your local machine.\nWe can execute queries, load and unload data, etc."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#stages",
    "href": "posts/software/snowflake/snowpro_notes.html#stages",
    "title": "Snowflake: SnowPro Core",
    "section": "3.1. Stages",
    "text": "3.1. Stages\nStages are locations used to store data.\nFrom the stage, say an S3 bucket, we can load data from stage -&gt; database. Likewise, we can unload data from database -&gt; stage (S3 bucket).\nStages can be internal (managed by Snowflake) or external (managed by your cloud provider, eg AWS S3).\n\n3.1.1. Internal Stage\nAn internal stage is managed by Snowflake.\nWe upload data into an internal stage using the PUT command. By default, files are compressed with gzip and encrypted.\nWe load it into the database using the COPY INTO command. We can also unload using the COPY INTO command by varying the destination.\nThere are three types of stage:\n\nUser stage\n\nCan only be accessed by one user\nEvery user has one by default\nCannot be altered or dropped\nAccessed with @~\n\nTable stage\n\nCan only be accessed by one table\nCannot be altered or dropped\nUse this to load to a specific table\nAccessed with @%\n\nNamed stage\n\nCREATE STAGE to create your own\nThis is then just like any other database object, so you can modify it or grant privileges\nMost commonly used stage\nAccessed with @\n\n\nA typical use case for an internal stage is when we have a file on our local system that we want to load into Snowflake, but we don’t have an external cloud provider set up.\n\n\n3.1.2. External Stage\nAn external stage connects to an external cloud provider, such as an S3 bucket.\nWe create it with the CREATE STAGE command as with an internal stage. This creates a Snowflake object that we can modify and grant privileges to.\nCREATE STAGE stage_name \n  URL='s3://bucket/path/'\nWe can add CREDENTIALS argument but this would store them in plain text. A better practice is to pass a STORAGE_INTEGRATION argument that points to credentials.\nWe can also specify the FILE_FORMAT.\n\n\n3.1.3. Commands For Stages\nSome of the most common commands for stages:\n\nLIST\n\nList all files (and additional properties) in the stage.\n\nCOPY INTO\n\nLoad data into the stage, or unload data from the stage.\n\nSELECT\n\nQuery from stage\n\nDESC\n\nDescribe the stage. Shows the default values or arguments."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#copy-into",
    "href": "posts/software/snowflake/snowpro_notes.html#copy-into",
    "title": "Snowflake: SnowPro Core",
    "section": "3.2. COPY INTO",
    "text": "3.2. COPY INTO\nThis can bulk load or unload data.\nA warehouse is needed. Data transfer costs may apply if moving across regions or cloud providers.\n\n3.2.1. Loading Data\nLoad data from a stage to a table with:\nCOPY INTO table_name \nFROM stage_name\nWe can specify a file or list of files with the FILES argument.\nSupported file formats are:\n\ncsv (default)\njson\navro\norc\nparquet\nxml\n\nWe can also use the PATTERN argument to match a file pattern with wildcards, e.g. order*.csv\n\n\n3.2.2. Unloading Data\nUnloading data from the table to a stage uses the same syntax:\nCOPY INTO stage_name \nFROM table_name\nAs with loading, we can specify a file format with the FILE_FORMAT arg, or pass a reusable FILE_FORMAT object.\nCOPY INTO stage_name \nFROM table_name\nFILE_FORMAT = ( FORMAT_NAME = 'file_format_name' |\n                TYPE = CSV )"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#file-format",
    "href": "posts/software/snowflake/snowpro_notes.html#file-format",
    "title": "Snowflake: SnowPro Core",
    "section": "3.3. File Format",
    "text": "3.3. File Format\nIf the file format is not specified, it defaults to csv. You can see this and other default values by describing the stage with:\nDESC STAGE stage_name\nWe can overrule the defaults by specifying FILE_FORMAT argument in the COPY INTO command.\nA better practice is to use the file_format arg to pass a file_format object such as\nFILE_FORMAT = (TYPE = CSV)\nWe create this object with\nCREATE FILE FORMAT file_format_name\nTYPE = CSV\nFIELD_DELIMITER = ‘,’\nSKIP_HEADER = 1\nWe write this file format to a table like manage_db. Then we can reuse it in multiple places when creating the stage or table, loading or unloading data, etc."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#insert-and-update",
    "href": "posts/software/snowflake/snowpro_notes.html#insert-and-update",
    "title": "Snowflake: SnowPro Core",
    "section": "3.4. Insert and Update",
    "text": "3.4. Insert and Update\nInsert is the same as standard SQL:\nINSERT INTO table_name\nVALUES (1, 0.5, 'string')\nTo only insert specific columns:\nINSERT INTO table_name (col1, col2)\nVALUES (1, 0.5)\nINSERT OVERWRITE will truncate any existing data and insert only the given values. Use with caution! Any previous data is dropped, the table with only have the rows in this command.\nUpdate also works like standard SQL:\nUPDATE table_name\nSET col1=10\nWHERE col1=1\nTRUNCATE removes all of the values in the table.\nDROP removes the entire table object and its contents."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#storage-integration-object",
    "href": "posts/software/snowflake/snowpro_notes.html#storage-integration-object",
    "title": "Snowflake: SnowPro Core",
    "section": "3.5. Storage Integration Object",
    "text": "3.5. Storage Integration Object\nThis object stores a generated identity for external cloud storage.\nWe create it as a Snowflake object which constrains the allowed location and grant permissions to it in AWS, Azure etc."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#snowpipe",
    "href": "posts/software/snowflake/snowpro_notes.html#snowpipe",
    "title": "Snowflake: SnowPro Core",
    "section": "3.6. Snowpipe",
    "text": "3.6. Snowpipe\nThe discussion so far has focused on bulk loading, i.e. manual loading of a batch of data.\nSnowpipe is used for continuous data loading.\nA pipe is a Snowflake object. It loads data immediately when a file appears in blob storage. It triggers a predefined COPY command. This is useful when data needs to be available immediately.\nSnowpipe uses serverless features rather than warehouses.\nWhen files are uploaded to an S3 bucket, it sends an event notification to a serverless process which executes the copy command into the Snowflake database.\nCREATE PIPE pipe_name\nAUTO_INGEST = TRUE\nINGESTION = notification integration from cloud storage \nCOMMENT = string\nAS COPY INTO table_name\nFROM stage_name\nSnowpipe can be triggered by cloud messages or REST API. Cloud messages are for external stages only with that cloud provider. REST API can be internal or external stage.\n\nCost is based on “per second per core” of the serverless process.\nTime depends on size and number of files.\nIdeal file size is between 100-250 MB.\n\nSnowflake stores metadata about the file loading. Old history is retained for 14 days. The location of the pipe is stored in a schema in the database.\nThe schedule can be paused or resumed by altering the pipe.\nALTER PIPE pipe_name\nSET PIPE_EXECUTION_PAUSED = True"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#copy-options",
    "href": "posts/software/snowflake/snowpro_notes.html#copy-options",
    "title": "Snowflake: SnowPro Core",
    "section": "3.7. Copy Options",
    "text": "3.7. Copy Options\nThese are arguments we can pass to COPY INTO for loading and unloading. Some options only apply to loading and do not apply to unloading.\nThey are properties of the stage object, so if the arguments are not passed Snowflake will fall back to these default values.\n\n3.7.1. ON_ERROR\n\nData Type: String\nDescription: Only for data loading. How to handle errors in files.\nPossible Values:\n\nCONTINUE - Continue loading file if errors are found.\nSKIP_FILE - Skip loading this file if errors are found. This is the default for Snowpipe.\nSKIP_FILE_&lt;num&gt; - Skip if &gt;= num errors are found (absolute).\nSKIP_FILE_&lt;pct&gt;% - Skip if &gt;= pct errors are found (percentage).\nABORT_STATEMENT - Abort loading if an error is found. This is the default for bulk load.\n\n\n\n\n3.7.2. SIZE_LIMIT\n\nData Type: Int\nDescription: Maximum cumulative size, in bytes, to load. Once this amount of data has been loaded, skip any remaining files.\nPossible Values: Int bytes.\n\n\n\n3.7.3. PURGE\n\nData Type: Bool\nDescription: Remove files from the stage after they have been loaded.\nPossible Values: FALSE (default) | TRUE\n\n\n\n3.7.4. MATCH_BY_COLUMN_NAME\n\nData Type: String\nDescription: Load semi structured data by matching field names.\nPossible Values: NONE (default) | CASE_SENSITIVE | CASE_INSENSITIVE\n\n\n\n3.7.5. ENFORCE_LENGTH\n\nData Type: Bool\nDescription: If we have a varchar(10) field, how should we handle data that is too long?\nPossible Values:\n\nTRUE (default) - Raise an error\nFALSE - Automatically truncate strings\n\n\nTRUNCATECOLUMNS is an alternative arg that does the opposite.\n\n\n3.7.6. FORCE\n\nData Type: Bool\nDescription: If we have loaded this file before, should we load it again?\nPossible Values: False (default) | TRUE\n\n\n\n3.7.7. LOAD_UNCERTAIN_FILES\n\nData Type: Bool\nDescription: Should we load files if the load status is unknown?\nPossible Values: False (default) | TRUE\n\n\n\n3.7.8. VALIDATION_MODE\n\nData Type: String\nDescription: Validate the data instead of actually loading it.\nPossible Values:\n\nRETURN_N_ROWS - Validate the first N rows and returns them (like a SELECT statement would). If there is one or more errors in those rows, raise the first.\nRETURN_ERRORS - Return all errors in the file."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#validate",
    "href": "posts/software/snowflake/snowpro_notes.html#validate",
    "title": "Snowflake: SnowPro Core",
    "section": "3.8. VALIDATE",
    "text": "3.8. VALIDATE\nThe VALIDATE function validates the files loaded in a previous COPY INTO.\nReturns a list of errors from that bulk load. This is a table function, which means it returns multiple rows.\nSELECT * \nFROM TABLE(VALIDATE(table_name, JOB_ID =&gt; ‘_last’))\nWe can pass a query ID instead of _last to use a specific job run rather than the last run."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#unloading",
    "href": "posts/software/snowflake/snowpro_notes.html#unloading",
    "title": "Snowflake: SnowPro Core",
    "section": "3.9. Unloading",
    "text": "3.9. Unloading\nThe syntax for unloading data from a table into a stage is the same as loading, we just swap the source and target.\nCOPY INTO stage_name FROM table_name\nWe can unload specific rows or columns by using a SELECT statement:\nCOPY INTO stage_name \nFROM (SELECT col1, col2 FROM table_name)\nWe can pass a FILE_FORMAT object and HEADER args.\nWe can also specify the prefix or suffix for each file. By default the prefix is data_ and the suffix is _0, _1, etc.\nCOPY INTO stage_name/myprefix\nThis is the default behaviour to split the output into multiple files once MAX_FILE_SIZE is reached, setting an upper limit on the output. The SINGLE parameter can be passed to override this, to force the unloading task to keep the output to a single file without splitting.\nIf unloading to an internal stage, to get the data on your local machine use SnowSQL to run a GET command on the internal stage after unloading.\nYou can then use the REMOVE command to delete from the internal stage."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#transformations-and-functions",
    "href": "posts/software/snowflake/snowpro_notes.html#transformations-and-functions",
    "title": "Snowflake: SnowPro Core",
    "section": "4.1. Transformations and Functions",
    "text": "4.1. Transformations and Functions\nWe can specify transformations in the SELECT statement of the COPY command.\nThis can simplify ETL pipelines when performing simple transformations such as: column reordering, casting data types, removing columns, truncating strings to a certain length. We can also use a subset of SQL functions inside the COPY command. Supports most standard SQL functions defined in SQL:1999.\nSnowflake does not support more complex SQL inside the COPY command, such as FLATTEN, aggregations, GROUP BY, filtering with WHERE, JOINs.\nSupported functions:\n\nScalar functions. Return one value per row. E.g. DAYNAME\nAggregate functions. Return one value per group / table. E.g. MAX.\nWindow functions. Aggregate functions that return one value per row. E.g. SELECT ORDER_ID, SUBCATEGORY, MAX(amount) OVER (PARTITION BY SUBCATEGORY) FROM ORDERS;\nTable functions. Return multiple rows per input row. E.g. SELECT * FROM TABLE(VALIDATE(table_name, JOB_ID =&gt; ‘_last’))\nSystem functions. Control and information functions. E.g. SYSTEM$CANCEL_ALL_QUERIES\nUDFs. User-defined functions.\nExternal functions. Stored and executed outside of Snowflake."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#estimation-functions",
    "href": "posts/software/snowflake/snowpro_notes.html#estimation-functions",
    "title": "Snowflake: SnowPro Core",
    "section": "4.2. Estimation Functions",
    "text": "4.2. Estimation Functions\nExact calculations on large tables can be very compute-intensive or memory-intensive. Sometimes an estimate is good enough.\nSnowflake has some algorithms implemented out of the box that can give useful estimates with fewer resources.\n\n4.2.1. Number of Distinct Values - HLL()\nHyperLogLog algorithm.\nAverage error is ~1.6%.\nReplace\nCOUNT(DISTINCT (col1, col2, ...))\nwith\nHLL(col1, col2, ...)\nor\nAPPROX_COUNT_DISTINCT (col1, col2, ...)\n\n\n4.2.2. Frequent Values - APPROX_TOP_K()\nEstimate the most frequent values and their frequencies. Space-saving algorithm.\nUse the following command. The k argument is optional and defaults to 1. The counters argument is optional and specifies the maximum number of distinct values to track. If using this, we should use counters &gt;&gt; k.\nAPPROX_TOP_K (col1, k[optional], counters[optional])\n\n\n4.2.3. Percentile Values - APPROX_PERCENTILE()\nt-Digest algorithm.\nAPPROX_PERCENTILE (col1, percentile)\n\n\n4.2.4. Similarity of Two or More Data Sets - MINHASH & APPROXIMATE_SIMILARITY()\nThe full calculation uses the Jaccard similarity cofficient. This returns a value between 0 and 1 indicating similarity. \\[\nJ(A, B) = |(A \\cap B)| / |A \\cup B|\n\\]\nThe approximation is a two-step process that uses the MinHash algorithm to hash each table, then the APPROXIMATE_SIMILARITY function to estimate \\(J(A, B)\\).\nThe argument k in MINHASH is the number of hash functions to use. Higher k is more accurate but slower. We can pass individual column names instead oif *.\nSELECT MINHASH(k, *) AS mh FROM table_name;\nThe full approximation command is:\nSELECT APPROXIMATE_SIMILARITY(mh)\nFROM (\n    SELECT MINHASH(100, *) AS mh FROM mhtab1\n    UNION ALL\n    SELECT MINHASH(100, *) AS mh FROM mhtab2\n);"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#user-defined-functions-udf",
    "href": "posts/software/snowflake/snowpro_notes.html#user-defined-functions-udf",
    "title": "Snowflake: SnowPro Core",
    "section": "4.3. User-Defined Functions (UDF)",
    "text": "4.3. User-Defined Functions (UDF)\nThese are one of several ways of extending functionality with additional functions.\n(The other approaches are stored procedures and external functions, which are covered in the next sections.)\nUDFs support the following languages: SQL, Python, Java, JavaScript\n\n4.3.1. Defining a UDF\nWe can define a SQL UDF using create function.\nCREATE FUNCTION add_two(n int)\nreturns int\n    AS\n    $$\n    n+2\n    $$;\nDefining a UDF in Python is similar, we just need to specify the language and some other options.\nCREATE FUNCTION add_two(n int)\nreturns int\nlanguage Python\nruntime_version =‘3.8’\nhandler = ‘addtwo’ \n    AS\n    $$\n    def add_two(n):\n        return n+2\n    $$;\n\n\n4.3.2. Using a UDF\nWe just call the UDF from SnowSQL, for example\nSELECT add_two(3);\n\n\n4.3.3. Function Properties\nFunctions can be:\n\nScalar functions: Return one output row per input row.\nTabular functions: Return a table per input row.\n\nFunctions are schema-level objects in Snowflake. We can see them under Schema.Public.Functions.\nWe can manage access and grant privileges to functions, just like any other Snowflake object."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#stored-procedures",
    "href": "posts/software/snowflake/snowpro_notes.html#stored-procedures",
    "title": "Snowflake: SnowPro Core",
    "section": "4.4. Stored Procedures",
    "text": "4.4. Stored Procedures\nAnother way of extending functionality, like UDFs.\nUDF vs stored procedures: - UDF is typically used to calculate a value. It needs to return a value. No need to have access to the objects referenced in the function. - A stored procedure is typically used for database operations like INSERT, UPDATE, etc. It doesn’t need to return a value.\nCan rely on the caller’s or the owner’s access rights.\nProcedures are securable objects like functions, so we can grant access to them.\nSupported languages:\n\nSnowflake scripting - Snowflake SQL + procedural logic\nJavaScript\nSnowpark API - Python, Scala, Java\n\n\n4.4.1. Creating a Stored Procedure\nCREATE PROCEDURE find_min(n1 int, n2 int)\nreturns int\nlanguage sql\n    AS\n    BEGIN\n    IF (n1 &lt; n2)\n        THEN RETURN n1;\n        ELSE RETURN n2;\n    END IF;\n    END;\nStored procedures can be run with the caller’s rights or the user’s rights. This is defined with the stored procedure. By default, they run as owner but we can override this with:\nexecute as caller\n\n\n4.4.2. Calling a Stored Procedure\nCALL find_min(5,7)\nWe can reference dynamic values, such as variables in the user’s sessions, in stored procedures. - If argument is referenced in SQL, use :argname - If an object such as a table is referenced, use IDENTIFIER(:table_name)"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#external-functions",
    "href": "posts/software/snowflake/snowpro_notes.html#external-functions",
    "title": "Snowflake: SnowPro Core",
    "section": "4.5. External Functions",
    "text": "4.5. External Functions\nThese are user-defined functions that are stored and executed outside of Snowflake. Remotely executed code is referred to as a “remote service”.\nThis means it can reference third-party libraries, services and data.\nExamples of external API integrations are: AWS lambda function, Microsoft Azure function, HTTPS server.\nCREATE EXTERNAL FUNCTION my_func(string_col VAR_CHAR)\nreturns variant\napi_integration = azure_external_api_integration\nAS 'https://url/goes/here'\nSecurity-related information is stored in an API integration. This is a schema-level object, so it is securable and can be granted access to .\nAdvantages:\n\nCan use other languages\nAccess 3rd-party libraries\nCan be called from elsewhere, not just Snowflake, so we can have one central repository.\n\nLimitations:\n\nOnly scalar functions\nSlower performance - overhead of external functions\nNot shareable"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#secure-udfs-and-procedures",
    "href": "posts/software/snowflake/snowpro_notes.html#secure-udfs-and-procedures",
    "title": "Snowflake: SnowPro Core",
    "section": "4.6. Secure UDFs and Procedures",
    "text": "4.6. Secure UDFs and Procedures\nWe may want to hide certain information such as the function definition, or prevent users from seeing underlying data.\nWe just use the SECURE keyword.\nCREATE SECURE FUNCTION ...\nDisadvantages: - Lower query performance. The optimiser exposes some security risks, so this is locked down which restricts the optimisation options available therefore impacting performance lock this down the optimisations are restricted.\nWe should use it for sensitive data, otherwise the performance trade-off isn’t worthwhile."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#sequences",
    "href": "posts/software/snowflake/snowpro_notes.html#sequences",
    "title": "Snowflake: SnowPro Core",
    "section": "4.7. Sequences",
    "text": "4.7. Sequences\nSequences are typically used for DEFAULT values in CREATE TABLE statements. Sequences are not guaranteed to be gap-free.\nSequences are securable objects that we can grant privileges to.\nIt is a schema object like: Functions, Stored Procedures, Tables, File Formats, Stages, Stored Procedures, UDFs, Views, and Materialized Views.\nCREATE SEQUENCE my_seq\nSTART = 1\nINCREMENT = 1\nBoth START and INCREMENT default to 1.\nWe invoke a sequence with my_seq.nextval. For example:\nCREATE TABLE my_table(\n    id int DEFAULT my_seq.nextval,\n    first_name varchar\n    last_name varchar\n);\n\nINSERT INTO my_table(first_name, last_name)\nVALUES ('John', 'Cena'), ('Dwayne', 'Johnson'),('Steve','Austin');"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#semi-structured-data",
    "href": "posts/software/snowflake/snowpro_notes.html#semi-structured-data",
    "title": "Snowflake: SnowPro Core",
    "section": "4.8. Semi-structured Data",
    "text": "4.8. Semi-structured Data\n\n4.8.1. What is Semi-structured Data?\nSemi-structured data has no fixed schema, but contains tags/labels and has a nested structure.\nThis is in contrast to structured data like a table. Or unstructured data which is a free-for-all.\nSupported formats: json, xml, parquet, orc, avro\nSnowflake deals with unstructured data using 3 different data types:\n\nObject - think of this in the JavaScript sense, i.e. key:value pairs\nArray\nVariant - this can store data of any other type, including arrays and objects. Native support for semi-structured data.\n\nWe typically let Snowflake convert semi-structured data into a hierarchy of arrays and objects within a variant object.\nNulls and non-native strings like dates are cast to strings within a variant.\nVariant can store up to 16 MB uncompressed per row. If the data exceeds this, we will need to restructure or split the input.\nThe standard “ELT” approach (Extract, Load, Transform) for semi-structured data is to load the data as is, then transform it later once we’ve eyeballed it. This is a tweak of the classic ETL approach.\nWe often need to FLATTEN the data.\n\n\n4.8.2. Querying Semi-structured Data\nTo access elements of a VARIANT column, we use a :.\nFor example, if the raw_column column has a top-level key of heading1, we can query:\nSELECT raw_column:heading1\nFROM table_with_variant\nWe can also refer to columns by their position. So to access the first column:\nSELECT $1:heading1\nFROM table_with_variant\nTo access nested elements, use .:\nSELECT raw_column:heading1.subheading2\nFROM table_with_variant\nIf there is an array, we can access elements of the array with [index]. Arrays are 0-indexed, so to access the first element:\nSELECT raw_column:heading1.subheading2.array_field[0]\nFROM table_with_variant\nWe may also need to cast the types of the element with ::\nSELECT raw_column:heading1.subheading2.array_field[0]::VARCHAR\nFROM table_with_variant\n\n\n4.8.3. Flatten Hierarchical Data\nWe may want to flatten the hierarchies within semi-structured datainto a relational table. We do this with the FLATTEN table function.\nFLATTEN(INPUT =&gt; &lt;expression&gt;)\nNote that FLATTEN cannot be used inside a COPY command.\nWe can use it alongside the other keys of the data. For example:\nSELECT\n    RAW_FILE:id,\n    RAW_FILE:first_name,\n    VALUE as prev_company  -- this is the flatten array\nFROM \n    MY_DB.PUBLIC.JSON_RAW_TABLE,\n    TABLE(FLATTEN(input =&gt; RAW_FILE:previous_companies))\nThere is an implicit lateral join between the table and the flattened table function result set. We can make this explicit using the LATERAL FLATTEN keywords. The result is the same.\n\n\n4.8.4. Insert JSON Data\nWe use the PARSE_JSON function to import JSON data.\nSELECT PARSE_JSON(' { \"key1\": \"value1\", \"key2\": \"value2\" } ');\nWhen inserting variant data, we don’t use INSERT VALUES, we use a SELECT statement.\nINSERT INTO semi_structured_table_name\nSELECT PARSE_JSON(' { \"key1\": \"value1\", \"key2\": \"value2\" } ');"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#unstructured-data",
    "href": "posts/software/snowflake/snowpro_notes.html#unstructured-data",
    "title": "Snowflake: SnowPro Core",
    "section": "4.9. Unstructured Data",
    "text": "4.9. Unstructured Data\nUnstructured data is any data which does not fit in a pre-defined data model. E.g. video files, audio files, documents.\nSnowflake handles unstructured data using file URLs.\nSnowflake supports the following for both internal and external stages:\n\nAccess through URL in cloud storage\nShare file access URLs\n\n\n4.9.1. File URLs\nThere are three types of URL we can share:\n\n\n\n\n\n\n\n\n\nURL Type\nUse Case\nExpiry\nCommand to Return URL\n\n\n\n\nScoped URL\nEncoded URL with temporary access to a file but not the stage.\nExpires when results cache expires (currently 24 hours)\nBUILD_SCOPED_FILE_URL\n\n\nFile URL\nPermits prolonged access to a specified file.\nDoes not expire.\nBUILD_STAGE_FILE_URL\n\n\nPre-signed URL\nHTTPS URL used to access a file via a web browser.\nConfigurable expiration time for access token.\nGET_PRESIGNED_URL\n\n\n\nFor example, run the following SQL file function to create a scope URL:\nSELECT BUILD_SCOPED_FILE_URL(@stage_azure, 'Logo.png')\nFor a pre-signed URL, we also set the expiry time in seconds.\nSELECT GET_PRESIGNED_URL(@stage_azure, 'Logo.png', 60)\n\n\n4.9.2. Directory Tables\nA directory table stores metadata of staged files.\nThis is layered on a stage, rather than being a separate table object in a database.\nIt can be queried, with sufficient privileges on the stage, to retrieve file URLs to access files on the stage.\nIt needs to be enabled as it is not by default. We either do this in the CREATE STAGE command with\nCREATE STAGE stage_azure\nURL = &lt;'url'&gt;\nSTORAGE_INTEGRATION = integration\nDIRECTORY = (ENABLE = TRUE)\nor ALTER an existing stage:\nALTER STAGE stage_azure\nSET DIRECTORY = (ENABLE = TRUE)\nWe query this directory with:\nSELECT * FROM DIRECTORY(@stage_azure)\nThe first time querying, so will not see any results. The directory needs to be manually populated with:\nALTER STAGE stage_azure REFRESH\nWe may want to use the cloud provider’s event notification to set up an automatic refresh."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#data-sampling",
    "href": "posts/software/snowflake/snowpro_notes.html#data-sampling",
    "title": "Snowflake: SnowPro Core",
    "section": "4.10. Data Sampling",
    "text": "4.10. Data Sampling\nWhen developing views and queries against large databases, it may take a lot of time/compute to query against the entire database, making it slow to iterate. So we often want to work with a smaller subset of the data.\nUse cases:\n\nQuery development\nData analysis, estimates\n\nThere are two sampling methods:\n\nThe ROW / BERNOULLI method (both keywords give identical results). The following command will sample 10% of rows. We can optionally add a SEED(69) argument to get reproducible results.\n\nSELECT * FROM table\nSAMPLE ROW(10)\nSEED(69)\n\nThe BLOCK / SYSTEM method\n\nSELECT * FROM table\nSAMPLE BLOCK(10)\nSEED(69)\nComparison of row vs block sampling:\n\n\n\n\n\n\n\nROW\nBLOCK\n\n\n\n\nEvery row has probability \\(p\\) of being chosen\nEvery block (i.e. micro partition in block storage) has probability \\(p\\) of being chosen\n\n\nMore randomness\nMore efficient processing\n\n\nBetter for smaller tables\nBetter for larger tables"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#tasks",
    "href": "posts/software/snowflake/snowpro_notes.html#tasks",
    "title": "Snowflake: SnowPro Core",
    "section": "4.11. Tasks",
    "text": "4.11. Tasks\nTasks are used to schedule the execution of SQL statements or stored procedures. The are schema-level objects, so can be cloned.\nThey are often combined with streams to set up continuous ETL workflows.\nCREATE TASK my_task\n    WAREHOUSE = my_wh\n    SCHEDULE = '15 MINUTE'\n    AS\n    INSERT INTO my_table(time_col) VALUES (CURRENT_TIMESTAMP);\nIf we omit the WAREHOUSE argument, the task will run using Snowflake-managed compute.\nThe task is run using the privileges of the task owner.\nTo create a new task we need the following privileges: EXECUTE MANAGED TASK on account, CREATE TASK on schema and USAGE on warehouse.\nTo start/stop a task, we use the following commands. We always need to RESUME a task the first time we use it after creating it.\nALTER TASK my_task RESUME;\nALTER TASK my_task SUSPEND;\nAs well as setting up individual tasks, we can set up a DAG of interconnected tasks. We specify the dependencies using the AFTER keyword. DAGs are limited to 1000 tasks in total and 100 child tasks for a single node.\nCREATE TASK my_task_b\n    WAREHOUSE = my_wh\n    AFTER mytask_a\n    AS\n    ..."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#streams",
    "href": "posts/software/snowflake/snowpro_notes.html#streams",
    "title": "Snowflake: SnowPro Core",
    "section": "4.12. Streams",
    "text": "4.12. Streams\nA stream is an object which records Data Manipulation Language (DML) changes made to a table. This is change data capture. It is a schema-level object and can be cloned. The stream will be cloned when a database is cloned.\nIf a stream is set up on a table, the stream will record any inserts, deletes, updates to the table. We can then query the stream to see what has changed.\nTo create a stream:\nCREATE STREAM my_stream\nON TABLE my_table;\nWe can query from the stream. This will return any changed rows along with three metadata columns: METADATA$ACTION, METADATA$ISUPDATE, METADATA$ROW_ID.\nSELECT * FROM my_stream;\nConsuming a stream means we query its contents and then empty it. This is a typical use case in ETL work flows where we want to monitor for deltas and update another table. We do this by inserting into a target table from the stream:\nINSERT INTO target_table\n    SELECT col1, col2\n    FROM my_stream;\nThree types of stream:\n\nStandard: Insert, update, delete.\nAppend-only: Insert. Does not apply to external tables, only standard tables, directory tables, views.\nInsert-only: Insert. Only applies to external tables.\n\nA stream becomes stale when the offset is outside the data retention period of the table, i.e. unconsumed records in the stream are lost.\nThis determines how frequently a stream should be consumed. The STALE_AFTER column of DESCRIBE STREAM or SHOW STREAMS indicated when the stream is predicted to go stale.\nA stream extends the data retention period of the source table to 14 days by default. This is true for all snowflake editions. This is the MAX_DATA_EXTENION_TIME_IN_DAYS variable which defaults to 14 and can be increased to 90.\nTo trigger a task when a stream has data using WHEN SYSTEM$STREAM_HAS_DATA. For example:\nCREATE TASK my_task\n    WAREHOUSE = my_wh\n    SCHEDULE = '15 MINUTE'\n    WHEN SYSTEM$STREAM_HAS_DATA('my_stream')\n    AS\n    INSERT INTO my_table(time_col) VALUES (CURRENT_TIMESTAMP);"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#connectors-and-drivers",
    "href": "posts/software/snowflake/snowpro_notes.html#connectors-and-drivers",
    "title": "Snowflake: SnowPro Core",
    "section": "5.1. Connectors and Drivers",
    "text": "5.1. Connectors and Drivers\nSnowflake provides two interfaces:\n\nSnowsight web UI\nSnowSQL command line tool\n\nDrivers:\n\nGo\nJDBC\n.NET\nnode.js\nODBC\nPHP\nPython\n\nConnectors:\n\nPython\nKafka\nSpark\n\nPartner Connect allows us to create a trial account for third-party add-ons to Snowflake and integrate them with Snowflake.\nThey span many different categories and tools - BI, CI/CD, etc."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#snowflake-scripting",
    "href": "posts/software/snowflake/snowpro_notes.html#snowflake-scripting",
    "title": "Snowflake: SnowPro Core",
    "section": "5.2. Snowflake Scripting",
    "text": "5.2. Snowflake Scripting\nMostly used in stored procedures but can also be used for writing procedural code outside of this.\nWe can use@ if, case, for, repeat, while, loop\nThis is written in a “block”:\nDECLARE\nBEGIN \nEXCEPTION\nEND\nDECLARE and EXCEPTION are optional.\nThis is available in Snowsight. The classic UI or SnowSQL requires $$ around the block.\nObjects created in the block are available outside of it too. Variables created in the block can only be used inside it.\nThis is a similar syntax, but different from transactions which use BEGIN and END."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#snowpark",
    "href": "posts/software/snowflake/snowpro_notes.html#snowpark",
    "title": "Snowflake: SnowPro Core",
    "section": "5.3. Snowpark",
    "text": "5.3. Snowpark\nSnowpark API provides support for three programming languages: Python, Java, Scala.\nPython code converts to SQL which then queries Snowflake with serverless Snowflake engine.\nThis means there is no need to move data outside of Snowflake.\nBenefits:\n\nLazy evaluation\nPushdown - query is executed in Snowflake rather than unloading all data outside of Snowflake and then transforming\nUDFs can be defined inline"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#time-travel",
    "href": "posts/software/snowflake/snowpro_notes.html#time-travel",
    "title": "Snowflake: SnowPro Core",
    "section": "6.1. Time Travel",
    "text": "6.1. Time Travel\nWhat if someone drops a database or table accidentally? We need a backup to recover data. Time travel enables accessing historical data.\nWe can:\n\nQuery data that has been deleted or updated\nRestore dropped tables, schemas and databases\nCreate clones of tables, schemas and databases from a previous date\n\nWe can use a SQL query to access time travel data within a retention period. The AT keyword allows us to time travel.\nSELECT * \nFROM table\nAT (TIMESTAMP &gt;= timestamp)\nOr use OFFSET in seconds. So for 10 minutes:\nSELECT * \nFROM table \nAT (OFFSET &gt;= 10*60)\nAlternatively we can use BEFORE, where query_ID is the ID where we messed things up:\nSELECT * \nFROM table \nBEFORE (STATEMENT&gt;= query_id)\nGo to Activity -&gt; Query History in the Snowsight UI to get the correct query ID.\nIt is best practice to recover to an intermediate backup table first to confirm the recovered version is correct. E.g.\nCREATE OR REPLACE TABLE table_name_backup\nFROM table\nAT (TIMESTAMP &gt;= timestamp)"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#undrop",
    "href": "posts/software/snowflake/snowpro_notes.html#undrop",
    "title": "Snowflake: SnowPro Core",
    "section": "6.2. UNDROP",
    "text": "6.2. UNDROP\nWe can use the UNDROP keyword to recover objects.\nUNDROP TABLE table_name\nSame for SCHEMA or DATABASE.\nConsiderations.\n\nUNDROP fails if an object with the same name already exists.\nWe need ownership privileges to UNDROP an object.\n\nWhen working with time zones, it can be helpful to change the time zone to match your local time. This only affects the current session, not the whole account.\nALTER SESSION SET TIMEZONE = 'UTC'"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#retention-period",
    "href": "posts/software/snowflake/snowpro_notes.html#retention-period",
    "title": "Snowflake: SnowPro Core",
    "section": "6.3. Retention Period",
    "text": "6.3. Retention Period\nRetention period is the number of days for which historical data is preserved. This determines how far back we can time travel.\nThe retention period is configurable for table, schema, database or account.\nThe default DATA_RETENTION_TIME_IN_DAYS=1. If we want to disable time travel, we can set this to 0.\nWe can set this when we create a table, or alter it for an existing table:\nALTER TABLE table_name (\n    SET DATA_RETENTION_TIME_IN_DAYS=2\n)\nWe can set a minimum value at the account level:\nALTER ACCOUNT SET \nMIN_DATA_RETENTION_TIME_IN_DAYS=3\nThe account type determines the maximum retention period:\n\nStandard: up to 1 day\nEnterprise or higher: up to 90 days"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#fail-safe",
    "href": "posts/software/snowflake/snowpro_notes.html#fail-safe",
    "title": "Snowflake: SnowPro Core",
    "section": "6.4. Fail Safe",
    "text": "6.4. Fail Safe\nThis is for disaster recovery beyond time travel. This is not configurable and set to 7 days beyond time travel period for permanent tables.\nWe as users cannot access this data. We have to contact Snowflake support to restore it for us."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#storage-costs",
    "href": "posts/software/snowflake/snowpro_notes.html#storage-costs",
    "title": "Snowflake: SnowPro Core",
    "section": "6.5. Storage Costs",
    "text": "6.5. Storage Costs\nTime travel and fail safe contribute to storage costs. You only pay for what is modified.\nYou can see the breakdown in the Admin -&gt; Usage tab of the UI.\nYou can see how much storage is being used with:\nSELECT * \nFROM SNOWFLAKE_ACCOUNT_USAGE.TABLE_STORAGE_METRICS"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#table-types",
    "href": "posts/software/snowflake/snowpro_notes.html#table-types",
    "title": "Snowflake: SnowPro Core",
    "section": "6.6. Table Types",
    "text": "6.6. Table Types\nThe following table summarises the differences between the 3 types of table.\n\n\n\n\n\n\n\n\n\n\nTable Type\nCommand\nTime Travel Retention Period\nFail Safe\nUse Case\n\n\n\n\nPermanent\nCREATE TABLE\n0-90\nY\nStandard tables. Persist data until dropped.\n\n\nTransient\nCREATE TRANSIENT TABLE\n0-1\nN\nFor large tables that do not need to be protected. Persist data until dropped.\n\n\nTemporary\nCREATE TEMPORARY TABLE\n0-1\nN\nNon-permanent data just for this session.Only in session - data is deleted after the worksheet is closed.\n\n\n\nThese types are also available for other Snowflake objects: tables, stages, schema, database.\nIf a database is transient, so are all of its objects.\nTemporary table names do not clash with permanent or transient tables. The temporary table name takes precedence in the session and hides the others with the same name.\nIt is not possible to change the type of an existing object."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#zero-copy-cloning",
    "href": "posts/software/snowflake/snowpro_notes.html#zero-copy-cloning",
    "title": "Snowflake: SnowPro Core",
    "section": "7.1. Zero Copy Cloning",
    "text": "7.1. Zero Copy Cloning\nZero copy cloning makes it easy to copy an existing object in a storage-efficient way.\nFor example:\nCREATE TABLE table_name\nCLONE table_source\nWe can clone almost* any object: database, schema, table, stream, file format, sequence, task, stage, pipe.\n\n*the exceptions are:\n\npipes which can only be cloned if referencing an external stage\nstages which cannot be cloned for named internal stages\n\n\nWhen we clone a database or schema, all of its child objects are also cloned.\n\n\n\nZero Copy Clone\n\n\nIt’s called a “zero copy clone” because it does not actually copy the data at clone time. It is a metadata operation occurring in the cloud service layer.\nThe “copy” is a snapshot of the “original”. Both reference the same underlying micro-partitions in block storage. Think of it like pass-by-reference rather than pass-by-value.\nWhen modifying the “copy” Snowflake only stores the deltas, not the entire database again. The “original” and “copy” can be modified independently without affecting the other.\nA typical use case is to create backups for dev work.\nWe can clone from a time travel version of a table:\nCREATE TABLE table_new\nCLONE table_source\nBEFORE (TIMESTAMP &gt;= ‘2025-01-31 08:00:00’)"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#privileges",
    "href": "posts/software/snowflake/snowpro_notes.html#privileges",
    "title": "Snowflake: SnowPro Core",
    "section": "7.2. Privileges",
    "text": "7.2. Privileges\nThe privileges of the CHILD objects are inherited from the clone source if it is a container like a table or schema. But the privileges of the cloned object itself are NOT inherited. They need to be specified separately by the administrator.\nThe privileges required to clone an object depends on the object:\n\nTable: SELECT privileges\nPipe, stream, task: OWNER privileges\nAll other objects: USAGE privileges\n\nWhen we clone a table, its load history metadata is NOT copied. This means loaded data can be loaded again without causing a metadata clash."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#data-sharing",
    "href": "posts/software/snowflake/snowpro_notes.html#data-sharing",
    "title": "Snowflake: SnowPro Core",
    "section": "7.3. Data Sharing",
    "text": "7.3. Data Sharing\n\n7.3.1. What is a Data Share?\nData sharing can usually be quite complicated; managing multiple extracts of a common database and ensuring they are in sync.\nSnowflake separates storage vs compute. This allows us to share data without actually making a copy of it. As with zero copy cloning, this is a metadata operation, so uses the cloud service layer.\nWe grant access to the storage, and the customer provides their own compute resources.\nThis is available for all Snowflake pricing tiers.\nIn a one-way data share, Account 1 is the provider and Account 2 is the Consumer. Account 2 has a read-only view of the shared data. The provider is billed for the storage, whereas the consumer is billed for the compute costs of their queries.\n\n\n\nData Sharing One Way\n\n\nAn account can be simultaneously both a provider and consumer of data. You can even do this within the same account.\n\n\n\nData Sharing Two Way\n\n\nA share contains a single database. We can add multiple accounts to a share.\n\n\n7.3.2. How to Set Up a Data Share\nTo set up a data share we need to:\n\nCreate the share - requires ACCOUNTADMIN role or CREATE SHARE privileges.\n\nCREATE SHARE my_share;\n\nGrant privileges to the share\n\nGRANT USAGE ON DATABASE my_db TO_SHARE my_share;\nGRANT USAGE ON SCHEMA my_schema.my_db TO_SHARE my_share;\nGRANT SELECT ON TABLE my_table.my_schema.my_db TO_SHARE my_share;\n\nAdd consumer accounts\n\nALTER SHARE my_share ADD ACCOUNT = accountid123;\n\nThe consumer imports the share - requires ACCOUNTADMIN role, or IMPORE SHARE & CREATE TABLE privileges\n\nCREATE DATABASE new_db FROM SHARE my_share;\n\n\n7.3.3. What Can Be Shared\nWe can share: tables, external tables, secure views, secure materialised views, secure UDFs.\nThe share itself is a container containing one database, against which we can grant usage on scehma(s) and grant privileges to object(s). We can have one or more accounts that we grant access to.\nIt is a best practice to create secure views to avoid revealing unintended data. With an unsecured view, the user can see the command used to create the table, which could reveal the internal table used to create the view, columns not exposed to the user, aliases, etc.\nSnowflake will raise an error if trying to grant privileges to a standard view; they cannot be shared.\nIf sharing data with a lower tier Snowflake account, by default it won’t let you add a consumer account. We can override this by specifying SHARE_RESTRICTIONS=false.\nALTER SHARE my_share \nADD ACCOUNT accountid123\nSHARE_RESTRICTIONS = false;"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#database-replication",
    "href": "posts/software/snowflake/snowpro_notes.html#database-replication",
    "title": "Snowflake: SnowPro Core",
    "section": "7.4. Database Replication",
    "text": "7.4. Database Replication\nData shares are only possible within the same region and the same cloud provider.\nDatabase replication allows us to share data across different regions or on different cloud providers. It replicates a database between accounts within the same organisation. Also called “cross-region sharing”. This is available at all price tiers.\nThis incurs data transfer costs because it does actually copy the data across to a different location. The data and objects therefore need to be synchronised periodically.\nThe provider is referred to as the primary database and the consumer is the secondary database or a read-only replica.\nTo set up database replication we need to:\n\nEnable this at the account level with the ORGADMIN role.\n\nSELECT system$global_account_set_parameter(org_name.account_name), 'ENABLE_ACCOUNT_DATABASSE_REPLICATION', 'true');\n\nPromote a local database to primary database with ACCOUNTADMIN role.\n\nALTER DATABASE my_db ENABLE REPLICATION TO ACCOUNTS myorg.account2, myorg.account3;\n\nCreate a replica in the consumer account.\n\nCREATE DATABASE my_db AS REPLICA OF myorg.account1.my_db\n\nRefresh the database periodically. Ownership privileges are needed. We may want to set up a task to run this periodically.\n\nALTER DATABASE my_db REFRESH;"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#access-control",
    "href": "posts/software/snowflake/snowpro_notes.html#access-control",
    "title": "Snowflake: SnowPro Core",
    "section": "8.1. Access Control",
    "text": "8.1. Access Control\nThere are two approaches to access control:\n\nDiscretionary Access Control (DAC)\n\nEach object has an owner. That owner can grant access to the object.\n\nRole-based Access Control (RBAC)\n\nPrivileges are assigned to objects. Those privileges can be granted to roles. These roles can be assigned to users.\n\n\nWe do not assign privileges directly to users.\n\n\n\n\n\nflowchart LR\n\nA(Object) --&gt; B(Privilege) --&gt; C(Role) --&gt; D(User)\n\n\n\n\n\n\nKey concepts:\n\nSecurable object: An object that access can be granted to. Access is denied unless explicitly granted.\nPrivilege: A defined level of access.\nRole: The entity to which privileges are granted. Roles can be assigned to users or other roles. We can create a hierarchy of roles.\nUser: The identity associated with a person or program.\n\nWe can GRANT privileges to a role:\nGRANT my_privilege\nON my_object \nTO my_role\nAnd GRANT a role to a user.\nGRANT my_role\nTO user_id\nWe can also grant access to all tables in a schema:\nGRANT SELECT\nON ALL TABLES IN SCHEMA MARKETING_SALES\nTO ROLE MARKETING_ADMIN\nAnd also all future tables too:\nGRANT SELECT\nON FUTURE TABLES IN SCHEMA MARKETING_SALES\nTO ROLE MARKETING_ADMIN\nWe can REVOKE privileges in the same way:\nREVOKE privilege\nON object\nFROM role"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#roles",
    "href": "posts/software/snowflake/snowpro_notes.html#roles",
    "title": "Snowflake: SnowPro Core",
    "section": "8.2. Roles",
    "text": "8.2. Roles\nA role is an entity that we assign privileges to.\nRoles are then assigned to users. Multiple roles can be assigned to each user.\nWe need to grant access to all parent objects. So if we want to grant SELECT access to a table, we also need to grant USAGE access to the parent schema and database.\nEvery object is owned by one single role. Ownership can be transferred.\nThe SHOW ROLES command shows all available roles.\nThere is a “current role” used in every session. USE ROLE can be used in Snowflake worksheets or tasks.\n\n8.2.1. System-Defined Roles\nThese roles can’t be dropped. Additional privileges can be added to the roles, but not revoked.\nThe roles inherit from parent roles in the hierarchy.\nIt is best practice for custom roles to inherit from the SYSADMIN role.\n\nORGADMIN\n\nManage actions at the organisation level.\nCreate and view accounts.\n\nACCOUNTADMIN\n\nTop-level in hierarchy.\nShould only be grant to a limited number of users as it is the most powerful. It can manage all objects in the account.\nWe can create reader accounts and shares. Modify account level parameters including billing and resource monitors.\nContains SECURITYADMIN AND SYSADMIN roles.\n\nSECURITYADMIN\n\nManage any object grants globally.\nMANAGE GRANTS privilege.\nCreate, monitor and manage users and roles.\nInherits from USERADMIN. The difference with USERADMIN is SECURITYADMIN can manage grants GLOBALLY.\n\nSYSADMIN\n\nManages objects - it can create warehouses, databases and other objects.\nAll custom roles should be assigned to SYSADMIN so that SYSADMIN always remains a superset of all other roles and can manage any object by them.\n\nUSERADMIN\n\nFor user and role management.\nCREATE USER and CREATE ROLE privileges.\n\nPUBLIC\n\nAutomatically granted by default.\nGranted when no access control is needed. Objects can be owned but are available to everyone.\n\nCUSTOM\n\nCan be created by USERADMIN or higher.\nCREATE ROLE privilege. Should be assigned to SYSADMIN so that they can still manage all objects created by the custom role. Custom roles can be created by the database owner.\n\n\n\n\n8.2.2. Hierarchy of Roles\nThere is a hierarchy of roles.\n\n\n\n\n\nflowchart BT\n\nF(PUBLIC) --&gt; E(USERADMIN) --&gt; D(SECURITYADMIN) --&gt; A(ACCOUNTADMIN)\nC1(Custom Role 1) --&gt; B(SYSADMIN) --&gt; A(ACCOUNTADMIN) \nC3(Custom Role 3) --&gt; C2(Custom Role 2) --&gt; B(SYSADMIN)\n\n\n\n\n\n\nThis is similar to the hierarchy of objects that we’ve seen before in Section 2.10."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#privileges-1",
    "href": "posts/software/snowflake/snowpro_notes.html#privileges-1",
    "title": "Snowflake: SnowPro Core",
    "section": "8.3. Privileges",
    "text": "8.3. Privileges\nPrivileges define granular level of access to an object.\nWe can GRANT and REVOKE privileges. The owner of an object can manage its privileges. The SECURITYADMIN role has the global MANAGE GRANTS privilege, so can manage privileges on any object.\nWe can see all privileges in the Snowflake docs.\nSome of the important ones below.\nGlobal privileges:\n\nCREATE SHARE\nIMPORT SHARE\nAPPLY MASKING POLICY\n\nVirtual warehouse:\n\nMODIFY - E.g. resizing\nMONITOR - View executed queries\nOPERATE - Suspend or resume\nUSAGE - Use the warehouse to execute queries\nOWNERSHIP - Full control over the warehouse. Only one role can be an owner.\nALL - All privileges except ownership\n\nDatabases:\n\nMODIFY - ALTER properties of the database\nMONITOR - Use the DESCRIBE command\nUSAGE - Query the database and execute SHOW DATABASES command\nREFERENCE_USAGE - Use an object (usually secured view) to reference another object in a different database\nOWNERSHIP - Full control\nALL - Everything except ownership\nCREATE SCHEMA\n\nStages:\n\nREAD - Only for internal stages. Use GET, LIST, COPY INTO commands\nWRITE - Only for internal stages. Use PUT, REMOVE, COPY INTO commands\nUSAGE - Only for external stages. Equivalent of read and write.\nALL\nOWNERSHIP\n\nTables:\n\nSELECT\nINSERT\nUPDATE\nDELETE\nTRUNCATED\nDELETE\nALL\nOWNERSHIP\n\nWe can see the grants of a role with:\nSHOW GRANTS TO ROLE MARKETING_ADMIN\nWe can see which users are assigned to a role with\nSHOW GRANTS OF ROLE MARKETING_ADMIN"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#authentication",
    "href": "posts/software/snowflake/snowpro_notes.html#authentication",
    "title": "Snowflake: SnowPro Core",
    "section": "8.4. Authentication",
    "text": "8.4. Authentication\nAuthentication is proving that you are who you say you are.\nSnowflake uses Multi-Factor Authentication (MFA). This is powered by Duo, managed by Snowflake. MFA is available for all Snowflake editions and is supported by all Snowflake interfaces: web UI, SnowSQL, ODBC and JDBC, Python connectors.\nEnabled by default for accounts but requires users to enroll. Strongly recommended to use MFA for ACCOUNTADMIN.\nThe SECURITYADMIN or ACCOUNTADMIN roles can disable MFA for a user.\nWe can enable MFA token caching to reduce the number of prompts. This needs to be enabled; it isn’t by default. It makes the MFA token valid for 4 hours. Available for ODBC driver, JDBC driver and Python connector."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#sso",
    "href": "posts/software/snowflake/snowpro_notes.html#sso",
    "title": "Snowflake: SnowPro Core",
    "section": "8.5. SSO",
    "text": "8.5. SSO\nFederated authentication enables users to login via Single Sign-On (SSO).\nThe federated environment has two components:\n\nService provider: Snowflake\nExternal identity provider: Maintains credentials and authenticates users. Native support for Okta and Microsoft AD FS. Most SAML 2.0 compliant vendors are supported.\n\nWorkflow for Snowflake-initiated login:\n\nUser navigates to web UI\nChoose login via configured Identity Provider (IdP)\nAuthenticate via IdP credentials\nIdP sends a SAML response to Snowflake\nSnowflake opens a new session\n\nWorkflow for IdP-initiated login:\n\nUser navigates to IdP\nAuthenticate via IdP credentials\nSelect Snowflake as an application\nIdP sends a SAML response to Snowflake\nSnowflake opens a new session\n\nIdP has SCIM support. This is an open standard for automating user provisioning. We can create a user in the IdP and this provisions the user in Snowflake."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#key-pair-authentication",
    "href": "posts/software/snowflake/snowpro_notes.html#key-pair-authentication",
    "title": "Snowflake: SnowPro Core",
    "section": "8.6. Key-Pair Authentication",
    "text": "8.6. Key-Pair Authentication\nEnhanced security as an alternative to basic username and password.\nUser has a public key and a private key. This is the key-pair. Minimum key size is 2048-bit RSA key-pair.\nSetting this up:\n\nGenerate private key\nGenerate public key\nStore keys locally\nAssign public key to user. See command below.\nConfigure client to use key-pair authentication\n\nWe can set the public key for a user with:\nALTER USER my_user SET\nRSA_PUBLIC_KEY 'Fuak_shakfjsb';"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#column-level-security",
    "href": "posts/software/snowflake/snowpro_notes.html#column-level-security",
    "title": "Snowflake: SnowPro Core",
    "section": "8.7. Column-level Security",
    "text": "8.7. Column-level Security\nColumn-level security masks data in tables and views enforced on columns.\nThis is an enterprise edition feature.\n\n8.7.1. Dynamic Data Masking\nWith “dynamic data masking” the unmasked data is stored in the database, then depending on the role querying the data it is masked at runtime.\nWe can define a masking policy with:\nCREATE MASKING POLICY my_policy\nAS (col1 varchar) RETURNS varchar -&gt;\n    CASE\n    WHEN CURRENT ROLE IN (role_name)\n    THEN col1\n    ELSE “##-##”\n    END;\nWe apply a masking policy with:\nALTER TABLE my_table MODIFY COLUMN phone;\nWe can remove the policy with:\nUNSET MASKING POLICY my_policy;"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#external-tokenisation",
    "href": "posts/software/snowflake/snowpro_notes.html#external-tokenisation",
    "title": "Snowflake: SnowPro Core",
    "section": "8.8. External Tokenisation",
    "text": "8.8. External Tokenisation\nData is tokenised.\nThe benefit is the analytical value is preserved; we can still group the data and draw conclusions, it’s just anonymised. Sensitive data is protected.\nThe tokenisation happens in a pre-load step and the detokenisation happens at query runtime."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#row-level-security",
    "href": "posts/software/snowflake/snowpro_notes.html#row-level-security",
    "title": "Snowflake: SnowPro Core",
    "section": "8.9. Row-level Security",
    "text": "8.9. Row-level Security\nRow access policies allow us to determine which rows are visible to which users.\nThis is only available in the enterprise edition.\nRows are filtered at runtime on a given condition based on user or role.\nDefine the policy with:\nCREATE ROW ACCESS POLICY my_policy\nAS (col1 varchar) returns Boolean -&gt;\n    CASE\n    WHEN current_role() = 'EXAMPLE_ROLE'\n    AND col1 = 'value1' THEN true\n    ELSE false\n    END;\nApply the policy with:\nALTER TABLE my_table ADD ROW POLICY my_policy\nON (col1);"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#network-policies",
    "href": "posts/software/snowflake/snowpro_notes.html#network-policies",
    "title": "Snowflake: SnowPro Core",
    "section": "8.10. Network Policies",
    "text": "8.10. Network Policies\nNetwork policies allow us to restrict access to accounts based on the user’s IP address. We can also specify ranges of addresses.\nThis is available in all Snowflake editions.\nWe can either whitelist or blacklist IP addresses. If an address is in both, the blacklist takes precedence and the IP address is blocked.\nTo create a network policy we need the SECURITYADMIN role since this has the global CREATE NETWORK POLICY privilege.\nCREATE NETWORK POLICY my_network_policy\nALLOWED_IP_LIST = ('192.168.1.95', '192.168.1.113'),\nBLOCKED_IP_LIST = ('192.168.1.95')  -- Blacklist takes precedence\nTo apply this to an account, we again need the SECURITYADMIN role. This is a bit of an exception because this is a change to the account but we don’t need to be ACCOUNTADMIN.\nALTER ACCOUNT SET NETWORK_POLICY;\nWe can also alter a USER instead of an ACCOUNT if we have ownership of the user and the network policy."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#data-encryption",
    "href": "posts/software/snowflake/snowpro_notes.html#data-encryption",
    "title": "Snowflake: SnowPro Core",
    "section": "8.11. Data Encryption",
    "text": "8.11. Data Encryption\nAll data is encrypted at rest and in transit.\nThis is in all editions and happens automatically by default.\n\n8.11.1. Encryption at Rest\nRelevant to data in tables and internal stages. AES 256-bit encryption managed by Snowflake.\nNew keys are generated for new data every 30 days. Data generated with the old key will still use that old key. Old keys are preserved as long as there is still data that uses them; if not they are destroyed.\nWe can manually enable a feature to re-key all data every year. This is in the enterprise edition.\n\n\n8.11.2. Data in Transit\nThis is relevant for all Snowflake interfaces: WebUI, SnowSQL, JDBc, ODBC, Python Connector. TLS 1.2 end-to-end encryption.\n\n\n8.11.3. Other Encryption Features\nWe can additionally enable client-side encryption in external stages.\n“Tri-secret secure” enables customers to use their own keys. It is a business-critical edition feature. There is a composite master key which combines this customer-managed key with a Snowflake-managed key."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#account-usage-and-information-schema",
    "href": "posts/software/snowflake/snowpro_notes.html#account-usage-and-information-schema",
    "title": "Snowflake: SnowPro Core",
    "section": "8.12. Account Usage and Information Schema",
    "text": "8.12. Account Usage and Information Schema\nThese are functions we can use to query object metadata and historical data usage.\nThe ACCOUNT_USAGE schema is available in the SNOWFLAKE database, which is a shared database available to all accounts. The ACCOUNTADMIN can see everything. There are object metadata views like COLUMNS, and historical usage data views like COPY_HISTORY. The data is not real-time; there is a lag of 45 mins to 3 hours depending on the view. Retention period is 365 days. This does include dropped objects so can include a DELETED column.\nREADER_ACCOUNT_USAGE lets us query object metadata and historical usage data for reader accounts. It is more limited.\nThe INFORMATION_SCHEMA is a default schema available in all databases. This is read-only data about the parent database and account-level information. The output depends on which privileges you have. There is a lot of data returned so it can return messages saying to filter the query down. Shorter retention period: 7 days to 6 months. This is real-time, there is no delay. Does not include dropped objects."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#release-process",
    "href": "posts/software/snowflake/snowpro_notes.html#release-process",
    "title": "Snowflake: SnowPro Core",
    "section": "8.13. Release Process",
    "text": "8.13. Release Process\nReleases are weekly with no downtime.\n\nFull releases: New features, enhancements, fixes, behaviour changes\n\nBehaviour changes are monthly (i.e. every 4 weekly releases)\n\nPatch releases: Bugfixes\n\nSnowflake operate a three stage release approach:\n\nDay 1: early access for enterprise accounts who request this\nDay 1/2: regular access for standard accounts\nDay 2: final access for enterprise or higher accounts"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#query-profile",
    "href": "posts/software/snowflake/snowpro_notes.html#query-profile",
    "title": "Snowflake: SnowPro Core",
    "section": "9.1. Query Profile",
    "text": "9.1. Query Profile\nThis is a graphical representation of the query execution. This helps us understand the components of the query and optimise its performance.\n\n9.1.1. Accessing Query History\nThere are three ways to view the query history:\n\nIn Snowsight. This is in the Activity -&gt; Query History tab, or the Query tab in a worksheet for the most recent query.\nInformation schema\n\nSELECT * FROM TABLE(INFORMATION_SCHEMA.QUERY_HISTORY())\nORDER BY START_TIME;\n\nAccount usage schema\n\nSELECT * FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY;\n\n\n9.1.2. The Graphical Representation of a Query\nThe “Operator Tree” is a graphical representation of the “Operator Types”, which are components of query processing. We can also see the data flow, i.e. the number of records processed. The is also a percentage on each node indicating the percentage of overall execution time.\nThe “Profile Overview” tab gives details of the total time taken. The “Statistics” tab shows number of bytes scanned, percentage from cache, number of paritions scan.\n\n\n9.1.3. Data Spilling\nIt also shows “data spilling”, when the data does not fit in memory and is therefore written to the local storage disk. This is called “spilling to disk”. If the local storage is filled, the data is further spilled to remote cloud storage. This is called “spilling to cloud”.\nTo reduce spilling, we can either run a smaller query or use a bigger warehouse."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#caching",
    "href": "posts/software/snowflake/snowpro_notes.html#caching",
    "title": "Snowflake: SnowPro Core",
    "section": "9.2. Caching",
    "text": "9.2. Caching\nThere are three mechanisms for caching:\n\nResult cache\nData cache\nMetadata cache\n\nIf the data is not in a cache, it is retrieved from storage, i.e. the remote disk.\n\n9.2.1. Result Cache\nStores the results of a query. This happens in the cloud service layer.\nThe result is very fast and avoids re-execution.\nThis can only happen if:\n\nThe table data and micro-partitions have not changed,\nThe query does not use UDFs or enternal functions\nSufficient privileges and results are still available (cache is valid for 24 hours)\n\nResult caching is enabled by default, but can be disabled using the USE_CACHED_RESULT parameter.\nThe result cache is purged after 24 hours. If the query is re-run, it resets the purge timer, up to a maximum of 31 days.\n\n\n9.2.2. Data Cache\nThe local SSD disk of the virtual warehouse. This cannot be shared with other warehouses.\nIt improves the performance of subsequent queries which use the same underlying data. We can therefore improve performance and costs by using the same warehouse for queries on similar data.\nThe data cache is purged if the warehouse is suspended or resized.\n\n\n9.2.3. Metadata cache\nStores statistics for tables and columns in the cloud services layer.\nThis is also known as the “metadata store”. The virtual private Snowflake edition allows for a dedicated metadata store.\nThe metadata cache stores results about the data, such as range of values in a micro-partition, count rows, count distinct values, max/min values. We can then query these without using the virtual warehouse.\nThis is used for functions like DESCRIBE and other system-defined functions."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#micro-partitions",
    "href": "posts/software/snowflake/snowpro_notes.html#micro-partitions",
    "title": "Snowflake: SnowPro Core",
    "section": "9.3. Micro Partitions",
    "text": "9.3. Micro Partitions\nThe data is cloud storage is stored in micro partitions.\nThese are chunks of data, usually about 50-500MB in uncompressed size (although they are compressed by Snowflake). The most efficient compression algorithm for each micro partition is used independently.\nThis is specific to Snowflake’s architecture.\nData is stored in a columnar form. We can improve read queries by only selecting required columns.\nMicro partitions are immutable - they cannot be changed once created. New data will be stored in new micro partitions, so the partitions depend on the order of insertion.\n\n9.3.1. Partition Pruning\nMicro partitions allow for very granular partition pruning. When we run a query on a table, we can eliminate unnecessary partitions.\nSnowflake stores metadata per micro partition. E.g. range of values, number of distinct values, and other properties for query optimisation.\nSnowflake optimises queries by using the metadata to determine if a micro partition needs to be read, so it can avoid reading all of the data.\n\n\n9.3.2. Clustering Keys\nClustering keys let us influence how the data is partitioned. This helps improve query performance by improving partition pruning. Grouping similar rows in micro-partitions also generally improves column compression.\nClustering a table on a specific column redistributes the data in the micro-partitions.\nSnowflake stored metadata about the micro-partitions in a table:\n\nNumber of micro-partitions\nOverlapping micro-partitions: Number of partitions with overlapping values\nClustering depth: Average depth of overlapping columns for a specific column\n\nA table with micro-partitions that are optimal are said to be in a “constant state”.\n\n\n9.3.3. Reclustering\nReclustering does not update immediately, it happens during periodic reclustering.\nOnce we define the clustering keys, Snowflake handles the automatic reclustering in the cloud services layer (serverless). This only adjusts the micro-partitions that would benefit from reclustering.\nNew partitions are created and the old partitions marked as deleted. They are not immediately deleted, they remain accessible for the time travel period, which incurs storage costs.\nThis incurs costs:\n\nServerless costs: Credit consumption of reclustering\nStorage costs: Old partitions are maintained for time travel\n\nClustering is therefore not appropriate for every table. It depends on the use case; does the usage (and improved query performance) justify the cost?\nThe columns which would benefit most from clustering:\n\nLarge number of micro-partitions\n\nIn the extreme case, if a table only had one micro-partition there would be no point pruning or reclustering.\n\nFrequently used in WHERE, JOIN, ORDER BY\nGoldilocks amount of cardinality\n\nIf there are only a small number of possible values, e.g. True/False, then pruning would not be very effective\nIf there are too many unique values, e.g. timestamps, then there would not be an efficient grouping of values within a micro-partition\n\n\n\n\n9.3.4. Defining Clustering Keys\nA cluster key can be added at any time.\nWhen clustering on multiple columns, it is best practice to cluster on the lowest cardinality first and go in order of increasing cardinality. In the example below, col1 has fewer unique values than col5.\nDefining a clustering key:\nALTER TABLE table_name \nCLUSTER BY (col1, col5);\nWe can use expressions in the clustering key. A common use case is converting timestamps to dates.\nALTER TABLE table_name \nCLUSTER BY (DATE(timestamp));\nWe can also define the clustering key at the point of creating the table.\nCREATE TABLE table_name\nCLUSTER BY (col1, col5);\nWe can remove clustering keys with:\nALTER TABLE table_name \nDROP CLUSTER KEY;\n\n\n9.3.5. Systems Functions for Clustering\nFor info on clustering on a table, we can run this command to get a json result.\nSYSTEM$CLUSTERING_INFORMATION('my_table', '(col1, col3)')\nThis contains info on: total_partition_count, total_constant_partition_count, average_overlaps, average_depth, partition_depth_histogram.\nThe notes key contains Snowflake info/warning messages, such as when a clustering key has high cardinality."
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#search-optimization-service",
    "href": "posts/software/snowflake/snowpro_notes.html#search-optimization-service",
    "title": "Snowflake: SnowPro Core",
    "section": "9.4. Search Optimization Service",
    "text": "9.4. Search Optimization Service\nThis can improve performance of lookup and analytical queries that use many predicates for filtering.\nIt does this by adding a search access path.\nThe queries that benefit most from this are:\n\nSelective point lookup: return very few rows\nEquality or IN predicates\nSubstring and regex searches\nSelective geospatial functions\n\nThis is in the enterprise edition. It is maintained by Snowflake once setup for a table. It runs in the cloud service layer, so incurs serverless costs and additional storage costs.\nWe can add this to a table with the following command. We need either OWNERSHIP privileges on the table or the ADD SEARCH OPTIMIZATION privilege on the schema.\nALTER TABLE mytable\nADD SEARCH OPTIMIZATION;\nAdding it to the entire table is equivalent to specifying ON EQUALITY(*). We can be more specific about the columns we optimise:\nALTER TABLE mytable\nADD SEARCH OPTIMIZATION ON GEO(mycol);\nWe can remove search optimization with\nALTER TABLE mytable\nDROP SEARCH OPTIMIZATION;"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#materialised-views",
    "href": "posts/software/snowflake/snowpro_notes.html#materialised-views",
    "title": "Snowflake: SnowPro Core",
    "section": "9.5. Materialised Views",
    "text": "9.5. Materialised Views\nThese can improve performance issues of views. This is useful for complex queries that are run frequently.\nAvailable in enterprise edition.\nWe would typically create a regular view for a frequently run query. If this is compute-intensive, we can create a materialised view which pre-computes the result and stores it in a physical table.\nThis differs to a regular Snowflake table because the underlying data may change. The materialised view is updated automatically by Snowflake using the cloud storage layer. This incurs serverless costs and additional storage costs.\nIt is best practice to start small with materialised views and incrementally use more. Resource monitors can’t control Snowflake-managed warehouses.\nWe can see info on materialized views using:\nSELECT * FROM TABLE(INFORMATION_SCHEMA.MATERIALIZED_VIEW_REFRESH_HISTORY());\nTo create a materialised view:\nCREATE MATERIALIZED VIEW V_1 AS\n    SELECT * FROM table1 WHERE c1=200;\nConstraints on materialised views:\n\nOnly query 1 table - no joins or self-joins\nCan’t query other views or materialized views\nNo window functions, UDFs or HAVING clauses\nSome aggregate functions aren’t allowed\n\nA typical use case is for queries on external tables which can be slow.\nWe can pause and resume materialized views. This will stop automatic updates to save credit consumption.\nALTER MATERIALIZED VIEW v_1 SUSPEND;\nALTER MATERIALIZED VIEW v_1 RESUME;\nWe can drop materialized views that we no longer need.\nDROP MATERIALIZED VIEW v_1;"
  },
  {
    "objectID": "posts/software/snowflake/snowpro_notes.html#warehouse-considerations",
    "href": "posts/software/snowflake/snowpro_notes.html#warehouse-considerations",
    "title": "Snowflake: SnowPro Core",
    "section": "9.6. Warehouse Considerations",
    "text": "9.6. Warehouse Considerations\nWarehouses can be altered to improve performance.\n\nResizing: Warehouses can be resized when queries are running, but the new warehouse size will only affect future queries. The warehouse can also be resized when the warehouse is suspended.\nScale up for more complex queries.\nScale out for more frequent queries. We may want to enable auto-scaling to automate this.\nDedicated warehouse: Isolate workload of specific users/team. Helpful to direct specific type of workload to a particular warehouse. Best practice to enable auto-suspend and auto-resume on dedicated warehouses."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html",
    "href": "posts/software/software_architecture/software_architect_notes.html",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "Notes from “The Complete Guide to Becoming a Software Architect” Udemy course\n\n\nA developer knows what can be done, an architect knows what should be done. How do we use technology to meet business requirements.\nGeneral system requirements:\n\nFast\nSecure\nReliable\nEasy to maintain\n\nArchitect’s need to know how to code for:\n\nArchitecture’s trustworthiness\nSupport developers\nRespect of developers\n\n\n\n\n\nUnderstand the business - Strengths, weaknesses, compettions, growth strategy.\nDefine the system’s goals - Goals are not requirements. Goals describe the effect on the organisation, requirements describe what the system should do.\nWork for your client’s clients - Prioritise the end user.\nTalk to the right people with the right language - What is the thing that really matters to the person I’m talking to?\n\nProject managers care about how things affect deadlines, developers care about the technologies used, CEOs care about business continuity and bottom line.\n\n\n\n\nUnderstand the system requirements - What the system should do.\nUnderstand the non-functional requirements - Technical and service level attributes, e.g. number of users, loads, volumes, performance.\nMap the components - Understand the system functionality and communicate this to your client. Completely non-technical at this point, no mention of specific technologies. A diagram of high level components helps.\nSelect the technology stack - Backend, frontend, data store.\nDesign the architecture\nWrite the architecture document\nSupport the team\n\nInclude developers in non-functional requirements and architecture design for two reasons:\n\nLearn about unknown scenarios early\nCreate ambassadors\n\n\n\n\nThe 2 types of requirements: functional and non-functional.\n\n\n“What the system should do”\n\nBusiness flows\nBusiness services\nUser interfaces\n\n\n\n\n“What the system should deal with”\n\nPerformance - latency and throughput\n\nLatency - How long does it take to perform a single task?\nThroughput - How many tasks can be performed in a given time unit?\n\nLoad - Quantity of work without crashing. Determines the availability of the system. Always look at peak (worst case) numbers.\nData volume - How much data will the system accumulate over time?\n\nData required on day one\nData growth (say, annually)\n\nConcurrent users - How many users will be using the system? This includes “dead times” which differentiates it from the load requirement. Rule of thumb is concurrent_users = load * 10\nSLA - Service Level Agreement for uptime.\n\nThe non-functional requirements are generally the more important in determining the architecture. The client will generally need guiding towards sensible values, otherwise they just want as much load as possible, as much uptime as possible etc.\n\n\n\n\nThe application type should be established early based on the use case and expected user interaction.\n\nWeb apps\n\nServe html pages.\nUI; user-initiated actions; large scale; short, focused actions.\nRequest-response model.\n\nWeb API\n\nServe data (often JSON).\nData retrieval and storage; client-initiated actions; large scale; short, focused actions.\nRequest-response model.\n\nMobile\n\nRequire user interaction; frontend for web API; location-based.\n\nConsole\n\nNo UI; limited interation; long-running processes; short actions for power users.\nRequire technical knowledge.\n\nService\n\nNo UI; managed by service manager; long-running processes.\n\nDesktop\n\nAll resources on the PC; UI; user-centric actions.\n\nFunction-as-a-service\n\nAWS lambda\n\n\n\n\n\nConsiderations:\n\nAppropriate for the task\nCommunity - e.g. stack overflow activity\nPopularity - google trends over 2 years\n\n\n\nCovers web app, web API, console, service.\nOptions: .NET, Java, node.js, PHP, Python\n\n\n\n\n\n\nFigure 1: Backend Tech\n\n\n\n\n\n\nCovers:\n\nWeb app - Angular, React\nMobile - Native (Swift, Java/Kotlin), Xamarin, React Native\nDesktop - depends on target OS\n\n\n\n\nSQL - small, structured data\n\nRelational tables\nTransactions\n\nAtomicity\nConsistency\nIsolation\nDurability\n\nQuerying language is universal\n\nNoSQL - huge, unstructured or semi-structured data\n\nEmphasis on scale and performance.\nSchema-less, with entities stored as JSON.\nEventual consistency - data can be temporarily inconsistent\nNo universal querying language\n\n\n\n\n\nQuality attributes that describe technical capabilities to fulfill the non-functional requirements. Non-functional requirements map to quality attributes, which are designed in the architecture.\n\nScalability - Adding computing resources without any interruption. Scale out (add more compute instances) is preferred over scale up (increase CPU, RAM on the existing instance). Scaling out introduces redundancy and is not limited by hardware.\nManageability - Know what’s going on and take action accordingly. The question “who reports the problems” determines if a system is manageable. The system should flag problems, not the suer.\nModularity - A system that is built from blcoks that can be changed or replaced without affecting the whole system.\nExtensibility - Functionality of the system can be extended without modifying existing code.\nTestability - How easy is it to test the application. Independent modules and methods and single responsibility principle aid testability.\n\n\n\n\nA software component is a piece of code that runs in a single process. Distributed systems are composed of independent software components deployed on separate processes/servers/containers.\nComponent architecture relates to the lower level details, whereas system architecture is the higher level view of how the components fit together to achieve the non-functional requirements.\n\n\nLayers represent horizontal functionality:\n\nUI/SI - user interface or service interface (i.e. an API), authentication\nBusiness logic - validation, enrichment, computation\nData access layer - connection handling, transaction handling, querying/saving data\n\nCode can flow downwards by one layer only. It can never skip a layer and can never go up a layer. This enforces modularity.\nA service might sit across layers, for example logging sits across the 3 layers described above. In this case, the logging service is called a “sross-cutting concern”.\nEach layer should be independent of the implementation of other layers. Layers should handle exceptions from those below them, log the error and then throw a more generic exception, so that there is no reference to other layers’ implementation.\nLayers are part of the same process. This is different from tiers, which are processes that are distributed across a network.\n\n\n\nAn interface declares the signature of an implementation. This means that each implementation must implement the methods described. The abstract base class in Python is an example of this.\nClose coupling should be avoided; “new is glue”.\n\n\n\n\nSingle responsibility principle: Each class, module or method should have exactly one responsibility.\nOpen/closed principle: Software should be open for extension but closed for modification. Can be implemented using class inheritance.\nLiskov substitution principle: If S is a subtype of T, then objects of type T can be replaced with objects of type S without altering the program. This looks similar to polymorphism but is stricter; not only should the classes implement the same methods, but the behaviour of those methods should be the same too, there should be no hidden functionality performed by S that is not performed by T or vice versa.\nInterface segregation principle: Many client-specific interfaces are preferable to one general purpose interface.\nDependency inversion principle: High-level modules should depend on abstractions rather than concrete implementations. Relies on dependency injection, where one object supplies the dependencies of another object. A factory method determines which class to load and returns an instance of that class. This also makes testing easier, as you can inject a mock class rather than having to mock out specific functionality.\n\n\n\n\n\nStructure - case, underscores\nContent - class names should be nouns, methods should be imperative verbs\n\n\n\n\nSome best practices:\n\nOnly catch exceptions if you are going to do something with it\nCatch specific exceptions\nUse try-catch on the smallest code fragments possible\nLayers should handle exceptions raised by layers below them, without exposing the specific implementation of the other layer. Catch, log, re-raise a more generic error.\n\nPurposes of logging:\n\nTrack errors\nGather data\n\n\n\n\n\nA collection of general, reusable solutions to common software design problems. Popularised in the book “Design patterns: elements of reusable object-oriented software”.\n\nThe factory pattern: Creating objects without specifying the exact class of the object. Avoids strong coupling between classes. Create an interface, then a factory method returns an instance of a class which implements that interface. If the implementation needs to change, we only need to change the factory method.\nThe repository pattern: Modules which don’t work with the data store should be oblivious to the type of data store. This is similar to the concept of the Data Access Layer. Layers are for architects, design patterns are for developers. They both seek to solve the same issue.\nThe facade pattern: Creating a layer of abstraction to mask complex actions. The facade does not create any new functionality, it is just a wrapper integrating existing modular functions together.\nThe command pattern: All the action’s information is encapsulated within an object.\n\n\n\n\nThe architecture design is the big picture that should answer the following:\n\nHow will the system work under heavy load?\nWhat will happen if the system crashes at a critical moment?\nHow complicated is it to update?\n\nThe architecture should:\n\nDefine components\nDefine how components communicate\nDefine the system’s quality attributes\n\nMake the correct choice as early as possible\n\n\nEnsuring the services are not strongly tied to other services. Avoid the “spiderweb” - when the graph of connections between services is densely connected.\nPrevents coupling of platforms and URLs.\nTo avoid URL coupling between services, two options are:\n\n“Yellow pages” directory\nGateway\n\nThe “yellow pages” contains a directory of all service URLs. If service A wants to query service D, A queries the yellow pages for D’s URL, then uses that URL to query D. This means that service A does not hardcode any information about service D. If D’s URLs change, then only the yellow pages need to be updated. Services only need to know the yellow pages directory’s URL.\n\n\n\n\n\n\nFigure 2: Yellow Pages\n\n\n\nThe gateway acts as a middleman. It holds a mapping table of all URLs Service A queries the gateway, which in turn queries service E. Service A doesn’t need to know about E or any other services. Services only need to know the gateway’s URL.\n\n\n\n\n\n\nFigure 3: Gateway\n\n\n\n\n\n\nThe application’s state is stored in only 2 places:\n\nThe data store\nThe user interface\n\nStateless architecture is preferred in virtually all circumstances.\nIn a stateful architecture, when a user logs in, the login service retrieves the user details from the database and stored them for future use. Data is stored in code.\nIf the login request was routed to server A, the user’s details are stored there. If the user later tried to add itemsto the cart and their cart service request is routed to server B, then their user details will not exist as those are stored on server A.\nDisadvantages of stateful:\n\nLack of scalability\nLack of redundancy\n\n\n\n\n\n\n\nFigure 4: Stateful\n\n\n\nIn a stateless architecture, no data is stored in the service itself. This means the behaviour will be the same regardless of which server a request was routed to.\n\n\n\n\n\n\nFigure 5: Stateless\n\n\n\n\n\n\nCaches store data locally to avoid retrieving the same data from the database multiple times. It trades reliability of data (it is stored in volatile memory) for improved performance.\nA cache should store data that is frequently accessed and rarely modified.\nTwo types of cache:\n\nIn-memory cache - Cache stored in memory on a single service\n\nPros:\n\nBest performance\nCan store any objects\nEasy to use\n\nCons:\n\nSize is limited by the process’s memory\nCan grow stale/inconsistent if the service is scaled out to multiple servers\n\n\nDistributed cache - Cache is independent of the services and can be accessed by all servers\n\nPros:\n\nSupports scaled out servers\nFailover capabilities\nStorage is unlimited\n\nCons:\n\nSetup is more complex\nOften only support primitive data types\nWorse performance than in-memory cache\n\n\n\n\n\n\nMessaging methods can be evaluated on these criteria:\n\nPerformance\nMessage size\nExecution model\nFeedback (handshaking) and reliability\nComplexity\n\nMessaging methods include:\n\nREST API\nHTTP Push\nQueue\nFile-based and database-based methods\n\n\n\nUniversal standard for HTTP-based systems.\nUseful for traditional web apps.\n\n\n\n\n\n\nFigure 6: REST API\n\n\n\n\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nVery fast\n\n\nMessage size\nSame as HTTP protocol limitations - GET 8KB, POST ~10MB\n\n\nExecution model\nRequest/response - ideal for quick, short actions\n\n\nFeedback\nImmediate feedback via response codes\n\n\nComplexity\nExtremely easy\n\n\n\n\n\n\nA client subscribes to the service waiting for an event. When that event occurs, the server notifies the client.\nFor real-time messaging, these often use web sockets to maintain the subscription connection, rather than a traditional request/response model.\nUseful for chat or monitoring.\n\n\n\n\n\n\nFigure 7: HTTP Push\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nVery fast\n\n\nMessage size\nLimited, few KB\n\n\nExecution model\nWeb socket connection / long polling\n\n\nFeedback\nNone (fire and forget)\n\n\nComplexity\nExtremely easy\n\n\n\n\n\n\nThe queue sits between two (or more) services. If service A wants to send a message to service B, A places the message in the queue and B periodically pulls from the queue.\nThis ensures messages will be handled exactly once and in the order received.\nUseful for complex systems with lots of data, when order and reliability are important.\n\n\n\n\n\n\nFigure 8: Queue\n\n\n\n\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nSlow - push/poll time and database persistence\n\n\nMessage size\nUnlimited but best practice to use small messages\n\n\nExecution model\nPolling\n\n\nFeedback\nVery reliable but feedback depends on the monitoring in place to ensure messages make it to the queue and get executed\n\n\nComplexity\nRequires training and setup to maintain a queue engine\n\n\n\n\n\n\nSimilar to queue, but rather han placing messages in a queue it places them in a file directory or database. There is no guarantee that messages are processed once and only once.\nIf multiple services are polling the same file folder, then when a new file is added we can get two issues:\n\nFile locked\nDuplicate processing\n\nSimilar use case to queues, but queues are generally preferred.\n\n\n\n\n\n\nFigure 9: File-based Messaging\n\n\n\n\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nSlow - push/poll time and database persistence\n\n\nMessage size\nUnlimited\n\n\nExecution model\nPolling\n\n\nFeedback\nVery reliable but feedback depends on the monitoring\n\n\nComplexity\nRequires training and setup to maintain the filestore or database\n\n\n\n\n\n\n\n\n\nCreate a central logging service that all other services call to write to a central database. This solves the problem where each microservice has its own log format, data and location. For example, some might be in files, SQL database, NoSQL database, etc.\nImplementation can be via an API or polling folders that each of the services write to.\n\n\n\n\n\n\nFigure 10: Central Logging Service\n\n\n\n\n\n\nCorrelation ID is an identifier attached to the beginning of a user flow and is attached to any action taken by that user, so that if there is an error at any point the user flow can be traced back through the different services’ logs.\n\n\n\n\n\nExternal considerations can affect architecture and design decisions.\n\nDeadlines\nDev team skills - New technologies can introduce uncertainty, delays and low quality\nIT support - Assign who will support the product from the outset. This should not be developers.\nCost - Build vs buy. Estimate cost vs value. Spend money to save time, don’t spend time to save money.\n\n\n\n\nThis should describe the basic elements of the system:\n\nTechnology stack\nComponents\nServices\nCommunication between components and services\n\nNo development should begin before the document is complete.\nGoals of the document:\n\nDescribe what should be developed and how\nList the functional and non-functional requirements\n\nAudience:\n\nEveryone involved with the system: project manager, CTO, QA leader, developers\nSections for management appear first as they are unlikely to read the whole document\nQA lead can begin preparing test infrastructure ahead of time\n\nFormat: UML (universal modeling language) is often used. It visualises the system’s design with concepts and diagrams. However, this assumes the audience is familiar with UML which is often not the case.\n\nKeep the contents as simple as possible\nUse plain English\nVisualise using whatever software is comfortable and appropriate\n\nStructure:\n\nBackground\nRequirements\nExecutive summary\nArchitecture overview\nComponents drill-down\n\n\n\nThis section validates your point of view and instils confidence that you understand the project.\nOne page for all team and management.\nDescribe the system from a business POV:\n\nThe system’s role\nReasons for replacing the old system\nExpected business impact\n\n\n\nFunctional and non-functional requirements - what should the system do and deal with? This section validates your understadning of the requirements. The requirements dictate the architecture so this section sets the scene for the chosen architecture.\nOne page for all team and management.\nThis should be a brief, bullet point list of requirements with no more than 3 lines on each.\n\n\n\n\nProvide a high-level view of the architecture.\nManagers will not read the whole document; &lt;3 pages for management.\n\nUse charts and diagrams\nWrite this after the rest of the document\nUse technical terms sparsely and only well known ones\nBe concise and don’t repeat yourself\n\n\n\n\nProvide a high-level view of the architecture in technical terms. Do not deep dive into specific components\n&lt;10 pages for developers and QA lead.\n\nGeneral description: type and major non-functional requirements\nHigh-level diagram of logical components, no specifics about hardware or technologies\nDiagram walkthrough: describe the various parts and their roles\nTechnology stack: iff there is a single stack include it here, otherwise include it in the components drill down section\n\n\n\n\nDetailed description of each component.\nUnlimited length, for developers and QA lead.\nFor each component:\n\nComponent’s role\nTechnology stack: Data store, backend, frontend\nComponent’s architecture: Describe the API (URL, logic, response code, comments). Describe the layers. Mention design patterns here.\nDevelopment instructions: Specific development guidelines\n\nBe specific and include rationale behind decisions.\n\n\n\n\nThese architecture patterns solve specific problems but add complexity to the system, so the costs and benefits should be compared.\n\n\nAn architecture in which functionalities are implemented as separate, loosely coupled services that interact with each other using a standard lightweight protocol.\nProblems with monolithic services:\n\nA single exception can crash the whole process\nUpdates impact all components\nLimited to one development platform/language\nUnoptimised compute resources\n\nWith microservices, each service is independent of others so can be updated separately, use a different platform, and be optimised separately.\nAn example of splitting a monolithic architecture into microservices:\n\n\n\n\n\n\nFigure 11: Monolith Example\n\n\n\nAs a microservice architecture, this becomes:\n\n\n\n\n\n\nFigure 12: Microservice Example\n\n\n\nProblems with microservices:\n\nComplex monitoring of all services and their interactions\nComplex architecture\nComplex testing\n\n\n\n\nStoring the deltas to each entity rather than updating it. The events can then be “rebuilt” from the start to give a view of the state at any given point in time.\nUse when history matters.\n\n\n\n\n\n\nFigure 13: Event Sourcing\n\n\n\nPros:\n\nTracing history\nSimple data model\nPerformance\nReporting\n\nCons:\n\nNo unified view - need to rebuild all events from the start to see the current state\nStorage usage\n\n\n\n\nCommand query responsibility segregation. Data storage and data retrieval are two separate databases, with a sync service between the two.\nThis integrates nicely with event sourcing, where events (deltas) are stored in one database and the current state is periodically built and stored in the retrieval database.\n\n\n\n\n\n\nFigure 14: CQRS\n\n\n\nPros:\n\nUseful with high-frequency updates that require near real-time querying\n\nCons:\n\nComplexity - need 2 databases, a sync service, ETL between the storage and retrieval database\n\n\n\n\n\nThe architect is often not the direct line manager of the developers implementing the software. You ultimately work with people, not software. Influence without authority is key.\n\nListening - assume you are not the smartest person in the room, collective wisdom is always better.\nDealing with criticism\n\nGenuine questioning: Provide facts and logic, be willing to go back and check again\nMocking: Don’t attack back, provide facts and logic\n\nBe smart not right\nOrganisational politics - be aware of it but don’t engage in it\nPublic speaking - define a goal, know your audience, be confident, don’t read, maintain eye contact\nLearning - blogs (DZone, InfoQ, O’Reilly), articles, conferences\n\n\n\n\n\n“The Complete Guide to Becoming a Software Architect” Udemy course\nDesign patterns\n\n\n\n\nFigure 1: Backend Tech\nFigure 2: Yellow Pages\nFigure 3: Gateway\nFigure 4: Stateful\nFigure 5: Stateless\nFigure 6: REST API\nFigure 7: HTTP Push\nFigure 8: Queue\nFigure 9: File-based Messaging\nFigure 10: Central Logging Service\nFigure 11: Monolith Example\nFigure 12: Microservice Example\nFigure 13: Event Sourcing\nFigure 14: CQRS"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#what-is-a-software-architect",
    "href": "posts/software/software_architecture/software_architect_notes.html#what-is-a-software-architect",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "A developer knows what can be done, an architect knows what should be done. How do we use technology to meet business requirements.\nGeneral system requirements:\n\nFast\nSecure\nReliable\nEasy to maintain\n\nArchitect’s need to know how to code for:\n\nArchitecture’s trustworthiness\nSupport developers\nRespect of developers"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#the-architects-mindset",
    "href": "posts/software/software_architecture/software_architect_notes.html#the-architects-mindset",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "Understand the business - Strengths, weaknesses, compettions, growth strategy.\nDefine the system’s goals - Goals are not requirements. Goals describe the effect on the organisation, requirements describe what the system should do.\nWork for your client’s clients - Prioritise the end user.\nTalk to the right people with the right language - What is the thing that really matters to the person I’m talking to?\n\nProject managers care about how things affect deadlines, developers care about the technologies used, CEOs care about business continuity and bottom line."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#the-architecture-process",
    "href": "posts/software/software_architecture/software_architect_notes.html#the-architecture-process",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "Understand the system requirements - What the system should do.\nUnderstand the non-functional requirements - Technical and service level attributes, e.g. number of users, loads, volumes, performance.\nMap the components - Understand the system functionality and communicate this to your client. Completely non-technical at this point, no mention of specific technologies. A diagram of high level components helps.\nSelect the technology stack - Backend, frontend, data store.\nDesign the architecture\nWrite the architecture document\nSupport the team\n\nInclude developers in non-functional requirements and architecture design for two reasons:\n\nLearn about unknown scenarios early\nCreate ambassadors"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#system-requirements",
    "href": "posts/software/software_architecture/software_architect_notes.html#system-requirements",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "The 2 types of requirements: functional and non-functional.\n\n\n“What the system should do”\n\nBusiness flows\nBusiness services\nUser interfaces\n\n\n\n\n“What the system should deal with”\n\nPerformance - latency and throughput\n\nLatency - How long does it take to perform a single task?\nThroughput - How many tasks can be performed in a given time unit?\n\nLoad - Quantity of work without crashing. Determines the availability of the system. Always look at peak (worst case) numbers.\nData volume - How much data will the system accumulate over time?\n\nData required on day one\nData growth (say, annually)\n\nConcurrent users - How many users will be using the system? This includes “dead times” which differentiates it from the load requirement. Rule of thumb is concurrent_users = load * 10\nSLA - Service Level Agreement for uptime.\n\nThe non-functional requirements are generally the more important in determining the architecture. The client will generally need guiding towards sensible values, otherwise they just want as much load as possible, as much uptime as possible etc."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#application-types",
    "href": "posts/software/software_architecture/software_architect_notes.html#application-types",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "The application type should be established early based on the use case and expected user interaction.\n\nWeb apps\n\nServe html pages.\nUI; user-initiated actions; large scale; short, focused actions.\nRequest-response model.\n\nWeb API\n\nServe data (often JSON).\nData retrieval and storage; client-initiated actions; large scale; short, focused actions.\nRequest-response model.\n\nMobile\n\nRequire user interaction; frontend for web API; location-based.\n\nConsole\n\nNo UI; limited interation; long-running processes; short actions for power users.\nRequire technical knowledge.\n\nService\n\nNo UI; managed by service manager; long-running processes.\n\nDesktop\n\nAll resources on the PC; UI; user-centric actions.\n\nFunction-as-a-service\n\nAWS lambda"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#select-technology-stack",
    "href": "posts/software/software_architecture/software_architect_notes.html#select-technology-stack",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "Considerations:\n\nAppropriate for the task\nCommunity - e.g. stack overflow activity\nPopularity - google trends over 2 years\n\n\n\nCovers web app, web API, console, service.\nOptions: .NET, Java, node.js, PHP, Python\n\n\n\n\n\n\nFigure 1: Backend Tech\n\n\n\n\n\n\nCovers:\n\nWeb app - Angular, React\nMobile - Native (Swift, Java/Kotlin), Xamarin, React Native\nDesktop - depends on target OS\n\n\n\n\nSQL - small, structured data\n\nRelational tables\nTransactions\n\nAtomicity\nConsistency\nIsolation\nDurability\n\nQuerying language is universal\n\nNoSQL - huge, unstructured or semi-structured data\n\nEmphasis on scale and performance.\nSchema-less, with entities stored as JSON.\nEventual consistency - data can be temporarily inconsistent\nNo universal querying language"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#the--ilities",
    "href": "posts/software/software_architecture/software_architect_notes.html#the--ilities",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "Quality attributes that describe technical capabilities to fulfill the non-functional requirements. Non-functional requirements map to quality attributes, which are designed in the architecture.\n\nScalability - Adding computing resources without any interruption. Scale out (add more compute instances) is preferred over scale up (increase CPU, RAM on the existing instance). Scaling out introduces redundancy and is not limited by hardware.\nManageability - Know what’s going on and take action accordingly. The question “who reports the problems” determines if a system is manageable. The system should flag problems, not the suer.\nModularity - A system that is built from blcoks that can be changed or replaced without affecting the whole system.\nExtensibility - Functionality of the system can be extended without modifying existing code.\nTestability - How easy is it to test the application. Independent modules and methods and single responsibility principle aid testability."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#components-architecture",
    "href": "posts/software/software_architecture/software_architect_notes.html#components-architecture",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "A software component is a piece of code that runs in a single process. Distributed systems are composed of independent software components deployed on separate processes/servers/containers.\nComponent architecture relates to the lower level details, whereas system architecture is the higher level view of how the components fit together to achieve the non-functional requirements.\n\n\nLayers represent horizontal functionality:\n\nUI/SI - user interface or service interface (i.e. an API), authentication\nBusiness logic - validation, enrichment, computation\nData access layer - connection handling, transaction handling, querying/saving data\n\nCode can flow downwards by one layer only. It can never skip a layer and can never go up a layer. This enforces modularity.\nA service might sit across layers, for example logging sits across the 3 layers described above. In this case, the logging service is called a “sross-cutting concern”.\nEach layer should be independent of the implementation of other layers. Layers should handle exceptions from those below them, log the error and then throw a more generic exception, so that there is no reference to other layers’ implementation.\nLayers are part of the same process. This is different from tiers, which are processes that are distributed across a network.\n\n\n\nAn interface declares the signature of an implementation. This means that each implementation must implement the methods described. The abstract base class in Python is an example of this.\nClose coupling should be avoided; “new is glue”.\n\n\n\n\nSingle responsibility principle: Each class, module or method should have exactly one responsibility.\nOpen/closed principle: Software should be open for extension but closed for modification. Can be implemented using class inheritance.\nLiskov substitution principle: If S is a subtype of T, then objects of type T can be replaced with objects of type S without altering the program. This looks similar to polymorphism but is stricter; not only should the classes implement the same methods, but the behaviour of those methods should be the same too, there should be no hidden functionality performed by S that is not performed by T or vice versa.\nInterface segregation principle: Many client-specific interfaces are preferable to one general purpose interface.\nDependency inversion principle: High-level modules should depend on abstractions rather than concrete implementations. Relies on dependency injection, where one object supplies the dependencies of another object. A factory method determines which class to load and returns an instance of that class. This also makes testing easier, as you can inject a mock class rather than having to mock out specific functionality.\n\n\n\n\n\nStructure - case, underscores\nContent - class names should be nouns, methods should be imperative verbs\n\n\n\n\nSome best practices:\n\nOnly catch exceptions if you are going to do something with it\nCatch specific exceptions\nUse try-catch on the smallest code fragments possible\nLayers should handle exceptions raised by layers below them, without exposing the specific implementation of the other layer. Catch, log, re-raise a more generic error.\n\nPurposes of logging:\n\nTrack errors\nGather data"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#design-patterns",
    "href": "posts/software/software_architecture/software_architect_notes.html#design-patterns",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "A collection of general, reusable solutions to common software design problems. Popularised in the book “Design patterns: elements of reusable object-oriented software”.\n\nThe factory pattern: Creating objects without specifying the exact class of the object. Avoids strong coupling between classes. Create an interface, then a factory method returns an instance of a class which implements that interface. If the implementation needs to change, we only need to change the factory method.\nThe repository pattern: Modules which don’t work with the data store should be oblivious to the type of data store. This is similar to the concept of the Data Access Layer. Layers are for architects, design patterns are for developers. They both seek to solve the same issue.\nThe facade pattern: Creating a layer of abstraction to mask complex actions. The facade does not create any new functionality, it is just a wrapper integrating existing modular functions together.\nThe command pattern: All the action’s information is encapsulated within an object."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#system-architecture",
    "href": "posts/software/software_architecture/software_architect_notes.html#system-architecture",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "The architecture design is the big picture that should answer the following:\n\nHow will the system work under heavy load?\nWhat will happen if the system crashes at a critical moment?\nHow complicated is it to update?\n\nThe architecture should:\n\nDefine components\nDefine how components communicate\nDefine the system’s quality attributes\n\nMake the correct choice as early as possible\n\n\nEnsuring the services are not strongly tied to other services. Avoid the “spiderweb” - when the graph of connections between services is densely connected.\nPrevents coupling of platforms and URLs.\nTo avoid URL coupling between services, two options are:\n\n“Yellow pages” directory\nGateway\n\nThe “yellow pages” contains a directory of all service URLs. If service A wants to query service D, A queries the yellow pages for D’s URL, then uses that URL to query D. This means that service A does not hardcode any information about service D. If D’s URLs change, then only the yellow pages need to be updated. Services only need to know the yellow pages directory’s URL.\n\n\n\n\n\n\nFigure 2: Yellow Pages\n\n\n\nThe gateway acts as a middleman. It holds a mapping table of all URLs Service A queries the gateway, which in turn queries service E. Service A doesn’t need to know about E or any other services. Services only need to know the gateway’s URL.\n\n\n\n\n\n\nFigure 3: Gateway\n\n\n\n\n\n\nThe application’s state is stored in only 2 places:\n\nThe data store\nThe user interface\n\nStateless architecture is preferred in virtually all circumstances.\nIn a stateful architecture, when a user logs in, the login service retrieves the user details from the database and stored them for future use. Data is stored in code.\nIf the login request was routed to server A, the user’s details are stored there. If the user later tried to add itemsto the cart and their cart service request is routed to server B, then their user details will not exist as those are stored on server A.\nDisadvantages of stateful:\n\nLack of scalability\nLack of redundancy\n\n\n\n\n\n\n\nFigure 4: Stateful\n\n\n\nIn a stateless architecture, no data is stored in the service itself. This means the behaviour will be the same regardless of which server a request was routed to.\n\n\n\n\n\n\nFigure 5: Stateless\n\n\n\n\n\n\nCaches store data locally to avoid retrieving the same data from the database multiple times. It trades reliability of data (it is stored in volatile memory) for improved performance.\nA cache should store data that is frequently accessed and rarely modified.\nTwo types of cache:\n\nIn-memory cache - Cache stored in memory on a single service\n\nPros:\n\nBest performance\nCan store any objects\nEasy to use\n\nCons:\n\nSize is limited by the process’s memory\nCan grow stale/inconsistent if the service is scaled out to multiple servers\n\n\nDistributed cache - Cache is independent of the services and can be accessed by all servers\n\nPros:\n\nSupports scaled out servers\nFailover capabilities\nStorage is unlimited\n\nCons:\n\nSetup is more complex\nOften only support primitive data types\nWorse performance than in-memory cache\n\n\n\n\n\n\nMessaging methods can be evaluated on these criteria:\n\nPerformance\nMessage size\nExecution model\nFeedback (handshaking) and reliability\nComplexity\n\nMessaging methods include:\n\nREST API\nHTTP Push\nQueue\nFile-based and database-based methods\n\n\n\nUniversal standard for HTTP-based systems.\nUseful for traditional web apps.\n\n\n\n\n\n\nFigure 6: REST API\n\n\n\n\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nVery fast\n\n\nMessage size\nSame as HTTP protocol limitations - GET 8KB, POST ~10MB\n\n\nExecution model\nRequest/response - ideal for quick, short actions\n\n\nFeedback\nImmediate feedback via response codes\n\n\nComplexity\nExtremely easy\n\n\n\n\n\n\nA client subscribes to the service waiting for an event. When that event occurs, the server notifies the client.\nFor real-time messaging, these often use web sockets to maintain the subscription connection, rather than a traditional request/response model.\nUseful for chat or monitoring.\n\n\n\n\n\n\nFigure 7: HTTP Push\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nVery fast\n\n\nMessage size\nLimited, few KB\n\n\nExecution model\nWeb socket connection / long polling\n\n\nFeedback\nNone (fire and forget)\n\n\nComplexity\nExtremely easy\n\n\n\n\n\n\nThe queue sits between two (or more) services. If service A wants to send a message to service B, A places the message in the queue and B periodically pulls from the queue.\nThis ensures messages will be handled exactly once and in the order received.\nUseful for complex systems with lots of data, when order and reliability are important.\n\n\n\n\n\n\nFigure 8: Queue\n\n\n\n\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nSlow - push/poll time and database persistence\n\n\nMessage size\nUnlimited but best practice to use small messages\n\n\nExecution model\nPolling\n\n\nFeedback\nVery reliable but feedback depends on the monitoring in place to ensure messages make it to the queue and get executed\n\n\nComplexity\nRequires training and setup to maintain a queue engine\n\n\n\n\n\n\nSimilar to queue, but rather han placing messages in a queue it places them in a file directory or database. There is no guarantee that messages are processed once and only once.\nIf multiple services are polling the same file folder, then when a new file is added we can get two issues:\n\nFile locked\nDuplicate processing\n\nSimilar use case to queues, but queues are generally preferred.\n\n\n\n\n\n\nFigure 9: File-based Messaging\n\n\n\n\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nSlow - push/poll time and database persistence\n\n\nMessage size\nUnlimited\n\n\nExecution model\nPolling\n\n\nFeedback\nVery reliable but feedback depends on the monitoring\n\n\nComplexity\nRequires training and setup to maintain the filestore or database\n\n\n\n\n\n\n\n\n\nCreate a central logging service that all other services call to write to a central database. This solves the problem where each microservice has its own log format, data and location. For example, some might be in files, SQL database, NoSQL database, etc.\nImplementation can be via an API or polling folders that each of the services write to.\n\n\n\n\n\n\nFigure 10: Central Logging Service\n\n\n\n\n\n\nCorrelation ID is an identifier attached to the beginning of a user flow and is attached to any action taken by that user, so that if there is an error at any point the user flow can be traced back through the different services’ logs."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#external-considerations",
    "href": "posts/software/software_architecture/software_architect_notes.html#external-considerations",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "External considerations can affect architecture and design decisions.\n\nDeadlines\nDev team skills - New technologies can introduce uncertainty, delays and low quality\nIT support - Assign who will support the product from the outset. This should not be developers.\nCost - Build vs buy. Estimate cost vs value. Spend money to save time, don’t spend time to save money."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#architecture-document",
    "href": "posts/software/software_architecture/software_architect_notes.html#architecture-document",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "This should describe the basic elements of the system:\n\nTechnology stack\nComponents\nServices\nCommunication between components and services\n\nNo development should begin before the document is complete.\nGoals of the document:\n\nDescribe what should be developed and how\nList the functional and non-functional requirements\n\nAudience:\n\nEveryone involved with the system: project manager, CTO, QA leader, developers\nSections for management appear first as they are unlikely to read the whole document\nQA lead can begin preparing test infrastructure ahead of time\n\nFormat: UML (universal modeling language) is often used. It visualises the system’s design with concepts and diagrams. However, this assumes the audience is familiar with UML which is often not the case.\n\nKeep the contents as simple as possible\nUse plain English\nVisualise using whatever software is comfortable and appropriate\n\nStructure:\n\nBackground\nRequirements\nExecutive summary\nArchitecture overview\nComponents drill-down\n\n\n\nThis section validates your point of view and instils confidence that you understand the project.\nOne page for all team and management.\nDescribe the system from a business POV:\n\nThe system’s role\nReasons for replacing the old system\nExpected business impact\n\n\n\nFunctional and non-functional requirements - what should the system do and deal with? This section validates your understadning of the requirements. The requirements dictate the architecture so this section sets the scene for the chosen architecture.\nOne page for all team and management.\nThis should be a brief, bullet point list of requirements with no more than 3 lines on each.\n\n\n\n\nProvide a high-level view of the architecture.\nManagers will not read the whole document; &lt;3 pages for management.\n\nUse charts and diagrams\nWrite this after the rest of the document\nUse technical terms sparsely and only well known ones\nBe concise and don’t repeat yourself\n\n\n\n\nProvide a high-level view of the architecture in technical terms. Do not deep dive into specific components\n&lt;10 pages for developers and QA lead.\n\nGeneral description: type and major non-functional requirements\nHigh-level diagram of logical components, no specifics about hardware or technologies\nDiagram walkthrough: describe the various parts and their roles\nTechnology stack: iff there is a single stack include it here, otherwise include it in the components drill down section\n\n\n\n\nDetailed description of each component.\nUnlimited length, for developers and QA lead.\nFor each component:\n\nComponent’s role\nTechnology stack: Data store, backend, frontend\nComponent’s architecture: Describe the API (URL, logic, response code, comments). Describe the layers. Mention design patterns here.\nDevelopment instructions: Specific development guidelines\n\nBe specific and include rationale behind decisions."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#advanced-architecture-topics",
    "href": "posts/software/software_architecture/software_architect_notes.html#advanced-architecture-topics",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "These architecture patterns solve specific problems but add complexity to the system, so the costs and benefits should be compared.\n\n\nAn architecture in which functionalities are implemented as separate, loosely coupled services that interact with each other using a standard lightweight protocol.\nProblems with monolithic services:\n\nA single exception can crash the whole process\nUpdates impact all components\nLimited to one development platform/language\nUnoptimised compute resources\n\nWith microservices, each service is independent of others so can be updated separately, use a different platform, and be optimised separately.\nAn example of splitting a monolithic architecture into microservices:\n\n\n\n\n\n\nFigure 11: Monolith Example\n\n\n\nAs a microservice architecture, this becomes:\n\n\n\n\n\n\nFigure 12: Microservice Example\n\n\n\nProblems with microservices:\n\nComplex monitoring of all services and their interactions\nComplex architecture\nComplex testing\n\n\n\n\nStoring the deltas to each entity rather than updating it. The events can then be “rebuilt” from the start to give a view of the state at any given point in time.\nUse when history matters.\n\n\n\n\n\n\nFigure 13: Event Sourcing\n\n\n\nPros:\n\nTracing history\nSimple data model\nPerformance\nReporting\n\nCons:\n\nNo unified view - need to rebuild all events from the start to see the current state\nStorage usage\n\n\n\n\nCommand query responsibility segregation. Data storage and data retrieval are two separate databases, with a sync service between the two.\nThis integrates nicely with event sourcing, where events (deltas) are stored in one database and the current state is periodically built and stored in the retrieval database.\n\n\n\n\n\n\nFigure 14: CQRS\n\n\n\nPros:\n\nUseful with high-frequency updates that require near real-time querying\n\nCons:\n\nComplexity - need 2 databases, a sync service, ETL between the storage and retrieval database"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#soft-skills",
    "href": "posts/software/software_architecture/software_architect_notes.html#soft-skills",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "The architect is often not the direct line manager of the developers implementing the software. You ultimately work with people, not software. Influence without authority is key.\n\nListening - assume you are not the smartest person in the room, collective wisdom is always better.\nDealing with criticism\n\nGenuine questioning: Provide facts and logic, be willing to go back and check again\nMocking: Don’t attack back, provide facts and logic\n\nBe smart not right\nOrganisational politics - be aware of it but don’t engage in it\nPublic speaking - define a goal, know your audience, be confident, don’t read, maintain eye contact\nLearning - blogs (DZone, InfoQ, O’Reilly), articles, conferences"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#references",
    "href": "posts/software/software_architecture/software_architect_notes.html#references",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "“The Complete Guide to Becoming a Software Architect” Udemy course\nDesign patterns\n\n\n\n\nFigure 1: Backend Tech\nFigure 2: Yellow Pages\nFigure 3: Gateway\nFigure 4: Stateful\nFigure 5: Stateless\nFigure 6: REST API\nFigure 7: HTTP Push\nFigure 8: Queue\nFigure 9: File-based Messaging\nFigure 10: Central Logging Service\nFigure 11: Monolith Example\nFigure 12: Microservice Example\nFigure 13: Event Sourcing\nFigure 14: CQRS"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html",
    "href": "posts/software/aws/aws_saa_notes.html",
    "title": "AWS Solutions Architect",
    "section": "",
    "text": "Regions are geographic locations, e.g. europe-west-3, us-east-1, etc.\nHow should we choose a region?\n\nCompliance - data governance rules may require data within a certain location\nProximity to reduce latency\nAvailable services vary by region\nPricing varies by region\n\nEach region can have multiple Availability Zones. There are usually between 3 and 6, e.g. ap-southeast-2a, ap-southeast-2b and ap-southeast-2c.\nEach AZ contains multiple data centers with redundant power, networking and connectivity.\nThere are multiple Edge Locations/Points of Presence; 400 locations around the world.\n\n\n\nSome services are global: IAM, Route 53, CloudFront, WAF\nMost are region-scoped: EC2, Elastic Beanstalk, Lambda, Rekognition\nThe region selector is in the top right. The service selector in top left, or alternatively use search bar.\n\n\n\nClick on Billing and Cost Management in the top right of the screen.\nThis needs to first be activated for administrator IAM users. From the root account: Account (top right) -&gt; IAM user and role access to billing information -&gt; tick the Activate IAM Access checkbox.\n\nBills tab - You can see bills per service and per region.\nFree Tier tab - Check what the free tier quotas are, and your current and forecasted usage.\nBudgets tab - set a budget. Use a template -&gt; Zero spend budget -&gt; Budget name and email recipients. This will alert as soon as you spend any money. There is also a monthly cost budget for regular reporting."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#aws-global-infrastructure",
    "href": "posts/software/aws/aws_saa_notes.html#aws-global-infrastructure",
    "title": "AWS Solutions Architect",
    "section": "",
    "text": "Regions are geographic locations, e.g. europe-west-3, us-east-1, etc.\nHow should we choose a region?\n\nCompliance - data governance rules may require data within a certain location\nProximity to reduce latency\nAvailable services vary by region\nPricing varies by region\n\nEach region can have multiple Availability Zones. There are usually between 3 and 6, e.g. ap-southeast-2a, ap-southeast-2b and ap-southeast-2c.\nEach AZ contains multiple data centers with redundant power, networking and connectivity.\nThere are multiple Edge Locations/Points of Presence; 400 locations around the world."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#tour-of-the-console",
    "href": "posts/software/aws/aws_saa_notes.html#tour-of-the-console",
    "title": "AWS Solutions Architect",
    "section": "",
    "text": "Some services are global: IAM, Route 53, CloudFront, WAF\nMost are region-scoped: EC2, Elastic Beanstalk, Lambda, Rekognition\nThe region selector is in the top right. The service selector in top left, or alternatively use search bar."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#aws-billing",
    "href": "posts/software/aws/aws_saa_notes.html#aws-billing",
    "title": "AWS Solutions Architect",
    "section": "",
    "text": "Click on Billing and Cost Management in the top right of the screen.\nThis needs to first be activated for administrator IAM users. From the root account: Account (top right) -&gt; IAM user and role access to billing information -&gt; tick the Activate IAM Access checkbox.\n\nBills tab - You can see bills per service and per region.\nFree Tier tab - Check what the free tier quotas are, and your current and forecasted usage.\nBudgets tab - set a budget. Use a template -&gt; Zero spend budget -&gt; Budget name and email recipients. This will alert as soon as you spend any money. There is also a monthly cost budget for regular reporting."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#overview",
    "href": "posts/software/aws/aws_saa_notes.html#overview",
    "title": "AWS Solutions Architect",
    "section": "2.1. Overview",
    "text": "2.1. Overview\nIdentity and access management. This is a global service.\nThe root account is created by default. It shouldn’t be used or shared; just use it to create users.\nUsers are people within the org and can be grouped. Groups cannot contain other groups. A user can belong to multiple groups (or none, but this is generally not best practice)."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#permissions",
    "href": "posts/software/aws/aws_saa_notes.html#permissions",
    "title": "AWS Solutions Architect",
    "section": "2.2. Permissions",
    "text": "2.2. Permissions\nUsers or groups can be assigned policies which are specified as a JSON document.\nLeast privilege principle means you shouldn’t give a user more permissions than they need."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#creating-users-and-groups",
    "href": "posts/software/aws/aws_saa_notes.html#creating-users-and-groups",
    "title": "AWS Solutions Architect",
    "section": "2.3. Creating Users and Groups",
    "text": "2.3. Creating Users and Groups\nIn the IAM dashboard, there is a Users tab.\nThere is a Create User button. We give them a user name and can choose a password (or autogenerate a password if this is for another user).\nThen we can add permissions directly, or create a group and add the user.\nTo create a group, specify the name and permissions policy.\nTags are optional key-value pairs we can add to assign custom metadata to different resources.\nWe can also create an account alias in IAM to simplify the account sign in, rather than having to remember the account ID.\nWhen signing in to the AWS console, you can choose to log in as root user or IAM user."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#iam-policies",
    "href": "posts/software/aws/aws_saa_notes.html#iam-policies",
    "title": "AWS Solutions Architect",
    "section": "2.4. IAM Policies",
    "text": "2.4. IAM Policies\nPolicies can be attached to groups, or assigned as inline policies to a specific user. Groups are best practice.\nComponents of JSON document:\n\nVersion: Policy language version (date)\nId: Identifier for the policy\nStatement: Specifies the permissions\n\nEach statement consists of:\n\nSid: Optional identifier for the statement\nEffect: “Allow” or “Deny”\nPrincipal: The account/user/role that this policy applies to\nAction: List of actions that this policy allows or denies\nResource: What the actions apply to, eg a bucket\nCondition: Optional, conditions when this policy should apply\n\n“*” is a wildcard that matches anything."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#mfa",
    "href": "posts/software/aws/aws_saa_notes.html#mfa",
    "title": "AWS Solutions Architect",
    "section": "2.5. MFA",
    "text": "2.5. MFA\nPassword policy can have different settings: minimum length, specific characters, password expiration, prevent password re-use.\nMulti-factor authentication requires the password you know and the device you own to log in.\nA hacker needs both to compromise the account.\nMFA devices:\n\nVirtual MFA devices - Google Authenticator, Authy. Support for multiple tokens on a single device.\nUniversal 2nd Factor Security Key (U2F) - eg YubiKey. Support for multiple root and IAM users on a single security key.\n\nHardware key fob MFA device\nHardware key fob MFA device for AWS GovCloud"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#access-keys",
    "href": "posts/software/aws/aws_saa_notes.html#access-keys",
    "title": "AWS Solutions Architect",
    "section": "2.6. Access Keys",
    "text": "2.6. Access Keys\nThere are 3 approaches to access AWS:\n\nManagement console (web UI) - password + MFA\nCommand line interface (CLI) - access keys\nSoftware Developer Kit (SDK) - access keys\n\nAccess keys are generated through the console and managed by the user. Access Key ID is like a username. Secret access key is like a password. Do not share access keys.\nAWS CLI gives programmatic access to public APIs of AWS. It is open source. Configure access keys in the CLI using aws configure.\nAWS SDK is for language-specific APIs."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#aws-cloudshell",
    "href": "posts/software/aws/aws_saa_notes.html#aws-cloudshell",
    "title": "AWS Solutions Architect",
    "section": "2.7. AWS CloudShell",
    "text": "2.7. AWS CloudShell\nAccess using the terminal icon in the toolbar next to the search bar.\nThis is an alternative to using your own terminal to access the AWS CLI. It is a cloud-based terminal.\nYou can pass --region to a command to run in a region other than the region selected in the AWS console.\nCloudShell has a file system attached so we can upload and download files."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#iam-roles-for-services",
    "href": "posts/software/aws/aws_saa_notes.html#iam-roles-for-services",
    "title": "AWS Solutions Architect",
    "section": "2.8. IAM Roles for Services",
    "text": "2.8. IAM Roles for Services\nSome AWS services can perform actions on your behalf. To do so, they need the correct permissions, which we can grant with an IAM role.\nFor example, EC2 instance roles, Lambda Function roles, CloudFormation roles.\nIn IAM, select Roles. Choose AWS Service and select the use case, e.g. EC2. Then we attach a permissions policy, such as IAMReadOnlyAccess."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#iam-security-tools",
    "href": "posts/software/aws/aws_saa_notes.html#iam-security-tools",
    "title": "AWS Solutions Architect",
    "section": "2.9. IAM Security Tools",
    "text": "2.9. IAM Security Tools\n\nIAM Credentials Report. Account-level report on all users and their credentials.\nIAM Access Advisor. User-level report on the service permissions granted to a user and when they were last accessed. This can help to see unused permissions to enforce principle of least privilege. This is in the Access Advisor tab under Users in IAM."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#iam-guidelines-and-best-practices",
    "href": "posts/software/aws/aws_saa_notes.html#iam-guidelines-and-best-practices",
    "title": "AWS Solutions Architect",
    "section": "2.10 IAM Guidelines and Best Practices",
    "text": "2.10 IAM Guidelines and Best Practices\n\nDon’t use root account except for account set up\nOne physical user = One AWS user\nAssign users to groups and assign permissions to groups\nCreate a strong password policy and use MFA\nUse Roles to give permissions to AWS services\nUse Access Keys for programmatic access via CLI and SDK\nAudit permissions using credentials report and access advisor\nNever share IAM users or access keys"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#summary",
    "href": "posts/software/aws/aws_saa_notes.html#summary",
    "title": "AWS Solutions Architect",
    "section": "2.11. Summary",
    "text": "2.11. Summary\n\nUsers map to a physical user\nGroups contain users. They can’t contain other groups.\nPolicies are JSON documents denoting the permissions for a user / group\nRoles grant permissions for AWS services like EC2 instances\nSecurity use MFA and password policy\nProgrammatic use of services via CLI or SDK. Access keys manage permissions for these.\nAudit usage via credentials report or access advisor"
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#ec2-overview",
    "href": "posts/software/aws/aws_saa_notes.html#ec2-overview",
    "title": "AWS Solutions Architect",
    "section": "3.1. EC2 Overview",
    "text": "3.1. EC2 Overview\nElastic Compute Cloud used for infrastructure-as-a-service.\nEncompasses a few different use cases:\n\nRenting virtual machines (EC2)\nStoring data on virtual drives (EBS)\nDistributing load across machines (ELB)\nScaling services using an auto-scaling group (ASG)\n\nSizing and configuration options:\n\nOS\nCPU\nRAM\nStorage - This can be network-attached (EBS and EFS) or hardware (EC2 Instance Store)\nNetwork Card - Speed of card and IP address\nFirewall rules - Security group\nBootstrap script - Configure a script to run at first launch using and EC2 User Data script. This runs as the root user so has sudo access.\n\nThere are different instance types that have different combinations of the configuration options above."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#creating-an-ec2-instance",
    "href": "posts/software/aws/aws_saa_notes.html#creating-an-ec2-instance",
    "title": "AWS Solutions Architect",
    "section": "3.2. Creating an EC2 Instance",
    "text": "3.2. Creating an EC2 Instance\n\nSpecify a “name” tag for the instance and any other optional tags.\nChoose a base image. OS.\nChoose an instance type.\nKey pair. This is optional and allows you to ssh into your instance.\nConfigure network settings. Public IP address, checkboxes to allow ssh access, http access\nConfigure storage amount and type. Delete on termination is an important selection to delete the EBS volume once the corresponding EC2 instance is terminated.\nThe “user data” box allows us to pass a bootstrap shell script.\nCheck the summary and click Launch Instance.\n\nThe Instance Details tab tells you the Instance ID, public IP address (to access from the internet) and the private IP address (to access from within AWS).\nWe can stop an instance to keep the storage state of the attached EBS volume but without incurring any more EC2 costs. The public IP address might change it stopping and starting. The private IP address stays the same.\nAlternatively, we can terminate it completely."
  },
  {
    "objectID": "posts/software/aws/aws_saa_notes.html#ec2-instance-types",
    "href": "posts/software/aws/aws_saa_notes.html#ec2-instance-types",
    "title": "AWS Solutions Architect",
    "section": "3.3. EC2 Instance Types",
    "text": "3.3. EC2 Instance Types\nThere are several families of instances: general purpose, compute-optimised, memory-optimised, accelerated computing, storage-optimised.\nSee the AWS website for an overview of all instances. There is also a handy comparison website here.\nThe naming convention is: \\[\nm5.large\n\\]\n\nm is the instance class\n5 is the generation (AWS releases new versions over time)\nlarge is the size within the class\n\nThe use cases for each of the instance types:\n\nGeneral purpose is for generic workloads like web servers. Balance between compute, memory and networking.\nCompute-optimized instances for tasks that require good processors, such as batch processing, HPC, scientific modelling.\nMemory-optimized instances for large RAM, e.g. in-memory databases and big unstructured data processing.\nStorage-optimised instances for tasks that require reading and writing a lot of data from lcoal storage, e.g. high-frequnecy transaction processing, cache for in-memory databases, data warehouse."
  },
  {
    "objectID": "posts/software/data_structures_algos/algorithms/algorithms.html",
    "href": "posts/software/data_structures_algos/algorithms/algorithms.html",
    "title": "Algorithms",
    "section": "",
    "text": "The mathematical explanation is given in terms of the upper bound of the growth rate of a function. Very briefly: if a function \\(g(x)\\) grows no faster than \\(f(x)\\) then \\(g\\) is a member of \\(O(f)\\). This isn’t particularly helpful for intuitive understanding though.\nThe fundamental question is:\n\nIf there are \\(N\\) data elements, how many steps will the algorithm take?\n\nThis helps us understand how the performance of the algorithm changes as the data increases.\nBasically, count the number of steps the algorithm takes as a function of N (and M if it involves two arrays), then drop any constants and only keep the “worst” term since we consider the worst case scenario.\nBe aware, though, that this doesn’t necessarily mean a lower complexity algorithm will always be faster in practice for every use case. Take for exmaple algorithm A, which always takes 100 steps regardless of input size so is \\(O(1)\\), and algorithm B which scales linearly with the input so in \\(O(N)\\).\nIf we apply these to an array with 10 elements, A will take 100 steps and B will take 10 steps. So the “worse” algorithm can perform better for small data.\nWhen an algorithm is \\(O(log(N))\\), the log is implicitly \\(log_2\\). These come up regularly in algorithms where we divide and conquer, as in the binary search, because we half the data with each iteration. If you see an algorithm has a log term in its complexity, that’s generally a clue that there is some binary split happening somewhere.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nDefinition: Recursion\nSee “Recursion”.\n\n\nRecursion is when a function calls itself.\nIn any case where you can use a loop, you could also write it recursively. I’m not saying you should, but you can.\nEvery recursive function should have a base case (or multiple base cases) where it does not recurse, to prevent it from entering an infinite loop.\n\n\nThere is a knack to reading recursive functions. Start from the base case and work backwards.\n\nIdentify the base case and walk through it\nIdentify the “second-to-last” case and walk through it\nRepeat for the next-to-last case before that and walk through it\n\n\ndef factorial(number):\n    \"\"\"Recursively calculate the factorial of a number.\"\"\"\n    # The base case\n    if number == 1:\n        return 1\n    # The recursive bit\n    return number * factorial(number - 1)\n\n\nfactorial(6)\n\n720\n\n\n\n\n\nThe computer uses a call stack to keep track of the functions to call. When we enter a new recursion, we push a function call on to the stack, and when we finish executing we pop it from the call stack.\nIf we don’t write appropriate base cases, the recursive function can loop infinitely, leading to stack overflow.\n\n\n\nUse recursion when the depth of the problem is unknown or arbitrary.\nIf we have a problem where we want to go through nested structures but we don’t know ahead of time how deep they go, we can’t solve this using regular loops but we can with recursion.\nFor example, if we want to traverse a directory and each of its subdirectories, and each of their subdirectories, etc.\n\n\n\nRecursive algorithms are useful for categories of problems where:\n\nThe goal is to repeatedly execute a task\nThe problem can be broken into subproblems, which are versions of the same problem but with a smaller input.\n\n\n\nIf we are modifying the data structure, say an array, in place, we can pass it as an extra parameter to the recursive function so that it can be passed up and down the call stack.\nThis is often useful for the repeatedly execute category of problems.\n\n\n\nRecursion is useful when coupled with a new, slightly unintuitive, mental model of top-down problem solving.\n\nImagine the function you’re writing has already been implemented\nIdentify the subproblem, often the next step along\nSee what happens when you call the function on the subproblem\nGo from there until you reach a base case\n\nThis is often useful for the subproblem category of problems.\n\n\n\n\n\nThe idea behind dynamic programming is similar to recursion: break the problem down into smaller subproblems. For dynamic programming problems, the subproblems are typically overlapping subproblems. We also want to remove any duplicate calls.\n\n\nA common problem with recursive approaches is that we end up calculating the same function multiple times in the call stack. This can lead to some horrendous complexities like \\(O(N!)\\) or \\(O(2^N)\\).\nThe key difference to recursion is we only solve each subproblem once and store its result. This way, we can just look it up for any other calls that require it. This storing of intermediate calculations is called memoization.\nWe will need to pass this memoised object as an extra parameter and modify it in place, as noted in the recusion section.\n\n\n\nDitch recursion and just use a loop.\nThis is technically another way of “solving a problem that can be solved using recursion but without making duplciate calls” - which is what dynamic programming essentially is. In this case, we do it by removing recursion altogether.\n\n\n\n\nOften the primary focus of an algorithm is its speed, characterised by its time complexity - how many steps does the algorithm take for an input of N elements?\nBut another useful dimension to analyse is its space complexity - how much memory does the algorithm consume for an input of N elements?\nIt is important to note that space complexity generally only considers the new data that the algorithm is generating, not the original input. The extra space consumed by the new data is called auxiliary space.\n(However, some textbooks do include the original data in the space complexity, so it’s important to check the convention being used.)\n\n\nA recursive function takes up a unit of memory for each call that it makes.\nEach time it is called, it adds an item (the function call itself and any additional parameters) on the call stack. So to understand its space complexity, we need to determine how big the call stack can get at its peak, i.e. how many recursive calls it makes.\n\n\n\n\nThe first step towards optimising your code is understanding its current complexity.\n\n\nWhat is the best complexity you can imagine?\nAlso called the best-conceivable runtime.\nFor example, if a problem requires processing every unit of a list, then you will have to visit all \\(N\\) elements, so the best you can do is probably \\(O(N)\\). You won’t necessarily be able to achieve this, but it gives you an indication of the potential.\n\nDetermine your current algorithm’s Big O\nDetermine the best-imaginable Big O\nIf they’re different, try optimising\n\nAnother mental trick that can be helpful is to pick a really fast Big and ask yourself “If someone told me they had an algorithm to achieve this Big O, would I believe them?”\n\n\n\nAsk yourself: “If I could magically look up a desired piece of information in \\(O(1)\\) time, could I make my algorithm faster?”\nIf so, you may be able to bring in an additional data structure to accommodate this look up. Often this is a hash table.\n\n\n\nStart with several smaller cases of the problem and work through the answers by hand.\nDoes a pattern emerge that might generalise to bigger cases?\n\n\n\nA greedy algorithm, at each step, chooses what appears to be the current best option.\nThis local optimisation doesn’t necessarily find the global optimal solution. But it can be sueful in situations where finding the absolute best option is not necessary or practical.\n\n\n\nIf the input data were stored as a different data structure, would it make the problem easier?\nIn some cases, it can be worthwhile performing a pre-compute step to convert the data structure if it then allows faster algorithms to be run.\n\n\n\n\n\nA common sense guide to data structures and algorithms, Jay Wengrow."
  },
  {
    "objectID": "posts/software/data_structures_algos/algorithms/algorithms.html#big-o",
    "href": "posts/software/data_structures_algos/algorithms/algorithms.html#big-o",
    "title": "Algorithms",
    "section": "",
    "text": "The mathematical explanation is given in terms of the upper bound of the growth rate of a function. Very briefly: if a function \\(g(x)\\) grows no faster than \\(f(x)\\) then \\(g\\) is a member of \\(O(f)\\). This isn’t particularly helpful for intuitive understanding though.\nThe fundamental question is:\n\nIf there are \\(N\\) data elements, how many steps will the algorithm take?\n\nThis helps us understand how the performance of the algorithm changes as the data increases.\nBasically, count the number of steps the algorithm takes as a function of N (and M if it involves two arrays), then drop any constants and only keep the “worst” term since we consider the worst case scenario.\nBe aware, though, that this doesn’t necessarily mean a lower complexity algorithm will always be faster in practice for every use case. Take for exmaple algorithm A, which always takes 100 steps regardless of input size so is \\(O(1)\\), and algorithm B which scales linearly with the input so in \\(O(N)\\).\nIf we apply these to an array with 10 elements, A will take 100 steps and B will take 10 steps. So the “worse” algorithm can perform better for small data.\nWhen an algorithm is \\(O(log(N))\\), the log is implicitly \\(log_2\\). These come up regularly in algorithms where we divide and conquer, as in the binary search, because we half the data with each iteration. If you see an algorithm has a log term in its complexity, that’s generally a clue that there is some binary split happening somewhere."
  },
  {
    "objectID": "posts/software/data_structures_algos/algorithms/algorithms.html#recursion",
    "href": "posts/software/data_structures_algos/algorithms/algorithms.html#recursion",
    "title": "Algorithms",
    "section": "",
    "text": "Tip\n\n\n\nDefinition: Recursion\nSee “Recursion”.\n\n\nRecursion is when a function calls itself.\nIn any case where you can use a loop, you could also write it recursively. I’m not saying you should, but you can.\nEvery recursive function should have a base case (or multiple base cases) where it does not recurse, to prevent it from entering an infinite loop.\n\n\nThere is a knack to reading recursive functions. Start from the base case and work backwards.\n\nIdentify the base case and walk through it\nIdentify the “second-to-last” case and walk through it\nRepeat for the next-to-last case before that and walk through it\n\n\ndef factorial(number):\n    \"\"\"Recursively calculate the factorial of a number.\"\"\"\n    # The base case\n    if number == 1:\n        return 1\n    # The recursive bit\n    return number * factorial(number - 1)\n\n\nfactorial(6)\n\n720\n\n\n\n\n\nThe computer uses a call stack to keep track of the functions to call. When we enter a new recursion, we push a function call on to the stack, and when we finish executing we pop it from the call stack.\nIf we don’t write appropriate base cases, the recursive function can loop infinitely, leading to stack overflow.\n\n\n\nUse recursion when the depth of the problem is unknown or arbitrary.\nIf we have a problem where we want to go through nested structures but we don’t know ahead of time how deep they go, we can’t solve this using regular loops but we can with recursion.\nFor example, if we want to traverse a directory and each of its subdirectories, and each of their subdirectories, etc.\n\n\n\nRecursive algorithms are useful for categories of problems where:\n\nThe goal is to repeatedly execute a task\nThe problem can be broken into subproblems, which are versions of the same problem but with a smaller input.\n\n\n\nIf we are modifying the data structure, say an array, in place, we can pass it as an extra parameter to the recursive function so that it can be passed up and down the call stack.\nThis is often useful for the repeatedly execute category of problems.\n\n\n\nRecursion is useful when coupled with a new, slightly unintuitive, mental model of top-down problem solving.\n\nImagine the function you’re writing has already been implemented\nIdentify the subproblem, often the next step along\nSee what happens when you call the function on the subproblem\nGo from there until you reach a base case\n\nThis is often useful for the subproblem category of problems."
  },
  {
    "objectID": "posts/software/data_structures_algos/algorithms/algorithms.html#dynamic-programming",
    "href": "posts/software/data_structures_algos/algorithms/algorithms.html#dynamic-programming",
    "title": "Algorithms",
    "section": "",
    "text": "The idea behind dynamic programming is similar to recursion: break the problem down into smaller subproblems. For dynamic programming problems, the subproblems are typically overlapping subproblems. We also want to remove any duplicate calls.\n\n\nA common problem with recursive approaches is that we end up calculating the same function multiple times in the call stack. This can lead to some horrendous complexities like \\(O(N!)\\) or \\(O(2^N)\\).\nThe key difference to recursion is we only solve each subproblem once and store its result. This way, we can just look it up for any other calls that require it. This storing of intermediate calculations is called memoization.\nWe will need to pass this memoised object as an extra parameter and modify it in place, as noted in the recusion section.\n\n\n\nDitch recursion and just use a loop.\nThis is technically another way of “solving a problem that can be solved using recursion but without making duplciate calls” - which is what dynamic programming essentially is. In this case, we do it by removing recursion altogether."
  },
  {
    "objectID": "posts/software/data_structures_algos/algorithms/algorithms.html#space-complexity",
    "href": "posts/software/data_structures_algos/algorithms/algorithms.html#space-complexity",
    "title": "Algorithms",
    "section": "",
    "text": "Often the primary focus of an algorithm is its speed, characterised by its time complexity - how many steps does the algorithm take for an input of N elements?\nBut another useful dimension to analyse is its space complexity - how much memory does the algorithm consume for an input of N elements?\nIt is important to note that space complexity generally only considers the new data that the algorithm is generating, not the original input. The extra space consumed by the new data is called auxiliary space.\n(However, some textbooks do include the original data in the space complexity, so it’s important to check the convention being used.)\n\n\nA recursive function takes up a unit of memory for each call that it makes.\nEach time it is called, it adds an item (the function call itself and any additional parameters) on the call stack. So to understand its space complexity, we need to determine how big the call stack can get at its peak, i.e. how many recursive calls it makes."
  },
  {
    "objectID": "posts/software/data_structures_algos/algorithms/algorithms.html#code-optimisation-techniques",
    "href": "posts/software/data_structures_algos/algorithms/algorithms.html#code-optimisation-techniques",
    "title": "Algorithms",
    "section": "",
    "text": "The first step towards optimising your code is understanding its current complexity.\n\n\nWhat is the best complexity you can imagine?\nAlso called the best-conceivable runtime.\nFor example, if a problem requires processing every unit of a list, then you will have to visit all \\(N\\) elements, so the best you can do is probably \\(O(N)\\). You won’t necessarily be able to achieve this, but it gives you an indication of the potential.\n\nDetermine your current algorithm’s Big O\nDetermine the best-imaginable Big O\nIf they’re different, try optimising\n\nAnother mental trick that can be helpful is to pick a really fast Big and ask yourself “If someone told me they had an algorithm to achieve this Big O, would I believe them?”\n\n\n\nAsk yourself: “If I could magically look up a desired piece of information in \\(O(1)\\) time, could I make my algorithm faster?”\nIf so, you may be able to bring in an additional data structure to accommodate this look up. Often this is a hash table.\n\n\n\nStart with several smaller cases of the problem and work through the answers by hand.\nDoes a pattern emerge that might generalise to bigger cases?\n\n\n\nA greedy algorithm, at each step, chooses what appears to be the current best option.\nThis local optimisation doesn’t necessarily find the global optimal solution. But it can be sueful in situations where finding the absolute best option is not necessary or practical.\n\n\n\nIf the input data were stored as a different data structure, would it make the problem easier?\nIn some cases, it can be worthwhile performing a pre-compute step to convert the data structure if it then allows faster algorithms to be run."
  },
  {
    "objectID": "posts/software/data_structures_algos/algorithms/algorithms.html#references",
    "href": "posts/software/data_structures_algos/algorithms/algorithms.html#references",
    "title": "Algorithms",
    "section": "",
    "text": "A common sense guide to data structures and algorithms, Jay Wengrow."
  },
  {
    "objectID": "posts/software/react/6_styling/post.html",
    "href": "posts/software/react/6_styling/post.html",
    "title": "React: Styling",
    "section": "",
    "text": "First stylin’, then profilin’. Woo!\n\n\nThere are several options of styling in React:\n\nVanilla CSS with separate modules\nIn-line CSS styles\nScoped CSS with modules\nCSS-in-React with styled components\nTailwind CSS\n\nWe will look at each in turn, along with their relative merits.\nIn all cases, styling can be static or dynamic.\n\n\n\nIn plain CSS we can use:\n\nElement selectors - header h1\nID selectors - #auth-inputs\nClass selectors - .controls\n\nImport the .css file into the JSX file you want to style. You can have multiple css files, placing each next to the component JSX file it relates to.\nAdvantages:\n\nDecouples styling from JSX.\nCSS can be modified independently of the code logic, when working with multiple developers .\nMore developers will be familiar with plain CSS.\n\nDisadvantages:\n\nStyles are not scoped to components, which can lead to clashes and unexpected behaviour. The CSS just gets injected into the styles part of the page by the build process, so they apply globally.\n\nDynamic styling can be achieved by having two different class names and conditionally switching between them with a ternary expression.\n\n\n\nYou can set the style prop of each component directly.\nAdvantages:\n\nQuick to add\nStyles are scoped to the component\nDynamic styling is easy\n\nDisadvantages:\n\nYou have to style each component individually\nNo separation between CSS and JSX code\n\n\n\n\nFile-specific scoping for CSS classes.\nUsing .module in the css file name, e.g. Header.module.css will scope the styles to the file that it’s imported into.\nThe import is done slightly differently as it now returns a JavaScript object.\nimport { styles } from ‘Header.module.css’\nThis module approach is not supported natively by browsers. Instead, the build tool takes each of your classes and renames it to ensure it is unique per file. These transformed styles are what you see in the rendered DOM.\nConditional switching for dynamic styles works the same as vanilla CSS.\nAdvantages:\n\nDecouple CSS from JS\nCSS classes are scoped to file\n\nDisadvantages:\n\nBigger projects end up with many small CSS module files\n\n\n\n\nInstall this with\nnpm install styled-components\nThis keeps components and styles linked as a combined object that can be reused in multiple places.\nUse backticks to define a style template :\nconst Container = styled.div`\n    Styling goes here\n`\nThis creates a regular div under the hood. Any props passed to it will get forwarded to the underlying component so we can use it like normal.\nWe can pass functions in between the backticks for dynamic styling. Any props passed to the component are forwarded, so we can use these in functions to set styles.\nconst Label = styled.label`\n    display: block;\n    margin-bottom: 0.5rem;\n    color: ${({invalid}) =&gt; invalid ? 'red' : 'green'}\n`\nA common convention is to name any props used only for styling with a $ at the start to ensure they don’t clash with any other built-in props of the component.\n\n\n\nThis is another 3rd-party framework. The idea is you don’t need to know CSS. Instead you apply small, pre-defined utility classes to each component to achieve a style, and these abstract away a lot of the CSS styles.\nUse a VSCode tailwind plugin to get autocomplete suggestions.\nMedia queries can be used to apply different styles depending on the screen size. E.g. md:, hover:, etc\nAdvantages:\n\nDon’t need to know CSS.\nStyles are scoped to the component.\nConfigurable and extensible.\nRapid development (once you know the built-in class names).\n\nDisadvantages:\n\nLong class names.\nChanging styling requires modifying JSX.\nYou end up with lots of small wrapper components or lots of copy paste.\nYou need to learn the built-in class names.\n\n\n\n\n\nSection 6 of “React: The Complete Guide” Udemy course"
  },
  {
    "objectID": "posts/software/react/6_styling/post.html#styling-options-in-react",
    "href": "posts/software/react/6_styling/post.html#styling-options-in-react",
    "title": "React: Styling",
    "section": "",
    "text": "There are several options of styling in React:\n\nVanilla CSS with separate modules\nIn-line CSS styles\nScoped CSS with modules\nCSS-in-React with styled components\nTailwind CSS\n\nWe will look at each in turn, along with their relative merits.\nIn all cases, styling can be static or dynamic."
  },
  {
    "objectID": "posts/software/react/6_styling/post.html#vanilla-css",
    "href": "posts/software/react/6_styling/post.html#vanilla-css",
    "title": "React: Styling",
    "section": "",
    "text": "In plain CSS we can use:\n\nElement selectors - header h1\nID selectors - #auth-inputs\nClass selectors - .controls\n\nImport the .css file into the JSX file you want to style. You can have multiple css files, placing each next to the component JSX file it relates to.\nAdvantages:\n\nDecouples styling from JSX.\nCSS can be modified independently of the code logic, when working with multiple developers .\nMore developers will be familiar with plain CSS.\n\nDisadvantages:\n\nStyles are not scoped to components, which can lead to clashes and unexpected behaviour. The CSS just gets injected into the styles part of the page by the build process, so they apply globally.\n\nDynamic styling can be achieved by having two different class names and conditionally switching between them with a ternary expression."
  },
  {
    "objectID": "posts/software/react/6_styling/post.html#inline-styles",
    "href": "posts/software/react/6_styling/post.html#inline-styles",
    "title": "React: Styling",
    "section": "",
    "text": "You can set the style prop of each component directly.\nAdvantages:\n\nQuick to add\nStyles are scoped to the component\nDynamic styling is easy\n\nDisadvantages:\n\nYou have to style each component individually\nNo separation between CSS and JSX code"
  },
  {
    "objectID": "posts/software/react/6_styling/post.html#css-modules",
    "href": "posts/software/react/6_styling/post.html#css-modules",
    "title": "React: Styling",
    "section": "",
    "text": "File-specific scoping for CSS classes.\nUsing .module in the css file name, e.g. Header.module.css will scope the styles to the file that it’s imported into.\nThe import is done slightly differently as it now returns a JavaScript object.\nimport { styles } from ‘Header.module.css’\nThis module approach is not supported natively by browsers. Instead, the build tool takes each of your classes and renames it to ensure it is unique per file. These transformed styles are what you see in the rendered DOM.\nConditional switching for dynamic styles works the same as vanilla CSS.\nAdvantages:\n\nDecouple CSS from JS\nCSS classes are scoped to file\n\nDisadvantages:\n\nBigger projects end up with many small CSS module files"
  },
  {
    "objectID": "posts/software/react/6_styling/post.html#styled-components-third-party-library",
    "href": "posts/software/react/6_styling/post.html#styled-components-third-party-library",
    "title": "React: Styling",
    "section": "",
    "text": "Install this with\nnpm install styled-components\nThis keeps components and styles linked as a combined object that can be reused in multiple places.\nUse backticks to define a style template :\nconst Container = styled.div`\n    Styling goes here\n`\nThis creates a regular div under the hood. Any props passed to it will get forwarded to the underlying component so we can use it like normal.\nWe can pass functions in between the backticks for dynamic styling. Any props passed to the component are forwarded, so we can use these in functions to set styles.\nconst Label = styled.label`\n    display: block;\n    margin-bottom: 0.5rem;\n    color: ${({invalid}) =&gt; invalid ? 'red' : 'green'}\n`\nA common convention is to name any props used only for styling with a $ at the start to ensure they don’t clash with any other built-in props of the component."
  },
  {
    "objectID": "posts/software/react/6_styling/post.html#tailwind-css",
    "href": "posts/software/react/6_styling/post.html#tailwind-css",
    "title": "React: Styling",
    "section": "",
    "text": "This is another 3rd-party framework. The idea is you don’t need to know CSS. Instead you apply small, pre-defined utility classes to each component to achieve a style, and these abstract away a lot of the CSS styles.\nUse a VSCode tailwind plugin to get autocomplete suggestions.\nMedia queries can be used to apply different styles depending on the screen size. E.g. md:, hover:, etc\nAdvantages:\n\nDon’t need to know CSS.\nStyles are scoped to the component.\nConfigurable and extensible.\nRapid development (once you know the built-in class names).\n\nDisadvantages:\n\nLong class names.\nChanging styling requires modifying JSX.\nYou end up with lots of small wrapper components or lots of copy paste.\nYou need to learn the built-in class names."
  },
  {
    "objectID": "posts/software/react/6_styling/post.html#references",
    "href": "posts/software/react/6_styling/post.html#references",
    "title": "React: Styling",
    "section": "",
    "text": "Section 6 of “React: The Complete Guide” Udemy course"
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html",
    "href": "posts/software/react/2_js_essentials/post.html",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "These notes serve as a JavaScript refresher, and for me a gentle introduction to JavaScript coming from Python.\n\n\n\n\n\nJavascript is supported natively by the browser, so we can add it directly to the html of a webpage.\nWe can add it either in the body of the &lt;script&gt; tag, or preferably as a separate .js file that’s then called as the src parameter of the script tag.\nThe defer parameter means the script won’t be called until the rest of the body is loaded.\nThe type=module parameter means the JavaScript file will be treated as a module rather than executed as a script.\n\n\n\nJSX is not natively supported by the browser, so a build tool transforms it to regular JavaScript.\nIt also minifies the project to optimise the size and loading times.\n\n\n\nWe need to use the export keyword to make a function or variable available outside of that file. Each file can have at most one default export.\nThe import keyword then lets us use this.\nUse curly braces for the import unless it is a default export. If it is a default export, you assign your own name to the imported variable. The path to import from is in single or double quotes, with the file extension in plain JS. In React, some build tools automatically populate the file extension so you don’t need it.\nWe can group the imports if there are many by using starred imports.\nimport * as utils from “./utils.js”;\nThen use utils.blah to use those imported values.\nWe can also alias individual imported variables with the as keyword.\n\n\n\nThe primitives in JavaScript are: string, number, boolean, null, undefined.\nThe are also complex types built in: object, array.\nVariables are defined with the let keyword. Camel case is most common in JS.\nConstants are defined with the const keyword. They cannot be reassigned. Prefer const where it is appropriate, to be clear about your intentions that this should not be reassigned.\nOlder versions of JavaScript did not make this distinction and used var in all cases. This is discouraged now.\n\n\n\nThese include add, subtract, divide, multiply.\nThese can be defined on any types, not just numbers.\nTriple equals === is used to compare values.\n\n\n\nFunctions can be defined using “regular” syntax or “arrow” syntax.\nRegular syntax:\nfunction sum(a, b) {\n    return a + b;\n}\nArrow function syntax:\nconst sum = (a, b) =&gt; {a + b};\nThe function can then be invoked as\nsum(1, 3)\nWe can set default values of variables as\nconst sum = (a, b = 1) =&gt; {a + b};\nFunctions can be passed as props to other functions. This is helpful when defining components which we want to pass state setters or other handler functions to (functional components are ultimately just functions themselves).\nWe can also define functions inside of other functions. This is helpful when we want the function to be scoped only to the outer function, not defined globally. This is again used a lot in React since we may want to define functions with our (functional) components.\n\n\n\nObjects are key-value pairs. The value for a given key can be accessed with ., for example:\nobj.key1\nObjects can also have methods. These are functions defined inside the object.\nconst obj = {\n    name: “Gurp”,\n    method1: greet() {return “Hello “ + this.name}\n}\nobj.greet()\nClass instances are essentially objects like above. If we want to create a reusable class, we can formally define a class.\nclass User (\n    constructor(name, age) {\n        this.name = name;\n        this.age = age;\n    }\n\n    greet() {\n        return “Hello “ + this.name;\n    }\n)\n\nconst user1 = new User(\"Gurp\", 30);\nObjects (and, by extension, arrays) are passed by reference. So when we modify the object, it does not create a new object, it mutates the original. The memory address is stored as a constant, not the value. So if we create an object as a const, we can modify it without reassigning it.\n\n\n\nArrays are technically a special case of object.\nconst array1 = [1, 2, 3, 4];\nThere are some built-in utility methods of arrays that are particularly helpful/common:\n\n\n\nMethod\nExample\nDocs\n\n\n\n\npush\narray1.push(5);\npush docs\n\n\nmap\nconst squares = array1.map((item) =&gt; item * 2);\nmap docs\n\n\nfind\nconst found = array1.find((element) =&gt; element &gt; 3);\nfind docs\n\n\nfindIndex\narr.findIndex((item) =&gt; item===2)\nfindIndex docs\n\n\nfilter\nconst result = array1.filter((item) =&gt; item &gt; 2);\nfilter docs\n\n\nreduce\nconst summedArray = array1.reduce((accumulator, currentValue) =&gt; accumulator + currentValue);\nreduce docs\n\n\nconcat\nconst array3 = array1.concat(array2);\nconcat docs\n\n\nslice\narray1.slice(1,3)  // returns [2, 3]\nslice docs\n\n\nsplice\nmonths.splice(4, 1, 'May');  // Replaces 1 element at index 4\nsplice docs\n\n\n\n\n\n\nArray destructuring allows us to pick out the values of an array rather than assigning them one-by-one.\nconst [firstName, lastName] = [\"Gurp\", \"Johl\"]\ninstead of\nconst nameArray = [\"Gurp\", \"Johl\"]\nconst firstName = nameArray[0]\nconst lastName =  nameArray[1]\nSimilarly, we can destructure objects too. We can also alias the keys with a :, as in the example below where the name key is aliased to userName.\nconst {name: userName, age} = {name: 'Gurp', age: 30}\ninstead of\nconst userObj = {name: 'Gurp', age: 30}\nconst userName = userObj.name\nconst age = userObj.age\n\n\n\nThe spread operator pulls out values of arrays and objects. This is useful for merging multiple arrays, e.g.\nconst arrayA = [1, 2, 3]\nconst arrayB = [4, 5, 6]\nconst mergedArray = [...arrayA, ...arrayB]\nThe same applies to merging objects.\n\n\n\nIf-else clauses work similarly to other languages:\nif userName === \"Gurp\" {\n    // Do something\n} else {\n    // Do something else\n}\nFor loops again are similar, although the syntax looks a bit janky at first compared to Python:\nconst hobbies = [\"Sports\", \"Music\"];\n\nfor (const hobby of hobbies) {\n    console.log(hobby);\n}\n\n\n\n\nMDN docs\nSection 2 of “React: The Complete Guide” Udemy course"
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#adding-javascript-to-a-page",
    "href": "posts/software/react/2_js_essentials/post.html#adding-javascript-to-a-page",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "Javascript is supported natively by the browser, so we can add it directly to the html of a webpage.\nWe can add it either in the body of the &lt;script&gt; tag, or preferably as a separate .js file that’s then called as the src parameter of the script tag.\nThe defer parameter means the script won’t be called until the rest of the body is loaded.\nThe type=module parameter means the JavaScript file will be treated as a module rather than executed as a script."
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#react-projects-use-a-build-process.",
    "href": "posts/software/react/2_js_essentials/post.html#react-projects-use-a-build-process.",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "JSX is not natively supported by the browser, so a build tool transforms it to regular JavaScript.\nIt also minifies the project to optimise the size and loading times."
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#imports-and-exports",
    "href": "posts/software/react/2_js_essentials/post.html#imports-and-exports",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "We need to use the export keyword to make a function or variable available outside of that file. Each file can have at most one default export.\nThe import keyword then lets us use this.\nUse curly braces for the import unless it is a default export. If it is a default export, you assign your own name to the imported variable. The path to import from is in single or double quotes, with the file extension in plain JS. In React, some build tools automatically populate the file extension so you don’t need it.\nWe can group the imports if there are many by using starred imports.\nimport * as utils from “./utils.js”;\nThen use utils.blah to use those imported values.\nWe can also alias individual imported variables with the as keyword."
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#variables",
    "href": "posts/software/react/2_js_essentials/post.html#variables",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "The primitives in JavaScript are: string, number, boolean, null, undefined.\nThe are also complex types built in: object, array.\nVariables are defined with the let keyword. Camel case is most common in JS.\nConstants are defined with the const keyword. They cannot be reassigned. Prefer const where it is appropriate, to be clear about your intentions that this should not be reassigned.\nOlder versions of JavaScript did not make this distinction and used var in all cases. This is discouraged now."
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#operators",
    "href": "posts/software/react/2_js_essentials/post.html#operators",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "These include add, subtract, divide, multiply.\nThese can be defined on any types, not just numbers.\nTriple equals === is used to compare values."
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#functions",
    "href": "posts/software/react/2_js_essentials/post.html#functions",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "Functions can be defined using “regular” syntax or “arrow” syntax.\nRegular syntax:\nfunction sum(a, b) {\n    return a + b;\n}\nArrow function syntax:\nconst sum = (a, b) =&gt; {a + b};\nThe function can then be invoked as\nsum(1, 3)\nWe can set default values of variables as\nconst sum = (a, b = 1) =&gt; {a + b};\nFunctions can be passed as props to other functions. This is helpful when defining components which we want to pass state setters or other handler functions to (functional components are ultimately just functions themselves).\nWe can also define functions inside of other functions. This is helpful when we want the function to be scoped only to the outer function, not defined globally. This is again used a lot in React since we may want to define functions with our (functional) components."
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#objects",
    "href": "posts/software/react/2_js_essentials/post.html#objects",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "Objects are key-value pairs. The value for a given key can be accessed with ., for example:\nobj.key1\nObjects can also have methods. These are functions defined inside the object.\nconst obj = {\n    name: “Gurp”,\n    method1: greet() {return “Hello “ + this.name}\n}\nobj.greet()\nClass instances are essentially objects like above. If we want to create a reusable class, we can formally define a class.\nclass User (\n    constructor(name, age) {\n        this.name = name;\n        this.age = age;\n    }\n\n    greet() {\n        return “Hello “ + this.name;\n    }\n)\n\nconst user1 = new User(\"Gurp\", 30);\nObjects (and, by extension, arrays) are passed by reference. So when we modify the object, it does not create a new object, it mutates the original. The memory address is stored as a constant, not the value. So if we create an object as a const, we can modify it without reassigning it."
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#arrays",
    "href": "posts/software/react/2_js_essentials/post.html#arrays",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "Arrays are technically a special case of object.\nconst array1 = [1, 2, 3, 4];\nThere are some built-in utility methods of arrays that are particularly helpful/common:\n\n\n\nMethod\nExample\nDocs\n\n\n\n\npush\narray1.push(5);\npush docs\n\n\nmap\nconst squares = array1.map((item) =&gt; item * 2);\nmap docs\n\n\nfind\nconst found = array1.find((element) =&gt; element &gt; 3);\nfind docs\n\n\nfindIndex\narr.findIndex((item) =&gt; item===2)\nfindIndex docs\n\n\nfilter\nconst result = array1.filter((item) =&gt; item &gt; 2);\nfilter docs\n\n\nreduce\nconst summedArray = array1.reduce((accumulator, currentValue) =&gt; accumulator + currentValue);\nreduce docs\n\n\nconcat\nconst array3 = array1.concat(array2);\nconcat docs\n\n\nslice\narray1.slice(1,3)  // returns [2, 3]\nslice docs\n\n\nsplice\nmonths.splice(4, 1, 'May');  // Replaces 1 element at index 4\nsplice docs"
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#destructuring",
    "href": "posts/software/react/2_js_essentials/post.html#destructuring",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "Array destructuring allows us to pick out the values of an array rather than assigning them one-by-one.\nconst [firstName, lastName] = [\"Gurp\", \"Johl\"]\ninstead of\nconst nameArray = [\"Gurp\", \"Johl\"]\nconst firstName = nameArray[0]\nconst lastName =  nameArray[1]\nSimilarly, we can destructure objects too. We can also alias the keys with a :, as in the example below where the name key is aliased to userName.\nconst {name: userName, age} = {name: 'Gurp', age: 30}\ninstead of\nconst userObj = {name: 'Gurp', age: 30}\nconst userName = userObj.name\nconst age = userObj.age"
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#spread-operator",
    "href": "posts/software/react/2_js_essentials/post.html#spread-operator",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "The spread operator pulls out values of arrays and objects. This is useful for merging multiple arrays, e.g.\nconst arrayA = [1, 2, 3]\nconst arrayB = [4, 5, 6]\nconst mergedArray = [...arrayA, ...arrayB]\nThe same applies to merging objects."
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#control-structures",
    "href": "posts/software/react/2_js_essentials/post.html#control-structures",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "If-else clauses work similarly to other languages:\nif userName === \"Gurp\" {\n    // Do something\n} else {\n    // Do something else\n}\nFor loops again are similar, although the syntax looks a bit janky at first compared to Python:\nconst hobbies = [\"Sports\", \"Music\"];\n\nfor (const hobby of hobbies) {\n    console.log(hobby);\n}"
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#references",
    "href": "posts/software/react/2_js_essentials/post.html#references",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "MDN docs\nSection 2 of “React: The Complete Guide” Udemy course"
  },
  {
    "objectID": "posts/software/react/30_typescript/post.html",
    "href": "posts/software/react/30_typescript/post.html",
    "title": "React: TypeScript Essentials",
    "section": "",
    "text": "TypeScript (TS) is a superset of JavaScript (JS) which adds static typing.\nVanilla JS is dynamically typed. Types do not have to be specified ahead of time. This can result in funky errors like the classic “1” + “2”= “12”.\n\n\n\nIt can be installed like any other package and managed in the project dependencies package.json\nnpm install typescript \nTypescript is compiled. It does not run in the browser directly. Instead, there is a compilation step which converts TS to JS which can run in the browser.\nThis compile step is where we will find type errors, before they hit production.\nnpx tsc\n\n\n\n\n\nThe primitives are number, string, boolean. We also have null and undefined.\nNote that primitive types are lowercase, e.g. number. The object itself, e.g. Number, is not what we want here.\nThere is also an any type which is a catch all. We generally avoid this, as it defeats the purpose of using TS.\n\n\n\nWe have built-in complex types: objects and arrays.\nDefine an array of strings like:\nlet myArray: string[] = [“this”, “is”, “an”, “array”];\nDefine an object type by specifying the keys and their types:\nlet person: {name: string, age: number} = {name: “Gurp”, age: 30}\n\n\n\nUse a pipe to denote where multiple types are allowed, e.g.\nlet val: string|number = 69;\n\n\n\nDefine a type with the type keyword.\nThis allows the type to be reusable if it’s used in multiple places.\n\n\n\nTS infers the output type based on the arguments.\nIf this is correct, it’s common practice to not override this, let it infer. But you may want to override if you want it to output a union of types which it hasn’t inferred.\nFunctions also have a special void return type if they do not return anything.\n\n\n\nIf you have a utility function that can accept any input type, but the output should be the same type as the input, you can denote this using generic types with angled brackets.\nconst myFunc&lt;T&gt;(inputArray: T[], inputValue: T) {\n    return [inputValue, …inputArray]\n}\nThis can be called with strings and would return an array of strings. Or called with numbers and return an array of numbers. Rather than having to use any, we can use the generic (T is arbitrary and just stands for Type) then when we call the function TypeScript will infer the output type correctly.\n\n\n\nThese are often interchangeable. The key difference is that interfaces can be extended, whereas type cannot.\nSee here\n\n\n\nWhen defining a class, you can specify the types of each attribute.\nclass Todo {\n    id: string;\n    text: string;\n\n    constructor(inputText: string) {\n        this.text = inputText;\n        this.id = new Date().toISOString();\n    }\n}\nWhen instances of this class are used, you can simply use the class itself as the type.\nThis is useful for defining data models.\n\n\n\n\nCreating a react project using TypeScript is largely the same, but you will have .tsx files rather than .jsx.\nCertain packages that you install may have additional type annotations packages if they were written in JS, to make them they play nicely with TS. Some packages don’t need it if they were written in TS to begin with.\n\n\nWhen passing props in React, it automatically passes certain default props like children. It would be cumbersome if we had to manually define the types of those default props on every component.\nInstead, we can set the output type of our component as React.FC (Functional Component) and this will handle the default props.\nIf we then want to define our custom prop types, we can do so in angle brackets after:\nReact.FC&lt;{prop1: string, prop2: number}&gt;\nHere we are using a generic type, React.FC. The angled brackets are defining what types are being used in this particular case for this generic type.\nProps are marked as optional by adding a ? after the variable name, i.e. the key in the object.\n\n\n\nThe form submit outputs an event object which can be used by other functions.\nThe type of this can be encapsulated by the React.FormEvent type. Similarly, there is a React.MouseEvent for the onClick listener.\n\n\n\nWe create a ref with useRef then attach it to a component (can be built-in or custom).\nTypeScript doesn’t know which component you intend to attach the ref to, so you need to specify this when creating the ref.\nBy default, useRef returns a generic type, so we need to set the specific type when we call it. We also need to provide a starting value (null) to convince the TypeScript compiler that the ref isn’t already assigned to something else.\nconst inputRef = useRef&lt;HTMLInputElement&gt;(null);\nThen use this in an input element\n&lt;input ref={inputRef} /&gt;\nWhen working with ref.current TypeScript will often demand a ? to indicate that this is possibly null. The resulting value’s type will then be, for example, string or null. If you know it will never be null, you can replace with the ! operator. This means the resulting value will have type string only.\n\n\n\nWhere we pass a function as a prop we define its type as an arrow function specifying inputs and outputs.\nmyFunc: (text: string) =&gt; void\nNote that this is similar to how object types look like an object but don’t actually create an object. Function types look like a function but don’t actually create a function.\nThe .bind method is similar to partial in Python. This is useful when we are passing a function down a prop chain and it will always have a certain argument. Bind saves us having to pass the value and declare its type and every stage of the prop chain.\n\n\n\nThe compilerOptions.target value defines which version of JavaScript the TypeScript compiler will transform the code to.\nThe compilerOptions.lib value defines which TypeScript default types are included out of the box. For example, “DOM” gives support for built-in html types like HTMLInputType.\nIf we want to allow plain JavaScript files in the project alongside TypeScript, we can set compilerOptions.allowJs to True. If False, everything must strictly be TypeScript.\nWe can set a strict compile with compilerOptions.strict. This will forbid implicit any types etc.\n\n\n\nWhen we create a state, we often initialise it with an empty value, e.g. null or an empty array. But then TypeScript does not know what type is going to go in that state later.\nThe useState function returns a generic type so we can overwrite it with our type.\nconst [myState, setMyState] = useState&lt;string[]&gt;([])\n\n\n\n\n\nSection 30 of “React: The Complete Guide” Udemy course\nTypes vs Interfaces"
  },
  {
    "objectID": "posts/software/react/30_typescript/post.html#wtf-is-ts",
    "href": "posts/software/react/30_typescript/post.html#wtf-is-ts",
    "title": "React: TypeScript Essentials",
    "section": "",
    "text": "TypeScript (TS) is a superset of JavaScript (JS) which adds static typing.\nVanilla JS is dynamically typed. Types do not have to be specified ahead of time. This can result in funky errors like the classic “1” + “2”= “12”."
  },
  {
    "objectID": "posts/software/react/30_typescript/post.html#installing-and-using-ts.",
    "href": "posts/software/react/30_typescript/post.html#installing-and-using-ts.",
    "title": "React: TypeScript Essentials",
    "section": "",
    "text": "It can be installed like any other package and managed in the project dependencies package.json\nnpm install typescript \nTypescript is compiled. It does not run in the browser directly. Instead, there is a compilation step which converts TS to JS which can run in the browser.\nThis compile step is where we will find type errors, before they hit production.\nnpx tsc"
  },
  {
    "objectID": "posts/software/react/30_typescript/post.html#types",
    "href": "posts/software/react/30_typescript/post.html#types",
    "title": "React: TypeScript Essentials",
    "section": "",
    "text": "The primitives are number, string, boolean. We also have null and undefined.\nNote that primitive types are lowercase, e.g. number. The object itself, e.g. Number, is not what we want here.\nThere is also an any type which is a catch all. We generally avoid this, as it defeats the purpose of using TS.\n\n\n\nWe have built-in complex types: objects and arrays.\nDefine an array of strings like:\nlet myArray: string[] = [“this”, “is”, “an”, “array”];\nDefine an object type by specifying the keys and their types:\nlet person: {name: string, age: number} = {name: “Gurp”, age: 30}\n\n\n\nUse a pipe to denote where multiple types are allowed, e.g.\nlet val: string|number = 69;\n\n\n\nDefine a type with the type keyword.\nThis allows the type to be reusable if it’s used in multiple places.\n\n\n\nTS infers the output type based on the arguments.\nIf this is correct, it’s common practice to not override this, let it infer. But you may want to override if you want it to output a union of types which it hasn’t inferred.\nFunctions also have a special void return type if they do not return anything.\n\n\n\nIf you have a utility function that can accept any input type, but the output should be the same type as the input, you can denote this using generic types with angled brackets.\nconst myFunc&lt;T&gt;(inputArray: T[], inputValue: T) {\n    return [inputValue, …inputArray]\n}\nThis can be called with strings and would return an array of strings. Or called with numbers and return an array of numbers. Rather than having to use any, we can use the generic (T is arbitrary and just stands for Type) then when we call the function TypeScript will infer the output type correctly.\n\n\n\nThese are often interchangeable. The key difference is that interfaces can be extended, whereas type cannot.\nSee here\n\n\n\nWhen defining a class, you can specify the types of each attribute.\nclass Todo {\n    id: string;\n    text: string;\n\n    constructor(inputText: string) {\n        this.text = inputText;\n        this.id = new Date().toISOString();\n    }\n}\nWhen instances of this class are used, you can simply use the class itself as the type.\nThis is useful for defining data models."
  },
  {
    "objectID": "posts/software/react/30_typescript/post.html#typescript-with-react",
    "href": "posts/software/react/30_typescript/post.html#typescript-with-react",
    "title": "React: TypeScript Essentials",
    "section": "",
    "text": "Creating a react project using TypeScript is largely the same, but you will have .tsx files rather than .jsx.\nCertain packages that you install may have additional type annotations packages if they were written in JS, to make them they play nicely with TS. Some packages don’t need it if they were written in TS to begin with.\n\n\nWhen passing props in React, it automatically passes certain default props like children. It would be cumbersome if we had to manually define the types of those default props on every component.\nInstead, we can set the output type of our component as React.FC (Functional Component) and this will handle the default props.\nIf we then want to define our custom prop types, we can do so in angle brackets after:\nReact.FC&lt;{prop1: string, prop2: number}&gt;\nHere we are using a generic type, React.FC. The angled brackets are defining what types are being used in this particular case for this generic type.\nProps are marked as optional by adding a ? after the variable name, i.e. the key in the object.\n\n\n\nThe form submit outputs an event object which can be used by other functions.\nThe type of this can be encapsulated by the React.FormEvent type. Similarly, there is a React.MouseEvent for the onClick listener.\n\n\n\nWe create a ref with useRef then attach it to a component (can be built-in or custom).\nTypeScript doesn’t know which component you intend to attach the ref to, so you need to specify this when creating the ref.\nBy default, useRef returns a generic type, so we need to set the specific type when we call it. We also need to provide a starting value (null) to convince the TypeScript compiler that the ref isn’t already assigned to something else.\nconst inputRef = useRef&lt;HTMLInputElement&gt;(null);\nThen use this in an input element\n&lt;input ref={inputRef} /&gt;\nWhen working with ref.current TypeScript will often demand a ? to indicate that this is possibly null. The resulting value’s type will then be, for example, string or null. If you know it will never be null, you can replace with the ! operator. This means the resulting value will have type string only.\n\n\n\nWhere we pass a function as a prop we define its type as an arrow function specifying inputs and outputs.\nmyFunc: (text: string) =&gt; void\nNote that this is similar to how object types look like an object but don’t actually create an object. Function types look like a function but don’t actually create a function.\nThe .bind method is similar to partial in Python. This is useful when we are passing a function down a prop chain and it will always have a certain argument. Bind saves us having to pass the value and declare its type and every stage of the prop chain.\n\n\n\nThe compilerOptions.target value defines which version of JavaScript the TypeScript compiler will transform the code to.\nThe compilerOptions.lib value defines which TypeScript default types are included out of the box. For example, “DOM” gives support for built-in html types like HTMLInputType.\nIf we want to allow plain JavaScript files in the project alongside TypeScript, we can set compilerOptions.allowJs to True. If False, everything must strictly be TypeScript.\nWe can set a strict compile with compilerOptions.strict. This will forbid implicit any types etc.\n\n\n\nWhen we create a state, we often initialise it with an empty value, e.g. null or an empty array. But then TypeScript does not know what type is going to go in that state later.\nThe useState function returns a generic type so we can overwrite it with our type.\nconst [myState, setMyState] = useState&lt;string[]&gt;([])"
  },
  {
    "objectID": "posts/software/react/30_typescript/post.html#references",
    "href": "posts/software/react/30_typescript/post.html#references",
    "title": "React: TypeScript Essentials",
    "section": "",
    "text": "Section 30 of “React: The Complete Guide” Udemy course\nTypes vs Interfaces"
  },
  {
    "objectID": "posts/software/react/23_deployment/post.html",
    "href": "posts/software/react/23_deployment/post.html",
    "title": "React: Deployment",
    "section": "",
    "text": "We should think about the following when we’re ready to deploy our app:\n\nTest code\nOptimise code\nBuild app\nUpload app\nConfigure server\n\nTesting is handled in a separate post.\nThis post contains some optimisation, build and configuration considerations.\n\n\nLoad code only when it’s needed.\nWhen we import files into other files, they are immediately resolved. This means that we need to load everything before any part of the site loads.\nWith lazy loading, we only load each file as it is needed by the site.\nInstead of:\nimport BlogPage from './pages/Blog';\nWe can use:\nimport { lazy, Suspense } from 'react';\n\nconst BlogPage = lazy(() =&gt; import('./pages/Blog'));\n\n// Then wherever the BlogPage component is used, wrap it with a Suspense component\n&lt;Suspense fallback={&lt;p&gt;Loading...&lt;/p&gt;}&gt;\n  &lt;BlogPost /&gt;\n&lt;/Suspense&gt;\nYou can verify how lazy loading works by looking at the network tab of the browser while navigating the website.\nIt should only load pages as required while you navigate.\n\n\n\nRunning npm run build will create an optimised build which transforms React code to Javascript, CSS and HTML, which are supported natively by browsers.\nThe contents of this build directory should be uploaded to the hosting server.\nA React Single-Page Application (SPA) is a static website. It does not require code to be executed by the server.\nThere are many static site hosts, e.g. Github Pages, Firebase.\n\n\n\nClient-side routing is commonly used in smaller apps to keep it as an SPA.\nThere are multiple “pages” which are handled by react-router. So the server only ever actually returns a single page, index.html, regardless of the URL and the page routing is handled in JavaScript on the client side.\nThis is in contrast to server-side routing, where each page is a separate html file, so different URLs return different HTML pages.\nIf creating the website as an SPA, the deployment job should be configured for this so that the server correctly resolves the different URLs internally rather than trying to serve different HTML files.\n\n\n\n\nSection 23 of “React: The Complete Guide” Udemy course"
  },
  {
    "objectID": "posts/software/react/23_deployment/post.html#lazy-loading",
    "href": "posts/software/react/23_deployment/post.html#lazy-loading",
    "title": "React: Deployment",
    "section": "",
    "text": "Load code only when it’s needed.\nWhen we import files into other files, they are immediately resolved. This means that we need to load everything before any part of the site loads.\nWith lazy loading, we only load each file as it is needed by the site.\nInstead of:\nimport BlogPage from './pages/Blog';\nWe can use:\nimport { lazy, Suspense } from 'react';\n\nconst BlogPage = lazy(() =&gt; import('./pages/Blog'));\n\n// Then wherever the BlogPage component is used, wrap it with a Suspense component\n&lt;Suspense fallback={&lt;p&gt;Loading...&lt;/p&gt;}&gt;\n  &lt;BlogPost /&gt;\n&lt;/Suspense&gt;\nYou can verify how lazy loading works by looking at the network tab of the browser while navigating the website.\nIt should only load pages as required while you navigate."
  },
  {
    "objectID": "posts/software/react/23_deployment/post.html#building-code-for-production",
    "href": "posts/software/react/23_deployment/post.html#building-code-for-production",
    "title": "React: Deployment",
    "section": "",
    "text": "Running npm run build will create an optimised build which transforms React code to Javascript, CSS and HTML, which are supported natively by browsers.\nThe contents of this build directory should be uploaded to the hosting server.\nA React Single-Page Application (SPA) is a static website. It does not require code to be executed by the server.\nThere are many static site hosts, e.g. Github Pages, Firebase."
  },
  {
    "objectID": "posts/software/react/23_deployment/post.html#server-side-routing",
    "href": "posts/software/react/23_deployment/post.html#server-side-routing",
    "title": "React: Deployment",
    "section": "",
    "text": "Client-side routing is commonly used in smaller apps to keep it as an SPA.\nThere are multiple “pages” which are handled by react-router. So the server only ever actually returns a single page, index.html, regardless of the URL and the page routing is handled in JavaScript on the client side.\nThis is in contrast to server-side routing, where each page is a separate html file, so different URLs return different HTML pages.\nIf creating the website as an SPA, the deployment job should be configured for this so that the server correctly resolves the different URLs internally rather than trying to serve different HTML files."
  },
  {
    "objectID": "posts/software/react/23_deployment/post.html#references",
    "href": "posts/software/react/23_deployment/post.html#references",
    "title": "React: Deployment",
    "section": "",
    "text": "Section 23 of “React: The Complete Guide” Udemy course"
  },
  {
    "objectID": "posts/ml/graph_ml/part_1/graph_notes.html",
    "href": "posts/ml/graph_ml/part_1/graph_notes.html",
    "title": "Graph ML: What’s a Graph?",
    "section": "",
    "text": "Graphs are a generic way of representing relationships (edges) between entities (nodes).\nThis makes them useful in a wide variety of applications, including modelling biological pathways, social networks, or even images.\nGraphs do not have a fixed structure and the relationships (edges) can be time-varying.\n\n\nEntities are nodes or vertices and their relationships are edges.\nNodes that are connected are adjacent and are called neighbours.\nGraphs are the general case of trees. A graph is broadly something with nodes and edges. A tree is a graph that has no cycles and all nodes are connected.\nA directed graph or digraph shows where relationships can be directional, e.g. social network followers.\nEdges can be weighted to represent quantities like distance or cost.\nA path is the sequence of edges used to get from one vertex to another.\nA simple path does not visit any node more than once.\nA graph is connected if there is a path between every pair of vertices, i.e. there are no isolated nodes or clusters.\nA cycle is a path where the first and last nodes are the same.\n\n\n\n\nTree: An undirected graph with no cycles.\nRooted tree: A tree where one node is designated ads the root.\nDirected Acyclic Graph (DAG): A directed graph with no cycles. This means edges can only be traversed in a particular direction.\nBipartite graphs: A graph where vertices can be divided into two disjoint sets.\nComplete graph: A graph where every pair of vertices is connected by an edge, i.e. it is fully connected.\n\n\n\n\n\nGraphs are an abstract data structure, so there are multiple ways we can represent them in memory.\nSome common options are:\n\nObjects and pointers\nAdjacency lists\nEdge lists\nAdjacency matrix\n\nEach approach has its own trade-offs in terms of memory usage, ease of implementation, and efficiency based on the specific characteristics of the graph you’re dealing with.\nWe’ll implement the following simple graph using each approach:\n\n\n\n\n\nflowchart LR\n\nA[1] &lt;---&gt; B[2]\nB[2] &lt;---&gt; C[3]\n\n\n\n\n\n\n\n\nIn this approach, we represent each node in the graph as an object, and you use pointers to connect these nodes. Each object typically contains a list of pointers to other nodes it’s connected to.\nFor a connected graph, if we have access to one node we can traverse the graph to find all other nodes. This is not true for graphs with unconnected nodes.\nThis approach will be familiar to computer scientists - it bears a resemblance to linked lists. In practice this isn’t used much in practical ML because traversing the graph is more cumbersome.\nPros:\n\nAllows for flexible representation of complex relationships.\nIdeal for graphs where nodes have additional properties beyond just connectivity.\n\nCons:\n\nMemory overhead due to object instantiation and pointer storage.\nCan be complex to implement and manage.\nIf the graph is not connected, an additional data structure is needed to keep track of isolated nodes.\n\nPython Implementation:\n\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.neighbors = []\n\n    def __repr__(self) -&gt; str:\n        return f\"Node {self.value} with neighbors {[k.value for k in self.neighbors]}\"\n\ndef add_edge(node1, node2):\n    node1.neighbors.append(node2)\n    node2.neighbors.append(node1)\n\nnode1 = Node(1)\nnode2 = Node(2)\nnode3 = Node(3)\n\nadd_edge(node1, node2)\nadd_edge(node2, node3)\n\n\nprint(node1)\n\nNode 1 with neighbors [2]\n\n\n\nprint(node2)\n\nNode 2 with neighbors [1, 3]\n\n\n\nprint(node3)\n\nNode 3 with neighbors [2]\n\n\n\n\n\nIn this approach, we store a list of edges per node.\nWe use a list or dictionary where the index or key represents a node, and the value is a list of its adjacent nodes.\nPros:\n\nEfficient memory usage for sparse graphs - \\(O(V + E)\\) space complexity.\nEasy to implement and understand.\nSuitable for graphs with varying degrees of connectivity.\nEasy to iterate through neighbors.\n\nCons:\n\nSlower for dense graphs.\nChecking if nodes are connected is slow - retrieving edge information may require linear search.\n\nPython Implementation:\n\nclass Graph:\n    def __init__(self):\n        self.adj_list = {}\n\n    def __repr__(self) -&gt; str:\n        return str(self.adj_list)\n\n    def add_edge(self, node1, node2):\n        if node1 not in self.adj_list:\n            self.adj_list[node1] = []\n        if node2 not in self.adj_list:\n            self.adj_list[node2] = []\n        self.adj_list[node1].append(node2)\n        self.adj_list[node2].append(node1)\n\n\ngraph = Graph()\ngraph.add_edge(1, 2)\ngraph.add_edge(2, 3)\n\nprint(graph)\n\n{1: [2], 2: [1, 3], 3: [2]}\n\n\n\n\n\nAn even more compact form is the edge list, which lists all of the edges as (source_node, target_node) tuples.\nPros:\n\nEven more efficient memory usage for sparse graphs - \\(O(E)\\) space complexity.\n\nCons:\n\nSlower for dense graphs.\nChecking if nodes are connected is slow - retrieving edge information may require linear search.\n\nPython Implementation:\n\nclass Graph:\n    def __init__(self):\n        self.edge_list = []\n\n    def __repr__(self) -&gt; str:\n        return str(self.edge_list)\n\n    def add_edge(self, node1, node2):\n        node_pair = (node1, node2)\n        if node_pair not in self.edge_list:\n            self.edge_list.append(node_pair)\n\n\ngraph = Graph()\ngraph.add_edge(1, 2)\ngraph.add_edge(2, 3)\n\nprint(graph)\n\n[(1, 2), (2, 3)]\n\n\n\n\n\nIn this approach, we represent the graph as a 2D matrix where rows and columns represent nodes, and matrix cells indicate whether there’s an edge between the nodes.\nFor weighted graphs, the values in the matrix correspond to the weights of the edges.\nPros:\n\nEfficient for dense graphs.\nConstant time access to check edge existence; checking if nodes are connected is \\(O(1)\\).\n\nCons:\n\nNot ideal for graphs with a large number of nodes due to \\(O(V^2)\\) space complexity. This is particularly bad for sparse graphs.\nAdding/removing nodes can be expensive as it requires resizing the matrix. Bad for dynamic graphs.\n\nPython Implementation:\n\nclass Graph:\n    def __init__(self, num_nodes):\n        self.adj_matrix = [[0] * num_nodes for row in range(num_nodes)]\n\n    def __repr__(self) -&gt; str:\n        return str(self.adj_matrix)\n\n    def add_edge(self, node1, node2):\n        self.adj_matrix[node1][node2] = 1\n        self.adj_matrix[node2][node1] = 1\n\n\ngraph = Graph(num_nodes=3)\ngraph.add_edge(0, 1)\ngraph.add_edge(1, 2)\n\nprint(graph)\n\n[[0, 1, 0], [1, 0, 1], [0, 1, 0]]\n\n\nThe adjacency matrix approach is useful in machine learning as it naturally fits into a tensor representation, which most ML libraries (e.g. pytorch, tensorflow) play nicely with. So we’ll stick with tthat going forward.\n\n\n\n\nGrpahs are common enough to work with that we don’t need to implement them from scratch every time we want ot use them (although doing so in the previous section is instructive).\nnetworkx is a useful third-party library for this task\n\nimport networkx as nx\n\n\ngraph = nx.Graph()\ngraph.add_edges_from([\n    ('A', 'B'),\n    ('A', 'C'), \n    ('B', 'D'), \n    ('B', 'E'), \n    ('C', 'F'), \n    ('C', 'G'), \n    ('G', 'G'), \n])\n\nprint(graph)\n\nGraph with 7 nodes and 7 edges\n\n\nFrom this Graph object we can see our familiar edge list:\n\ngraph.adj\n\nAdjacencyView({'A': {'B': {}, 'C': {}}, 'B': {'A': {}, 'D': {}, 'E': {}}, 'C': {'A': {}, 'F': {}, 'G': {}}, 'D': {'B': {}}, 'E': {'B': {}}, 'F': {'C': {}}, 'G': {'C': {}, 'G': {}}})\n\n\nOr the adjacency matrix:\n\nnx.adjacency_matrix(graph).todense()\n\n/var/folders/8k/8jqhnfbd1t99blb07r1hs5440000gn/T/ipykernel_50230/1468791322.py:1: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n  nx.adjacency_matrix(graph).todense()\n\n\nmatrix([[0, 1, 1, 0, 0, 0, 0],\n        [1, 0, 0, 1, 1, 0, 0],\n        [1, 0, 0, 0, 0, 1, 1],\n        [0, 1, 0, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0, 0, 0],\n        [0, 0, 1, 0, 0, 0, 0],\n        [0, 0, 1, 0, 0, 0, 1]])\n\n\n\n\n\n\n\nThis is the number of edges which are incident on a node.\nFor an undirected graph, this is the number of edges connected to the node. Self-loops add 2 to the degree.\nFor a directed graph, we can distinguish the indegree, \\(deg^-(V)\\), as the edges that point towards the node. The outdegree, \\(deg^+(V)\\), denotes the edges that start from the node. Self-loops add 1 to the indegree and 1 to the outdegree.\n\ngraph.degree\n\nDegreeView({'A': 2, 'B': 3, 'C': 3, 'D': 1, 'E': 1, 'F': 1, 'G': 3})\n\n\n\n\n\nCentrality measures the importance of a given node in a network.\nThere a different formulations of centrality:\n\nDegree centrality: This is simply the degree of the node. How many neighbours?\nCloseness centrality: The average length of the shortest path between this node and all others. How fast can I reach my neighbours?\nBetweenness centrality: The number of times a node lies on the shortest path between pairs of other nodes in the graph. How often am I the bottleneck/bridge for others?\n\n\nnx.degree_centrality(graph)\n\n{'A': 0.3333333333333333,\n 'B': 0.5,\n 'C': 0.5,\n 'D': 0.16666666666666666,\n 'E': 0.16666666666666666,\n 'F': 0.16666666666666666,\n 'G': 0.5}\n\n\n\nnx.closeness_centrality(graph)\n\n{'A': 0.6,\n 'B': 0.5454545454545454,\n 'C': 0.5454545454545454,\n 'D': 0.375,\n 'E': 0.375,\n 'F': 0.375,\n 'G': 0.375}\n\n\n\nnx.betweenness_centrality(graph)\n\n{'A': 0.6, 'B': 0.6, 'C': 0.6, 'D': 0.0, 'E': 0.0, 'F': 0.0, 'G': 0.0}\n\n\n\n\n\n\n\nThis is the minimum number of edges that would need to be removed to make the graph disconnected.\nHow easy is it to create isolated nodes/clusters?\nThis is a measure of connectedness.\n\n\n\nThis is the ratio of the number of edges in the grap to the maximum possible number of edges between all nodes.\nFor a directed graph with \\(n\\) nodes, the maximum number of edges is \\(n(n-1)\\).\nFor an undirected graph with \\(n\\) nodes, the maximum number of edges is \\(\\frac{n(n-1)}{2}\\).\nBroadly speaking, a graph is considered dense if \\(density &gt; 0.5\\) and sparse if \\(density &lt; 0.1\\).\n\n\n\n\n\nThere are several categories of learning tasks we may want to perform on a graph:\n\nNode classification: Predict the category of each node. E.g. categorising songs by genre.\nLink prediction: Predict missing links between nodes. E.g. friend recommnedations in a social network.\nGraph classification: Categorise an entire graph.\nGraph generation: Generate a new graph based on desired properties.\n\nThere are several prominent families of graph learning techniques:\n\nGraph signal processing: Apply traditional signal processing techniques like Fourier analysis to graphs to study its connectivity and structure.\nMatrix factorisation: Find low-dimensional representations of large matrices\nRandom walk: Simulate random walks over a graph, e.g. to generate training date for downstream models.\nDeep learning: Encode the graph as tensors and perform deep learning.\n\nTraditional tabular datasets focus on the entities which are represented as rows. But often, the relationships between entities contain vital information. Global features, such as network-wide statistics, may also provide useful information."
  },
  {
    "objectID": "posts/ml/graph_ml/part_1/graph_notes.html#graphs-what-and-why",
    "href": "posts/ml/graph_ml/part_1/graph_notes.html#graphs-what-and-why",
    "title": "Graph ML: What’s a Graph?",
    "section": "",
    "text": "Graphs are a generic way of representing relationships (edges) between entities (nodes).\nThis makes them useful in a wide variety of applications, including modelling biological pathways, social networks, or even images.\nGraphs do not have a fixed structure and the relationships (edges) can be time-varying.\n\n\nEntities are nodes or vertices and their relationships are edges.\nNodes that are connected are adjacent and are called neighbours.\nGraphs are the general case of trees. A graph is broadly something with nodes and edges. A tree is a graph that has no cycles and all nodes are connected.\nA directed graph or digraph shows where relationships can be directional, e.g. social network followers.\nEdges can be weighted to represent quantities like distance or cost.\nA path is the sequence of edges used to get from one vertex to another.\nA simple path does not visit any node more than once.\nA graph is connected if there is a path between every pair of vertices, i.e. there are no isolated nodes or clusters.\nA cycle is a path where the first and last nodes are the same.\n\n\n\n\nTree: An undirected graph with no cycles.\nRooted tree: A tree where one node is designated ads the root.\nDirected Acyclic Graph (DAG): A directed graph with no cycles. This means edges can only be traversed in a particular direction.\nBipartite graphs: A graph where vertices can be divided into two disjoint sets.\nComplete graph: A graph where every pair of vertices is connected by an edge, i.e. it is fully connected."
  },
  {
    "objectID": "posts/ml/graph_ml/part_1/graph_notes.html#implementing-a-graph-from-scratch",
    "href": "posts/ml/graph_ml/part_1/graph_notes.html#implementing-a-graph-from-scratch",
    "title": "Graph ML: What’s a Graph?",
    "section": "",
    "text": "Graphs are an abstract data structure, so there are multiple ways we can represent them in memory.\nSome common options are:\n\nObjects and pointers\nAdjacency lists\nEdge lists\nAdjacency matrix\n\nEach approach has its own trade-offs in terms of memory usage, ease of implementation, and efficiency based on the specific characteristics of the graph you’re dealing with.\nWe’ll implement the following simple graph using each approach:\n\n\n\n\n\nflowchart LR\n\nA[1] &lt;---&gt; B[2]\nB[2] &lt;---&gt; C[3]\n\n\n\n\n\n\n\n\nIn this approach, we represent each node in the graph as an object, and you use pointers to connect these nodes. Each object typically contains a list of pointers to other nodes it’s connected to.\nFor a connected graph, if we have access to one node we can traverse the graph to find all other nodes. This is not true for graphs with unconnected nodes.\nThis approach will be familiar to computer scientists - it bears a resemblance to linked lists. In practice this isn’t used much in practical ML because traversing the graph is more cumbersome.\nPros:\n\nAllows for flexible representation of complex relationships.\nIdeal for graphs where nodes have additional properties beyond just connectivity.\n\nCons:\n\nMemory overhead due to object instantiation and pointer storage.\nCan be complex to implement and manage.\nIf the graph is not connected, an additional data structure is needed to keep track of isolated nodes.\n\nPython Implementation:\n\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.neighbors = []\n\n    def __repr__(self) -&gt; str:\n        return f\"Node {self.value} with neighbors {[k.value for k in self.neighbors]}\"\n\ndef add_edge(node1, node2):\n    node1.neighbors.append(node2)\n    node2.neighbors.append(node1)\n\nnode1 = Node(1)\nnode2 = Node(2)\nnode3 = Node(3)\n\nadd_edge(node1, node2)\nadd_edge(node2, node3)\n\n\nprint(node1)\n\nNode 1 with neighbors [2]\n\n\n\nprint(node2)\n\nNode 2 with neighbors [1, 3]\n\n\n\nprint(node3)\n\nNode 3 with neighbors [2]\n\n\n\n\n\nIn this approach, we store a list of edges per node.\nWe use a list or dictionary where the index or key represents a node, and the value is a list of its adjacent nodes.\nPros:\n\nEfficient memory usage for sparse graphs - \\(O(V + E)\\) space complexity.\nEasy to implement and understand.\nSuitable for graphs with varying degrees of connectivity.\nEasy to iterate through neighbors.\n\nCons:\n\nSlower for dense graphs.\nChecking if nodes are connected is slow - retrieving edge information may require linear search.\n\nPython Implementation:\n\nclass Graph:\n    def __init__(self):\n        self.adj_list = {}\n\n    def __repr__(self) -&gt; str:\n        return str(self.adj_list)\n\n    def add_edge(self, node1, node2):\n        if node1 not in self.adj_list:\n            self.adj_list[node1] = []\n        if node2 not in self.adj_list:\n            self.adj_list[node2] = []\n        self.adj_list[node1].append(node2)\n        self.adj_list[node2].append(node1)\n\n\ngraph = Graph()\ngraph.add_edge(1, 2)\ngraph.add_edge(2, 3)\n\nprint(graph)\n\n{1: [2], 2: [1, 3], 3: [2]}\n\n\n\n\n\nAn even more compact form is the edge list, which lists all of the edges as (source_node, target_node) tuples.\nPros:\n\nEven more efficient memory usage for sparse graphs - \\(O(E)\\) space complexity.\n\nCons:\n\nSlower for dense graphs.\nChecking if nodes are connected is slow - retrieving edge information may require linear search.\n\nPython Implementation:\n\nclass Graph:\n    def __init__(self):\n        self.edge_list = []\n\n    def __repr__(self) -&gt; str:\n        return str(self.edge_list)\n\n    def add_edge(self, node1, node2):\n        node_pair = (node1, node2)\n        if node_pair not in self.edge_list:\n            self.edge_list.append(node_pair)\n\n\ngraph = Graph()\ngraph.add_edge(1, 2)\ngraph.add_edge(2, 3)\n\nprint(graph)\n\n[(1, 2), (2, 3)]\n\n\n\n\n\nIn this approach, we represent the graph as a 2D matrix where rows and columns represent nodes, and matrix cells indicate whether there’s an edge between the nodes.\nFor weighted graphs, the values in the matrix correspond to the weights of the edges.\nPros:\n\nEfficient for dense graphs.\nConstant time access to check edge existence; checking if nodes are connected is \\(O(1)\\).\n\nCons:\n\nNot ideal for graphs with a large number of nodes due to \\(O(V^2)\\) space complexity. This is particularly bad for sparse graphs.\nAdding/removing nodes can be expensive as it requires resizing the matrix. Bad for dynamic graphs.\n\nPython Implementation:\n\nclass Graph:\n    def __init__(self, num_nodes):\n        self.adj_matrix = [[0] * num_nodes for row in range(num_nodes)]\n\n    def __repr__(self) -&gt; str:\n        return str(self.adj_matrix)\n\n    def add_edge(self, node1, node2):\n        self.adj_matrix[node1][node2] = 1\n        self.adj_matrix[node2][node1] = 1\n\n\ngraph = Graph(num_nodes=3)\ngraph.add_edge(0, 1)\ngraph.add_edge(1, 2)\n\nprint(graph)\n\n[[0, 1, 0], [1, 0, 1], [0, 1, 0]]\n\n\nThe adjacency matrix approach is useful in machine learning as it naturally fits into a tensor representation, which most ML libraries (e.g. pytorch, tensorflow) play nicely with. So we’ll stick with tthat going forward."
  },
  {
    "objectID": "posts/ml/graph_ml/part_1/graph_notes.html#implementing-a-graph-with-networkx",
    "href": "posts/ml/graph_ml/part_1/graph_notes.html#implementing-a-graph-with-networkx",
    "title": "Graph ML: What’s a Graph?",
    "section": "",
    "text": "Grpahs are common enough to work with that we don’t need to implement them from scratch every time we want ot use them (although doing so in the previous section is instructive).\nnetworkx is a useful third-party library for this task\n\nimport networkx as nx\n\n\ngraph = nx.Graph()\ngraph.add_edges_from([\n    ('A', 'B'),\n    ('A', 'C'), \n    ('B', 'D'), \n    ('B', 'E'), \n    ('C', 'F'), \n    ('C', 'G'), \n    ('G', 'G'), \n])\n\nprint(graph)\n\nGraph with 7 nodes and 7 edges\n\n\nFrom this Graph object we can see our familiar edge list:\n\ngraph.adj\n\nAdjacencyView({'A': {'B': {}, 'C': {}}, 'B': {'A': {}, 'D': {}, 'E': {}}, 'C': {'A': {}, 'F': {}, 'G': {}}, 'D': {'B': {}}, 'E': {'B': {}}, 'F': {'C': {}}, 'G': {'C': {}, 'G': {}}})\n\n\nOr the adjacency matrix:\n\nnx.adjacency_matrix(graph).todense()\n\n/var/folders/8k/8jqhnfbd1t99blb07r1hs5440000gn/T/ipykernel_50230/1468791322.py:1: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n  nx.adjacency_matrix(graph).todense()\n\n\nmatrix([[0, 1, 1, 0, 0, 0, 0],\n        [1, 0, 0, 1, 1, 0, 0],\n        [1, 0, 0, 0, 0, 1, 1],\n        [0, 1, 0, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0, 0, 0],\n        [0, 0, 1, 0, 0, 0, 0],\n        [0, 0, 1, 0, 0, 0, 1]])"
  },
  {
    "objectID": "posts/ml/graph_ml/part_1/graph_notes.html#graph-metrics",
    "href": "posts/ml/graph_ml/part_1/graph_notes.html#graph-metrics",
    "title": "Graph ML: What’s a Graph?",
    "section": "",
    "text": "This is the number of edges which are incident on a node.\nFor an undirected graph, this is the number of edges connected to the node. Self-loops add 2 to the degree.\nFor a directed graph, we can distinguish the indegree, \\(deg^-(V)\\), as the edges that point towards the node. The outdegree, \\(deg^+(V)\\), denotes the edges that start from the node. Self-loops add 1 to the indegree and 1 to the outdegree.\n\ngraph.degree\n\nDegreeView({'A': 2, 'B': 3, 'C': 3, 'D': 1, 'E': 1, 'F': 1, 'G': 3})\n\n\n\n\n\nCentrality measures the importance of a given node in a network.\nThere a different formulations of centrality:\n\nDegree centrality: This is simply the degree of the node. How many neighbours?\nCloseness centrality: The average length of the shortest path between this node and all others. How fast can I reach my neighbours?\nBetweenness centrality: The number of times a node lies on the shortest path between pairs of other nodes in the graph. How often am I the bottleneck/bridge for others?\n\n\nnx.degree_centrality(graph)\n\n{'A': 0.3333333333333333,\n 'B': 0.5,\n 'C': 0.5,\n 'D': 0.16666666666666666,\n 'E': 0.16666666666666666,\n 'F': 0.16666666666666666,\n 'G': 0.5}\n\n\n\nnx.closeness_centrality(graph)\n\n{'A': 0.6,\n 'B': 0.5454545454545454,\n 'C': 0.5454545454545454,\n 'D': 0.375,\n 'E': 0.375,\n 'F': 0.375,\n 'G': 0.375}\n\n\n\nnx.betweenness_centrality(graph)\n\n{'A': 0.6, 'B': 0.6, 'C': 0.6, 'D': 0.0, 'E': 0.0, 'F': 0.0, 'G': 0.0}\n\n\n\n\n\n\n\nThis is the minimum number of edges that would need to be removed to make the graph disconnected.\nHow easy is it to create isolated nodes/clusters?\nThis is a measure of connectedness.\n\n\n\nThis is the ratio of the number of edges in the grap to the maximum possible number of edges between all nodes.\nFor a directed graph with \\(n\\) nodes, the maximum number of edges is \\(n(n-1)\\).\nFor an undirected graph with \\(n\\) nodes, the maximum number of edges is \\(\\frac{n(n-1)}{2}\\).\nBroadly speaking, a graph is considered dense if \\(density &gt; 0.5\\) and sparse if \\(density &lt; 0.1\\)."
  },
  {
    "objectID": "posts/ml/graph_ml/part_1/graph_notes.html#why-use-graph-learning",
    "href": "posts/ml/graph_ml/part_1/graph_notes.html#why-use-graph-learning",
    "title": "Graph ML: What’s a Graph?",
    "section": "",
    "text": "There are several categories of learning tasks we may want to perform on a graph:\n\nNode classification: Predict the category of each node. E.g. categorising songs by genre.\nLink prediction: Predict missing links between nodes. E.g. friend recommnedations in a social network.\nGraph classification: Categorise an entire graph.\nGraph generation: Generate a new graph based on desired properties.\n\nThere are several prominent families of graph learning techniques:\n\nGraph signal processing: Apply traditional signal processing techniques like Fourier analysis to graphs to study its connectivity and structure.\nMatrix factorisation: Find low-dimensional representations of large matrices\nRandom walk: Simulate random walks over a graph, e.g. to generate training date for downstream models.\nDeep learning: Encode the graph as tensors and perform deep learning.\n\nTraditional tabular datasets focus on the entities which are represented as rows. But often, the relationships between entities contain vital information. Global features, such as network-wide statistics, may also provide useful information."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson4/chapter4.html",
    "href": "posts/ml/generative_deep_learning/lesson4/chapter4.html",
    "title": "Generative AI: GANs",
    "section": "",
    "text": "Notes on Generative Adversarial Networks (GANs).\n\n\n\n\n\n\nStory Time\n\n\n\nImagine a forger trying to forge £20 notes and the police trying to stop them.\nThe police learn to spot the fakes. But then the forger learns to improve their forging skills to make better fakes.\nThis goes back and forth. With each iteration, the forger keeps getting better but then the police learn to spot these more sophisticated fakes.\nThe results in a forger (generator) learning to create convincing fakes and the popo (discriminator) learning to spot fakes.\n\n\n\n\nThe idea of GANs is that we can train two competing models:\n\nThe generator tries to convert random noise into convincing observations.\nThe discriminator tries to predict whether an observation came from the original training dataset or is a “fake”.\n\nWe initialise both as random models; the generator outputs noise and the discriminator predicts randomly. We then alternate the training of the two networks so that the generator gets incrementally better at fooling the discriminator, then the discriminator gets incrementally better at spotting fakes.\n\n\n\n\n\nflowchart LR\n\n  A([Random noise]) --&gt; B[Generator] --&gt; C([Generated image]) \n\n  D([Image]) --&gt; E[Discriminator] --&gt; F([Prediction of realness probability])\n\n\n\n\n\n\n\n\n\nWe will implement a GAN to generate pictures of bricks.\n\n\nLoad image data of lego bricks. We will train a model that can generate novel lego brick images.\n\n\nThe original data is scaled from [0, 255].\nOften we will rescale this to [0, 1] so that we can use sigmoid activation functions.\nIn this case we will scale to [-1, 1] so that we can use tanh activation functions, which tend to give stronger gradients than sigmoid.\n\nfrom pathlib import Path\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras import (\n    layers,\n    models,\n    callbacks,\n    losses,\n    utils,\n    metrics,\n    optimizers,\n)\n\n\ndef sample_batch(dataset):\n    batch = dataset.take(1).get_single_element()\n    if isinstance(batch, tuple):\n        batch = batch[0]\n    return batch.numpy()\n\n\ndef display_images(\n    images, n=10, size=(20, 3), cmap=\"gray_r\", as_type=\"float32\", save_to=None\n):\n    \"\"\"Displays n random images from each one of the supplied arrays.\"\"\"\n    if images.max() &gt; 1.0:\n        images = images / 255.0\n    elif images.min() &lt; 0.0:\n        images = (images + 1.0) / 2.0\n\n    plt.figure(figsize=size)\n    for i in range(n):\n        _ = plt.subplot(1, n, i + 1)\n        plt.imshow(images[i].astype(as_type), cmap=cmap)\n        plt.axis(\"off\")\n\n    if save_to:\n        plt.savefig(save_to)\n        print(f\"\\nSaved to {save_to}\")\n\n    plt.show()\n\nModel and data parameters:\n\nDATA_DIR = Path(\"/Users/gurpreetjohl/workspace/datasets/lego-brick-images\")\n\nIMAGE_SIZE = 64\nCHANNELS = 1\nBATCH_SIZE = 128\nZ_DIM = 100\nEPOCHS = 100\nLOAD_MODEL = False\nADAM_BETA_1 = 0.5\nADAM_BETA_2 = 0.999\nLEARNING_RATE = 0.0002\nNOISE_PARAM = 0.1\n\nLoad and pre-process the training data:\n\ndef preprocess(img):\n    \"\"\"Normalize and reshape the images.\"\"\"\n    img = (tf.cast(img, \"float32\") - 127.5) / 127.5\n    return img\n\n\ntraining_data = utils.image_dataset_from_directory(\n    DATA_DIR / \"dataset\",\n    labels=None,\n    color_mode=\"grayscale\",\n    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    seed=42,\n    interpolation=\"bilinear\",\n)\ntrain = training_data.map(lambda x: preprocess(x))\n\nFound 40000 files belonging to 1 classes.\n\n\nSome sample input images:\n\ndisplay_images(sample_batch(train))\n\n\n\n\n\n\n\n\n\n\n\n\nThe goal of the discriminator is to predict whether an image is real or fake.\nThis is a supervised binary classification problem, so we can use CNN architecture with a single output node. We stack Conv2D layers with BatchNormalization, LeakyReLU and Dropout layers sandwiched between.\n\ndiscriminator_input = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS))\n\nx = layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(discriminator_input)\nx = layers.LeakyReLU(0.2)(x)\nx = layers.Dropout(0.3)(x)\n\nx = layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\nx = layers.Dropout(0.3)(x)\n\nx = layers.Conv2D(256, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\nx = layers.Dropout(0.3)(x)\n\nx = layers.Conv2D(512, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\nx = layers.Dropout(0.3)(x)\n\nx = layers.Conv2D(1, kernel_size=4, strides=1, padding=\"valid\", use_bias=False, activation=\"sigmoid\")(x)\ndiscriminator_output = layers.Flatten()(x)  # The shape is already 1x1 so no need for a Dense layer after this\n\ndiscriminator = models.Model(discriminator_input, discriminator_output)\ndiscriminator.summary()\n\nModel: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 64, 64, 1)]       0         \n                                                                 \n conv2d_5 (Conv2D)           (None, 32, 32, 64)        1024      \n                                                                 \n leaky_re_lu_4 (LeakyReLU)   (None, 32, 32, 64)        0         \n                                                                 \n dropout_4 (Dropout)         (None, 32, 32, 64)        0         \n                                                                 \n conv2d_6 (Conv2D)           (None, 16, 16, 128)       131072    \n                                                                 \n batch_normalization_3 (Bat  (None, 16, 16, 128)       512       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_5 (LeakyReLU)   (None, 16, 16, 128)       0         \n                                                                 \n dropout_5 (Dropout)         (None, 16, 16, 128)       0         \n                                                                 \n conv2d_7 (Conv2D)           (None, 8, 8, 256)         524288    \n                                                                 \n batch_normalization_4 (Bat  (None, 8, 8, 256)         1024      \n chNormalization)                                                \n                                                                 \n leaky_re_lu_6 (LeakyReLU)   (None, 8, 8, 256)         0         \n                                                                 \n dropout_6 (Dropout)         (None, 8, 8, 256)         0         \n                                                                 \n conv2d_8 (Conv2D)           (None, 4, 4, 512)         2097152   \n                                                                 \n batch_normalization_5 (Bat  (None, 4, 4, 512)         2048      \n chNormalization)                                                \n                                                                 \n leaky_re_lu_7 (LeakyReLU)   (None, 4, 4, 512)         0         \n                                                                 \n dropout_7 (Dropout)         (None, 4, 4, 512)         0         \n                                                                 \n conv2d_9 (Conv2D)           (None, 1, 1, 1)           8192      \n                                                                 \n flatten_1 (Flatten)         (None, 1)                 0         \n                                                                 \n=================================================================\nTotal params: 2765312 (10.55 MB)\nTrainable params: 2763520 (10.54 MB)\nNon-trainable params: 1792 (7.00 KB)\n_________________________________________________________________\n\n\n\n\n\nThe purpose of the generator is to turn random noise into convincing images.\nThe input is a vector sampled from a multivariate Normal distribution, and the output is an image of the same size as the training data.\nThe discriminator-generator relationship in a GAN is similar to that of the encoder-decoder relations in a VAE.\nThe architecture of the discriminator is similar to the discriminator but in reverse (like a decoder). We pass stack Conv2DTranspose layers with BatchNormalization and LeakyReLU layers sandwiched in between.\n\n\nWe use Conv2DTranspose layers to scale the image size up.\nAn alternative would be to use stacks of Upsampling2D and Conv2D layers, i.e. the following serves the same purpose as a Conv2DTranspose layer:\nx = layers.Upsampling2D(size=2)(x)\nx = layers.Conv2D(256, kernel_size=4, strides=1, padding=\"same\")(x)\nThe Upsampling2D layer simply repeats each row and column to double its size, then Conv2D applies a convolution.\nThe idea is similar with Conv2DTranspose, but the extra rows and columns are filled with zeros rather than repeated existing values.\nConv2DTranspose layers can result in checkerboard pattern artifacts. Both options are used in practice, so it is often helpful to experiment and see which gives better results.\n\ngenerator_input = layers.Input(shape=(Z_DIM,))\n\nx = layers.Reshape((1, 1, Z_DIM))(generator_input)  # Reshape the input vector so we can apply conv transpose operations to it\n\nx = layers.Conv2DTranspose(512, kernel_size=4, strides=1, padding=\"valid\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\n\nx = layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\n\nx = layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\n\nx = layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\n\ngenerator_output = layers.Conv2DTranspose(\n    CHANNELS,\n    kernel_size=4,\n    strides=2,\n    padding=\"same\",\n    use_bias=False,\n    activation=\"tanh\",\n)(x)\n\ngenerator = models.Model(generator_input, generator_output)\ngenerator.summary()\n\nModel: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_3 (InputLayer)        [(None, 100)]             0         \n                                                                 \n reshape (Reshape)           (None, 1, 1, 100)         0         \n                                                                 \n conv2d_transpose (Conv2DTr  (None, 4, 4, 512)         819200    \n anspose)                                                        \n                                                                 \n batch_normalization_6 (Bat  (None, 4, 4, 512)         2048      \n chNormalization)                                                \n                                                                 \n leaky_re_lu_8 (LeakyReLU)   (None, 4, 4, 512)         0         \n                                                                 \n conv2d_transpose_1 (Conv2D  (None, 8, 8, 256)         2097152   \n Transpose)                                                      \n                                                                 \n batch_normalization_7 (Bat  (None, 8, 8, 256)         1024      \n chNormalization)                                                \n                                                                 \n leaky_re_lu_9 (LeakyReLU)   (None, 8, 8, 256)         0         \n                                                                 \n conv2d_transpose_2 (Conv2D  (None, 16, 16, 128)       524288    \n Transpose)                                                      \n                                                                 \n batch_normalization_8 (Bat  (None, 16, 16, 128)       512       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_10 (LeakyReLU)  (None, 16, 16, 128)       0         \n                                                                 \n conv2d_transpose_3 (Conv2D  (None, 32, 32, 64)        131072    \n Transpose)                                                      \n                                                                 \n batch_normalization_9 (Bat  (None, 32, 32, 64)        256       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_11 (LeakyReLU)  (None, 32, 32, 64)        0         \n                                                                 \n conv2d_transpose_4 (Conv2D  (None, 64, 64, 1)         1024      \n Transpose)                                                      \n                                                                 \n=================================================================\nTotal params: 3576576 (13.64 MB)\nTrainable params: 3574656 (13.64 MB)\nNon-trainable params: 1920 (7.50 KB)\n_________________________________________________________________\n\n\n\n\n\n\nWe alternate between training the discriminator and generator. They are not trained simultaneously. We want the generated images to be predicted close to 1 because the generator is good, not because the discriminator is weak.\nFor the discriminator, we create a training set where some images are real images from the training data and some are outputs from the generator. This is then a supervised binary classification problem.\nFor the generator, we want a way of scoring each generated image on its realness so that we can optimise this. The discriminator provides exactly this. We pass the generated images through the discriminator to get probabilities. The generator wants to fool the discriminator, so ideally this would be a vector of 1s. So the loss function is the binary crossentropy between these probabilities and a vector of 1s.\n\nclass DCGAN(models.Model):\n    def __init__(self, discriminator, generator, latent_dim):\n        super(DCGAN, self).__init__()\n        self.discriminator = discriminator\n        self.generator = generator\n        self.latent_dim = latent_dim\n\n    def compile(self, d_optimizer, g_optimizer):\n        super(DCGAN, self).compile()\n        self.loss_fn = losses.BinaryCrossentropy()\n        self.d_optimizer = d_optimizer\n        self.g_optimizer = g_optimizer\n        self.d_loss_metric = metrics.Mean(name=\"d_loss\")\n        self.d_real_acc_metric = metrics.BinaryAccuracy(name=\"d_real_acc\")\n        self.d_fake_acc_metric = metrics.BinaryAccuracy(name=\"d_fake_acc\")\n        self.d_acc_metric = metrics.BinaryAccuracy(name=\"d_acc\")\n        self.g_loss_metric = metrics.Mean(name=\"g_loss\")\n        self.g_acc_metric = metrics.BinaryAccuracy(name=\"g_acc\")\n\n    @property\n    def metrics(self):\n        return [\n            self.d_loss_metric,\n            self.d_real_acc_metric,\n            self.d_fake_acc_metric,\n            self.d_acc_metric,\n            self.g_loss_metric,\n            self.g_acc_metric,\n        ]\n\n    def train_step(self, real_images):\n        # Sample random points in the latent space\n        batch_size = tf.shape(real_images)[0]\n        random_latent_vectors = tf.random.normal(\n            shape=(batch_size, self.latent_dim)\n        )\n\n        # Train the discriminator on fake images\n        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n            generated_images = self.generator(\n                random_latent_vectors, training=True\n            )\n\n            # Evaluate the discriminator on the real and fake images\n            real_predictions = self.discriminator(real_images, training=True)\n            fake_predictions = self.discriminator(generated_images, training=True)\n\n            real_labels = tf.ones_like(real_predictions)\n            real_noisy_labels = real_labels + NOISE_PARAM * tf.random.uniform(\n                tf.shape(real_predictions)\n            )\n            fake_labels = tf.zeros_like(fake_predictions)\n            fake_noisy_labels = fake_labels - NOISE_PARAM * tf.random.uniform(\n                tf.shape(fake_predictions)\n            )\n\n            # Calculate the losses\n            d_real_loss = self.loss_fn(real_noisy_labels, real_predictions)\n            d_fake_loss = self.loss_fn(fake_noisy_labels, fake_predictions)\n            d_loss = (d_real_loss + d_fake_loss) / 2.0\n\n            g_loss = self.loss_fn(real_labels, fake_predictions)\n\n        # Update gradients\n        gradients_of_discriminator = disc_tape.gradient(\n            d_loss, self.discriminator.trainable_variables\n        )\n        gradients_of_generator = gen_tape.gradient(\n            g_loss, self.generator.trainable_variables\n        )\n\n        self.d_optimizer.apply_gradients(\n            zip(gradients_of_discriminator, discriminator.trainable_variables)\n        )\n        self.g_optimizer.apply_gradients(\n            zip(gradients_of_generator, generator.trainable_variables)\n        )\n\n        # Update metrics\n        self.d_loss_metric.update_state(d_loss)\n        self.d_real_acc_metric.update_state(real_labels, real_predictions)\n        self.d_fake_acc_metric.update_state(fake_labels, fake_predictions)\n        self.d_acc_metric.update_state(\n            [real_labels, fake_labels], [real_predictions, fake_predictions]\n        )\n        self.g_loss_metric.update_state(g_loss)\n        self.g_acc_metric.update_state(real_labels, fake_predictions)\n\n        return {m.name: m.result() for m in self.metrics}\n\n\n# Create a DCGAN\ndcgan = DCGAN(\n    discriminator=discriminator, generator=generator, latent_dim=Z_DIM\n)\n\n\ndcgan.compile(\n    d_optimizer=optimizers.legacy.Adam(\n        learning_rate=LEARNING_RATE, beta_1=ADAM_BETA_1, beta_2=ADAM_BETA_2\n    ),\n    g_optimizer=optimizers.legacy.Adam(\n        learning_rate=LEARNING_RATE, beta_1=ADAM_BETA_1, beta_2=ADAM_BETA_2\n    ),\n)\ndcgan.fit(train,  epochs=EPOCHS)\n\nEpoch 1/100\n313/313 [==============================] - 459s 1s/step - d_loss: 0.0252 - d_real_acc: 0.9011 - d_fake_acc: 0.9013 - d_acc: 0.9012 - g_loss: 5.3464 - g_acc: 0.0987\nEpoch 2/100\n313/313 [==============================] - 459s 1s/step - d_loss: 0.0454 - d_real_acc: 0.8986 - d_fake_acc: 0.8997 - d_acc: 0.8992 - g_loss: 5.2642 - g_acc: 0.1002\nEpoch 3/100\n313/313 [==============================] - 458s 1s/step - d_loss: 0.0556 - d_real_acc: 0.8958 - d_fake_acc: 0.8975 - d_acc: 0.8967 - g_loss: 4.9679 - g_acc: 0.1025\nEpoch 4/100\n313/313 [==============================] - 467s 1s/step - d_loss: 0.0246 - d_real_acc: 0.9065 - d_fake_acc: 0.9091 - d_acc: 0.9078 - g_loss: 5.1611 - g_acc: 0.0909\nEpoch 5/100\n313/313 [==============================] - 470s 1s/step - d_loss: 0.0178 - d_real_acc: 0.9067 - d_fake_acc: 0.9088 - d_acc: 0.9078 - g_loss: 5.1731 - g_acc: 0.0912\nEpoch 6/100\n313/313 [==============================] - 463s 1s/step - d_loss: 0.0314 - d_real_acc: 0.9116 - d_fake_acc: 0.9105 - d_acc: 0.9110 - g_loss: 5.2774 - g_acc: 0.0895\nEpoch 7/100\n313/313 [==============================] - 559s 2s/step - d_loss: 0.0229 - d_real_acc: 0.9085 - d_fake_acc: 0.9079 - d_acc: 0.9082 - g_loss: 5.3445 - g_acc: 0.0921\nEpoch 8/100\n313/313 [==============================] - 467s 1s/step - d_loss: -0.0155 - d_real_acc: 0.9161 - d_fake_acc: 0.9161 - d_acc: 0.9161 - g_loss: 5.7091 - g_acc: 0.0839\nEpoch 9/100\n313/313 [==============================] - 438s 1s/step - d_loss: -0.0077 - d_real_acc: 0.9220 - d_fake_acc: 0.9224 - d_acc: 0.9222 - g_loss: 5.8731 - g_acc: 0.0776\nEpoch 10/100\n313/313 [==============================] - 468s 1s/step - d_loss: -0.0472 - d_real_acc: 0.9228 - d_fake_acc: 0.9241 - d_acc: 0.9234 - g_loss: 5.9693 - g_acc: 0.0759\nEpoch 11/100\n313/313 [==============================] - 430s 1s/step - d_loss: -0.0839 - d_real_acc: 0.9404 - d_fake_acc: 0.9424 - d_acc: 0.9414 - g_loss: 6.1212 - g_acc: 0.0576\nEpoch 12/100\n313/313 [==============================] - 457s 1s/step - d_loss: 0.0431 - d_real_acc: 0.9053 - d_fake_acc: 0.9046 - d_acc: 0.9049 - g_loss: 6.0708 - g_acc: 0.0954\nEpoch 13/100\n313/313 [==============================] - 448s 1s/step - d_loss: -0.0154 - d_real_acc: 0.9236 - d_fake_acc: 0.9244 - d_acc: 0.9240 - g_loss: 6.3106 - g_acc: 0.0756\nEpoch 14/100\n313/313 [==============================] - 432s 1s/step - d_loss: -0.0720 - d_real_acc: 0.9320 - d_fake_acc: 0.9342 - d_acc: 0.9331 - g_loss: 6.6509 - g_acc: 0.0658\nEpoch 15/100\n313/313 [==============================] - 443s 1s/step - d_loss: 0.0057 - d_real_acc: 0.9072 - d_fake_acc: 0.9097 - d_acc: 0.9085 - g_loss: 6.1399 - g_acc: 0.0903\nEpoch 16/100\n313/313 [==============================] - 427s 1s/step - d_loss: 0.0069 - d_real_acc: 0.9185 - d_fake_acc: 0.9167 - d_acc: 0.9176 - g_loss: 6.3255 - g_acc: 0.0833\nEpoch 17/100\n313/313 [==============================] - 439s 1s/step - d_loss: -0.0709 - d_real_acc: 0.9220 - d_fake_acc: 0.9239 - d_acc: 0.9230 - g_loss: 6.8108 - g_acc: 0.0761\nEpoch 18/100\n313/313 [==============================] - 437s 1s/step - d_loss: 0.0373 - d_real_acc: 0.9288 - d_fake_acc: 0.9512 - d_acc: 0.9400 - g_loss: 8.1066 - g_acc: 0.0488\nEpoch 19/100\n313/313 [==============================] - 1029s 3s/step - d_loss: -0.1154 - d_real_acc: 0.9408 - d_fake_acc: 0.9420 - d_acc: 0.9414 - g_loss: 7.6274 - g_acc: 0.0580\nEpoch 20/100\n313/313 [==============================] - 5781s 19s/step - d_loss: -0.0431 - d_real_acc: 0.9222 - d_fake_acc: 0.9231 - d_acc: 0.9227 - g_loss: 7.1953 - g_acc: 0.0769\nEpoch 21/100\n313/313 [==============================] - 2696s 9s/step - d_loss: -0.0542 - d_real_acc: 0.9176 - d_fake_acc: 0.9205 - d_acc: 0.9191 - g_loss: 7.1675 - g_acc: 0.0794\nEpoch 22/100\n313/313 [==============================] - 1481s 5s/step - d_loss: -0.1424 - d_real_acc: 0.9398 - d_fake_acc: 0.9400 - d_acc: 0.9399 - g_loss: 7.7399 - g_acc: 0.0600\nEpoch 23/100\n313/313 [==============================] - 439s 1s/step - d_loss: -0.0263 - d_real_acc: 0.9154 - d_fake_acc: 0.9148 - d_acc: 0.9151 - g_loss: 7.3322 - g_acc: 0.0852\nEpoch 24/100\n313/313 [==============================] - 443s 1s/step - d_loss: -0.1420 - d_real_acc: 0.9406 - d_fake_acc: 0.9430 - d_acc: 0.9418 - g_loss: 8.1502 - g_acc: 0.0570\nEpoch 25/100\n313/313 [==============================] - 475s 2s/step - d_loss: -0.1650 - d_real_acc: 0.9393 - d_fake_acc: 0.9395 - d_acc: 0.9394 - g_loss: 7.9602 - g_acc: 0.0605\nEpoch 26/100\n313/313 [==============================] - 447s 1s/step - d_loss: -0.1247 - d_real_acc: 0.9419 - d_fake_acc: 0.9437 - d_acc: 0.9428 - g_loss: 7.7939 - g_acc: 0.0563\nEpoch 27/100\n313/313 [==============================] - 1356s 4s/step - d_loss: -0.0182 - d_real_acc: 0.9160 - d_fake_acc: 0.9212 - d_acc: 0.9186 - g_loss: 6.9180 - g_acc: 0.0787\nEpoch 28/100\n313/313 [==============================] - 2219s 7s/step - d_loss: -0.2227 - d_real_acc: 0.9511 - d_fake_acc: 0.9519 - d_acc: 0.9515 - g_loss: 8.1970 - g_acc: 0.0481\nEpoch 29/100\n313/313 [==============================] - 5807s 19s/step - d_loss: -0.1091 - d_real_acc: 0.9318 - d_fake_acc: 0.9320 - d_acc: 0.9319 - g_loss: 7.5829 - g_acc: 0.0680\nEpoch 30/100\n313/313 [==============================] - 2511s 8s/step - d_loss: -0.3131 - d_real_acc: 0.9571 - d_fake_acc: 0.9604 - d_acc: 0.9588 - g_loss: 9.8839 - g_acc: 0.0395\nEpoch 31/100\n313/313 [==============================] - 2768s 9s/step - d_loss: -0.0996 - d_real_acc: 0.9269 - d_fake_acc: 0.9286 - d_acc: 0.9277 - g_loss: 8.3337 - g_acc: 0.0714\nEpoch 32/100\n313/313 [==============================] - 3046s 10s/step - d_loss: -0.1619 - d_real_acc: 0.9423 - d_fake_acc: 0.9482 - d_acc: 0.9453 - g_loss: 8.2435 - g_acc: 0.0518\nEpoch 33/100\n313/313 [==============================] - 3478s 11s/step - d_loss: -0.1182 - d_real_acc: 0.9284 - d_fake_acc: 0.9304 - d_acc: 0.9294 - g_loss: 8.1681 - g_acc: 0.0696\nEpoch 34/100\n313/313 [==============================] - 2776s 9s/step - d_loss: -0.2214 - d_real_acc: 0.9459 - d_fake_acc: 0.9582 - d_acc: 0.9520 - g_loss: 9.9168 - g_acc: 0.0417\nEpoch 35/100\n313/313 [==============================] - 2724s 9s/step - d_loss: -0.3101 - d_real_acc: 0.9421 - d_fake_acc: 0.9293 - d_acc: 0.9357 - g_loss: 13.2857 - g_acc: 0.0707\nEpoch 36/100\n313/313 [==============================] - 2648s 8s/step - d_loss: -0.0441 - d_real_acc: 0.8963 - d_fake_acc: 0.8961 - d_acc: 0.8962 - g_loss: 7.5664 - g_acc: 0.1038\nEpoch 37/100\n313/313 [==============================] - 3262s 10s/step - d_loss: -0.0859 - d_real_acc: 0.9314 - d_fake_acc: 0.9402 - d_acc: 0.9358 - g_loss: 8.3591 - g_acc: 0.0598\nEpoch 38/100\n313/313 [==============================] - 2612s 8s/step - d_loss: -0.2979 - d_real_acc: 0.9554 - d_fake_acc: 0.9577 - d_acc: 0.9566 - g_loss: 9.2534 - g_acc: 0.0423\nEpoch 39/100\n313/313 [==============================] - 2235s 7s/step - d_loss: -0.3387 - d_real_acc: 0.9607 - d_fake_acc: 0.9622 - d_acc: 0.9615 - g_loss: 9.9397 - g_acc: 0.0378\nEpoch 40/100\n313/313 [==============================] - 3453s 11s/step - d_loss: -0.1056 - d_real_acc: 0.9279 - d_fake_acc: 0.9310 - d_acc: 0.9294 - g_loss: 8.9394 - g_acc: 0.0690\nEpoch 41/100\n313/313 [==============================] - 2316s 7s/step - d_loss: -0.2147 - d_real_acc: 0.9318 - d_fake_acc: 0.9327 - d_acc: 0.9323 - g_loss: 9.0337 - g_acc: 0.0673\nEpoch 42/100\n313/313 [==============================] - 3134s 10s/step - d_loss: -0.2554 - d_real_acc: 0.9511 - d_fake_acc: 0.9540 - d_acc: 0.9526 - g_loss: 9.5571 - g_acc: 0.0460\nEpoch 43/100\n313/313 [==============================] - 3933s 13s/step - d_loss: -0.2871 - d_real_acc: 0.9490 - d_fake_acc: 0.9526 - d_acc: 0.9508 - g_loss: 10.3316 - g_acc: 0.0474\nEpoch 44/100\n313/313 [==============================] - 3248s 10s/step - d_loss: -0.3456 - d_real_acc: 0.9635 - d_fake_acc: 0.9635 - d_acc: 0.9635 - g_loss: 9.8675 - g_acc: 0.0364\nEpoch 45/100\n313/313 [==============================] - 3043s 10s/step - d_loss: -0.3274 - d_real_acc: 0.9603 - d_fake_acc: 0.9618 - d_acc: 0.9610 - g_loss: 10.4185 - g_acc: 0.0382\nEpoch 46/100\n313/313 [==============================] - 2706s 9s/step - d_loss: -0.6160 - d_real_acc: 0.9902 - d_fake_acc: 0.9908 - d_acc: 0.9905 - g_loss: 13.0574 - g_acc: 0.0092\nEpoch 47/100\n313/313 [==============================] - 2453s 8s/step - d_loss: 0.3413 - d_real_acc: 0.8073 - d_fake_acc: 0.8054 - d_acc: 0.8064 - g_loss: 6.5391 - g_acc: 0.1946\nEpoch 48/100\n313/313 [==============================] - 2898s 9s/step - d_loss: -0.4416 - d_real_acc: 0.9764 - d_fake_acc: 0.9784 - d_acc: 0.9774 - g_loss: 10.8318 - g_acc: 0.0216\nEpoch 49/100\n313/313 [==============================] - 3358s 11s/step - d_loss: 6.8776 - d_real_acc: 0.1058 - d_fake_acc: 0.9910 - d_acc: 0.5484 - g_loss: 14.8921 - g_acc: 0.0090\nEpoch 50/100\n313/313 [==============================] - 2940s 9s/step - d_loss: 7.7113 - d_real_acc: 0.0000e+00 - d_fake_acc: 1.0000 - d_acc: 0.5000 - g_loss: 15.4250 - g_acc: 0.0000e+00\nEpoch 51/100\n313/313 [==============================] - 2983s 10s/step - d_loss: 7.7121 - d_real_acc: 0.0000e+00 - d_fake_acc: 1.0000 - d_acc: 0.5000 - g_loss: 15.4250 - g_acc: 0.0000e+00\nEpoch 52/100\n313/313 [==============================] - 3458s 11s/step - d_loss: 7.7149 - d_real_acc: 0.0000e+00 - d_fake_acc: 1.0000 - d_acc: 0.5000 - g_loss: 15.4250 - g_acc: 0.0000e+00\nEpoch 53/100\n313/313 [==============================] - 2404s 8s/step - d_loss: 4.4950 - d_real_acc: 0.3543 - d_fake_acc: 0.8865 - d_acc: 0.6204 - g_loss: 10.6772 - g_acc: 0.1135\nEpoch 54/100\n313/313 [==============================] - 3297s 11s/step - d_loss: -0.0132 - d_real_acc: 0.9068 - d_fake_acc: 0.9010 - d_acc: 0.9039 - g_loss: 7.9660 - g_acc: 0.0990\nEpoch 55/100\n313/313 [==============================] - 2486s 8s/step - d_loss: -0.3508 - d_real_acc: 0.9615 - d_fake_acc: 0.9612 - d_acc: 0.9614 - g_loss: 10.2242 - g_acc: 0.0388\nEpoch 56/100\n313/313 [==============================] - 2995s 10s/step - d_loss: -0.3125 - d_real_acc: 0.9525 - d_fake_acc: 0.9533 - d_acc: 0.9529 - g_loss: 10.4182 - g_acc: 0.0467\nEpoch 57/100\n313/313 [==============================] - 1791s 6s/step - d_loss: -0.3201 - d_real_acc: 0.9532 - d_fake_acc: 0.9560 - d_acc: 0.9546 - g_loss: 10.4752 - g_acc: 0.0441\nEpoch 58/100\n313/313 [==============================] - 2792s 9s/step - d_loss: -0.2649 - d_real_acc: 0.9509 - d_fake_acc: 0.9532 - d_acc: 0.9520 - g_loss: 9.3587 - g_acc: 0.0468\nEpoch 59/100\n313/313 [==============================] - 3665s 12s/step - d_loss: -0.1747 - d_real_acc: 0.9413 - d_fake_acc: 0.9584 - d_acc: 0.9499 - g_loss: 10.1369 - g_acc: 0.0416\nEpoch 60/100\n313/313 [==============================] - 2493s 8s/step - d_loss: -0.2692 - d_real_acc: 0.9499 - d_fake_acc: 0.9534 - d_acc: 0.9517 - g_loss: 9.7124 - g_acc: 0.0466\nEpoch 61/100\n313/313 [==============================] - 2293s 7s/step - d_loss: -0.2869 - d_real_acc: 0.9520 - d_fake_acc: 0.9556 - d_acc: 0.9538 - g_loss: 10.2684 - g_acc: 0.0444\nEpoch 62/100\n313/313 [==============================] - 2865s 9s/step - d_loss: -0.6188 - d_real_acc: 0.9900 - d_fake_acc: 0.9901 - d_acc: 0.9901 - g_loss: 12.9584 - g_acc: 0.0099\nEpoch 63/100\n313/313 [==============================] - 2301s 7s/step - d_loss: -0.7197 - d_real_acc: 0.9984 - d_fake_acc: 0.9985 - d_acc: 0.9985 - g_loss: 14.5762 - g_acc: 0.0015\nEpoch 64/100\n313/313 [==============================] - 2404s 8s/step - d_loss: -0.4320 - d_real_acc: 0.9665 - d_fake_acc: 0.9702 - d_acc: 0.9683 - g_loss: 12.0177 - g_acc: 0.0298\nEpoch 65/100\n313/313 [==============================] - 4723s 15s/step - d_loss: 6.7591 - d_real_acc: 0.9940 - d_fake_acc: 0.1077 - d_acc: 0.5509 - g_loss: 1.4402 - g_acc: 0.8923\nEpoch 66/100\n313/313 [==============================] - 2726s 9s/step - d_loss: 7.6259 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 67/100\n313/313 [==============================] - 3325s 11s/step - d_loss: 7.6250 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 68/100\n313/313 [==============================] - 2513s 8s/step - d_loss: 7.6251 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 7.4387e-12 - g_acc: 1.0000\nEpoch 69/100\n313/313 [==============================] - 3304s 11s/step - d_loss: 3.4076 - d_real_acc: 0.8552 - d_fake_acc: 0.5472 - d_acc: 0.7012 - g_loss: 5.2072 - g_acc: 0.4528\nEpoch 70/100\n313/313 [==============================] - 2276s 7s/step - d_loss: -0.2424 - d_real_acc: 0.9448 - d_fake_acc: 0.9581 - d_acc: 0.9514 - g_loss: 10.5206 - g_acc: 0.0419\nEpoch 71/100\n313/313 [==============================] - 3315s 11s/step - d_loss: -0.3093 - d_real_acc: 0.9563 - d_fake_acc: 0.9640 - d_acc: 0.9602 - g_loss: 10.6076 - g_acc: 0.0360\nEpoch 72/100\n313/313 [==============================] - 2306s 7s/step - d_loss: -0.2440 - d_real_acc: 0.9466 - d_fake_acc: 0.9581 - d_acc: 0.9523 - g_loss: 10.1996 - g_acc: 0.0419\nEpoch 73/100\n313/313 [==============================] - 3218s 10s/step - d_loss: -0.7206 - d_real_acc: 0.9985 - d_fake_acc: 0.9983 - d_acc: 0.9984 - g_loss: 14.6350 - g_acc: 0.0017\nEpoch 74/100\n313/313 [==============================] - 3258s 10s/step - d_loss: -0.6281 - d_real_acc: 0.9828 - d_fake_acc: 0.9841 - d_acc: 0.9834 - g_loss: 14.5219 - g_acc: 0.0159\nEpoch 75/100\n313/313 [==============================] - 2874s 9s/step - d_loss: -0.0555 - d_real_acc: 0.9254 - d_fake_acc: 0.9371 - d_acc: 0.9312 - g_loss: 9.3443 - g_acc: 0.0629\nEpoch 76/100\n313/313 [==============================] - 2559s 8s/step - d_loss: -0.2825 - d_real_acc: 0.9515 - d_fake_acc: 0.9611 - d_acc: 0.9563 - g_loss: 10.7583 - g_acc: 0.0388\nEpoch 77/100\n313/313 [==============================] - 3663s 12s/step - d_loss: -0.3945 - d_real_acc: 0.9667 - d_fake_acc: 0.9691 - d_acc: 0.9679 - g_loss: 10.8566 - g_acc: 0.0309\nEpoch 78/100\n313/313 [==============================] - 2314s 7s/step - d_loss: -0.3953 - d_real_acc: 0.9508 - d_fake_acc: 0.9529 - d_acc: 0.9519 - g_loss: 12.0037 - g_acc: 0.0471\nEpoch 79/100\n313/313 [==============================] - 2816s 9s/step - d_loss: -0.6059 - d_real_acc: 0.9841 - d_fake_acc: 0.9838 - d_acc: 0.9840 - g_loss: 13.3516 - g_acc: 0.0162\nEpoch 80/100\n313/313 [==============================] - 3232s 10s/step - d_loss: -0.3555 - d_real_acc: 0.9587 - d_fake_acc: 0.9649 - d_acc: 0.9618 - g_loss: 11.2453 - g_acc: 0.0351\nEpoch 81/100\n313/313 [==============================] - 3705s 12s/step - d_loss: -0.4501 - d_real_acc: 0.9731 - d_fake_acc: 0.9743 - d_acc: 0.9737 - g_loss: 11.4553 - g_acc: 0.0258\nEpoch 82/100\n313/313 [==============================] - 2364s 8s/step - d_loss: -0.3827 - d_real_acc: 0.9588 - d_fake_acc: 0.9639 - d_acc: 0.9613 - g_loss: 11.6275 - g_acc: 0.0361\nEpoch 83/100\n313/313 [==============================] - 1122s 4s/step - d_loss: -0.4355 - d_real_acc: 0.9642 - d_fake_acc: 0.9674 - d_acc: 0.9658 - g_loss: 12.1025 - g_acc: 0.0326\nEpoch 84/100\n313/313 [==============================] - 4065s 13s/step - d_loss: -0.4456 - d_real_acc: 0.9695 - d_fake_acc: 0.9714 - d_acc: 0.9704 - g_loss: 11.6065 - g_acc: 0.0287\nEpoch 85/100\n313/313 [==============================] - 4461s 14s/step - d_loss: -0.6405 - d_real_acc: 0.9901 - d_fake_acc: 0.9899 - d_acc: 0.9900 - g_loss: 13.4694 - g_acc: 0.0101\nEpoch 86/100\n313/313 [==============================] - 2630s 8s/step - d_loss: -0.6431 - d_real_acc: 0.9857 - d_fake_acc: 0.9856 - d_acc: 0.9857 - g_loss: 14.3623 - g_acc: 0.0144\nEpoch 87/100\n313/313 [==============================] - 2567s 8s/step - d_loss: -0.3870 - d_real_acc: 0.9534 - d_fake_acc: 0.9578 - d_acc: 0.9556 - g_loss: 12.0201 - g_acc: 0.0422\nEpoch 88/100\n313/313 [==============================] - 2597s 8s/step - d_loss: -0.7624 - d_real_acc: 0.9999 - d_fake_acc: 0.9998 - d_acc: 0.9998 - g_loss: 15.3547 - g_acc: 2.5000e-04\nEpoch 89/100\n313/313 [==============================] - 1477s 5s/step - d_loss: -0.5787 - d_real_acc: 0.9764 - d_fake_acc: 0.9759 - d_acc: 0.9762 - g_loss: 13.8500 - g_acc: 0.0241\nEpoch 90/100\n313/313 [==============================] - 522s 2s/step - d_loss: -0.6747 - d_real_acc: 0.9885 - d_fake_acc: 0.9897 - d_acc: 0.9891 - g_loss: 14.5329 - g_acc: 0.0104\nEpoch 91/100\n313/313 [==============================] - 512s 2s/step - d_loss: 6.4703 - d_real_acc: 0.9892 - d_fake_acc: 0.1438 - d_acc: 0.5665 - g_loss: 2.1184 - g_acc: 0.8562\nEpoch 92/100\n313/313 [==============================] - 514s 2s/step - d_loss: 7.6245 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 93/100\n313/313 [==============================] - 533s 2s/step - d_loss: 7.6249 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 94/100\n313/313 [==============================] - 499s 2s/step - d_loss: 7.6236 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 95/100\n313/313 [==============================] - 483s 2s/step - d_loss: 7.6240 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 96/100\n313/313 [==============================] - 481s 2s/step - d_loss: 7.6248 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 97/100\n313/313 [==============================] - 488s 2s/step - d_loss: 7.6247 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 98/100\n313/313 [==============================] - 481s 2s/step - d_loss: 7.6263 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 99/100\n313/313 [==============================] - 459s 1s/step - d_loss: 7.6235 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 100/100\n313/313 [==============================] - 4669s 15s/step - d_loss: 7.6231 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\n\n\n&lt;keras.src.callbacks.History at 0x10f686690&gt;\n\n\n\n# Save the final models\ngenerator.save(\"./models/generator\")\ndiscriminator.save(\"./models/discriminator\")\n\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nINFO:tensorflow:Assets written to: ./models/generator/assets\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nINFO:tensorflow:Assets written to: ./models/discriminator/assets\n\n\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nINFO:tensorflow:Assets written to: ./models/generator/assets\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nINFO:tensorflow:Assets written to: ./models/discriminator/assets\n\n\n\n\n\n\nWe can see some examples of images produced by the GAN.\n(I don’t have a GPU so training is slow and I only trained 100 epochs… they’re a bit crap)\n\n# Sample some points in the latent space, from the standard normal distribution\ngrid_width, grid_height = (10, 3)\nz_sample = np.random.normal(size=(grid_width * grid_height, Z_DIM))\n\n# Decode the sampled points\nreconstructions = generator.predict(z_sample)\n\n# Draw a plot of decoded images\nfig = plt.figure(figsize=(18, 5))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\n# Output the grid of faces\nfor i in range(grid_width * grid_height):\n    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n    ax.axis(\"off\")\n    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")\n\n\n\n\n\n\n\n\nWe also want to make sure a generative model doesn’t simply recreate images that are already in the training set.\nAs a sanity check, we plot some generated images and the closest training images (using the L1 distance). This confirms that the generator is able to understand high-level features, even though we didn’t provide anything other than raw pixels, and it can generate examples distinct from those encountered before.\n\ndef compare_images(img1, img2):\n    return np.mean(np.abs(img1 - img2))\n\nall_data = []\nfor i in train.as_numpy_iterator():\n    all_data.extend(i)\nall_data = np.array(all_data)\n\n# Plot the images\nr, c = 3, 5\nfig, axs = plt.subplots(r, c, figsize=(10, 6))\nfig.suptitle(\"Generated images\", fontsize=20)\n\nnoise = np.random.normal(size=(r * c, Z_DIM))\ngen_imgs = generator.predict(noise)\n\ncnt = 0\nfor i in range(r):\n    for j in range(c):\n        axs[i, j].imshow(gen_imgs[cnt], cmap=\"gray_r\")\n        axs[i, j].axis(\"off\")\n        cnt += 1\n\nplt.show()\n\n1/1 [==============================] - 0s 80ms/step\n\n\n\n\n\n\n\n\n\n\nfig, axs = plt.subplots(r, c, figsize=(10, 6))\nfig.suptitle(\"Closest images in the training set\", fontsize=20)\n\ncnt = 0\nfor i in range(r):\n    for j in range(c):\n        c_diff = 99999\n        c_img = None\n        for k_idx, k in enumerate(all_data):\n            diff = compare_images(gen_imgs[cnt], k)\n            if diff &lt; c_diff:\n                c_img = np.copy(k)\n                c_diff = diff\n        axs[i, j].imshow(c_img, cmap=\"gray_r\")\n        axs[i, j].axis(\"off\")\n        cnt += 1\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nGANs are notoriously difficult to train because there is a balancing act between the generator and discriminator; neither should grow so strong that it overpowers the other.\n\n\nThe discriminator can always spot the fakes, so the signal from the loss function becomes too weak to cause any meaningful improvements in the generator.\nIn the extreme case, the discriminator distinguishes fakes perfectly, so gradients vanish and no training takes place.\nWe need to weaken the discriminator in this case. Some possible options are:\n\nMore dropout\nLower learning rate\nSimplify the discriminator architecture - use fewer layers\nAdd noise to the labels when training the discriminator\nAdd intentional labelling errors - randomly flip the labels of some images when training the discriminator\n\n\n\n\nIf the discriminator is too weak, the generator will learn that it can trick the discriminator using a small sample of nearly identical images. This is known as mode collapse. The generator would map every point in the latent space to this image, so the gradients of the loss function would vanish and it would not be able to recover.\nStrengthening the discriminator would not help because the generator would just learn to find a different mode that fools the discriminator with no diversity; it is numb to its input.\nSome possible options are:\n\nStrengthen the discriminator - do the opposite of the previous section\nReduce the learning rate of both generator and discriminator\nIncrease the batch size\n\n\n\n\n\nThe value of the loss is not meaningful as a measure of the generator’s strength when training.\nThe loss function is relative to the discriminator, and since the discriminator is also being trained, the goalposts are constantly shifting. Also, we don’t want the loss function to reach 0 or else we may reach mode collapse as described above.\nThis makes GAN training difficult to monitor.\n\n\n\nThere are a lot of hyperparameters involved with GANs because we are now training two networks.\nThe performance is highly sensitive to these hyperparameter choices, and involves a lot of trial and error.\n\n\n\nThe Wasserstein GAN replaces the binary crossentropy loss function with the Wassserstein lss function in both the discriminator and generator.\nThis results in two desirable properties:\n\nA meaningful loss metric that correlates with generator convergence. This allows for better monitoring of training.\nMore stable optimisation process.\n\n\n\n\n\nFirst, recall the binary cross-entropy loss, which is defined as: \\[\n-\\frac{1}{n} \\sum_{i=1}^{n}{y_i log(p_i) + (1 - y_i)log(1-p_i)}\n\\]\nIn a regular GAN, the discriminator compares the predictions for real images \\(p_i = D(x_i)\\) to the response \\(y_i = 1\\), and it compares the predictions for generated images \\(p_i = D(G(z_i))\\) to the response \\(y_i = 0\\).\nSo the discriminator loss minimisation can be written as: \\[\n\\min_{D} -(E_{x \\sim p_X}[log D(x)] + E_{z \\sim p_Z}[log (1 - D(G(z))] )\n\\]\nThe generator aims to trick the discriminator into believing the images are real, so it compares the discriminator’s predicted response to the desired response of \\(y_i=1\\). So the generator loss minimisation can be written as: \\[\n\\min_{G} -(E_{z \\sim p_Z}[log D(G(z)] )\n\\]\n\n\n\nIn contrast , the Wasserstein loss function is defined as: \\[\n-\\frac{1}{n} \\sum_{i=1}^{n}{y_i p_i}\n\\]\nIt requires that we use \\(y_i=1\\) and \\(y_i=-1\\) as labels rather than 1 and 0. We also remove the final sigmoid activation layer, which constrained the output to the range \\([0, 1]\\), meaning the output can now be any real number in the range \\((-\\infty, \\infty)\\).\nBecause of these changes, rather than referring to a discriminator which outputs a probability, for WGANs we refer to a critic which outputs a score.\nWith these changes of labels to -1 and 1, the critic loss minimisation becomes: \\[\n\\min_{D} -(E_{x \\sim p_X}[D(x)] - E_{z \\sim p_Z}[D(G(z)] )\n\\]\ni.e. it aims to maximise the difference in predictions between real and generated images.\nThe generator is still trying to trick the critic as in a regular GAN, so it wants the critic to score its generated images as highly as possible. To this end, it still compares to the desired critic response of \\(y_i=1\\) corresponding to the critic believing the generated image is real. So the generator loss minimisation remains similar, just without the log: \\[\n\\min_{G} -(E_{z \\sim p_Z}[D(G(z)] )\n\\]\n\n\n\n\nAllowing the critic to use the range \\((-\\infty, \\infty)\\) initially seems a bit counterintuitive - usually we want to avoid large numbers in neural networks otherwise gradients explode!\nThere is an additional constraint placed on the critic requiring it to be a 1-Lipschitz continuous function. This means that for any two input images, \\(x_1\\) and \\(x_2\\), it satisfied the following inequality: \\[\n\\frac{| D(x_1) - D(x_2) |}{| x_1 - x_2 |} \\le 1\n\\]\nRecall that the critic is a function \\(D\\) which converts an image into a scalar prediction. The numerator is the change in predictions, and the denominator is the average pixelwise difference. So this is essentially a limit on how sharply the critic predictions are allowed to change for a given image perturbation.\nAn in-depth exploration of why the Wasserstein loss requires this constraint is given here.\n\n\nOne crude way of enforcing the constraint suggested by the original authors (and described as “a terrible way to enforce a Lipschitz constraint) is to clip the weights to \\([-0.01, 0.01]\\) after each training batch. However, this diminishes the critic’s ability to learn.\nAn improved approach is to introduce a gradient penalty term to penalise the gradient norm when it deviates from 1.\n\n\n\n\nThe gradient penalty (GP) loss term encourages gradients towards 1 to conform to the 1-Lipschitz continuous constraint.\nThe GP loss measures the squared difference between the norm of the gradient of predictions w.r.t. input images and 1, i.e. \\[\nloss_{GP} = (||\\frac{\\delta predictions}{\\delta input}|| - 1)^2\n\\]\nCalculating this everywhere during the training process would be computationally intensive. Instead we evaluate it at a sample of points. To encourage a balanced mix, we use an interpolated image which is a pairwise weighted average of a real image and a generated image.\nThe combined loss function is then a weighted sum of the Wasserstein loss and the GP loss: \\[\nloss = loss_{Wasserstein} + \\lambda_{GP} * loss_{GP}\n\\]\n\n\n\nWhen training WGANs, we train the critic to convergence and then the train the generator. This ensures the gradients used for the generator update are accurate.\nThis is a major benefit over traditional GANs, where we must balance the alternating training of generator and discriminator.\nWe train the critic several times between each generator training step. A typical ratio is 3-5 critic updates for each generator update.\nNote that batch normalisation shouldn’t be used for WGAN-GP because it creates correlation between images of a given batch, which makes the GP term less effective.\n\n\n\nImages produced by VAEs tend to produce softer images that blur colour boundaries, whereas GANs produce sharper, more well-defined.\nHowever, GANs typically take longer to train and can be more sensitive to hyperparameter choices to reach a satisfactory result.\n\n\n\n\nConditional GANs allow us to control the type of image that is generated. For example, should we generate a large or small brick? A male or female face?\n\n\nThe key difference of a CGAN is we pass a one-hot encoded label.\nFor the generator, we append the one-hot encoded label vector to the random noise input to the generator. For the critic, we append the one-hot encoded label vector to the image. If it has multiple channels, as in an RGB image, the label vector is repeated on each of the channels to fit the required shape.\nThe critic can now see the label, which means the generator needs to create images that match the generated labels. If the generated image was inconsistent with the generated label, then the critic could use this to easily distinguish the fakes.\n\nThe only change required to the architecture is to concatenate the label to the inputs of the discriminator (actual training labels) and the generator (initially randomised vectors of the correct shape).\n\nThe images and labels are unpacked during the training step.\n\n\n\nWe can control the type of image generated by passing a particular one-hot encoded label into the input of the generator.\nIf labels are available for your training data, it is generally a good idea to include them in the inputs even if you don’t intend to create a conditional GAN from it. They tend to improve the quality of the output generated, since the labels act as a highly informative extension of the pixel inputs.\n\n\n\n\n\nChapter 4 of Generative Deep Learning by David Foster.\nWasserstein loss and Lipschitz constraint"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson4/chapter4.html#gans-1",
    "href": "posts/ml/generative_deep_learning/lesson4/chapter4.html#gans-1",
    "title": "Generative AI: GANs",
    "section": "",
    "text": "The idea of GANs is that we can train two competing models:\n\nThe generator tries to convert random noise into convincing observations.\nThe discriminator tries to predict whether an observation came from the original training dataset or is a “fake”.\n\nWe initialise both as random models; the generator outputs noise and the discriminator predicts randomly. We then alternate the training of the two networks so that the generator gets incrementally better at fooling the discriminator, then the discriminator gets incrementally better at spotting fakes.\n\n\n\n\n\nflowchart LR\n\n  A([Random noise]) --&gt; B[Generator] --&gt; C([Generated image]) \n\n  D([Image]) --&gt; E[Discriminator] --&gt; F([Prediction of realness probability])"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson4/chapter4.html#building-a-deep-convolutional-gan-dcgan",
    "href": "posts/ml/generative_deep_learning/lesson4/chapter4.html#building-a-deep-convolutional-gan-dcgan",
    "title": "Generative AI: GANs",
    "section": "",
    "text": "We will implement a GAN to generate pictures of bricks.\n\n\nLoad image data of lego bricks. We will train a model that can generate novel lego brick images.\n\n\nThe original data is scaled from [0, 255].\nOften we will rescale this to [0, 1] so that we can use sigmoid activation functions.\nIn this case we will scale to [-1, 1] so that we can use tanh activation functions, which tend to give stronger gradients than sigmoid.\n\nfrom pathlib import Path\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras import (\n    layers,\n    models,\n    callbacks,\n    losses,\n    utils,\n    metrics,\n    optimizers,\n)\n\n\ndef sample_batch(dataset):\n    batch = dataset.take(1).get_single_element()\n    if isinstance(batch, tuple):\n        batch = batch[0]\n    return batch.numpy()\n\n\ndef display_images(\n    images, n=10, size=(20, 3), cmap=\"gray_r\", as_type=\"float32\", save_to=None\n):\n    \"\"\"Displays n random images from each one of the supplied arrays.\"\"\"\n    if images.max() &gt; 1.0:\n        images = images / 255.0\n    elif images.min() &lt; 0.0:\n        images = (images + 1.0) / 2.0\n\n    plt.figure(figsize=size)\n    for i in range(n):\n        _ = plt.subplot(1, n, i + 1)\n        plt.imshow(images[i].astype(as_type), cmap=cmap)\n        plt.axis(\"off\")\n\n    if save_to:\n        plt.savefig(save_to)\n        print(f\"\\nSaved to {save_to}\")\n\n    plt.show()\n\nModel and data parameters:\n\nDATA_DIR = Path(\"/Users/gurpreetjohl/workspace/datasets/lego-brick-images\")\n\nIMAGE_SIZE = 64\nCHANNELS = 1\nBATCH_SIZE = 128\nZ_DIM = 100\nEPOCHS = 100\nLOAD_MODEL = False\nADAM_BETA_1 = 0.5\nADAM_BETA_2 = 0.999\nLEARNING_RATE = 0.0002\nNOISE_PARAM = 0.1\n\nLoad and pre-process the training data:\n\ndef preprocess(img):\n    \"\"\"Normalize and reshape the images.\"\"\"\n    img = (tf.cast(img, \"float32\") - 127.5) / 127.5\n    return img\n\n\ntraining_data = utils.image_dataset_from_directory(\n    DATA_DIR / \"dataset\",\n    labels=None,\n    color_mode=\"grayscale\",\n    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    seed=42,\n    interpolation=\"bilinear\",\n)\ntrain = training_data.map(lambda x: preprocess(x))\n\nFound 40000 files belonging to 1 classes.\n\n\nSome sample input images:\n\ndisplay_images(sample_batch(train))\n\n\n\n\n\n\n\n\n\n\n\n\nThe goal of the discriminator is to predict whether an image is real or fake.\nThis is a supervised binary classification problem, so we can use CNN architecture with a single output node. We stack Conv2D layers with BatchNormalization, LeakyReLU and Dropout layers sandwiched between.\n\ndiscriminator_input = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS))\n\nx = layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(discriminator_input)\nx = layers.LeakyReLU(0.2)(x)\nx = layers.Dropout(0.3)(x)\n\nx = layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\nx = layers.Dropout(0.3)(x)\n\nx = layers.Conv2D(256, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\nx = layers.Dropout(0.3)(x)\n\nx = layers.Conv2D(512, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\nx = layers.Dropout(0.3)(x)\n\nx = layers.Conv2D(1, kernel_size=4, strides=1, padding=\"valid\", use_bias=False, activation=\"sigmoid\")(x)\ndiscriminator_output = layers.Flatten()(x)  # The shape is already 1x1 so no need for a Dense layer after this\n\ndiscriminator = models.Model(discriminator_input, discriminator_output)\ndiscriminator.summary()\n\nModel: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 64, 64, 1)]       0         \n                                                                 \n conv2d_5 (Conv2D)           (None, 32, 32, 64)        1024      \n                                                                 \n leaky_re_lu_4 (LeakyReLU)   (None, 32, 32, 64)        0         \n                                                                 \n dropout_4 (Dropout)         (None, 32, 32, 64)        0         \n                                                                 \n conv2d_6 (Conv2D)           (None, 16, 16, 128)       131072    \n                                                                 \n batch_normalization_3 (Bat  (None, 16, 16, 128)       512       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_5 (LeakyReLU)   (None, 16, 16, 128)       0         \n                                                                 \n dropout_5 (Dropout)         (None, 16, 16, 128)       0         \n                                                                 \n conv2d_7 (Conv2D)           (None, 8, 8, 256)         524288    \n                                                                 \n batch_normalization_4 (Bat  (None, 8, 8, 256)         1024      \n chNormalization)                                                \n                                                                 \n leaky_re_lu_6 (LeakyReLU)   (None, 8, 8, 256)         0         \n                                                                 \n dropout_6 (Dropout)         (None, 8, 8, 256)         0         \n                                                                 \n conv2d_8 (Conv2D)           (None, 4, 4, 512)         2097152   \n                                                                 \n batch_normalization_5 (Bat  (None, 4, 4, 512)         2048      \n chNormalization)                                                \n                                                                 \n leaky_re_lu_7 (LeakyReLU)   (None, 4, 4, 512)         0         \n                                                                 \n dropout_7 (Dropout)         (None, 4, 4, 512)         0         \n                                                                 \n conv2d_9 (Conv2D)           (None, 1, 1, 1)           8192      \n                                                                 \n flatten_1 (Flatten)         (None, 1)                 0         \n                                                                 \n=================================================================\nTotal params: 2765312 (10.55 MB)\nTrainable params: 2763520 (10.54 MB)\nNon-trainable params: 1792 (7.00 KB)\n_________________________________________________________________\n\n\n\n\n\nThe purpose of the generator is to turn random noise into convincing images.\nThe input is a vector sampled from a multivariate Normal distribution, and the output is an image of the same size as the training data.\nThe discriminator-generator relationship in a GAN is similar to that of the encoder-decoder relations in a VAE.\nThe architecture of the discriminator is similar to the discriminator but in reverse (like a decoder). We pass stack Conv2DTranspose layers with BatchNormalization and LeakyReLU layers sandwiched in between.\n\n\nWe use Conv2DTranspose layers to scale the image size up.\nAn alternative would be to use stacks of Upsampling2D and Conv2D layers, i.e. the following serves the same purpose as a Conv2DTranspose layer:\nx = layers.Upsampling2D(size=2)(x)\nx = layers.Conv2D(256, kernel_size=4, strides=1, padding=\"same\")(x)\nThe Upsampling2D layer simply repeats each row and column to double its size, then Conv2D applies a convolution.\nThe idea is similar with Conv2DTranspose, but the extra rows and columns are filled with zeros rather than repeated existing values.\nConv2DTranspose layers can result in checkerboard pattern artifacts. Both options are used in practice, so it is often helpful to experiment and see which gives better results.\n\ngenerator_input = layers.Input(shape=(Z_DIM,))\n\nx = layers.Reshape((1, 1, Z_DIM))(generator_input)  # Reshape the input vector so we can apply conv transpose operations to it\n\nx = layers.Conv2DTranspose(512, kernel_size=4, strides=1, padding=\"valid\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\n\nx = layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\n\nx = layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\n\nx = layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\n\ngenerator_output = layers.Conv2DTranspose(\n    CHANNELS,\n    kernel_size=4,\n    strides=2,\n    padding=\"same\",\n    use_bias=False,\n    activation=\"tanh\",\n)(x)\n\ngenerator = models.Model(generator_input, generator_output)\ngenerator.summary()\n\nModel: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_3 (InputLayer)        [(None, 100)]             0         \n                                                                 \n reshape (Reshape)           (None, 1, 1, 100)         0         \n                                                                 \n conv2d_transpose (Conv2DTr  (None, 4, 4, 512)         819200    \n anspose)                                                        \n                                                                 \n batch_normalization_6 (Bat  (None, 4, 4, 512)         2048      \n chNormalization)                                                \n                                                                 \n leaky_re_lu_8 (LeakyReLU)   (None, 4, 4, 512)         0         \n                                                                 \n conv2d_transpose_1 (Conv2D  (None, 8, 8, 256)         2097152   \n Transpose)                                                      \n                                                                 \n batch_normalization_7 (Bat  (None, 8, 8, 256)         1024      \n chNormalization)                                                \n                                                                 \n leaky_re_lu_9 (LeakyReLU)   (None, 8, 8, 256)         0         \n                                                                 \n conv2d_transpose_2 (Conv2D  (None, 16, 16, 128)       524288    \n Transpose)                                                      \n                                                                 \n batch_normalization_8 (Bat  (None, 16, 16, 128)       512       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_10 (LeakyReLU)  (None, 16, 16, 128)       0         \n                                                                 \n conv2d_transpose_3 (Conv2D  (None, 32, 32, 64)        131072    \n Transpose)                                                      \n                                                                 \n batch_normalization_9 (Bat  (None, 32, 32, 64)        256       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_11 (LeakyReLU)  (None, 32, 32, 64)        0         \n                                                                 \n conv2d_transpose_4 (Conv2D  (None, 64, 64, 1)         1024      \n Transpose)                                                      \n                                                                 \n=================================================================\nTotal params: 3576576 (13.64 MB)\nTrainable params: 3574656 (13.64 MB)\nNon-trainable params: 1920 (7.50 KB)\n_________________________________________________________________\n\n\n\n\n\n\nWe alternate between training the discriminator and generator. They are not trained simultaneously. We want the generated images to be predicted close to 1 because the generator is good, not because the discriminator is weak.\nFor the discriminator, we create a training set where some images are real images from the training data and some are outputs from the generator. This is then a supervised binary classification problem.\nFor the generator, we want a way of scoring each generated image on its realness so that we can optimise this. The discriminator provides exactly this. We pass the generated images through the discriminator to get probabilities. The generator wants to fool the discriminator, so ideally this would be a vector of 1s. So the loss function is the binary crossentropy between these probabilities and a vector of 1s.\n\nclass DCGAN(models.Model):\n    def __init__(self, discriminator, generator, latent_dim):\n        super(DCGAN, self).__init__()\n        self.discriminator = discriminator\n        self.generator = generator\n        self.latent_dim = latent_dim\n\n    def compile(self, d_optimizer, g_optimizer):\n        super(DCGAN, self).compile()\n        self.loss_fn = losses.BinaryCrossentropy()\n        self.d_optimizer = d_optimizer\n        self.g_optimizer = g_optimizer\n        self.d_loss_metric = metrics.Mean(name=\"d_loss\")\n        self.d_real_acc_metric = metrics.BinaryAccuracy(name=\"d_real_acc\")\n        self.d_fake_acc_metric = metrics.BinaryAccuracy(name=\"d_fake_acc\")\n        self.d_acc_metric = metrics.BinaryAccuracy(name=\"d_acc\")\n        self.g_loss_metric = metrics.Mean(name=\"g_loss\")\n        self.g_acc_metric = metrics.BinaryAccuracy(name=\"g_acc\")\n\n    @property\n    def metrics(self):\n        return [\n            self.d_loss_metric,\n            self.d_real_acc_metric,\n            self.d_fake_acc_metric,\n            self.d_acc_metric,\n            self.g_loss_metric,\n            self.g_acc_metric,\n        ]\n\n    def train_step(self, real_images):\n        # Sample random points in the latent space\n        batch_size = tf.shape(real_images)[0]\n        random_latent_vectors = tf.random.normal(\n            shape=(batch_size, self.latent_dim)\n        )\n\n        # Train the discriminator on fake images\n        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n            generated_images = self.generator(\n                random_latent_vectors, training=True\n            )\n\n            # Evaluate the discriminator on the real and fake images\n            real_predictions = self.discriminator(real_images, training=True)\n            fake_predictions = self.discriminator(generated_images, training=True)\n\n            real_labels = tf.ones_like(real_predictions)\n            real_noisy_labels = real_labels + NOISE_PARAM * tf.random.uniform(\n                tf.shape(real_predictions)\n            )\n            fake_labels = tf.zeros_like(fake_predictions)\n            fake_noisy_labels = fake_labels - NOISE_PARAM * tf.random.uniform(\n                tf.shape(fake_predictions)\n            )\n\n            # Calculate the losses\n            d_real_loss = self.loss_fn(real_noisy_labels, real_predictions)\n            d_fake_loss = self.loss_fn(fake_noisy_labels, fake_predictions)\n            d_loss = (d_real_loss + d_fake_loss) / 2.0\n\n            g_loss = self.loss_fn(real_labels, fake_predictions)\n\n        # Update gradients\n        gradients_of_discriminator = disc_tape.gradient(\n            d_loss, self.discriminator.trainable_variables\n        )\n        gradients_of_generator = gen_tape.gradient(\n            g_loss, self.generator.trainable_variables\n        )\n\n        self.d_optimizer.apply_gradients(\n            zip(gradients_of_discriminator, discriminator.trainable_variables)\n        )\n        self.g_optimizer.apply_gradients(\n            zip(gradients_of_generator, generator.trainable_variables)\n        )\n\n        # Update metrics\n        self.d_loss_metric.update_state(d_loss)\n        self.d_real_acc_metric.update_state(real_labels, real_predictions)\n        self.d_fake_acc_metric.update_state(fake_labels, fake_predictions)\n        self.d_acc_metric.update_state(\n            [real_labels, fake_labels], [real_predictions, fake_predictions]\n        )\n        self.g_loss_metric.update_state(g_loss)\n        self.g_acc_metric.update_state(real_labels, fake_predictions)\n\n        return {m.name: m.result() for m in self.metrics}\n\n\n# Create a DCGAN\ndcgan = DCGAN(\n    discriminator=discriminator, generator=generator, latent_dim=Z_DIM\n)\n\n\ndcgan.compile(\n    d_optimizer=optimizers.legacy.Adam(\n        learning_rate=LEARNING_RATE, beta_1=ADAM_BETA_1, beta_2=ADAM_BETA_2\n    ),\n    g_optimizer=optimizers.legacy.Adam(\n        learning_rate=LEARNING_RATE, beta_1=ADAM_BETA_1, beta_2=ADAM_BETA_2\n    ),\n)\ndcgan.fit(train,  epochs=EPOCHS)\n\nEpoch 1/100\n313/313 [==============================] - 459s 1s/step - d_loss: 0.0252 - d_real_acc: 0.9011 - d_fake_acc: 0.9013 - d_acc: 0.9012 - g_loss: 5.3464 - g_acc: 0.0987\nEpoch 2/100\n313/313 [==============================] - 459s 1s/step - d_loss: 0.0454 - d_real_acc: 0.8986 - d_fake_acc: 0.8997 - d_acc: 0.8992 - g_loss: 5.2642 - g_acc: 0.1002\nEpoch 3/100\n313/313 [==============================] - 458s 1s/step - d_loss: 0.0556 - d_real_acc: 0.8958 - d_fake_acc: 0.8975 - d_acc: 0.8967 - g_loss: 4.9679 - g_acc: 0.1025\nEpoch 4/100\n313/313 [==============================] - 467s 1s/step - d_loss: 0.0246 - d_real_acc: 0.9065 - d_fake_acc: 0.9091 - d_acc: 0.9078 - g_loss: 5.1611 - g_acc: 0.0909\nEpoch 5/100\n313/313 [==============================] - 470s 1s/step - d_loss: 0.0178 - d_real_acc: 0.9067 - d_fake_acc: 0.9088 - d_acc: 0.9078 - g_loss: 5.1731 - g_acc: 0.0912\nEpoch 6/100\n313/313 [==============================] - 463s 1s/step - d_loss: 0.0314 - d_real_acc: 0.9116 - d_fake_acc: 0.9105 - d_acc: 0.9110 - g_loss: 5.2774 - g_acc: 0.0895\nEpoch 7/100\n313/313 [==============================] - 559s 2s/step - d_loss: 0.0229 - d_real_acc: 0.9085 - d_fake_acc: 0.9079 - d_acc: 0.9082 - g_loss: 5.3445 - g_acc: 0.0921\nEpoch 8/100\n313/313 [==============================] - 467s 1s/step - d_loss: -0.0155 - d_real_acc: 0.9161 - d_fake_acc: 0.9161 - d_acc: 0.9161 - g_loss: 5.7091 - g_acc: 0.0839\nEpoch 9/100\n313/313 [==============================] - 438s 1s/step - d_loss: -0.0077 - d_real_acc: 0.9220 - d_fake_acc: 0.9224 - d_acc: 0.9222 - g_loss: 5.8731 - g_acc: 0.0776\nEpoch 10/100\n313/313 [==============================] - 468s 1s/step - d_loss: -0.0472 - d_real_acc: 0.9228 - d_fake_acc: 0.9241 - d_acc: 0.9234 - g_loss: 5.9693 - g_acc: 0.0759\nEpoch 11/100\n313/313 [==============================] - 430s 1s/step - d_loss: -0.0839 - d_real_acc: 0.9404 - d_fake_acc: 0.9424 - d_acc: 0.9414 - g_loss: 6.1212 - g_acc: 0.0576\nEpoch 12/100\n313/313 [==============================] - 457s 1s/step - d_loss: 0.0431 - d_real_acc: 0.9053 - d_fake_acc: 0.9046 - d_acc: 0.9049 - g_loss: 6.0708 - g_acc: 0.0954\nEpoch 13/100\n313/313 [==============================] - 448s 1s/step - d_loss: -0.0154 - d_real_acc: 0.9236 - d_fake_acc: 0.9244 - d_acc: 0.9240 - g_loss: 6.3106 - g_acc: 0.0756\nEpoch 14/100\n313/313 [==============================] - 432s 1s/step - d_loss: -0.0720 - d_real_acc: 0.9320 - d_fake_acc: 0.9342 - d_acc: 0.9331 - g_loss: 6.6509 - g_acc: 0.0658\nEpoch 15/100\n313/313 [==============================] - 443s 1s/step - d_loss: 0.0057 - d_real_acc: 0.9072 - d_fake_acc: 0.9097 - d_acc: 0.9085 - g_loss: 6.1399 - g_acc: 0.0903\nEpoch 16/100\n313/313 [==============================] - 427s 1s/step - d_loss: 0.0069 - d_real_acc: 0.9185 - d_fake_acc: 0.9167 - d_acc: 0.9176 - g_loss: 6.3255 - g_acc: 0.0833\nEpoch 17/100\n313/313 [==============================] - 439s 1s/step - d_loss: -0.0709 - d_real_acc: 0.9220 - d_fake_acc: 0.9239 - d_acc: 0.9230 - g_loss: 6.8108 - g_acc: 0.0761\nEpoch 18/100\n313/313 [==============================] - 437s 1s/step - d_loss: 0.0373 - d_real_acc: 0.9288 - d_fake_acc: 0.9512 - d_acc: 0.9400 - g_loss: 8.1066 - g_acc: 0.0488\nEpoch 19/100\n313/313 [==============================] - 1029s 3s/step - d_loss: -0.1154 - d_real_acc: 0.9408 - d_fake_acc: 0.9420 - d_acc: 0.9414 - g_loss: 7.6274 - g_acc: 0.0580\nEpoch 20/100\n313/313 [==============================] - 5781s 19s/step - d_loss: -0.0431 - d_real_acc: 0.9222 - d_fake_acc: 0.9231 - d_acc: 0.9227 - g_loss: 7.1953 - g_acc: 0.0769\nEpoch 21/100\n313/313 [==============================] - 2696s 9s/step - d_loss: -0.0542 - d_real_acc: 0.9176 - d_fake_acc: 0.9205 - d_acc: 0.9191 - g_loss: 7.1675 - g_acc: 0.0794\nEpoch 22/100\n313/313 [==============================] - 1481s 5s/step - d_loss: -0.1424 - d_real_acc: 0.9398 - d_fake_acc: 0.9400 - d_acc: 0.9399 - g_loss: 7.7399 - g_acc: 0.0600\nEpoch 23/100\n313/313 [==============================] - 439s 1s/step - d_loss: -0.0263 - d_real_acc: 0.9154 - d_fake_acc: 0.9148 - d_acc: 0.9151 - g_loss: 7.3322 - g_acc: 0.0852\nEpoch 24/100\n313/313 [==============================] - 443s 1s/step - d_loss: -0.1420 - d_real_acc: 0.9406 - d_fake_acc: 0.9430 - d_acc: 0.9418 - g_loss: 8.1502 - g_acc: 0.0570\nEpoch 25/100\n313/313 [==============================] - 475s 2s/step - d_loss: -0.1650 - d_real_acc: 0.9393 - d_fake_acc: 0.9395 - d_acc: 0.9394 - g_loss: 7.9602 - g_acc: 0.0605\nEpoch 26/100\n313/313 [==============================] - 447s 1s/step - d_loss: -0.1247 - d_real_acc: 0.9419 - d_fake_acc: 0.9437 - d_acc: 0.9428 - g_loss: 7.7939 - g_acc: 0.0563\nEpoch 27/100\n313/313 [==============================] - 1356s 4s/step - d_loss: -0.0182 - d_real_acc: 0.9160 - d_fake_acc: 0.9212 - d_acc: 0.9186 - g_loss: 6.9180 - g_acc: 0.0787\nEpoch 28/100\n313/313 [==============================] - 2219s 7s/step - d_loss: -0.2227 - d_real_acc: 0.9511 - d_fake_acc: 0.9519 - d_acc: 0.9515 - g_loss: 8.1970 - g_acc: 0.0481\nEpoch 29/100\n313/313 [==============================] - 5807s 19s/step - d_loss: -0.1091 - d_real_acc: 0.9318 - d_fake_acc: 0.9320 - d_acc: 0.9319 - g_loss: 7.5829 - g_acc: 0.0680\nEpoch 30/100\n313/313 [==============================] - 2511s 8s/step - d_loss: -0.3131 - d_real_acc: 0.9571 - d_fake_acc: 0.9604 - d_acc: 0.9588 - g_loss: 9.8839 - g_acc: 0.0395\nEpoch 31/100\n313/313 [==============================] - 2768s 9s/step - d_loss: -0.0996 - d_real_acc: 0.9269 - d_fake_acc: 0.9286 - d_acc: 0.9277 - g_loss: 8.3337 - g_acc: 0.0714\nEpoch 32/100\n313/313 [==============================] - 3046s 10s/step - d_loss: -0.1619 - d_real_acc: 0.9423 - d_fake_acc: 0.9482 - d_acc: 0.9453 - g_loss: 8.2435 - g_acc: 0.0518\nEpoch 33/100\n313/313 [==============================] - 3478s 11s/step - d_loss: -0.1182 - d_real_acc: 0.9284 - d_fake_acc: 0.9304 - d_acc: 0.9294 - g_loss: 8.1681 - g_acc: 0.0696\nEpoch 34/100\n313/313 [==============================] - 2776s 9s/step - d_loss: -0.2214 - d_real_acc: 0.9459 - d_fake_acc: 0.9582 - d_acc: 0.9520 - g_loss: 9.9168 - g_acc: 0.0417\nEpoch 35/100\n313/313 [==============================] - 2724s 9s/step - d_loss: -0.3101 - d_real_acc: 0.9421 - d_fake_acc: 0.9293 - d_acc: 0.9357 - g_loss: 13.2857 - g_acc: 0.0707\nEpoch 36/100\n313/313 [==============================] - 2648s 8s/step - d_loss: -0.0441 - d_real_acc: 0.8963 - d_fake_acc: 0.8961 - d_acc: 0.8962 - g_loss: 7.5664 - g_acc: 0.1038\nEpoch 37/100\n313/313 [==============================] - 3262s 10s/step - d_loss: -0.0859 - d_real_acc: 0.9314 - d_fake_acc: 0.9402 - d_acc: 0.9358 - g_loss: 8.3591 - g_acc: 0.0598\nEpoch 38/100\n313/313 [==============================] - 2612s 8s/step - d_loss: -0.2979 - d_real_acc: 0.9554 - d_fake_acc: 0.9577 - d_acc: 0.9566 - g_loss: 9.2534 - g_acc: 0.0423\nEpoch 39/100\n313/313 [==============================] - 2235s 7s/step - d_loss: -0.3387 - d_real_acc: 0.9607 - d_fake_acc: 0.9622 - d_acc: 0.9615 - g_loss: 9.9397 - g_acc: 0.0378\nEpoch 40/100\n313/313 [==============================] - 3453s 11s/step - d_loss: -0.1056 - d_real_acc: 0.9279 - d_fake_acc: 0.9310 - d_acc: 0.9294 - g_loss: 8.9394 - g_acc: 0.0690\nEpoch 41/100\n313/313 [==============================] - 2316s 7s/step - d_loss: -0.2147 - d_real_acc: 0.9318 - d_fake_acc: 0.9327 - d_acc: 0.9323 - g_loss: 9.0337 - g_acc: 0.0673\nEpoch 42/100\n313/313 [==============================] - 3134s 10s/step - d_loss: -0.2554 - d_real_acc: 0.9511 - d_fake_acc: 0.9540 - d_acc: 0.9526 - g_loss: 9.5571 - g_acc: 0.0460\nEpoch 43/100\n313/313 [==============================] - 3933s 13s/step - d_loss: -0.2871 - d_real_acc: 0.9490 - d_fake_acc: 0.9526 - d_acc: 0.9508 - g_loss: 10.3316 - g_acc: 0.0474\nEpoch 44/100\n313/313 [==============================] - 3248s 10s/step - d_loss: -0.3456 - d_real_acc: 0.9635 - d_fake_acc: 0.9635 - d_acc: 0.9635 - g_loss: 9.8675 - g_acc: 0.0364\nEpoch 45/100\n313/313 [==============================] - 3043s 10s/step - d_loss: -0.3274 - d_real_acc: 0.9603 - d_fake_acc: 0.9618 - d_acc: 0.9610 - g_loss: 10.4185 - g_acc: 0.0382\nEpoch 46/100\n313/313 [==============================] - 2706s 9s/step - d_loss: -0.6160 - d_real_acc: 0.9902 - d_fake_acc: 0.9908 - d_acc: 0.9905 - g_loss: 13.0574 - g_acc: 0.0092\nEpoch 47/100\n313/313 [==============================] - 2453s 8s/step - d_loss: 0.3413 - d_real_acc: 0.8073 - d_fake_acc: 0.8054 - d_acc: 0.8064 - g_loss: 6.5391 - g_acc: 0.1946\nEpoch 48/100\n313/313 [==============================] - 2898s 9s/step - d_loss: -0.4416 - d_real_acc: 0.9764 - d_fake_acc: 0.9784 - d_acc: 0.9774 - g_loss: 10.8318 - g_acc: 0.0216\nEpoch 49/100\n313/313 [==============================] - 3358s 11s/step - d_loss: 6.8776 - d_real_acc: 0.1058 - d_fake_acc: 0.9910 - d_acc: 0.5484 - g_loss: 14.8921 - g_acc: 0.0090\nEpoch 50/100\n313/313 [==============================] - 2940s 9s/step - d_loss: 7.7113 - d_real_acc: 0.0000e+00 - d_fake_acc: 1.0000 - d_acc: 0.5000 - g_loss: 15.4250 - g_acc: 0.0000e+00\nEpoch 51/100\n313/313 [==============================] - 2983s 10s/step - d_loss: 7.7121 - d_real_acc: 0.0000e+00 - d_fake_acc: 1.0000 - d_acc: 0.5000 - g_loss: 15.4250 - g_acc: 0.0000e+00\nEpoch 52/100\n313/313 [==============================] - 3458s 11s/step - d_loss: 7.7149 - d_real_acc: 0.0000e+00 - d_fake_acc: 1.0000 - d_acc: 0.5000 - g_loss: 15.4250 - g_acc: 0.0000e+00\nEpoch 53/100\n313/313 [==============================] - 2404s 8s/step - d_loss: 4.4950 - d_real_acc: 0.3543 - d_fake_acc: 0.8865 - d_acc: 0.6204 - g_loss: 10.6772 - g_acc: 0.1135\nEpoch 54/100\n313/313 [==============================] - 3297s 11s/step - d_loss: -0.0132 - d_real_acc: 0.9068 - d_fake_acc: 0.9010 - d_acc: 0.9039 - g_loss: 7.9660 - g_acc: 0.0990\nEpoch 55/100\n313/313 [==============================] - 2486s 8s/step - d_loss: -0.3508 - d_real_acc: 0.9615 - d_fake_acc: 0.9612 - d_acc: 0.9614 - g_loss: 10.2242 - g_acc: 0.0388\nEpoch 56/100\n313/313 [==============================] - 2995s 10s/step - d_loss: -0.3125 - d_real_acc: 0.9525 - d_fake_acc: 0.9533 - d_acc: 0.9529 - g_loss: 10.4182 - g_acc: 0.0467\nEpoch 57/100\n313/313 [==============================] - 1791s 6s/step - d_loss: -0.3201 - d_real_acc: 0.9532 - d_fake_acc: 0.9560 - d_acc: 0.9546 - g_loss: 10.4752 - g_acc: 0.0441\nEpoch 58/100\n313/313 [==============================] - 2792s 9s/step - d_loss: -0.2649 - d_real_acc: 0.9509 - d_fake_acc: 0.9532 - d_acc: 0.9520 - g_loss: 9.3587 - g_acc: 0.0468\nEpoch 59/100\n313/313 [==============================] - 3665s 12s/step - d_loss: -0.1747 - d_real_acc: 0.9413 - d_fake_acc: 0.9584 - d_acc: 0.9499 - g_loss: 10.1369 - g_acc: 0.0416\nEpoch 60/100\n313/313 [==============================] - 2493s 8s/step - d_loss: -0.2692 - d_real_acc: 0.9499 - d_fake_acc: 0.9534 - d_acc: 0.9517 - g_loss: 9.7124 - g_acc: 0.0466\nEpoch 61/100\n313/313 [==============================] - 2293s 7s/step - d_loss: -0.2869 - d_real_acc: 0.9520 - d_fake_acc: 0.9556 - d_acc: 0.9538 - g_loss: 10.2684 - g_acc: 0.0444\nEpoch 62/100\n313/313 [==============================] - 2865s 9s/step - d_loss: -0.6188 - d_real_acc: 0.9900 - d_fake_acc: 0.9901 - d_acc: 0.9901 - g_loss: 12.9584 - g_acc: 0.0099\nEpoch 63/100\n313/313 [==============================] - 2301s 7s/step - d_loss: -0.7197 - d_real_acc: 0.9984 - d_fake_acc: 0.9985 - d_acc: 0.9985 - g_loss: 14.5762 - g_acc: 0.0015\nEpoch 64/100\n313/313 [==============================] - 2404s 8s/step - d_loss: -0.4320 - d_real_acc: 0.9665 - d_fake_acc: 0.9702 - d_acc: 0.9683 - g_loss: 12.0177 - g_acc: 0.0298\nEpoch 65/100\n313/313 [==============================] - 4723s 15s/step - d_loss: 6.7591 - d_real_acc: 0.9940 - d_fake_acc: 0.1077 - d_acc: 0.5509 - g_loss: 1.4402 - g_acc: 0.8923\nEpoch 66/100\n313/313 [==============================] - 2726s 9s/step - d_loss: 7.6259 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 67/100\n313/313 [==============================] - 3325s 11s/step - d_loss: 7.6250 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 68/100\n313/313 [==============================] - 2513s 8s/step - d_loss: 7.6251 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 7.4387e-12 - g_acc: 1.0000\nEpoch 69/100\n313/313 [==============================] - 3304s 11s/step - d_loss: 3.4076 - d_real_acc: 0.8552 - d_fake_acc: 0.5472 - d_acc: 0.7012 - g_loss: 5.2072 - g_acc: 0.4528\nEpoch 70/100\n313/313 [==============================] - 2276s 7s/step - d_loss: -0.2424 - d_real_acc: 0.9448 - d_fake_acc: 0.9581 - d_acc: 0.9514 - g_loss: 10.5206 - g_acc: 0.0419\nEpoch 71/100\n313/313 [==============================] - 3315s 11s/step - d_loss: -0.3093 - d_real_acc: 0.9563 - d_fake_acc: 0.9640 - d_acc: 0.9602 - g_loss: 10.6076 - g_acc: 0.0360\nEpoch 72/100\n313/313 [==============================] - 2306s 7s/step - d_loss: -0.2440 - d_real_acc: 0.9466 - d_fake_acc: 0.9581 - d_acc: 0.9523 - g_loss: 10.1996 - g_acc: 0.0419\nEpoch 73/100\n313/313 [==============================] - 3218s 10s/step - d_loss: -0.7206 - d_real_acc: 0.9985 - d_fake_acc: 0.9983 - d_acc: 0.9984 - g_loss: 14.6350 - g_acc: 0.0017\nEpoch 74/100\n313/313 [==============================] - 3258s 10s/step - d_loss: -0.6281 - d_real_acc: 0.9828 - d_fake_acc: 0.9841 - d_acc: 0.9834 - g_loss: 14.5219 - g_acc: 0.0159\nEpoch 75/100\n313/313 [==============================] - 2874s 9s/step - d_loss: -0.0555 - d_real_acc: 0.9254 - d_fake_acc: 0.9371 - d_acc: 0.9312 - g_loss: 9.3443 - g_acc: 0.0629\nEpoch 76/100\n313/313 [==============================] - 2559s 8s/step - d_loss: -0.2825 - d_real_acc: 0.9515 - d_fake_acc: 0.9611 - d_acc: 0.9563 - g_loss: 10.7583 - g_acc: 0.0388\nEpoch 77/100\n313/313 [==============================] - 3663s 12s/step - d_loss: -0.3945 - d_real_acc: 0.9667 - d_fake_acc: 0.9691 - d_acc: 0.9679 - g_loss: 10.8566 - g_acc: 0.0309\nEpoch 78/100\n313/313 [==============================] - 2314s 7s/step - d_loss: -0.3953 - d_real_acc: 0.9508 - d_fake_acc: 0.9529 - d_acc: 0.9519 - g_loss: 12.0037 - g_acc: 0.0471\nEpoch 79/100\n313/313 [==============================] - 2816s 9s/step - d_loss: -0.6059 - d_real_acc: 0.9841 - d_fake_acc: 0.9838 - d_acc: 0.9840 - g_loss: 13.3516 - g_acc: 0.0162\nEpoch 80/100\n313/313 [==============================] - 3232s 10s/step - d_loss: -0.3555 - d_real_acc: 0.9587 - d_fake_acc: 0.9649 - d_acc: 0.9618 - g_loss: 11.2453 - g_acc: 0.0351\nEpoch 81/100\n313/313 [==============================] - 3705s 12s/step - d_loss: -0.4501 - d_real_acc: 0.9731 - d_fake_acc: 0.9743 - d_acc: 0.9737 - g_loss: 11.4553 - g_acc: 0.0258\nEpoch 82/100\n313/313 [==============================] - 2364s 8s/step - d_loss: -0.3827 - d_real_acc: 0.9588 - d_fake_acc: 0.9639 - d_acc: 0.9613 - g_loss: 11.6275 - g_acc: 0.0361\nEpoch 83/100\n313/313 [==============================] - 1122s 4s/step - d_loss: -0.4355 - d_real_acc: 0.9642 - d_fake_acc: 0.9674 - d_acc: 0.9658 - g_loss: 12.1025 - g_acc: 0.0326\nEpoch 84/100\n313/313 [==============================] - 4065s 13s/step - d_loss: -0.4456 - d_real_acc: 0.9695 - d_fake_acc: 0.9714 - d_acc: 0.9704 - g_loss: 11.6065 - g_acc: 0.0287\nEpoch 85/100\n313/313 [==============================] - 4461s 14s/step - d_loss: -0.6405 - d_real_acc: 0.9901 - d_fake_acc: 0.9899 - d_acc: 0.9900 - g_loss: 13.4694 - g_acc: 0.0101\nEpoch 86/100\n313/313 [==============================] - 2630s 8s/step - d_loss: -0.6431 - d_real_acc: 0.9857 - d_fake_acc: 0.9856 - d_acc: 0.9857 - g_loss: 14.3623 - g_acc: 0.0144\nEpoch 87/100\n313/313 [==============================] - 2567s 8s/step - d_loss: -0.3870 - d_real_acc: 0.9534 - d_fake_acc: 0.9578 - d_acc: 0.9556 - g_loss: 12.0201 - g_acc: 0.0422\nEpoch 88/100\n313/313 [==============================] - 2597s 8s/step - d_loss: -0.7624 - d_real_acc: 0.9999 - d_fake_acc: 0.9998 - d_acc: 0.9998 - g_loss: 15.3547 - g_acc: 2.5000e-04\nEpoch 89/100\n313/313 [==============================] - 1477s 5s/step - d_loss: -0.5787 - d_real_acc: 0.9764 - d_fake_acc: 0.9759 - d_acc: 0.9762 - g_loss: 13.8500 - g_acc: 0.0241\nEpoch 90/100\n313/313 [==============================] - 522s 2s/step - d_loss: -0.6747 - d_real_acc: 0.9885 - d_fake_acc: 0.9897 - d_acc: 0.9891 - g_loss: 14.5329 - g_acc: 0.0104\nEpoch 91/100\n313/313 [==============================] - 512s 2s/step - d_loss: 6.4703 - d_real_acc: 0.9892 - d_fake_acc: 0.1438 - d_acc: 0.5665 - g_loss: 2.1184 - g_acc: 0.8562\nEpoch 92/100\n313/313 [==============================] - 514s 2s/step - d_loss: 7.6245 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 93/100\n313/313 [==============================] - 533s 2s/step - d_loss: 7.6249 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 94/100\n313/313 [==============================] - 499s 2s/step - d_loss: 7.6236 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 95/100\n313/313 [==============================] - 483s 2s/step - d_loss: 7.6240 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 96/100\n313/313 [==============================] - 481s 2s/step - d_loss: 7.6248 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 97/100\n313/313 [==============================] - 488s 2s/step - d_loss: 7.6247 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 98/100\n313/313 [==============================] - 481s 2s/step - d_loss: 7.6263 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 99/100\n313/313 [==============================] - 459s 1s/step - d_loss: 7.6235 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 100/100\n313/313 [==============================] - 4669s 15s/step - d_loss: 7.6231 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\n\n\n&lt;keras.src.callbacks.History at 0x10f686690&gt;\n\n\n\n# Save the final models\ngenerator.save(\"./models/generator\")\ndiscriminator.save(\"./models/discriminator\")\n\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nINFO:tensorflow:Assets written to: ./models/generator/assets\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nINFO:tensorflow:Assets written to: ./models/discriminator/assets\n\n\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nINFO:tensorflow:Assets written to: ./models/generator/assets\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nINFO:tensorflow:Assets written to: ./models/discriminator/assets"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson4/chapter4.html#analysing-the-gan",
    "href": "posts/ml/generative_deep_learning/lesson4/chapter4.html#analysing-the-gan",
    "title": "Generative AI: GANs",
    "section": "",
    "text": "We can see some examples of images produced by the GAN.\n(I don’t have a GPU so training is slow and I only trained 100 epochs… they’re a bit crap)\n\n# Sample some points in the latent space, from the standard normal distribution\ngrid_width, grid_height = (10, 3)\nz_sample = np.random.normal(size=(grid_width * grid_height, Z_DIM))\n\n# Decode the sampled points\nreconstructions = generator.predict(z_sample)\n\n# Draw a plot of decoded images\nfig = plt.figure(figsize=(18, 5))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\n# Output the grid of faces\nfor i in range(grid_width * grid_height):\n    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n    ax.axis(\"off\")\n    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")\n\n\n\n\n\n\n\n\nWe also want to make sure a generative model doesn’t simply recreate images that are already in the training set.\nAs a sanity check, we plot some generated images and the closest training images (using the L1 distance). This confirms that the generator is able to understand high-level features, even though we didn’t provide anything other than raw pixels, and it can generate examples distinct from those encountered before.\n\ndef compare_images(img1, img2):\n    return np.mean(np.abs(img1 - img2))\n\nall_data = []\nfor i in train.as_numpy_iterator():\n    all_data.extend(i)\nall_data = np.array(all_data)\n\n# Plot the images\nr, c = 3, 5\nfig, axs = plt.subplots(r, c, figsize=(10, 6))\nfig.suptitle(\"Generated images\", fontsize=20)\n\nnoise = np.random.normal(size=(r * c, Z_DIM))\ngen_imgs = generator.predict(noise)\n\ncnt = 0\nfor i in range(r):\n    for j in range(c):\n        axs[i, j].imshow(gen_imgs[cnt], cmap=\"gray_r\")\n        axs[i, j].axis(\"off\")\n        cnt += 1\n\nplt.show()\n\n1/1 [==============================] - 0s 80ms/step\n\n\n\n\n\n\n\n\n\n\nfig, axs = plt.subplots(r, c, figsize=(10, 6))\nfig.suptitle(\"Closest images in the training set\", fontsize=20)\n\ncnt = 0\nfor i in range(r):\n    for j in range(c):\n        c_diff = 99999\n        c_img = None\n        for k_idx, k in enumerate(all_data):\n            diff = compare_images(gen_imgs[cnt], k)\n            if diff &lt; c_diff:\n                c_img = np.copy(k)\n                c_diff = diff\n        axs[i, j].imshow(c_img, cmap=\"gray_r\")\n        axs[i, j].axis(\"off\")\n        cnt += 1\n\nplt.show()"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson4/chapter4.html#gan-training-tips",
    "href": "posts/ml/generative_deep_learning/lesson4/chapter4.html#gan-training-tips",
    "title": "Generative AI: GANs",
    "section": "",
    "text": "GANs are notoriously difficult to train because there is a balancing act between the generator and discriminator; neither should grow so strong that it overpowers the other.\n\n\nThe discriminator can always spot the fakes, so the signal from the loss function becomes too weak to cause any meaningful improvements in the generator.\nIn the extreme case, the discriminator distinguishes fakes perfectly, so gradients vanish and no training takes place.\nWe need to weaken the discriminator in this case. Some possible options are:\n\nMore dropout\nLower learning rate\nSimplify the discriminator architecture - use fewer layers\nAdd noise to the labels when training the discriminator\nAdd intentional labelling errors - randomly flip the labels of some images when training the discriminator\n\n\n\n\nIf the discriminator is too weak, the generator will learn that it can trick the discriminator using a small sample of nearly identical images. This is known as mode collapse. The generator would map every point in the latent space to this image, so the gradients of the loss function would vanish and it would not be able to recover.\nStrengthening the discriminator would not help because the generator would just learn to find a different mode that fools the discriminator with no diversity; it is numb to its input.\nSome possible options are:\n\nStrengthen the discriminator - do the opposite of the previous section\nReduce the learning rate of both generator and discriminator\nIncrease the batch size"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson4/chapter4.html#uninformative-loss",
    "href": "posts/ml/generative_deep_learning/lesson4/chapter4.html#uninformative-loss",
    "title": "Generative AI: GANs",
    "section": "",
    "text": "The value of the loss is not meaningful as a measure of the generator’s strength when training.\nThe loss function is relative to the discriminator, and since the discriminator is also being trained, the goalposts are constantly shifting. Also, we don’t want the loss function to reach 0 or else we may reach mode collapse as described above.\nThis makes GAN training difficult to monitor."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson4/chapter4.html#hyperparameters",
    "href": "posts/ml/generative_deep_learning/lesson4/chapter4.html#hyperparameters",
    "title": "Generative AI: GANs",
    "section": "",
    "text": "There are a lot of hyperparameters involved with GANs because we are now training two networks.\nThe performance is highly sensitive to these hyperparameter choices, and involves a lot of trial and error."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson4/chapter4.html#wasserstein-gan-with-gradient-penalty-wgan-gp",
    "href": "posts/ml/generative_deep_learning/lesson4/chapter4.html#wasserstein-gan-with-gradient-penalty-wgan-gp",
    "title": "Generative AI: GANs",
    "section": "",
    "text": "The Wasserstein GAN replaces the binary crossentropy loss function with the Wassserstein lss function in both the discriminator and generator.\nThis results in two desirable properties:\n\nA meaningful loss metric that correlates with generator convergence. This allows for better monitoring of training.\nMore stable optimisation process.\n\n\n\n\n\nFirst, recall the binary cross-entropy loss, which is defined as: \\[\n-\\frac{1}{n} \\sum_{i=1}^{n}{y_i log(p_i) + (1 - y_i)log(1-p_i)}\n\\]\nIn a regular GAN, the discriminator compares the predictions for real images \\(p_i = D(x_i)\\) to the response \\(y_i = 1\\), and it compares the predictions for generated images \\(p_i = D(G(z_i))\\) to the response \\(y_i = 0\\).\nSo the discriminator loss minimisation can be written as: \\[\n\\min_{D} -(E_{x \\sim p_X}[log D(x)] + E_{z \\sim p_Z}[log (1 - D(G(z))] )\n\\]\nThe generator aims to trick the discriminator into believing the images are real, so it compares the discriminator’s predicted response to the desired response of \\(y_i=1\\). So the generator loss minimisation can be written as: \\[\n\\min_{G} -(E_{z \\sim p_Z}[log D(G(z)] )\n\\]\n\n\n\nIn contrast , the Wasserstein loss function is defined as: \\[\n-\\frac{1}{n} \\sum_{i=1}^{n}{y_i p_i}\n\\]\nIt requires that we use \\(y_i=1\\) and \\(y_i=-1\\) as labels rather than 1 and 0. We also remove the final sigmoid activation layer, which constrained the output to the range \\([0, 1]\\), meaning the output can now be any real number in the range \\((-\\infty, \\infty)\\).\nBecause of these changes, rather than referring to a discriminator which outputs a probability, for WGANs we refer to a critic which outputs a score.\nWith these changes of labels to -1 and 1, the critic loss minimisation becomes: \\[\n\\min_{D} -(E_{x \\sim p_X}[D(x)] - E_{z \\sim p_Z}[D(G(z)] )\n\\]\ni.e. it aims to maximise the difference in predictions between real and generated images.\nThe generator is still trying to trick the critic as in a regular GAN, so it wants the critic to score its generated images as highly as possible. To this end, it still compares to the desired critic response of \\(y_i=1\\) corresponding to the critic believing the generated image is real. So the generator loss minimisation remains similar, just without the log: \\[\n\\min_{G} -(E_{z \\sim p_Z}[D(G(z)] )\n\\]\n\n\n\n\nAllowing the critic to use the range \\((-\\infty, \\infty)\\) initially seems a bit counterintuitive - usually we want to avoid large numbers in neural networks otherwise gradients explode!\nThere is an additional constraint placed on the critic requiring it to be a 1-Lipschitz continuous function. This means that for any two input images, \\(x_1\\) and \\(x_2\\), it satisfied the following inequality: \\[\n\\frac{| D(x_1) - D(x_2) |}{| x_1 - x_2 |} \\le 1\n\\]\nRecall that the critic is a function \\(D\\) which converts an image into a scalar prediction. The numerator is the change in predictions, and the denominator is the average pixelwise difference. So this is essentially a limit on how sharply the critic predictions are allowed to change for a given image perturbation.\nAn in-depth exploration of why the Wasserstein loss requires this constraint is given here.\n\n\nOne crude way of enforcing the constraint suggested by the original authors (and described as “a terrible way to enforce a Lipschitz constraint) is to clip the weights to \\([-0.01, 0.01]\\) after each training batch. However, this diminishes the critic’s ability to learn.\nAn improved approach is to introduce a gradient penalty term to penalise the gradient norm when it deviates from 1.\n\n\n\n\nThe gradient penalty (GP) loss term encourages gradients towards 1 to conform to the 1-Lipschitz continuous constraint.\nThe GP loss measures the squared difference between the norm of the gradient of predictions w.r.t. input images and 1, i.e. \\[\nloss_{GP} = (||\\frac{\\delta predictions}{\\delta input}|| - 1)^2\n\\]\nCalculating this everywhere during the training process would be computationally intensive. Instead we evaluate it at a sample of points. To encourage a balanced mix, we use an interpolated image which is a pairwise weighted average of a real image and a generated image.\nThe combined loss function is then a weighted sum of the Wasserstein loss and the GP loss: \\[\nloss = loss_{Wasserstein} + \\lambda_{GP} * loss_{GP}\n\\]\n\n\n\nWhen training WGANs, we train the critic to convergence and then the train the generator. This ensures the gradients used for the generator update are accurate.\nThis is a major benefit over traditional GANs, where we must balance the alternating training of generator and discriminator.\nWe train the critic several times between each generator training step. A typical ratio is 3-5 critic updates for each generator update.\nNote that batch normalisation shouldn’t be used for WGAN-GP because it creates correlation between images of a given batch, which makes the GP term less effective.\n\n\n\nImages produced by VAEs tend to produce softer images that blur colour boundaries, whereas GANs produce sharper, more well-defined.\nHowever, GANs typically take longer to train and can be more sensitive to hyperparameter choices to reach a satisfactory result."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson4/chapter4.html#conditional-gan-cgan",
    "href": "posts/ml/generative_deep_learning/lesson4/chapter4.html#conditional-gan-cgan",
    "title": "Generative AI: GANs",
    "section": "",
    "text": "Conditional GANs allow us to control the type of image that is generated. For example, should we generate a large or small brick? A male or female face?\n\n\nThe key difference of a CGAN is we pass a one-hot encoded label.\nFor the generator, we append the one-hot encoded label vector to the random noise input to the generator. For the critic, we append the one-hot encoded label vector to the image. If it has multiple channels, as in an RGB image, the label vector is repeated on each of the channels to fit the required shape.\nThe critic can now see the label, which means the generator needs to create images that match the generated labels. If the generated image was inconsistent with the generated label, then the critic could use this to easily distinguish the fakes.\n\nThe only change required to the architecture is to concatenate the label to the inputs of the discriminator (actual training labels) and the generator (initially randomised vectors of the correct shape).\n\nThe images and labels are unpacked during the training step.\n\n\n\nWe can control the type of image generated by passing a particular one-hot encoded label into the input of the generator.\nIf labels are available for your training data, it is generally a good idea to include them in the inputs even if you don’t intend to create a conditional GAN from it. They tend to improve the quality of the output generated, since the labels act as a highly informative extension of the pixel inputs."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson4/chapter4.html#references",
    "href": "posts/ml/generative_deep_learning/lesson4/chapter4.html#references",
    "title": "Generative AI: GANs",
    "section": "",
    "text": "Chapter 4 of Generative Deep Learning by David Foster.\nWasserstein loss and Lipschitz constraint"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html",
    "href": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html",
    "title": "Generative AI: Deep Learning Foundations",
    "section": "",
    "text": "Notes on deep learning concepts in the context of generative AI.\n\n\n\nDeep learning is a class of ML algorithms that use multiple stacked layers of processing units to learn high-level representations from unstructured data.\n\nStructured data is naturally arranged into columns of features. Think of relational database tables.\nBy contrast, unstructured data refers to any data that is not structured in tabular format. E.g. images, audio, text. Individual pixels – or frequencies or words – are not informative alone, but higher-level representations are, so we aim to learn these.\n\n\n\nMost deep learning systems in practice are ANNs,so deep learning has become synonymous with deep deural networks, vut there are other forms such as deep belief networks.\nA neural network where all adjacent layers are fully connected is called a Multilayer Perceptron (MLP) for historical reasons.\nBatches of inputs (e.g. images) are passed through and the predicted outputs are compared to the ground truth. This gives a loss function.\nThe prediction error is then back propagated through the network to adjust the weights incrementally.\nThis allows the model to learn features without manual feature engineering. Earlier layers learn low-level features (like edges) and these are combined in intermediate leayers to learn features like teeth, through to later layers which learning high-level representations like smiling.\n\n\n\nWe will implement an MLP for a supervised (read: not generative) learning problem to classify images.\nThe following uses the CIFAR-10 dataset.\n\n\nLoad the dataset, then scale pixel values to be in the 0 to 1 range and one-hot encode the labels.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras import layers, models, optimizers, utils, datasets\n\n\n# Load the data set\n(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n\n# Scale pixel values to be between 0 and 1.\nx_train = x_train.astype(\"float32\") / 255.\nx_test = x_test.astype(\"float32\") / 255.\n\n# One-hot encode the labels. The CIFAR-10 dataset contains 10 categories.\nNUM_CLASSES = 10\ny_train = utils.to_categorical(y_train, NUM_CLASSES)\ny_test = utils.to_categorical(y_test, NUM_CLASSES)\n\n\nDownloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n170498071/170498071 [==============================] - 64s 0us/step\n\n\n\nplt.imshow(x_train[1])\n\n\n\n\n\n\n\n\n\nprint(y_train[:5])\n\n[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n\n\n\n\n\nThe model is a series of fully connected dense layers.\n\n\nCode\nINPUT_SHAPE = (32, 32, 3,)\n\ninput_layer = layers.Input(INPUT_SHAPE)\nx = layers.Flatten()(input_layer)\nx = layers.Dense(units=200, activation='relu')(x)\nx = layers.Dense(units=150, activation='relu')(x)\noutput_layer = layers.Dense(units=10, activation='softmax')(x)\n\nmodel = models.Model(input_layer, output_layer)\n\n\nThe nonlinear activation function is critical, as this ensures the model is able to learn complex functions of the input rather than simply a linear combination of inputs.\nEach unit within a given layer has a bias term that alwways output 1, which ensures that the output of a unit can be non-zero even if the inputs were all 0. This helps prevent vanishing gradients.\nThe number of parameters in the 200-unit Dense layer is thus: \\(200 * (3072 + 1) = 614600\\)\n\nmodel.summary()\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n                                                                 \n flatten (Flatten)           (None, 3072)              0         \n                                                                 \n dense (Dense)               (None, 200)               614600    \n                                                                 \n dense_1 (Dense)             (None, 150)               30150     \n                                                                 \n dense_2 (Dense)             (None, 10)                1510      \n                                                                 \n=================================================================\nTotal params: 646260 (2.47 MB)\nTrainable params: 646260 (2.47 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\nTo train the model, we require an optimizer and a loss function.\n\n\nThis is used by the network to compare predicted outputs \\(p_i\\) with ground truth labels \\(y_i\\).\nFor regression tasks use mean-squared error: \\[\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - p_i)^2\n\\]\nFor classification tasks with exclusive categories uses categorical cross-entropy: \\[\nCCE = - \\sum_{i=1}^{n} y_i \\log(p_i)\n\\]\nFor classification tasks with binary one output unit or if an observation can belong to multiple classes simultaneously use binary cross-entropy: \\[\nBCE = - \\frac{1}{n} \\sum_{i=1}^{n} (y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) )\n\\]\n\n\n\nThis is the algorithm that updates the weights of the model based on the loss function.\nAdaptive Moment Estimation (ADAM) or Root Mean Squared Propagation (RMSProp) are commonly used.\nThe learning rate is the hyperparameter that is most likely to need tweaking.\n\n\n\nThis determines gow many images are shown to the model in each training step.\nThe larger the batch size, the more stable the gradient calculation but at the expense of a slower training step.\n\nIt is recommend to increase the batch size as training progresses\n\n\n\nCode\n# WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n# opt = optimizers.Adam(learning_rate=0.0005)\nopt = optimizers.legacy.Adam(learning_rate=0.0005)\n\nmodel.compile(\n    loss=\"categorical_crossentropy\",\n    optimizer=opt,\n    metrics=[\"accuracy\"]\n)\n\n\n\n\nCode\nmodel.fit(x_train, y_train, batch_size=32, epochs=10, shuffle=True)\n\n\nEpoch 1/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.8519 - accuracy: 0.3320\nEpoch 2/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.6692 - accuracy: 0.4044\nEpoch 3/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.5840 - accuracy: 0.4360\nEpoch 4/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.5353 - accuracy: 0.4532\nEpoch 5/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4925 - accuracy: 0.4711\nEpoch 6/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4675 - accuracy: 0.4764\nEpoch 7/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4386 - accuracy: 0.4878\nEpoch 8/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4146 - accuracy: 0.4971\nEpoch 9/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.3917 - accuracy: 0.5055\nEpoch 10/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.3691 - accuracy: 0.5124\n\n\n&lt;keras.src.callbacks.History at 0x149e41610&gt;\n\n\n\n\n\n\nUse the model to predict the classification of unseen images from the test set.\nEvaluate the model loss and accuracy over the test set:\n\n\nCode\nmodel.evaluate(x_test, y_test)\n\n\n313/313 [==============================] - 0s 560us/step - loss: 1.4571 - accuracy: 0.4813\n\n\n[1.4570728540420532, 0.4812999963760376]\n\n\n\n\nCode\n# Look at the predictions and ground truth labels from the test set\nCLASSES = np.array(\n    [\n        \"airplane\",\n        \"automobile\",\n        \"bird\",\n        \"cat\",\n        \"deer\",\n        \"dog\",\n        \"frog\",\n        \"horse\",\n        \"ship\",\n        \"truck\",\n    ]\n)\n\ny_pred = model.predict(x_test)\ny_pred_single = CLASSES[np.argmax(y_pred, axis=1)]\ny_test_single = CLASSES[np.argmax(y_test, axis=1)]\n\n\n313/313 [==============================] - 0s 493us/step\n\n\nPlot a comparison of a random selection of test images.\n\n\nCode\nN_IMAGES = 10\nindices = np.random.choice(range(len(x_test)), N_IMAGES)\n\nfig = plt.figure(figsize=(15, 3))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i, idx in enumerate(indices):\n    img = x_test[idx]\n    ax = fig.add_subplot(1, N_IMAGES, i + 1)\n    ax.axis(\"off\")\n    ax.text(\n        0.5,\n        -0.35,\n        \"pred = \" + str(y_pred_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.text(\n        0.5,\n        -0.7,\n        \"act = \" + str(y_test_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.imshow(img)\n\n\n\n\n\n\n\n\n\n\n\n\n\nA simple MLP does not take into account the spatial structure of images, so we can improve performance by using a convolutional layer.\nThis slides a kernel or filter (say, a 3x3 window) over the image and multiply the values elementwise.\nA convolutional layer is a collection of kernels where the (initially random) weights are learned through training.\n\n\nThis is the step size used when moving the kernel window across the image.\nThis determines the output size of the layer. For example, a stride=2 layer will half the height and width of the input image.\n\n\n\nWhen stride=1, we may still end up with a smaller images as the kernel does not overhang the edges of the image.\nUsing padding='same' adds 0s around the border to ensure that the output size is the same as the input size in this case (i.e. it lets the kernel overhang the edges of the original image). This helps to keep track of the size of the tensor.\n\n\n\nThe output of a convolutional layer is another 4-dimensional tensor with shape (batch_size, height, width, num_kernels).\nSo we can stack convolutional layers in series to learn higher-level representations.\n\n\n\nThe exploding gradient problem is common in deep learning: when errors are back-propagated, the gradient values in earlier layers can grow exponentially large.\nIn practice, this can cause overflow errors resulting in the loss function returning NaN.\nThe input layer is scaled, so we may naively expect the activations of all future layers to be well-scaled too. This may initially be true, but as training moves the weights further from their initial values the assumption of outputs being well-scaled breaks down. This is called covariate shift.\nBatch normalization z-scores each input channel, i.e. it subtract the mean and divides by the standard deviation of each mini-batch \\(B\\): \\[\n\\hat{x_i} = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_{B}^2 + \\epsilon}}\n\\]\nWe then learn two parameters, scale \\(\\gamma\\) and shift \\(\\beta\\). Then the output of the BatchNormalization layer is: \\[\ny_i = \\gamma \\hat{x_i} + \\beta\n\\]\nAt test time we may predict only a single image, so the batch normalization value would not be meaningful. Instead, the BatchNormalization layer also stores the moving average of \\(\\gamma\\) and \\(\\beta\\) as tow additional (derived therefore non-trainable) parameters. These moving average values are used at test time.\nBatch normalization has the added benefit of reducing overfitting in practice.\nUsually, batch normalization layers are placed before activation layers, although this is a matter of preference. The acronym BAD can be helpful to remember the general order: Batchnorm, Activation, Dropout.\n\n\n\nWe want to ensure that the model generalises well to unseen data, so it cannot rely on any single layer or neuron too heavily. Dropout acts as a regularization technique to prevent overfitting.\nEach Dropout layer chooses a random set of units from the previous layer and sets their value to 0\nAt test time the Dropout layer does not drop any connections, utilising the full network.\n\n\n\n\n\n\nThis is the same data set as the MLP case so we can re-use the same steps.\n\n\n\nThis is a sequence of stacks containing Conv2D layers with nonlinear acitvation functions.\n\n\nCode\ninput_layer = layers.Input((32, 32, 3))\n\n# Stack 1\nx = layers.Conv2D(filters=32, kernel_size=3, strides=1, padding=\"same\")(\n    input_layer\n)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Stack 2\nx = layers.Conv2D(filters=32, kernel_size=3, strides=2, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Stack 3\nx = layers.Conv2D(filters=64, kernel_size=3, strides=1, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Stack 4\nx = layers.Conv2D(filters=64, kernel_size=3, strides=2, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Dense final layers\nx = layers.Flatten()(x)\nx = layers.Dense(128)(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\nx = layers.Dropout(rate=0.5)(x)\nx = layers.Dense(NUM_CLASSES)(x)\n\n# Output\noutput_layer = layers.Activation(\"softmax\")(x)\nmodel = models.Model(input_layer, output_layer)\n\n\n\nmodel.summary()\n\nModel: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_4 (InputLayer)        [(None, 32, 32, 3)]       0         \n                                                                 \n conv2d_4 (Conv2D)           (None, 32, 32, 32)        896       \n                                                                 \n batch_normalization_5 (Bat  (None, 32, 32, 32)        128       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_5 (LeakyReLU)   (None, 32, 32, 32)        0         \n                                                                 \n conv2d_5 (Conv2D)           (None, 16, 16, 32)        9248      \n                                                                 \n batch_normalization_6 (Bat  (None, 16, 16, 32)        128       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_6 (LeakyReLU)   (None, 16, 16, 32)        0         \n                                                                 \n conv2d_6 (Conv2D)           (None, 16, 16, 64)        18496     \n                                                                 \n batch_normalization_7 (Bat  (None, 16, 16, 64)        256       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_7 (LeakyReLU)   (None, 16, 16, 64)        0         \n                                                                 \n conv2d_7 (Conv2D)           (None, 8, 8, 64)          36928     \n                                                                 \n batch_normalization_8 (Bat  (None, 8, 8, 64)          256       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_8 (LeakyReLU)   (None, 8, 8, 64)          0         \n                                                                 \n flatten_2 (Flatten)         (None, 4096)              0         \n                                                                 \n dense_5 (Dense)             (None, 128)               524416    \n                                                                 \n batch_normalization_9 (Bat  (None, 128)               512       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_9 (LeakyReLU)   (None, 128)               0         \n                                                                 \n dropout_1 (Dropout)         (None, 128)               0         \n                                                                 \n dense_6 (Dense)             (None, 10)                1290      \n                                                                 \n activation_1 (Activation)   (None, 10)                0         \n                                                                 \n=================================================================\nTotal params: 592554 (2.26 MB)\nTrainable params: 591914 (2.26 MB)\nNon-trainable params: 640 (2.50 KB)\n_________________________________________________________________\n\n\n\n\n\nThis is identical to the previous MLP model\n\n\nCode\n# WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n# opt = optimizers.Adam(learning_rate=0.0005)\nopt = optimizers.legacy.Adam(learning_rate=0.0005)\n\nmodel.compile(\n    loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"]\n)\n\n\n\n\nCode\nmodel.fit(\n    x_train,\n    y_train,\n    batch_size=32,\n    epochs=10,\n    shuffle=True,\n    validation_data=(x_test, y_test),\n)\n\n\nEpoch 1/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 1.5393 - accuracy: 0.4599 - val_loss: 1.5656 - val_accuracy: 0.4825\nEpoch 2/10\n1563/1563 [==============================] - 31s 20ms/step - loss: 1.1390 - accuracy: 0.5980 - val_loss: 1.2575 - val_accuracy: 0.5584\nEpoch 3/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 0.9980 - accuracy: 0.6515 - val_loss: 0.9970 - val_accuracy: 0.6513\nEpoch 4/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 0.9146 - accuracy: 0.6798 - val_loss: 0.9783 - val_accuracy: 0.6613\nEpoch 5/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.8515 - accuracy: 0.7044 - val_loss: 0.8269 - val_accuracy: 0.7094\nEpoch 6/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 0.7940 - accuracy: 0.7230 - val_loss: 0.8417 - val_accuracy: 0.7102\nEpoch 7/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.7511 - accuracy: 0.7362 - val_loss: 0.8542 - val_accuracy: 0.7044\nEpoch 8/10\n1563/1563 [==============================] - 29s 18ms/step - loss: 0.7167 - accuracy: 0.7495 - val_loss: 0.8554 - val_accuracy: 0.7083\nEpoch 9/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.6833 - accuracy: 0.7622 - val_loss: 1.0474 - val_accuracy: 0.6651\nEpoch 10/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.6510 - accuracy: 0.7726 - val_loss: 0.8449 - val_accuracy: 0.7020\n\n\n&lt;keras.src.callbacks.History at 0x14aa81d50&gt;\n\n\n\n\n\nAs before, we use the model to predict the classification of unseen images from the test set.\nEvaluate the model loss and accuracy over the test set:\n\n\nCode\nmodel.evaluate(x_test, y_test)\n\n\n313/313 [==============================] - 2s 5ms/step - loss: 0.8449 - accuracy: 0.7020\n\n\n[0.8448793888092041, 0.7020000219345093]\n\n\n\n\nCode\ny_pred = model.predict(x_test)\ny_pred_single = CLASSES[np.argmax(y_pred, axis=1)]\ny_test_single = CLASSES[np.argmax(y_test, axis=1)]\n\n\n313/313 [==============================] - 2s 5ms/step\n\n\nPlot a comparison of a random selection of test images.\n\n\nCode\nN_IMAGES = 10\nindices = np.random.choice(range(len(x_test)), N_IMAGES)\n\nfig = plt.figure(figsize=(15, 3))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i, idx in enumerate(indices):\n    img = x_test[idx]\n    ax = fig.add_subplot(1, N_IMAGES, i + 1)\n    ax.axis(\"off\")\n    ax.text(\n        0.5,\n        -0.35,\n        \"pred = \" + str(y_pred_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.text(\n        0.5,\n        -0.7,\n        \"act = \" + str(y_test_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.imshow(img)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 2 of Generative Deep Learning by David Foster."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#what-is-deep-learning",
    "href": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#what-is-deep-learning",
    "title": "Generative AI: Deep Learning Foundations",
    "section": "",
    "text": "Deep learning is a class of ML algorithms that use multiple stacked layers of processing units to learn high-level representations from unstructured data.\n\nStructured data is naturally arranged into columns of features. Think of relational database tables.\nBy contrast, unstructured data refers to any data that is not structured in tabular format. E.g. images, audio, text. Individual pixels – or frequencies or words – are not informative alone, but higher-level representations are, so we aim to learn these."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#deep-neural-networks",
    "href": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#deep-neural-networks",
    "title": "Generative AI: Deep Learning Foundations",
    "section": "",
    "text": "Most deep learning systems in practice are ANNs,so deep learning has become synonymous with deep deural networks, vut there are other forms such as deep belief networks.\nA neural network where all adjacent layers are fully connected is called a Multilayer Perceptron (MLP) for historical reasons.\nBatches of inputs (e.g. images) are passed through and the predicted outputs are compared to the ground truth. This gives a loss function.\nThe prediction error is then back propagated through the network to adjust the weights incrementally.\nThis allows the model to learn features without manual feature engineering. Earlier layers learn low-level features (like edges) and these are combined in intermediate leayers to learn features like teeth, through to later layers which learning high-level representations like smiling."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#building-a-multilayer-perceptron",
    "href": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#building-a-multilayer-perceptron",
    "title": "Generative AI: Deep Learning Foundations",
    "section": "",
    "text": "We will implement an MLP for a supervised (read: not generative) learning problem to classify images.\nThe following uses the CIFAR-10 dataset.\n\n\nLoad the dataset, then scale pixel values to be in the 0 to 1 range and one-hot encode the labels.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras import layers, models, optimizers, utils, datasets\n\n\n# Load the data set\n(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n\n# Scale pixel values to be between 0 and 1.\nx_train = x_train.astype(\"float32\") / 255.\nx_test = x_test.astype(\"float32\") / 255.\n\n# One-hot encode the labels. The CIFAR-10 dataset contains 10 categories.\nNUM_CLASSES = 10\ny_train = utils.to_categorical(y_train, NUM_CLASSES)\ny_test = utils.to_categorical(y_test, NUM_CLASSES)\n\n\nDownloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n170498071/170498071 [==============================] - 64s 0us/step\n\n\n\nplt.imshow(x_train[1])\n\n\n\n\n\n\n\n\n\nprint(y_train[:5])\n\n[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n\n\n\n\n\nThe model is a series of fully connected dense layers.\n\n\nCode\nINPUT_SHAPE = (32, 32, 3,)\n\ninput_layer = layers.Input(INPUT_SHAPE)\nx = layers.Flatten()(input_layer)\nx = layers.Dense(units=200, activation='relu')(x)\nx = layers.Dense(units=150, activation='relu')(x)\noutput_layer = layers.Dense(units=10, activation='softmax')(x)\n\nmodel = models.Model(input_layer, output_layer)\n\n\nThe nonlinear activation function is critical, as this ensures the model is able to learn complex functions of the input rather than simply a linear combination of inputs.\nEach unit within a given layer has a bias term that alwways output 1, which ensures that the output of a unit can be non-zero even if the inputs were all 0. This helps prevent vanishing gradients.\nThe number of parameters in the 200-unit Dense layer is thus: \\(200 * (3072 + 1) = 614600\\)\n\nmodel.summary()\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n                                                                 \n flatten (Flatten)           (None, 3072)              0         \n                                                                 \n dense (Dense)               (None, 200)               614600    \n                                                                 \n dense_1 (Dense)             (None, 150)               30150     \n                                                                 \n dense_2 (Dense)             (None, 10)                1510      \n                                                                 \n=================================================================\nTotal params: 646260 (2.47 MB)\nTrainable params: 646260 (2.47 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\nTo train the model, we require an optimizer and a loss function.\n\n\nThis is used by the network to compare predicted outputs \\(p_i\\) with ground truth labels \\(y_i\\).\nFor regression tasks use mean-squared error: \\[\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - p_i)^2\n\\]\nFor classification tasks with exclusive categories uses categorical cross-entropy: \\[\nCCE = - \\sum_{i=1}^{n} y_i \\log(p_i)\n\\]\nFor classification tasks with binary one output unit or if an observation can belong to multiple classes simultaneously use binary cross-entropy: \\[\nBCE = - \\frac{1}{n} \\sum_{i=1}^{n} (y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) )\n\\]\n\n\n\nThis is the algorithm that updates the weights of the model based on the loss function.\nAdaptive Moment Estimation (ADAM) or Root Mean Squared Propagation (RMSProp) are commonly used.\nThe learning rate is the hyperparameter that is most likely to need tweaking.\n\n\n\nThis determines gow many images are shown to the model in each training step.\nThe larger the batch size, the more stable the gradient calculation but at the expense of a slower training step.\n\nIt is recommend to increase the batch size as training progresses\n\n\n\nCode\n# WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n# opt = optimizers.Adam(learning_rate=0.0005)\nopt = optimizers.legacy.Adam(learning_rate=0.0005)\n\nmodel.compile(\n    loss=\"categorical_crossentropy\",\n    optimizer=opt,\n    metrics=[\"accuracy\"]\n)\n\n\n\n\nCode\nmodel.fit(x_train, y_train, batch_size=32, epochs=10, shuffle=True)\n\n\nEpoch 1/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.8519 - accuracy: 0.3320\nEpoch 2/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.6692 - accuracy: 0.4044\nEpoch 3/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.5840 - accuracy: 0.4360\nEpoch 4/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.5353 - accuracy: 0.4532\nEpoch 5/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4925 - accuracy: 0.4711\nEpoch 6/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4675 - accuracy: 0.4764\nEpoch 7/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4386 - accuracy: 0.4878\nEpoch 8/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4146 - accuracy: 0.4971\nEpoch 9/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.3917 - accuracy: 0.5055\nEpoch 10/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.3691 - accuracy: 0.5124\n\n\n&lt;keras.src.callbacks.History at 0x149e41610&gt;\n\n\n\n\n\n\nUse the model to predict the classification of unseen images from the test set.\nEvaluate the model loss and accuracy over the test set:\n\n\nCode\nmodel.evaluate(x_test, y_test)\n\n\n313/313 [==============================] - 0s 560us/step - loss: 1.4571 - accuracy: 0.4813\n\n\n[1.4570728540420532, 0.4812999963760376]\n\n\n\n\nCode\n# Look at the predictions and ground truth labels from the test set\nCLASSES = np.array(\n    [\n        \"airplane\",\n        \"automobile\",\n        \"bird\",\n        \"cat\",\n        \"deer\",\n        \"dog\",\n        \"frog\",\n        \"horse\",\n        \"ship\",\n        \"truck\",\n    ]\n)\n\ny_pred = model.predict(x_test)\ny_pred_single = CLASSES[np.argmax(y_pred, axis=1)]\ny_test_single = CLASSES[np.argmax(y_test, axis=1)]\n\n\n313/313 [==============================] - 0s 493us/step\n\n\nPlot a comparison of a random selection of test images.\n\n\nCode\nN_IMAGES = 10\nindices = np.random.choice(range(len(x_test)), N_IMAGES)\n\nfig = plt.figure(figsize=(15, 3))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i, idx in enumerate(indices):\n    img = x_test[idx]\n    ax = fig.add_subplot(1, N_IMAGES, i + 1)\n    ax.axis(\"off\")\n    ax.text(\n        0.5,\n        -0.35,\n        \"pred = \" + str(y_pred_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.text(\n        0.5,\n        -0.7,\n        \"act = \" + str(y_test_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.imshow(img)"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#cnn-concepts-and-layers",
    "href": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#cnn-concepts-and-layers",
    "title": "Generative AI: Deep Learning Foundations",
    "section": "",
    "text": "A simple MLP does not take into account the spatial structure of images, so we can improve performance by using a convolutional layer.\nThis slides a kernel or filter (say, a 3x3 window) over the image and multiply the values elementwise.\nA convolutional layer is a collection of kernels where the (initially random) weights are learned through training.\n\n\nThis is the step size used when moving the kernel window across the image.\nThis determines the output size of the layer. For example, a stride=2 layer will half the height and width of the input image.\n\n\n\nWhen stride=1, we may still end up with a smaller images as the kernel does not overhang the edges of the image.\nUsing padding='same' adds 0s around the border to ensure that the output size is the same as the input size in this case (i.e. it lets the kernel overhang the edges of the original image). This helps to keep track of the size of the tensor.\n\n\n\nThe output of a convolutional layer is another 4-dimensional tensor with shape (batch_size, height, width, num_kernels).\nSo we can stack convolutional layers in series to learn higher-level representations.\n\n\n\nThe exploding gradient problem is common in deep learning: when errors are back-propagated, the gradient values in earlier layers can grow exponentially large.\nIn practice, this can cause overflow errors resulting in the loss function returning NaN.\nThe input layer is scaled, so we may naively expect the activations of all future layers to be well-scaled too. This may initially be true, but as training moves the weights further from their initial values the assumption of outputs being well-scaled breaks down. This is called covariate shift.\nBatch normalization z-scores each input channel, i.e. it subtract the mean and divides by the standard deviation of each mini-batch \\(B\\): \\[\n\\hat{x_i} = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_{B}^2 + \\epsilon}}\n\\]\nWe then learn two parameters, scale \\(\\gamma\\) and shift \\(\\beta\\). Then the output of the BatchNormalization layer is: \\[\ny_i = \\gamma \\hat{x_i} + \\beta\n\\]\nAt test time we may predict only a single image, so the batch normalization value would not be meaningful. Instead, the BatchNormalization layer also stores the moving average of \\(\\gamma\\) and \\(\\beta\\) as tow additional (derived therefore non-trainable) parameters. These moving average values are used at test time.\nBatch normalization has the added benefit of reducing overfitting in practice.\nUsually, batch normalization layers are placed before activation layers, although this is a matter of preference. The acronym BAD can be helpful to remember the general order: Batchnorm, Activation, Dropout.\n\n\n\nWe want to ensure that the model generalises well to unseen data, so it cannot rely on any single layer or neuron too heavily. Dropout acts as a regularization technique to prevent overfitting.\nEach Dropout layer chooses a random set of units from the previous layer and sets their value to 0\nAt test time the Dropout layer does not drop any connections, utilising the full network."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#building-a-convolutional-neural-network",
    "href": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#building-a-convolutional-neural-network",
    "title": "Generative AI: Deep Learning Foundations",
    "section": "",
    "text": "This is the same data set as the MLP case so we can re-use the same steps.\n\n\n\nThis is a sequence of stacks containing Conv2D layers with nonlinear acitvation functions.\n\n\nCode\ninput_layer = layers.Input((32, 32, 3))\n\n# Stack 1\nx = layers.Conv2D(filters=32, kernel_size=3, strides=1, padding=\"same\")(\n    input_layer\n)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Stack 2\nx = layers.Conv2D(filters=32, kernel_size=3, strides=2, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Stack 3\nx = layers.Conv2D(filters=64, kernel_size=3, strides=1, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Stack 4\nx = layers.Conv2D(filters=64, kernel_size=3, strides=2, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Dense final layers\nx = layers.Flatten()(x)\nx = layers.Dense(128)(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\nx = layers.Dropout(rate=0.5)(x)\nx = layers.Dense(NUM_CLASSES)(x)\n\n# Output\noutput_layer = layers.Activation(\"softmax\")(x)\nmodel = models.Model(input_layer, output_layer)\n\n\n\nmodel.summary()\n\nModel: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_4 (InputLayer)        [(None, 32, 32, 3)]       0         \n                                                                 \n conv2d_4 (Conv2D)           (None, 32, 32, 32)        896       \n                                                                 \n batch_normalization_5 (Bat  (None, 32, 32, 32)        128       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_5 (LeakyReLU)   (None, 32, 32, 32)        0         \n                                                                 \n conv2d_5 (Conv2D)           (None, 16, 16, 32)        9248      \n                                                                 \n batch_normalization_6 (Bat  (None, 16, 16, 32)        128       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_6 (LeakyReLU)   (None, 16, 16, 32)        0         \n                                                                 \n conv2d_6 (Conv2D)           (None, 16, 16, 64)        18496     \n                                                                 \n batch_normalization_7 (Bat  (None, 16, 16, 64)        256       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_7 (LeakyReLU)   (None, 16, 16, 64)        0         \n                                                                 \n conv2d_7 (Conv2D)           (None, 8, 8, 64)          36928     \n                                                                 \n batch_normalization_8 (Bat  (None, 8, 8, 64)          256       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_8 (LeakyReLU)   (None, 8, 8, 64)          0         \n                                                                 \n flatten_2 (Flatten)         (None, 4096)              0         \n                                                                 \n dense_5 (Dense)             (None, 128)               524416    \n                                                                 \n batch_normalization_9 (Bat  (None, 128)               512       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_9 (LeakyReLU)   (None, 128)               0         \n                                                                 \n dropout_1 (Dropout)         (None, 128)               0         \n                                                                 \n dense_6 (Dense)             (None, 10)                1290      \n                                                                 \n activation_1 (Activation)   (None, 10)                0         \n                                                                 \n=================================================================\nTotal params: 592554 (2.26 MB)\nTrainable params: 591914 (2.26 MB)\nNon-trainable params: 640 (2.50 KB)\n_________________________________________________________________\n\n\n\n\n\nThis is identical to the previous MLP model\n\n\nCode\n# WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n# opt = optimizers.Adam(learning_rate=0.0005)\nopt = optimizers.legacy.Adam(learning_rate=0.0005)\n\nmodel.compile(\n    loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"]\n)\n\n\n\n\nCode\nmodel.fit(\n    x_train,\n    y_train,\n    batch_size=32,\n    epochs=10,\n    shuffle=True,\n    validation_data=(x_test, y_test),\n)\n\n\nEpoch 1/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 1.5393 - accuracy: 0.4599 - val_loss: 1.5656 - val_accuracy: 0.4825\nEpoch 2/10\n1563/1563 [==============================] - 31s 20ms/step - loss: 1.1390 - accuracy: 0.5980 - val_loss: 1.2575 - val_accuracy: 0.5584\nEpoch 3/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 0.9980 - accuracy: 0.6515 - val_loss: 0.9970 - val_accuracy: 0.6513\nEpoch 4/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 0.9146 - accuracy: 0.6798 - val_loss: 0.9783 - val_accuracy: 0.6613\nEpoch 5/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.8515 - accuracy: 0.7044 - val_loss: 0.8269 - val_accuracy: 0.7094\nEpoch 6/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 0.7940 - accuracy: 0.7230 - val_loss: 0.8417 - val_accuracy: 0.7102\nEpoch 7/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.7511 - accuracy: 0.7362 - val_loss: 0.8542 - val_accuracy: 0.7044\nEpoch 8/10\n1563/1563 [==============================] - 29s 18ms/step - loss: 0.7167 - accuracy: 0.7495 - val_loss: 0.8554 - val_accuracy: 0.7083\nEpoch 9/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.6833 - accuracy: 0.7622 - val_loss: 1.0474 - val_accuracy: 0.6651\nEpoch 10/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.6510 - accuracy: 0.7726 - val_loss: 0.8449 - val_accuracy: 0.7020\n\n\n&lt;keras.src.callbacks.History at 0x14aa81d50&gt;\n\n\n\n\n\nAs before, we use the model to predict the classification of unseen images from the test set.\nEvaluate the model loss and accuracy over the test set:\n\n\nCode\nmodel.evaluate(x_test, y_test)\n\n\n313/313 [==============================] - 2s 5ms/step - loss: 0.8449 - accuracy: 0.7020\n\n\n[0.8448793888092041, 0.7020000219345093]\n\n\n\n\nCode\ny_pred = model.predict(x_test)\ny_pred_single = CLASSES[np.argmax(y_pred, axis=1)]\ny_test_single = CLASSES[np.argmax(y_test, axis=1)]\n\n\n313/313 [==============================] - 2s 5ms/step\n\n\nPlot a comparison of a random selection of test images.\n\n\nCode\nN_IMAGES = 10\nindices = np.random.choice(range(len(x_test)), N_IMAGES)\n\nfig = plt.figure(figsize=(15, 3))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i, idx in enumerate(indices):\n    img = x_test[idx]\n    ax = fig.add_subplot(1, N_IMAGES, i + 1)\n    ax.axis(\"off\")\n    ax.text(\n        0.5,\n        -0.35,\n        \"pred = \" + str(y_pred_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.text(\n        0.5,\n        -0.7,\n        \"act = \" + str(y_test_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.imshow(img)"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#references",
    "href": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#references",
    "title": "Generative AI: Deep Learning Foundations",
    "section": "",
    "text": "Chapter 2 of Generative Deep Learning by David Foster."
  },
  {
    "objectID": "posts/ml/fastai/lesson3/lesson.html",
    "href": "posts/ml/fastai/lesson3/lesson.html",
    "title": "FastAI Lesson 3: How Does a Neural Network Learn?",
    "section": "",
    "text": "These are notes from lesson 3 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\nRecreate the spreadsheet to train a linear model and a neural network from scratch: see spreadsheet\n\n\n\n\nSome options for cloud environments: Kaggle, Colab, Paperspace.\nA comparison of performance vs training time for different image models useful for model selection is provided here. Resnet and convnext are generally good to start with.\nThe best practice is to start with a “good enough” model with a quick training time, so you can iterate quickly. Then as you get further on with your research and need a better model, you can move to a slower, more accurate model.\nThe criteria we generally care about are:\n\nHow fast are they\nHow much memory do they use\nHow accurate are they\n\nA learner object contains the pre-processing steps and the model itself.\n\n\n\nHow do we fit a function to data? An ML model is just a function fitted to data. Deep learning is just fitting an infinitely flexible model to data.\nIn this notebook there is an interactive cell to fit a quadratic function to some noisy data: y = ax^2 +bx + c\nWe can vary a, b and c to get a better fit by eye. We can make this more rigorous by defining a loss function to quantify how good the fit is.\nIn this case, use the mean absolute error: mae = mean(abs(actual - preds)) We can then use stochastic gradient descent to autome the tweaking of a, b and c to minimise the loss.\nWe can store the parameters as a tensor, then pytorch will calculate gradients of the loss function based on that tensor.\nabc = torch.tensor([1.1,1.1,1.1]) \nabc.requires_grad_()  # Modifies abc in place so that it will include gradient calculations on anything which uses abc downstream\nloss = quad_mae(abc)  # `loss` uses abc so pytorch will give the gradient of loss too\nloss.backward()  # Back-propagate the gradients\nabc.grad  # Returns a tensor of the loss gradients\nWe can then take a “small step” in the direction that will decrease the gradient. The size of the step should be proportional to the size of the gradient. We define a learning rate hyperparameter to determine how much to scale the gradients by.\nlearning_rate = 0.01\nabc -= abc.grad * learning_rate\nloss = quad_mae(abc)  # The loss should now be smaller\nWe can repeat this process to take multiple steps to minimise the gradient.\nfor step in range(10):\n    loss = quad_mae(abc)\n    loss.backward()\n    with torch.no_grad():\n        abc -= abc.grad * learning_rate\n    print(f'step={step}; loss={loss:.2f}')\nThe learning rate should decrease as we get closer to the minimum to ensure we don’t overshoot the minimum and increase the loss. A learning rate schedule can be specified to do this.\n\n\n\nFor deep learning, the premise is the same but instead of quadratic functions, we fit ReLUs and other non-linear functions.\nUniversal approximation theorem states that this is infinitely expressive if enough ReLUs (or other non-linear units) are combined. This means we can learn any computable function.\nA ReLU is essentially a linear function with the negative values clipped to 0.\ndef relu(m,c,x):\n    y = m * x + c\n    return np.clip(y, 0.)\nThis is all that deep learning is! All we need is:\n\nA model - a bunch of ReLUs combined will be flexible\nA loss function - mean absolute error between the actual data values and the values predicted by the model\nAn optimiser - stochastic gradient descent can start from random weights to incrementally improve the loss until we get a “good enough” fit\n\nWe just need enough time and data. There are a few hacks to decrease the time and data required:\n\nData augmentation\nRunning on GPUs to parallelise matrix multiplications\nConvolutions to skip over values to reduce the number of matrix multiplications required\nTransfer learning - initialise with parameters from another pre-trained model instead of random weights.\n\nThis spreadsheet from the homework task is a worked example of manually training a multivariate linear model, then extending that to a neural network summing two ReLUs.\n\n\n\n\nCourse lesson page\nComparison of computer vision models\n“How does a neural network really work?” notebook"
  },
  {
    "objectID": "posts/ml/fastai/lesson3/lesson.html#model-selection",
    "href": "posts/ml/fastai/lesson3/lesson.html#model-selection",
    "title": "FastAI Lesson 3: How Does a Neural Network Learn?",
    "section": "",
    "text": "Some options for cloud environments: Kaggle, Colab, Paperspace.\nA comparison of performance vs training time for different image models useful for model selection is provided here. Resnet and convnext are generally good to start with.\nThe best practice is to start with a “good enough” model with a quick training time, so you can iterate quickly. Then as you get further on with your research and need a better model, you can move to a slower, more accurate model.\nThe criteria we generally care about are:\n\nHow fast are they\nHow much memory do they use\nHow accurate are they\n\nA learner object contains the pre-processing steps and the model itself."
  },
  {
    "objectID": "posts/ml/fastai/lesson3/lesson.html#fitting-a-quadratic-function",
    "href": "posts/ml/fastai/lesson3/lesson.html#fitting-a-quadratic-function",
    "title": "FastAI Lesson 3: How Does a Neural Network Learn?",
    "section": "",
    "text": "How do we fit a function to data? An ML model is just a function fitted to data. Deep learning is just fitting an infinitely flexible model to data.\nIn this notebook there is an interactive cell to fit a quadratic function to some noisy data: y = ax^2 +bx + c\nWe can vary a, b and c to get a better fit by eye. We can make this more rigorous by defining a loss function to quantify how good the fit is.\nIn this case, use the mean absolute error: mae = mean(abs(actual - preds)) We can then use stochastic gradient descent to autome the tweaking of a, b and c to minimise the loss.\nWe can store the parameters as a tensor, then pytorch will calculate gradients of the loss function based on that tensor.\nabc = torch.tensor([1.1,1.1,1.1]) \nabc.requires_grad_()  # Modifies abc in place so that it will include gradient calculations on anything which uses abc downstream\nloss = quad_mae(abc)  # `loss` uses abc so pytorch will give the gradient of loss too\nloss.backward()  # Back-propagate the gradients\nabc.grad  # Returns a tensor of the loss gradients\nWe can then take a “small step” in the direction that will decrease the gradient. The size of the step should be proportional to the size of the gradient. We define a learning rate hyperparameter to determine how much to scale the gradients by.\nlearning_rate = 0.01\nabc -= abc.grad * learning_rate\nloss = quad_mae(abc)  # The loss should now be smaller\nWe can repeat this process to take multiple steps to minimise the gradient.\nfor step in range(10):\n    loss = quad_mae(abc)\n    loss.backward()\n    with torch.no_grad():\n        abc -= abc.grad * learning_rate\n    print(f'step={step}; loss={loss:.2f}')\nThe learning rate should decrease as we get closer to the minimum to ensure we don’t overshoot the minimum and increase the loss. A learning rate schedule can be specified to do this."
  },
  {
    "objectID": "posts/ml/fastai/lesson3/lesson.html#fit-a-deep-learning-model",
    "href": "posts/ml/fastai/lesson3/lesson.html#fit-a-deep-learning-model",
    "title": "FastAI Lesson 3: How Does a Neural Network Learn?",
    "section": "",
    "text": "For deep learning, the premise is the same but instead of quadratic functions, we fit ReLUs and other non-linear functions.\nUniversal approximation theorem states that this is infinitely expressive if enough ReLUs (or other non-linear units) are combined. This means we can learn any computable function.\nA ReLU is essentially a linear function with the negative values clipped to 0.\ndef relu(m,c,x):\n    y = m * x + c\n    return np.clip(y, 0.)\nThis is all that deep learning is! All we need is:\n\nA model - a bunch of ReLUs combined will be flexible\nA loss function - mean absolute error between the actual data values and the values predicted by the model\nAn optimiser - stochastic gradient descent can start from random weights to incrementally improve the loss until we get a “good enough” fit\n\nWe just need enough time and data. There are a few hacks to decrease the time and data required:\n\nData augmentation\nRunning on GPUs to parallelise matrix multiplications\nConvolutions to skip over values to reduce the number of matrix multiplications required\nTransfer learning - initialise with parameters from another pre-trained model instead of random weights.\n\nThis spreadsheet from the homework task is a worked example of manually training a multivariate linear model, then extending that to a neural network summing two ReLUs."
  },
  {
    "objectID": "posts/ml/fastai/lesson3/lesson.html#references",
    "href": "posts/ml/fastai/lesson3/lesson.html#references",
    "title": "FastAI Lesson 3: How Does a Neural Network Learn?",
    "section": "",
    "text": "Course lesson page\nComparison of computer vision models\n“How does a neural network really work?” notebook"
  },
  {
    "objectID": "posts/ml/fastai/lesson5/lesson.html",
    "href": "posts/ml/fastai/lesson5/lesson.html",
    "title": "FastAI Lesson 5: Natural Language Processing",
    "section": "",
    "text": "These are notes from lesson 5 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\n\nRecreate the Jupyter notebook to train a linear model and a neural network from scratch - see from scratch notebook\nThen repeat the exercise using the fastai framework (it’s much easier!) - see framework notebook\nRead numpy broadcasting rules\n\n\n\n\n\nTrain a linear model from scratch in Python using on the Titanic dataset. Then train a neural network from scratch in a similar way. This is an extension of the spreadsheet approach to make a neural network from scratch in lesson 3.\n\n\nNever throw away data.\nAn easy way of imputing missing values is to fill them with the mode. This is good enough for a first pass at creating a model.\n\n\n\nNumeric values that can grow exponentially like prices or population sizes often have long-tailed distributions.\nAn easy way to scale is to take log(x+1). The +1 term is just to avoid taking log of 0.\n\n\n\nOne-hot encode any categorical variables.\nWe should include an “other” category for each in case the validation or test set contains a category we didn’t encounter in the training set.\nIf there are categories with ony a small number of observations we can group them into an “other” category too.\n\n\n\nBroadcasting arrays together avoids boilerplate code to make dimensions match.\nFor reference, see numpy’s broadcasting rules and try APL\n\n\n\nFor a binary classification model, the outputs should be between 0 and 1. If you train a linear model, it might result in negative values or values &gt;1.\nThis means we can improve the loss by just clipping to ensure they stay between 0 and 1.\nThis is the idea behind the sigmoid function for output layers: smaller values will tend to 0 and larger values will tend to 1. In general, for any binary classification model, we should always have a sigmoid as the final layer. If the model isn’t training well, it’s worth checking that the final activation function is a sigmoid.\nThe sigmoid function is defined as: \\[ y = \\frac{1}{1+e^{-x}} \\]\n\n\n\nIn general, the middle layers of a neural network are similar between different problems.\nThe input layer will depend on the data for our specific problem. The output will depend on the target for our specific problem.\nSo we spend most of our time thinking about the correct input and output layers, and the hidden layers are less important.\n\n\n\n\nWhen creating the models from scratch, there was a lot of boilerplate code to:\n\nImpute missing values using the “obvious” method (fill with mode)\nNormalise continuous variables to be between 0 and 1\nOne-hot encode categorical variables\nRepeat all of these steps in the same order for the test set\n\nThe benefits of using a framework like fastai:\n\nLess boilerplate, so the obvious things are done automatically unless you say otherwise\nRepeating all of the feature engineering steps on the output is trivial with DataLoaders\n\n\n\n\nCreating independent models and taking the mean of their predictions improves the accuracy. There are a few different approaches to ensembling a categorical variable:\n\nTake the mean of the predictions (binary 0 or 1)\nTake the mean of the probabilities (continuous between 0 and 1) then threshold the result\nTake the mode of the predictions (binary 0 or 1)\n\nIn general the mean approaches work better but there’s no rule as to why, so try all of them.\n\n\n\n\nCourse lesson page\nNumpy broadcasting rules\nAPL"
  },
  {
    "objectID": "posts/ml/fastai/lesson5/lesson.html#training-a-model-from-scratch",
    "href": "posts/ml/fastai/lesson5/lesson.html#training-a-model-from-scratch",
    "title": "FastAI Lesson 5: Natural Language Processing",
    "section": "",
    "text": "Train a linear model from scratch in Python using on the Titanic dataset. Then train a neural network from scratch in a similar way. This is an extension of the spreadsheet approach to make a neural network from scratch in lesson 3.\n\n\nNever throw away data.\nAn easy way of imputing missing values is to fill them with the mode. This is good enough for a first pass at creating a model.\n\n\n\nNumeric values that can grow exponentially like prices or population sizes often have long-tailed distributions.\nAn easy way to scale is to take log(x+1). The +1 term is just to avoid taking log of 0.\n\n\n\nOne-hot encode any categorical variables.\nWe should include an “other” category for each in case the validation or test set contains a category we didn’t encounter in the training set.\nIf there are categories with ony a small number of observations we can group them into an “other” category too.\n\n\n\nBroadcasting arrays together avoids boilerplate code to make dimensions match.\nFor reference, see numpy’s broadcasting rules and try APL\n\n\n\nFor a binary classification model, the outputs should be between 0 and 1. If you train a linear model, it might result in negative values or values &gt;1.\nThis means we can improve the loss by just clipping to ensure they stay between 0 and 1.\nThis is the idea behind the sigmoid function for output layers: smaller values will tend to 0 and larger values will tend to 1. In general, for any binary classification model, we should always have a sigmoid as the final layer. If the model isn’t training well, it’s worth checking that the final activation function is a sigmoid.\nThe sigmoid function is defined as: \\[ y = \\frac{1}{1+e^{-x}} \\]\n\n\n\nIn general, the middle layers of a neural network are similar between different problems.\nThe input layer will depend on the data for our specific problem. The output will depend on the target for our specific problem.\nSo we spend most of our time thinking about the correct input and output layers, and the hidden layers are less important."
  },
  {
    "objectID": "posts/ml/fastai/lesson5/lesson.html#using-a-framework",
    "href": "posts/ml/fastai/lesson5/lesson.html#using-a-framework",
    "title": "FastAI Lesson 5: Natural Language Processing",
    "section": "",
    "text": "When creating the models from scratch, there was a lot of boilerplate code to:\n\nImpute missing values using the “obvious” method (fill with mode)\nNormalise continuous variables to be between 0 and 1\nOne-hot encode categorical variables\nRepeat all of these steps in the same order for the test set\n\nThe benefits of using a framework like fastai:\n\nLess boilerplate, so the obvious things are done automatically unless you say otherwise\nRepeating all of the feature engineering steps on the output is trivial with DataLoaders"
  },
  {
    "objectID": "posts/ml/fastai/lesson5/lesson.html#ensembles",
    "href": "posts/ml/fastai/lesson5/lesson.html#ensembles",
    "title": "FastAI Lesson 5: Natural Language Processing",
    "section": "",
    "text": "Creating independent models and taking the mean of their predictions improves the accuracy. There are a few different approaches to ensembling a categorical variable:\n\nTake the mean of the predictions (binary 0 or 1)\nTake the mean of the probabilities (continuous between 0 and 1) then threshold the result\nTake the mode of the predictions (binary 0 or 1)\n\nIn general the mean approaches work better but there’s no rule as to why, so try all of them."
  },
  {
    "objectID": "posts/ml/fastai/lesson5/lesson.html#references",
    "href": "posts/ml/fastai/lesson5/lesson.html#references",
    "title": "FastAI Lesson 5: Natural Language Processing",
    "section": "",
    "text": "Course lesson page\nNumpy broadcasting rules\nAPL"
  },
  {
    "objectID": "posts/ml/fastai/lesson6_1/lesson.html",
    "href": "posts/ml/fastai/lesson6_1/lesson.html",
    "title": "FastAI Lesson 6.5: Road to the Top",
    "section": "",
    "text": "These are notes on the “Road to the Top” notebooks that span lessons 6 and 7 of Fast AI Practical Deep Learning for Coders. I’ve separated these from the main topics of those lectires to keep the posts focused.\n\n\n\n\n\n\nHomework Task\n\n\n\n\nRecreate the softmax and cross-entropy loss spreadsheet example\nRead the “Road to the Top” notebook series - parts 1, 2 and 3\nRead “Things that confused me about cross entropy” by Chris Said.\n\n\n\n\n\nThe focus should be:\n\nCreate an effective validation set\nIterate quickly to find changes which improve the validation set\n\nTrain a simple model straight away, get a result and submit it. You can iterate and improve later.\nData augmentation, in particular test-time augmentation (TTA), can improve results.\n\n\n\nRules of thumb on model selection by problem type:\n\nFor computer vision use CNNs, fine-tune a pre-trained model. See comparison of pre-trained models here.\n\nIf unsure which model to choose, start with (small) convnext.\nDifferent models can have big differences in accuracy so try a few small models from different families.\nOnce you are happy with the model, try a size up in that family.\nResizing images to a square is a good, easy compromise to accomodate many different image sizes, orientations and aspect ratios. A more involved approach that may improve results is to batch images together and resize them to the median rectangle size.\n\nFor tabular data, random forests will give a reasonably good result.\n\nGBMs will give a better result eventually, but with more effort required to get a good model.\nWorth running a hyperparameter grid search for GBM because it’s fiddly.\n\n\nRules of thumb on hardware:\n\nYou generally want ~8 physical CPUs per GPU\nIf model training is CPU bound, it can help to resize images to be smaller. The file I/O on the CPU may be taking too long.\nIf model training is CPU bound and GPU usage is low, you might as well go up to a “better” (i.e. bigger) model with no increase of overall execution time.\n\n\n\n\nThis is a technique that allows you to run larger batch sizes without running out of GPU memory.\nRather than needing to fit a whole batch (say, 64 images) in memory, we can split this up into sub-batches. Let’s say we use an accumulation factor of 4; this means each sub-batch would have 16 images.\nWe calculate the gradient for each sub-batch but don’t immediately subtract it from the weights. We also keep a running count of the number of images we have seen. Once we have seen 64 images, i.e. 4 batches, then apply the total accumulated gradient and reset the count. Remember that in Pytorch, if we calculate another gradient without zeroing in between, it will add the new gradient to the old one, so this is in effect the “default behaviour”.\nGradient accumulation gives an identical result unless using batch normalisation, since the moving averages will be more volatile. Most big models use layer normalisation rather than batch normalisation so often this is not an issue.\nHow do we pick a batch size? Just pick the largest you can without running out of GPU memory.\nWhy not just reduce the batch size? If using a pretrained model, the other parameters such as learning rate work for that batch size and might not work for others. So we’d have to perform a hyperparameter search again to find a good combination for the new batch size. Gradient accumulation sidesteps this issue.\n\n\n\nConsider the case where we want to train a model with two different predicted labels, say plant species and disease type. If we have 10 plant species and 10 diseases, we create a model with 20 output neurons.\nHow does the model know what each should do? Through the loss function. We make one loss function for disease which takes the first 10 columns of the input to the final layer, and a second loss function for variety which takes the last 10 columns of the input to the final layer. The overall loss function is the sum of the two.\nIf you train it for longer, the multi target model can get better at predicting one target, say disease, than a single target model trained on just disease alone.\n\nThe early layers that are useful for variety may also be useful for disease\nKnowing the variety may be useful in predicting disease if there are confounding factors between disease and variety. For example, certain diseases affect certain varieties.\n\n\n\n\n\nCourse lesson page\n“Road to the Top” notebook\n“Things that confused me about cross entropy” by Chris Said."
  },
  {
    "objectID": "posts/ml/fastai/lesson6_1/lesson.html#general-points",
    "href": "posts/ml/fastai/lesson6_1/lesson.html#general-points",
    "title": "FastAI Lesson 6.5: Road to the Top",
    "section": "",
    "text": "The focus should be:\n\nCreate an effective validation set\nIterate quickly to find changes which improve the validation set\n\nTrain a simple model straight away, get a result and submit it. You can iterate and improve later.\nData augmentation, in particular test-time augmentation (TTA), can improve results."
  },
  {
    "objectID": "posts/ml/fastai/lesson6_1/lesson.html#some-rules-of-thumb",
    "href": "posts/ml/fastai/lesson6_1/lesson.html#some-rules-of-thumb",
    "title": "FastAI Lesson 6.5: Road to the Top",
    "section": "",
    "text": "Rules of thumb on model selection by problem type:\n\nFor computer vision use CNNs, fine-tune a pre-trained model. See comparison of pre-trained models here.\n\nIf unsure which model to choose, start with (small) convnext.\nDifferent models can have big differences in accuracy so try a few small models from different families.\nOnce you are happy with the model, try a size up in that family.\nResizing images to a square is a good, easy compromise to accomodate many different image sizes, orientations and aspect ratios. A more involved approach that may improve results is to batch images together and resize them to the median rectangle size.\n\nFor tabular data, random forests will give a reasonably good result.\n\nGBMs will give a better result eventually, but with more effort required to get a good model.\nWorth running a hyperparameter grid search for GBM because it’s fiddly.\n\n\nRules of thumb on hardware:\n\nYou generally want ~8 physical CPUs per GPU\nIf model training is CPU bound, it can help to resize images to be smaller. The file I/O on the CPU may be taking too long.\nIf model training is CPU bound and GPU usage is low, you might as well go up to a “better” (i.e. bigger) model with no increase of overall execution time."
  },
  {
    "objectID": "posts/ml/fastai/lesson6_1/lesson.html#gradient-accumulation",
    "href": "posts/ml/fastai/lesson6_1/lesson.html#gradient-accumulation",
    "title": "FastAI Lesson 6.5: Road to the Top",
    "section": "",
    "text": "This is a technique that allows you to run larger batch sizes without running out of GPU memory.\nRather than needing to fit a whole batch (say, 64 images) in memory, we can split this up into sub-batches. Let’s say we use an accumulation factor of 4; this means each sub-batch would have 16 images.\nWe calculate the gradient for each sub-batch but don’t immediately subtract it from the weights. We also keep a running count of the number of images we have seen. Once we have seen 64 images, i.e. 4 batches, then apply the total accumulated gradient and reset the count. Remember that in Pytorch, if we calculate another gradient without zeroing in between, it will add the new gradient to the old one, so this is in effect the “default behaviour”.\nGradient accumulation gives an identical result unless using batch normalisation, since the moving averages will be more volatile. Most big models use layer normalisation rather than batch normalisation so often this is not an issue.\nHow do we pick a batch size? Just pick the largest you can without running out of GPU memory.\nWhy not just reduce the batch size? If using a pretrained model, the other parameters such as learning rate work for that batch size and might not work for others. So we’d have to perform a hyperparameter search again to find a good combination for the new batch size. Gradient accumulation sidesteps this issue."
  },
  {
    "objectID": "posts/ml/fastai/lesson6_1/lesson.html#multi-target-classification",
    "href": "posts/ml/fastai/lesson6_1/lesson.html#multi-target-classification",
    "title": "FastAI Lesson 6.5: Road to the Top",
    "section": "",
    "text": "Consider the case where we want to train a model with two different predicted labels, say plant species and disease type. If we have 10 plant species and 10 diseases, we create a model with 20 output neurons.\nHow does the model know what each should do? Through the loss function. We make one loss function for disease which takes the first 10 columns of the input to the final layer, and a second loss function for variety which takes the last 10 columns of the input to the final layer. The overall loss function is the sum of the two.\nIf you train it for longer, the multi target model can get better at predicting one target, say disease, than a single target model trained on just disease alone.\n\nThe early layers that are useful for variety may also be useful for disease\nKnowing the variety may be useful in predicting disease if there are confounding factors between disease and variety. For example, certain diseases affect certain varieties."
  },
  {
    "objectID": "posts/ml/fastai/lesson6_1/lesson.html#references",
    "href": "posts/ml/fastai/lesson6_1/lesson.html#references",
    "title": "FastAI Lesson 6.5: Road to the Top",
    "section": "",
    "text": "Course lesson page\n“Road to the Top” notebook\n“Things that confused me about cross entropy” by Chris Said."
  },
  {
    "objectID": "posts/ml/fastai/lesson8/lesson.html",
    "href": "posts/ml/fastai/lesson8/lesson.html",
    "title": "FastAI Lesson 8: Convolutions",
    "section": "",
    "text": "These are notes from lesson 8 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\nRecreate the convolutions in a spreadsheet which underpins the discussion below.\n\n\n\n\n\n\nConvolutions slide a window of numbers, say 3x3, across our original image.\nDepending on the values in the filter, it will be able to pick out different features like horizontal or vertical edges. Subsequent layers can combine these into more sophisticated layers, like corners. These can eventually be combined to detect complex features of the image.\nAn interactive example of this to see the intuition behind the sliding window is here.\n\n\n\nThis is a technique to reduce the size of the input tensor.\nA 2x2 max pool layer slides a 2x2 filter over the input and replaces each value with the max of the 4 values in the image.\nNowadays, using the stride length is generally preferred over max pool layers.\n\n\n\nAn alternative technique to reduce the size of the input is to skip pixels when we slide our filter over the image.\nFor example, a stride=2 convolution would apply to every second pixel and therefore halves the image size in each axis, having the same effect as a 2x2 max pool.\n\n\n\nAs a regularisation technique to make sure the model is not overly reliant on any single pixel or region, we can add a dropout layer.\nConceptually, this is the same as initialisng a random tensor the same size as the input and masking the input based on whether the random weight is above a threshold.\n\n\n\nWe eventually want to reduce our input image size to output a tensor with one value per class.\nOne approach is to apply a dense layer when the image has been reduced “enough”, taking the dot product between the reduced image tensor and the dense layer. This is again deprecated in favour of the next approach…\n\n\n\nNowadays, we use stride=2 convolutions until we get a small (7x7) tensor, then apply a single average pool layer to it.\nThis 7x7 tensor (for a bear detector, say) effectively gives a value quantifying “is there a bear in this part of the image?”. It then takes the average of all of these to determine if there is a bear in the overall photo.\nThis works fine if the bear occupies most of the photo, but less well if the bear occupies a small region in the corner of the photo. So the details of the model depend on the use case. If we want ot be able to detect small bears in the corner, max pooling would work better here.\nConcat pool is a hybrid approach which does the max pool AND the average pool and concatentates the two results together.\n\n\n\n\nConvolutions can be reframed in different ways, as matrix multiplications or as systems of linear equations.\nThis article is a a helpful exploration of the topic.\n\n\n\nA summary of questions to end the course.\n\nRead Meta Learning.\nDon’t try to know everything. Pick a bit that you’re interested and dig in. You’ll start to recognise the same ideas cropping up with slight tweaks.\nDoes success in deep learning boil down to more compute power? No, we can be smarter about our approach. Also pick your problems to be ones that you can actually manage with smaller compute resources.\nDragonbox Algebra 5+ can teach little kids algebra.\nTurning a model into a startup. The key to a legitimate business venture is to solve a legitimate problem, and one that people will pay you to solve. Start with the problem, not the prototype. The LEan Startup by Eric Ries: create the MVP as quick as possible (and fake the solution) then gradually make it “less fake” as more people use it (and pay you).\nMake the things you want to do easier, then you’ll want to do them more.\n\n\n\n\n\nCourse lesson page\nMeta Learning"
  },
  {
    "objectID": "posts/ml/fastai/lesson8/lesson.html#the-intuition-behind-cnns",
    "href": "posts/ml/fastai/lesson8/lesson.html#the-intuition-behind-cnns",
    "title": "FastAI Lesson 8: Convolutions",
    "section": "",
    "text": "Convolutions slide a window of numbers, say 3x3, across our original image.\nDepending on the values in the filter, it will be able to pick out different features like horizontal or vertical edges. Subsequent layers can combine these into more sophisticated layers, like corners. These can eventually be combined to detect complex features of the image.\nAn interactive example of this to see the intuition behind the sliding window is here.\n\n\n\nThis is a technique to reduce the size of the input tensor.\nA 2x2 max pool layer slides a 2x2 filter over the input and replaces each value with the max of the 4 values in the image.\nNowadays, using the stride length is generally preferred over max pool layers.\n\n\n\nAn alternative technique to reduce the size of the input is to skip pixels when we slide our filter over the image.\nFor example, a stride=2 convolution would apply to every second pixel and therefore halves the image size in each axis, having the same effect as a 2x2 max pool.\n\n\n\nAs a regularisation technique to make sure the model is not overly reliant on any single pixel or region, we can add a dropout layer.\nConceptually, this is the same as initialisng a random tensor the same size as the input and masking the input based on whether the random weight is above a threshold.\n\n\n\nWe eventually want to reduce our input image size to output a tensor with one value per class.\nOne approach is to apply a dense layer when the image has been reduced “enough”, taking the dot product between the reduced image tensor and the dense layer. This is again deprecated in favour of the next approach…\n\n\n\nNowadays, we use stride=2 convolutions until we get a small (7x7) tensor, then apply a single average pool layer to it.\nThis 7x7 tensor (for a bear detector, say) effectively gives a value quantifying “is there a bear in this part of the image?”. It then takes the average of all of these to determine if there is a bear in the overall photo.\nThis works fine if the bear occupies most of the photo, but less well if the bear occupies a small region in the corner of the photo. So the details of the model depend on the use case. If we want ot be able to detect small bears in the corner, max pooling would work better here.\nConcat pool is a hybrid approach which does the max pool AND the average pool and concatentates the two results together."
  },
  {
    "objectID": "posts/ml/fastai/lesson8/lesson.html#convolutions-from-different-viewpoints",
    "href": "posts/ml/fastai/lesson8/lesson.html#convolutions-from-different-viewpoints",
    "title": "FastAI Lesson 8: Convolutions",
    "section": "",
    "text": "Convolutions can be reframed in different ways, as matrix multiplications or as systems of linear equations.\nThis article is a a helpful exploration of the topic."
  },
  {
    "objectID": "posts/ml/fastai/lesson8/lesson.html#assorted-thoughts-from-jeremy",
    "href": "posts/ml/fastai/lesson8/lesson.html#assorted-thoughts-from-jeremy",
    "title": "FastAI Lesson 8: Convolutions",
    "section": "",
    "text": "A summary of questions to end the course.\n\nRead Meta Learning.\nDon’t try to know everything. Pick a bit that you’re interested and dig in. You’ll start to recognise the same ideas cropping up with slight tweaks.\nDoes success in deep learning boil down to more compute power? No, we can be smarter about our approach. Also pick your problems to be ones that you can actually manage with smaller compute resources.\nDragonbox Algebra 5+ can teach little kids algebra.\nTurning a model into a startup. The key to a legitimate business venture is to solve a legitimate problem, and one that people will pay you to solve. Start with the problem, not the prototype. The LEan Startup by Eric Ries: create the MVP as quick as possible (and fake the solution) then gradually make it “less fake” as more people use it (and pay you).\nMake the things you want to do easier, then you’ll want to do them more."
  },
  {
    "objectID": "posts/ml/fastai/lesson8/lesson.html#references",
    "href": "posts/ml/fastai/lesson8/lesson.html#references",
    "title": "FastAI Lesson 8: Convolutions",
    "section": "",
    "text": "Course lesson page\nMeta Learning"
  },
  {
    "objectID": "posts/ml/fastai/lesson6/lesson.html",
    "href": "posts/ml/fastai/lesson6/lesson.html",
    "title": "FastAI Lesson 6: Random Forests",
    "section": "",
    "text": "These are notes from lesson 6 of Fast AI Practical Deep Learning for Coders.\n\n\nIt’s worth starting with a random forest as a baseline model because “it’s very hard to screw up”. Decision trees only consider the ordering of data, so are not sensitive to outliers, skewed distributions, dummy variables etc.\nA random forest is an ensemble of trees. A tree is an ensemble of binary splits.\nbinary split -&gt; tree -&gt; forest\n\n\nPick a column of the data set and split the rows into two groups. Predict the outcome based just on that.\nFor example, in the titanic dataset, pick the Sex column. This splits into male vs female. If we predict that all female passengers survived and all male passengers died, that’s a reasonably accurate prediction.\nA model which iterates through the variables and finds the best binary split is called a “OneR” or one rule model.\nThis was actually the state-of-the-art in the 90s and performs reasonably well across a lot of problems.\n\n\n\nThis extends the “OneR” idea to two or more splits.\nFor example, if we first split by sex, then find the next best variable to split on to create a two layer tree.\nLeaf nodes are the number of terminating nodes in the tree. OneR creates 2 leaf nodes. If we split one side again, we’ll have 3 leaf nodes. If we split the other side, to create a balanced two layer tree, we’ll have 4 leaf nodes.\n\n\n\nThe idea behind bagging is that if we have many unbiased, uncorrelated models, we can average their predictions to get an aggregate model that is better than any of them.\nEach individual model will overfit, so either be too high or too low for a given point, a positive or negative error term respectively. By combining multiple uncorrelated models, the error will average to 0.\nAn easy way to get many uncorrelated models is to only use a random subset of the data each time. Then build a decision tree for each subset.\nThis is the idea behind random forests.\nThe error decreases with the number of trees, with diminishing returns.\n\nJeremy’s rule of thumb: improvements level off after about 30 trees and he doesn’t often use &gt;100.\n\n\n\n\n\n\n\nA nice side effect of using decision trees is that we get feature importance plots for free.\nGini is a measure of inequality, similar to the score used in the notebook to quantify how good a split was.\nIntuitively, this measures the likelihood that if you picked two samples from a group, how likely is it they’d be the same every time. If the group is all the same, the probability is 1. If every item is different, the probability is 0.\nThe idea is for each split of a decision tree, we can track the column used to split and the amount that the gini decreased by. If we loop through the tree and accumulate the “gini reduction” for each column, we have a metric of how important that column was in splitting the dataset.\nThis makes random forests a useful first model when tackling a new big dataset, as it can give an insight into how useful each column is.\n\n\n\nA comment on explainability, regarding feature importance compared to SHAP, LIME etc. These explain the model, so to be useful the model needs to be accurate.\nIf you use feature importance from a bad model then the columns it claims are important might not actually be. So the usefulness of explainability techniques boils down to what models can they explain and how accurate are those models.\n\n\n\nFor each tree, we trained on a subset of the rows. We can then see how each one performed on the held out data; this is the out-of-bag (OOB) error per tree.\nWe can average this over all of the trees to get an overall OOB error for the forest.\n\n\n\nThese are not specific to random forests and can be applied to any ML model including deep neural networks.\nIf we want to know how an independent variable affects the dependent variable, a naive approach would be to just plot them.\nBut this could include a confounding variable. For example, the price of bulldozers increases over time, but driven by the presence of air conditioning which also increased over time.\nA partial dependence plot takes each row in turn and sets the column(s) of interest to the first value, say, year=1950. Then we predict the target variable using our model. Then repeat this for year=1951, 1952, etc.\nWe can then average the target variable per year to get a view of how it depends on the independent variable, all else being equal.\n\n\n\n\nWe make multiple trees, but instead of fitting all to different data subsets, we fit to residuals.\nSo we fit a very small tree (OneR even) to the data to get a first prediction. Then we calculate the error term. Then we fit another tree to predict the error term. Then calculate the second order error term. Then fit a tree to this, etc.\nThen our prediction is the sum of these trees, rather than the average like with a random forest.\nThis is “boosting”; calculating an error term then fitting another model to it.\nContrast this with “bagging” which was when we calculate multiple models to different subsets of data and average them.\n\n\n\n\nCourse lesson page\nComparison of computer vision models for fine-tuning\nBagging predictors"
  },
  {
    "objectID": "posts/ml/fastai/lesson6/lesson.html#background-on-random-forests",
    "href": "posts/ml/fastai/lesson6/lesson.html#background-on-random-forests",
    "title": "FastAI Lesson 6: Random Forests",
    "section": "",
    "text": "It’s worth starting with a random forest as a baseline model because “it’s very hard to screw up”. Decision trees only consider the ordering of data, so are not sensitive to outliers, skewed distributions, dummy variables etc.\nA random forest is an ensemble of trees. A tree is an ensemble of binary splits.\nbinary split -&gt; tree -&gt; forest\n\n\nPick a column of the data set and split the rows into two groups. Predict the outcome based just on that.\nFor example, in the titanic dataset, pick the Sex column. This splits into male vs female. If we predict that all female passengers survived and all male passengers died, that’s a reasonably accurate prediction.\nA model which iterates through the variables and finds the best binary split is called a “OneR” or one rule model.\nThis was actually the state-of-the-art in the 90s and performs reasonably well across a lot of problems.\n\n\n\nThis extends the “OneR” idea to two or more splits.\nFor example, if we first split by sex, then find the next best variable to split on to create a two layer tree.\nLeaf nodes are the number of terminating nodes in the tree. OneR creates 2 leaf nodes. If we split one side again, we’ll have 3 leaf nodes. If we split the other side, to create a balanced two layer tree, we’ll have 4 leaf nodes.\n\n\n\nThe idea behind bagging is that if we have many unbiased, uncorrelated models, we can average their predictions to get an aggregate model that is better than any of them.\nEach individual model will overfit, so either be too high or too low for a given point, a positive or negative error term respectively. By combining multiple uncorrelated models, the error will average to 0.\nAn easy way to get many uncorrelated models is to only use a random subset of the data each time. Then build a decision tree for each subset.\nThis is the idea behind random forests.\nThe error decreases with the number of trees, with diminishing returns.\n\nJeremy’s rule of thumb: improvements level off after about 30 trees and he doesn’t often use &gt;100."
  },
  {
    "objectID": "posts/ml/fastai/lesson6/lesson.html#attributes-of-random-forests",
    "href": "posts/ml/fastai/lesson6/lesson.html#attributes-of-random-forests",
    "title": "FastAI Lesson 6: Random Forests",
    "section": "",
    "text": "A nice side effect of using decision trees is that we get feature importance plots for free.\nGini is a measure of inequality, similar to the score used in the notebook to quantify how good a split was.\nIntuitively, this measures the likelihood that if you picked two samples from a group, how likely is it they’d be the same every time. If the group is all the same, the probability is 1. If every item is different, the probability is 0.\nThe idea is for each split of a decision tree, we can track the column used to split and the amount that the gini decreased by. If we loop through the tree and accumulate the “gini reduction” for each column, we have a metric of how important that column was in splitting the dataset.\nThis makes random forests a useful first model when tackling a new big dataset, as it can give an insight into how useful each column is.\n\n\n\nA comment on explainability, regarding feature importance compared to SHAP, LIME etc. These explain the model, so to be useful the model needs to be accurate.\nIf you use feature importance from a bad model then the columns it claims are important might not actually be. So the usefulness of explainability techniques boils down to what models can they explain and how accurate are those models.\n\n\n\nFor each tree, we trained on a subset of the rows. We can then see how each one performed on the held out data; this is the out-of-bag (OOB) error per tree.\nWe can average this over all of the trees to get an overall OOB error for the forest.\n\n\n\nThese are not specific to random forests and can be applied to any ML model including deep neural networks.\nIf we want to know how an independent variable affects the dependent variable, a naive approach would be to just plot them.\nBut this could include a confounding variable. For example, the price of bulldozers increases over time, but driven by the presence of air conditioning which also increased over time.\nA partial dependence plot takes each row in turn and sets the column(s) of interest to the first value, say, year=1950. Then we predict the target variable using our model. Then repeat this for year=1951, 1952, etc.\nWe can then average the target variable per year to get a view of how it depends on the independent variable, all else being equal."
  },
  {
    "objectID": "posts/ml/fastai/lesson6/lesson.html#gradient-boosting-forests",
    "href": "posts/ml/fastai/lesson6/lesson.html#gradient-boosting-forests",
    "title": "FastAI Lesson 6: Random Forests",
    "section": "",
    "text": "We make multiple trees, but instead of fitting all to different data subsets, we fit to residuals.\nSo we fit a very small tree (OneR even) to the data to get a first prediction. Then we calculate the error term. Then we fit another tree to predict the error term. Then calculate the second order error term. Then fit a tree to this, etc.\nThen our prediction is the sum of these trees, rather than the average like with a random forest.\nThis is “boosting”; calculating an error term then fitting another model to it.\nContrast this with “bagging” which was when we calculate multiple models to different subsets of data and average them."
  },
  {
    "objectID": "posts/ml/fastai/lesson6/lesson.html#references",
    "href": "posts/ml/fastai/lesson6/lesson.html#references",
    "title": "FastAI Lesson 6: Random Forests",
    "section": "",
    "text": "Course lesson page\nComparison of computer vision models for fine-tuning\nBagging predictors"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter1/lesson.html",
    "href": "posts/ml/hands_on_llm/chapter1/lesson.html",
    "title": "Hands-On LLMs: Intro",
    "section": "",
    "text": "This approach originated in the 1950s but gained popularity in the 2000s.\nIt treats unstructured text as a bag or words, throwing away any information from the position / ordering of the words and any semantic meaning of text.\n\nTokenise the text. A straightforward way to do this is to split on the spaces so we have a list of words.\nCreate a vocabulary of length N, containing every word in our training data.\nWe can then represent any sentence or document as a one-hot encoded N-dimensional vector.\nUse those vectors for downstream tasks, e.g. cosine similarity between vectors to measure the similarity of documents for recommender systems.\n\nFor example, if our vocabulary contains the words: that is a cute dog my cat\nThen we can encode the sentence “that is a cute dog” as:\n\n\n\nthat\nis\na\ncute\ndog\nmy\ncat\n\n\n\n\n1\n1\n1\n1\n1\n0\n0\n\n\n\nAnd another sentence “my cat is cute” as:\n\n\n\nthat\nis\na\ncute\ndog\nmy\ncat\n\n\n\n\n0\n1\n0\n1\n0\n1\n1\n\n\n\nThen to compare how similar the two sentences are, we can compare those vectors, for example using cosine similarity.\n\n\n\nA limitation of bag-of-words is that it makes no attempt to capture meaning from the text, treating each word as an unrelated token. By encoding text as one-hot encoded vectors, it does not capture that the word “cute” might be similar to “adorable” or “scrum-diddly-umptious”; every word is simply an arbitrary element of the vocabulary.\nDense vector embeddings attempt to capture these differences; rather than treating words as discrete elements, we can introduce a continuous scale for each embedding dimension, and learn where each word falls on the scale. Word2Vec was an early, and successful, approach to generating these embeddings.\nThe approach is to:\n\nAssign every word in the vocabulary an (initial random) vector of the embedding dimension, say 50.\nTake pairs of words from the training data, and train a model to predict whether they are likely to be neighbors in a sentence.\nIf two words typically share the same neighbouring words, they are likely to share similar embedding vectors, and vice versa.\n\nIllustrated word2vec provides a deeper dive.\nThese embeddings then have interesting properties. The classic example is that adding/subtracting the vectors for the corresponding words gives: \\[\nking - man + woman \\approx queen\n\\]\n\nThe numbers don’t lie and they spell disaster\n- “Big Poppa Pump” Scott Steiner\n\n\nN-grams are sliding windows of N words sampled from text. These can be used to train a model where the input is N words and the output is the predicted next word.\nContinuous bag or words (CBOW) tries to predict a missing word given the N preceding and following words.\nSkip-grams take a single word and try to predict the surrounding words. The are the “opposite” of CBOW.\n\n\n\n\n\n\n\n\n\n\nArchitecture\nTask\nInputs\nOutput\n\n\n\n\nN-gram\nThe numbers ___\n[The, numbers]\ndon’t\n\n\nCBOW\nThe numbers ___ lie and\n[The, numbers, lie, and]\ndon’t\n\n\nSkip-gram\n___ ___ don’t ___ ___\ndon’t\n[The, numbers, lie, and]\n\n\n\nNegative sampling is used to speed up the next-word prediction process. Instead of predicting the next token (a computationally expensive neural network), we reframe the task as “given two words, what is the probability that they are neighbours?” (a much faster logistic regression problem.)\nBut the issue is, our training dataset only has positive examples of neighbours. So the model could just always output 1 to get 100% accuracy. To avoid this, we introduce negative exmaples by taking random combinations of words in the vocabulary that aren’t neighbours. This idea is called noise-contrastive estimation.\nWord2vec is then just “skip gram with negative sampling” to generate word embeddings.\n\n\n\n\n\n\nTypes of embeddings\n\n\n\nThere are different types of embeddings that indicate different levels of abstraction.\nWe can create an embedding for a sub-word, a word, a sentence or a whole document. In each case, the result is an N-dimensional vector where N is the embedding size.\n\n\n\n\n\nThe embeddings so far have been static: the embedding for “bank” will be the same regardless of whether it’s referring to the bank of a river or a branch of Santander.\nThe next development notes that the embeddings should vary depending on their context, i.e. the surrounding words.\nRecurrent Neural Networks (RNNs) were initially used with attention mechanisms. These would:\n\nTake pre-generated embeddings (say, from word2vec) as inputs\nPass this to an encoder RNN to generate a context embedding\nPass this to a decoder RNN to generate an output, such as the input text translated to another language.\n\n\n\n\n\n\nflowchart LR\n\n  A(Pre-generated embeddings) --&gt; B[[Encoder RNN]] --&gt; C(Context embedding) --&gt; D[[Decoder RNN]] --&gt; E(Output)\n\n\n\n\n\n\n\n\n\nTransformers were introduced in the 2017 paper “Attention is All You Need”, which solely used the attention mechanism and removed the RNNs.\nThe original transformer was an encoder-decoder model for machine translation.\n\n\n\n\n\nflowchart LR\n\n  A(Pre-generated embeddings) --&gt; B[[Transformer Encoder]] --&gt; C[[Transformer Decoder]] --&gt; E(Output)\n\n\n\n\n\n\n\n\n\nBy splitting the encoder-decoder architecture and focusing only on the encoder, we can create models the excel at creating meaningful representations of language.\nThis is the premise behind models like BERT. The classification token is appended to the input, and the encoder alone is trained.\n\n\n\nSimilarly, we can split the encoder-decoder architecture and focusing only on the decoder. These excel at text generation.\nThis is the premise behind models like GPT.\nGenerative LLMs are essentially sequence-to-sequence machines: given some input text, predict the next tokens. The primary use case these days is being fine-tuned for “instruct” or “chat” models that are trained to provide an answer when given a question.\nFoundation models are open-source base models that can be fine-tuned for specific tasks.\n\n\n\nAside from Transformers, Mamba and RWKV perform well.\n\n\n\n\nThis size of a large larnguage model is a moving target as the field develops and model sizes scale.\nConsiderations:\n\nWhat if a new model has the same capabilities as an existing LLM but with a fraction of the parameters. Is this new model still “large”?\nWhat if we train a model the same size as GPT-4 but for text classification instead of generation? Is it still an LLM?\n\nThe creation of LLMs is typically done in two stages:\n\nLanguage modeling: Create a foundation model by (unsuperivsed) training on a vast corpus of text. This step allows the model to learn the grammar, structure and patterns of the language. It is not yet directed at a specific task. This takes the majority of the training time.\nFine-tuning: Using the foundation model for (supervised) training on a specific task.\n\n\n\n\n\nBias and fariness: Training data is seldom shared, so may contain implicit biases\nTransparency and accountability: Unintended consequences when there is no “human in the loop”. Who is accountable for the outcomes of the LLM? The company that trained it? Or the one that used it? Or the patient?\nGenerating harmful content\nIntellectual property: Who owns the output of an LLM? The user? The company that trained it? Or the original creators of the training data?\nRegulation\n\n\n\n\nThe following can be run in Google Colab on a free GPU.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\n# Load model and tokenizer \nmodel = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/Phi-3-mini-4k-instruct\", device_map=\"cuda\", torch_dtype=\"auto\", trust_remote_code=True\n) \ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini4k-instruct\")\n\n# Create a pipeline \ngenerator = pipeline(\n    \"text-generation\", model=model, tokenizer=tokenizer, return_full_text=False, max_new_tokens=500, do_sample=False\n)\n\n# The prompt (user input / query) \nmessages = [{\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"}]\n\n# Generate output \noutput = generator(messages)\nprint(output[0][\"generated_text\"])\n\n&gt;&gt;&gt; Why don't chickens like to go to the gym? Because they can't crack the egg-sistence of it!\n\n\n\n\nChapter 1 of Hands-On Large Language Models by Jay Alammar & Marten Grootendoorst\nhttps://jalammar.github.io/illustrated-word2vec/"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter1/lesson.html#a-brief-history-of-language-ai",
    "href": "posts/ml/hands_on_llm/chapter1/lesson.html#a-brief-history-of-language-ai",
    "title": "Hands-On LLMs: Intro",
    "section": "",
    "text": "This approach originated in the 1950s but gained popularity in the 2000s.\nIt treats unstructured text as a bag or words, throwing away any information from the position / ordering of the words and any semantic meaning of text.\n\nTokenise the text. A straightforward way to do this is to split on the spaces so we have a list of words.\nCreate a vocabulary of length N, containing every word in our training data.\nWe can then represent any sentence or document as a one-hot encoded N-dimensional vector.\nUse those vectors for downstream tasks, e.g. cosine similarity between vectors to measure the similarity of documents for recommender systems.\n\nFor example, if our vocabulary contains the words: that is a cute dog my cat\nThen we can encode the sentence “that is a cute dog” as:\n\n\n\nthat\nis\na\ncute\ndog\nmy\ncat\n\n\n\n\n1\n1\n1\n1\n1\n0\n0\n\n\n\nAnd another sentence “my cat is cute” as:\n\n\n\nthat\nis\na\ncute\ndog\nmy\ncat\n\n\n\n\n0\n1\n0\n1\n0\n1\n1\n\n\n\nThen to compare how similar the two sentences are, we can compare those vectors, for example using cosine similarity.\n\n\n\nA limitation of bag-of-words is that it makes no attempt to capture meaning from the text, treating each word as an unrelated token. By encoding text as one-hot encoded vectors, it does not capture that the word “cute” might be similar to “adorable” or “scrum-diddly-umptious”; every word is simply an arbitrary element of the vocabulary.\nDense vector embeddings attempt to capture these differences; rather than treating words as discrete elements, we can introduce a continuous scale for each embedding dimension, and learn where each word falls on the scale. Word2Vec was an early, and successful, approach to generating these embeddings.\nThe approach is to:\n\nAssign every word in the vocabulary an (initial random) vector of the embedding dimension, say 50.\nTake pairs of words from the training data, and train a model to predict whether they are likely to be neighbors in a sentence.\nIf two words typically share the same neighbouring words, they are likely to share similar embedding vectors, and vice versa.\n\nIllustrated word2vec provides a deeper dive.\nThese embeddings then have interesting properties. The classic example is that adding/subtracting the vectors for the corresponding words gives: \\[\nking - man + woman \\approx queen\n\\]\n\nThe numbers don’t lie and they spell disaster\n- “Big Poppa Pump” Scott Steiner\n\n\nN-grams are sliding windows of N words sampled from text. These can be used to train a model where the input is N words and the output is the predicted next word.\nContinuous bag or words (CBOW) tries to predict a missing word given the N preceding and following words.\nSkip-grams take a single word and try to predict the surrounding words. The are the “opposite” of CBOW.\n\n\n\n\n\n\n\n\n\n\nArchitecture\nTask\nInputs\nOutput\n\n\n\n\nN-gram\nThe numbers ___\n[The, numbers]\ndon’t\n\n\nCBOW\nThe numbers ___ lie and\n[The, numbers, lie, and]\ndon’t\n\n\nSkip-gram\n___ ___ don’t ___ ___\ndon’t\n[The, numbers, lie, and]\n\n\n\nNegative sampling is used to speed up the next-word prediction process. Instead of predicting the next token (a computationally expensive neural network), we reframe the task as “given two words, what is the probability that they are neighbours?” (a much faster logistic regression problem.)\nBut the issue is, our training dataset only has positive examples of neighbours. So the model could just always output 1 to get 100% accuracy. To avoid this, we introduce negative exmaples by taking random combinations of words in the vocabulary that aren’t neighbours. This idea is called noise-contrastive estimation.\nWord2vec is then just “skip gram with negative sampling” to generate word embeddings.\n\n\n\n\n\n\nTypes of embeddings\n\n\n\nThere are different types of embeddings that indicate different levels of abstraction.\nWe can create an embedding for a sub-word, a word, a sentence or a whole document. In each case, the result is an N-dimensional vector where N is the embedding size.\n\n\n\n\n\nThe embeddings so far have been static: the embedding for “bank” will be the same regardless of whether it’s referring to the bank of a river or a branch of Santander.\nThe next development notes that the embeddings should vary depending on their context, i.e. the surrounding words.\nRecurrent Neural Networks (RNNs) were initially used with attention mechanisms. These would:\n\nTake pre-generated embeddings (say, from word2vec) as inputs\nPass this to an encoder RNN to generate a context embedding\nPass this to a decoder RNN to generate an output, such as the input text translated to another language.\n\n\n\n\n\n\nflowchart LR\n\n  A(Pre-generated embeddings) --&gt; B[[Encoder RNN]] --&gt; C(Context embedding) --&gt; D[[Decoder RNN]] --&gt; E(Output)\n\n\n\n\n\n\n\n\n\nTransformers were introduced in the 2017 paper “Attention is All You Need”, which solely used the attention mechanism and removed the RNNs.\nThe original transformer was an encoder-decoder model for machine translation.\n\n\n\n\n\nflowchart LR\n\n  A(Pre-generated embeddings) --&gt; B[[Transformer Encoder]] --&gt; C[[Transformer Decoder]] --&gt; E(Output)\n\n\n\n\n\n\n\n\n\nBy splitting the encoder-decoder architecture and focusing only on the encoder, we can create models the excel at creating meaningful representations of language.\nThis is the premise behind models like BERT. The classification token is appended to the input, and the encoder alone is trained.\n\n\n\nSimilarly, we can split the encoder-decoder architecture and focusing only on the decoder. These excel at text generation.\nThis is the premise behind models like GPT.\nGenerative LLMs are essentially sequence-to-sequence machines: given some input text, predict the next tokens. The primary use case these days is being fine-tuned for “instruct” or “chat” models that are trained to provide an answer when given a question.\nFoundation models are open-source base models that can be fine-tuned for specific tasks.\n\n\n\nAside from Transformers, Mamba and RWKV perform well."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter1/lesson.html#how-large-is-a-large-language-model",
    "href": "posts/ml/hands_on_llm/chapter1/lesson.html#how-large-is-a-large-language-model",
    "title": "Hands-On LLMs: Intro",
    "section": "",
    "text": "This size of a large larnguage model is a moving target as the field develops and model sizes scale.\nConsiderations:\n\nWhat if a new model has the same capabilities as an existing LLM but with a fraction of the parameters. Is this new model still “large”?\nWhat if we train a model the same size as GPT-4 but for text classification instead of generation? Is it still an LLM?\n\nThe creation of LLMs is typically done in two stages:\n\nLanguage modeling: Create a foundation model by (unsuperivsed) training on a vast corpus of text. This step allows the model to learn the grammar, structure and patterns of the language. It is not yet directed at a specific task. This takes the majority of the training time.\nFine-tuning: Using the foundation model for (supervised) training on a specific task."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter1/lesson.html#ethical-considerations-of-llms",
    "href": "posts/ml/hands_on_llm/chapter1/lesson.html#ethical-considerations-of-llms",
    "title": "Hands-On LLMs: Intro",
    "section": "",
    "text": "Bias and fariness: Training data is seldom shared, so may contain implicit biases\nTransparency and accountability: Unintended consequences when there is no “human in the loop”. Who is accountable for the outcomes of the LLM? The company that trained it? Or the one that used it? Or the patient?\nGenerating harmful content\nIntellectual property: Who owns the output of an LLM? The user? The company that trained it? Or the original creators of the training data?\nRegulation"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter1/lesson.html#using-an-llm-locally",
    "href": "posts/ml/hands_on_llm/chapter1/lesson.html#using-an-llm-locally",
    "title": "Hands-On LLMs: Intro",
    "section": "",
    "text": "The following can be run in Google Colab on a free GPU.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\n# Load model and tokenizer \nmodel = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/Phi-3-mini-4k-instruct\", device_map=\"cuda\", torch_dtype=\"auto\", trust_remote_code=True\n) \ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini4k-instruct\")\n\n# Create a pipeline \ngenerator = pipeline(\n    \"text-generation\", model=model, tokenizer=tokenizer, return_full_text=False, max_new_tokens=500, do_sample=False\n)\n\n# The prompt (user input / query) \nmessages = [{\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"}]\n\n# Generate output \noutput = generator(messages)\nprint(output[0][\"generated_text\"])\n\n&gt;&gt;&gt; Why don't chickens like to go to the gym? Because they can't crack the egg-sistence of it!"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter1/lesson.html#references",
    "href": "posts/ml/hands_on_llm/chapter1/lesson.html#references",
    "title": "Hands-On LLMs: Intro",
    "section": "",
    "text": "Chapter 1 of Hands-On Large Language Models by Jay Alammar & Marten Grootendoorst\nhttps://jalammar.github.io/illustrated-word2vec/"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter7/lesson.html",
    "href": "posts/ml/hands_on_llm/chapter7/lesson.html",
    "title": "Hands-On LLMs: Advanced Text Generation Techniques and Tools",
    "section": "",
    "text": "Going beyond prompt engineering, there are several areas where we can improve the quality of generated text:\n\nModel I/O\nMemory\nAgents\nChains\n\nLangchain is a framework that provides useful abstractions for these kinds of things and helps connect them together. We will use LangChain here, but alternatives include LlamaIndex, DSPy and Haystack.\n\n\n\nLangChain example"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter7/lesson.html#quantization",
    "href": "posts/ml/hands_on_llm/chapter7/lesson.html#quantization",
    "title": "Hands-On LLMs: Advanced Text Generation Techniques and Tools",
    "section": "1.1. Quantization",
    "text": "1.1. Quantization\nWe can load quantized models using the GGUF file format which is a binary format optimised for fast loading of pytorch models.\nThe benefit of a quantized model is a smaller size in memory while retaining most of the original information. For example, if the model was trained using 32-bit floats for parameters, we can use 16-bit floats instead. This reduces the memory requirement but also reduces the precision. Often this trade-off is worthwhile.\nThis page goes into detail on the mechanics of quantisation.\nThe “best” model is constantly changing, so we can refer to the Open LLM leaderboard.\nWe can download a 16-bit quantized version of the Phi-3 mini model from HuggingFace.\n\n!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instructgguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\n\nThen we can use LangChain to load the GGUF file.\nNote: an alternative is to use the langchain_huggingface library.\n\n# This cell *should* work, but due to some funkiness with incompatible langchain vs llama versions (I think)\n#  it's easier to just create a custom LangChain wrapper in the following cell.\nfrom langchain import LlamaCpp\n\n# Make sure model_path points at the file location of the GGUF file\nMODEL_DIR = \"/Users/gurpreetjohl/workspace/python/ml-practice/ml-practice/models/\"\nMODEL_NAME = \"Phi-3-mini-4k-instruct-fp16.gguf\"\nmodel_path = MODEL_DIR+MODEL_NAME\n\nllm = LlamaCpp(\n    model_path=MODEL_DIR+MODEL_NAME,\n    n_gpu_layers=-1,\n    max_tokens=500,\n    n_ctx=2048,\n    seed=42,\n    verbose=False\n)\n\n/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/langchain_community/llms/__init__.py:312: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n\nFor example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\nwith: `from pydantic import BaseModel`\nor the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet.     from pydantic.v1 import BaseModel\n\n  from langchain_community.llms.llamacpp import LlamaCpp\n\n\nValidationError: 1 validation error for LlamaCpp\nclient\n  Field required [type=missing, input_value={'model_path': '/Users/gu...': 42, 'verbose': False}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\n\n\n\nfrom typing import Any, Dict, List, Optional\nfrom langchain_core.language_models import LLM\nfrom llama_cpp import Llama\n\nclass CustomLlamaLLM(LLM):\n    model_path: str\n    n_gpu_layers: int = -1\n    max_tokens: int = 500\n    n_ctx: int = 2048\n    seed: Optional[int] = 42\n    verbose: bool = False\n    client: Any = None\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.client = Llama(\n            model_path=self.model_path,\n            n_gpu_layers=self.n_gpu_layers,\n            max_tokens=self.max_tokens,\n            n_ctx=self.n_ctx,\n            seed=self.seed,\n            verbose=self.verbose\n        )\n\n    @property\n    def _llm_type(self) -&gt; str:\n        return \"CustomLlama\"\n\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -&gt; str:\n        response = self.client(prompt, stop=stop)\n        return response[\"choices\"][0][\"text\"]\n\n\n# Instantiate the custom LLM class\nllm = CustomLlamaLLM(\n    model_path=model_path,\n    n_gpu_layers=-1,\n    max_tokens=500,\n    n_ctx=2048,\n    seed=42,\n    verbose=True\n)\n\nllama_model_loader: loaded meta data with 23 key-value pairs and 195 tensors from /Users/gurpreetjohl/workspace/python/ml-practice/ml-practice/models/Phi-3-mini-4k-instruct-fp16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = phi3\nllama_model_loader: - kv   1:                               general.name str              = Phi3\nllama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\nllama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\nllama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\nllama_model_loader: - kv   5:                           phi3.block_count u32              = 32\nllama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\nllama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\nllama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\nllama_model_loader: - kv  10:                          general.file_type u32              = 1\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"&lt;unk&gt;\", \"&lt;s&gt;\", \"&lt;/s&gt;\", \"&lt;0x00&gt;\", \"&lt;...\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\nllama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  130 tensors\nllm_load_vocab: special tokens cache size = 323\nllm_load_vocab: token to piece cache size = 0.1687 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = phi3\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32064\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 3072\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_rot            = 96\nllm_load_print_meta: n_embd_head_k    = 96\nllm_load_print_meta: n_embd_head_v    = 96\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 3072\nllm_load_print_meta: n_embd_v_gqa     = 3072\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 8192\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 2\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: model type       = 3B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 3.82 B\nllm_load_print_meta: model size       = 7.12 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = Phi3\nllm_load_print_meta: BOS token        = 1 '&lt;s&gt;'\nllm_load_print_meta: EOS token        = 32000 '&lt;|endoftext|&gt;'\nllm_load_print_meta: UNK token        = 0 '&lt;unk&gt;'\nllm_load_print_meta: PAD token        = 32000 '&lt;|endoftext|&gt;'\nllm_load_print_meta: LF token         = 13 '&lt;0x0A&gt;'\nllm_load_print_meta: EOT token        = 32007 '&lt;|end|&gt;'\nllm_load_tensors: ggml ctx size =    0.22 MiB\nggml_backend_metal_log_allocated_size: allocated buffer, size =  7100.64 MiB, (23176.83 / 10922.67)ggml_backend_metal_log_allocated_size: warning: current allocated size is greater than the recommended max working set size\nllm_load_tensors: offloading 32 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloaded 33/33 layers to GPU\nllm_load_tensors:        CPU buffer size =   187.88 MiB\nllm_load_tensors:      Metal buffer size =  7100.64 MiB\n....................................................................................\nllama_new_context_with_model: n_ctx      = 2048\nllama_new_context_with_model: n_batch    = 512\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nggml_metal_init: allocating\nggml_metal_init: found device: Apple M2\nggml_metal_init: picking default device: Apple M2\nggml_metal_init: using embedded metal library\nggml_metal_init: GPU name:   Apple M2\nggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\nggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\nggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\nggml_metal_init: simdgroup reduction support   = true\nggml_metal_init: simdgroup matrix mul. support = true\nggml_metal_init: hasUnifiedMemory              = true\nggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\nllama_kv_cache_init:      Metal KV buffer size =   768.00 MiB\nllama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB\nllama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\nllama_new_context_with_model:      Metal compute buffer size =   168.00 MiB\nllama_new_context_with_model:        CPU compute buffer size =    10.01 MiB\nllama_new_context_with_model: graph nodes  = 1286\nllama_new_context_with_model: graph splits = 2\nAVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \nModel metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'&lt;|user|&gt;' + '\\n' + message['content'] + '&lt;|end|&gt;' + '\\n' + '&lt;|assistant|&gt;' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '&lt;|end|&gt;' + '\\n'}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.pre': 'default', 'general.file_type': '1', 'phi3.rope.dimension_count': '96', 'tokenizer.ggml.bos_token_id': '1', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.attention.head_count_kv': '32', 'phi3.attention.head_count': '32', 'tokenizer.ggml.model': 'llama', 'phi3.block_count': '32', 'general.architecture': 'phi3', 'phi3.feed_forward_length': '8192', 'phi3.embedding_length': '3072', 'general.name': 'Phi3', 'phi3.context_length': '4096'}\nAvailable chat formats from metadata: chat_template.default\nUsing gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'&lt;|user|&gt;' + '\n' + message['content'] + '&lt;|end|&gt;' + '\n' + '&lt;|assistant|&gt;' + '\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '&lt;|end|&gt;' + '\n'}}{% endif %}{% endfor %}\nUsing chat eos_token: &lt;|endoftext|&gt;\nUsing chat bos_token: &lt;s&gt;\nggml_metal_free: deallocating"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter7/lesson.html#basic-prompt-chain",
    "href": "posts/ml/hands_on_llm/chapter7/lesson.html#basic-prompt-chain",
    "title": "Hands-On LLMs: Advanced Text Generation Techniques and Tools",
    "section": "2.1. Basic Prompt Chain",
    "text": "2.1. Basic Prompt Chain\nIn LangChain, we use the invoke function to generate an output.\nHowever, each model requires a specific prompt template. If we blindly called invoke on our model, we get no response:\n\nllm.invoke(\"Hi! My name is Gurp. What is 1 + 1?\")\n\n\nllama_print_timings:        load time =     840.73 ms\nllama_print_timings:      sample time =       0.89 ms /     1 runs   (    0.89 ms per token,  1123.60 tokens per second)\nllama_print_timings: prompt eval time =     840.28 ms /    18 tokens (   46.68 ms per token,    21.42 tokens per second)\nllama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_print_timings:       total time =     841.04 ms /    19 tokens\n\n\n''\n\n\nThis is where the LangChain abstractions come in useful.\nWe will create a simple chain with a single link:\n\n\n\n\n\nflowchart LR\n\n  subgraph PromptChain\n    B(Prompt template) --&gt; C[LLM]\n  end\n\n  A(User prompt) --&gt; B\n  C --&gt; D(Output)\n\n\n\n\n\n\nFor our particular case, Phi-3 prompts require start, end , user and assistant tokens.\n\nfrom langchain import PromptTemplate \n\n# Create a prompt template with the \"input_prompt\" variable \ntemplate = \"\"\"&lt;s&gt;&lt;|user|&gt; {input_prompt}&lt;|end|&gt; &lt;|assistant|&gt;\"\"\" \nprompt = PromptTemplate(template=template, input_variables=[\"input_prompt\"])\n\nWe can then create a chain by chaining the prompt and LLM together. Then we can call invoke and get the intended text generation.\n\nbasic_chain = prompt | llm\nbasic_chain.invoke({\"input_prompt\": \"Hi! My name is Gurp. What is 1 + 1?\"})\n\nllama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading \"&lt;s&gt;\" in prompt, this will likely reduce response quality, consider removing it...\n  inputs = [input]\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     840.73 ms\nllama_print_timings:      sample time =       5.89 ms /    16 runs   (    0.37 ms per token,  2716.93 tokens per second)\nllama_print_timings: prompt eval time =     295.30 ms /    21 tokens (   14.06 ms per token,    71.11 tokens per second)\nllama_print_timings:        eval time =    1349.54 ms /    15 runs   (   89.97 ms per token,    11.11 tokens per second)\nllama_print_timings:       total time =    1668.23 ms /    36 tokens\n\n\n' Hello Gurp! The answer to 1 + 1 is 2'\n\n\nNote that we just passed the entire input_prompt as a variable to the chain, but we can define other variables. For example, if we wanted a more specialised use case where we don’t give te user as much flexibility, we can pre-define some of the prompt.\ntemplate = \"Create a funny name for a business that sells {product}.\" \nname_prompt = PromptTemplate(template=template, input_variables=[\"product\"])"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter7/lesson.html#multiple-prompt-chain",
    "href": "posts/ml/hands_on_llm/chapter7/lesson.html#multiple-prompt-chain",
    "title": "Hands-On LLMs: Advanced Text Generation Techniques and Tools",
    "section": "2.2. Multiple Prompt Chain",
    "text": "2.2. Multiple Prompt Chain\nIf we have more complex prompts or use cases, we can split the task into smaller subtasks that run sequentially. Each link in the chain deals with a specific subtask.\n\n\n\n\n\nflowchart LR\n\n  subgraph MultiPromptChain\n    B1(Prompt 1) --&gt; B2(Prompt 2)\n    B1 &lt;--&gt; C[LLM]\n    B2 &lt;--&gt; C\n  end\n\n  A(User input) --&gt; B1\n\n  B2 --&gt; D(Output)\n\n\n\n\n\n\nAs an example, we can prompt the LLM to create a story. First we ask it for a title based on the user prompt, then characters based on the title, then a story based on the characters and title. The first link is the only one that requires user input.\n\n\n\nStory prompt chain\n\n\n\nfrom langchain import LLMChain\n\n# Create a chain for the title of our story \ntitle_template = \"\"\"&lt;s&gt;&lt;|user|&gt; Create a title for a story about {summary}. Only return the title. &lt;|end|&gt; &lt;|assistant|&gt;\"\"\" \ntitle_prompt = PromptTemplate(template=title_template, input_variables= [\"summary\"]) \ntitle = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")\n\n# Create a chain for the character description using the summary and title \ncharacter_template = \"\"\"&lt;s&gt;&lt;|user|&gt; Describe the main character of a story about {summary} with the title {title}. Use only two sentences.&lt;|end|&gt; &lt;|assistant|&gt;\"\"\" \ncharacter_prompt = PromptTemplate(template=character_template, input_variables=[\"summary\", \"title\"]) \ncharacter = LLMChain(llm=llm, prompt=character_prompt, output_key=\"character\")\n\n# Create a chain for the story using the summary, title, and character description \nstory_template = \"\"\"&lt;s&gt;&lt;|user|&gt; Create a story about {summary} with the title {title}. The main character is: {character}. Only return the story and it cannot be longer than one paragraph. &lt;|end|&gt; &lt;|assistant|&gt;\"\"\" \nstory_prompt = PromptTemplate(template=story_template, input_variables=[\"summary\", \"title\", \"character\"]) \nstory = LLMChain(llm=llm, prompt=story_prompt, output_key=\"story\")\n\n\n# Combine all three components to create the full chain \nllm_chain = title | character | story\n\nggml_metal_free: deallocating\n\n\nNow we can invoke the chain just like before:\n\nllm_chain.invoke({\"summary\": \"a dog that can smell danger\"})\n\nllama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading \"&lt;s&gt;\" in prompt, this will likely reduce response quality, consider removing it...\n  inputs = [input]\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     840.73 ms\nllama_print_timings:      sample time =       1.34 ms /    16 runs   (    0.08 ms per token, 11958.15 tokens per second)\nllama_print_timings: prompt eval time =     295.27 ms /    23 tokens (   12.84 ms per token,    77.89 tokens per second)\nllama_print_timings:        eval time =    1424.98 ms /    15 runs   (   95.00 ms per token,    10.53 tokens per second)\nllama_print_timings:       total time =    1725.28 ms /    38 tokens\nllama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading \"&lt;s&gt;\" in prompt, this will likely reduce response quality, consider removing it...\n  inputs = [input]\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     840.73 ms\nllama_print_timings:      sample time =       1.97 ms /    16 runs   (    0.12 ms per token,  8109.48 tokens per second)\nllama_print_timings: prompt eval time =     190.41 ms /    44 tokens (    4.33 ms per token,   231.08 tokens per second)\nllama_print_timings:        eval time =    1315.12 ms /    15 runs   (   87.67 ms per token,    11.41 tokens per second)\nllama_print_timings:       total time =    1511.43 ms /    59 tokens\nllama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading \"&lt;s&gt;\" in prompt, this will likely reduce response quality, consider removing it...\n  inputs = [input]\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     840.73 ms\nllama_print_timings:      sample time =       1.48 ms /    16 runs   (    0.09 ms per token, 10803.51 tokens per second)\nllama_print_timings: prompt eval time =     290.88 ms /    70 tokens (    4.16 ms per token,   240.65 tokens per second)\nllama_print_timings:        eval time =    1480.88 ms /    15 runs   (   98.73 ms per token,    10.13 tokens per second)\nllama_print_timings:       total time =    1777.98 ms /    85 tokens\n\n\n{'summary': 'a dog that can smell danger',\n 'title': ' \"Scent of Peril: The Canine Detective\\'s N',\n 'character': ' In the heartwarming tale, Scent of Peril follows Max,',\n 'story': ' In a quaint suburban neighborhood, there lived an extraordinary German Shepherd'}"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter7/lesson.html#conversation-buffer",
    "href": "posts/ml/hands_on_llm/chapter7/lesson.html#conversation-buffer",
    "title": "Hands-On LLMs: Advanced Text Generation Techniques and Tools",
    "section": "3.1. Conversation Buffer",
    "text": "3.1. Conversation Buffer\nThe simplest way we can force the LLM to remember previous conversation is by passing the full conversation history in our prompt.\nThis approach is called conversation buffer memory. We update our prompt with the history of the chat.\n\n3.1.1. Simple Conversation Buffer\n\n# Create an updated prompt template to include a chat history \ntemplate = \"\"\"&lt;s&gt;&lt;|user|&gt;Current conversation:{chat_history} {input_prompt}&lt;|end|&gt; &lt;|assistant|&gt;\"\"\" \nprompt = PromptTemplate(template=template, input_variables=[\"input_prompt\", \"chat_history\"])\n\nNext, we create a ConversationBufferMemory link in the chain that will store the conversations we have previously had with the LLM.\n\nfrom langchain.memory import ConversationBufferMemory\n\n# Define the type of memory we will use \nmemory = ConversationBufferMemory(memory_key=\"chat_history\") \n\n# Chain the LLM, prompt, and memory together \nllm_chain = LLMChain(prompt=prompt, llm=llm, memory=memory)\n\nWe can verify if this has worked by seeing if it now remembers our name in later prompts:\n\n# Generate a conversation and ask a basic question \nllm_chain.invoke({\"input_prompt\": \"Hi! My name is Gurp. What is 1 + 1?\"})\n\nllama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading \"&lt;s&gt;\" in prompt, this will likely reduce response quality, consider removing it...\n  inputs = [input]\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     840.73 ms\nllama_print_timings:      sample time =       5.11 ms /    16 runs   (    0.32 ms per token,  3129.89 tokens per second)\nllama_print_timings: prompt eval time =     262.37 ms /    19 tokens (   13.81 ms per token,    72.42 tokens per second)\nllama_print_timings:        eval time =    1334.30 ms /    15 runs   (   88.95 ms per token,    11.24 tokens per second)\nllama_print_timings:       total time =    1608.21 ms /    34 tokens\n\n\n{'input_prompt': 'Hi! My name is Gurp. What is 1 + 1?',\n 'chat_history': '',\n 'text': ' Hello, Gurp! The answer to 1 + 1 is '}\n\n\n\n# Does the LLM remember my name? \nresponse = llm_chain.invoke({\"input_prompt\": \"What is my name?\"})\nprint(response)\n\nllama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading \"&lt;s&gt;\" in prompt, this will likely reduce response quality, consider removing it...\n  inputs = [input]\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     840.73 ms\nllama_print_timings:      sample time =       1.28 ms /    16 runs   (    0.08 ms per token, 12539.18 tokens per second)\nllama_print_timings: prompt eval time =     344.80 ms /    47 tokens (    7.34 ms per token,   136.31 tokens per second)\nllama_print_timings:        eval time =    1425.89 ms /    15 runs   (   95.06 ms per token,    10.52 tokens per second)\nllama_print_timings:       total time =    1775.85 ms /    62 tokens\n\n\n{'input_prompt': 'What is my name?', 'chat_history': 'Human: Hi! My name is Gurp. What is 1 + 1?\\nAI:  Hello, Gurp! The answer to 1 + 1 is ', 'text': ' Hi Gurp! The answer to 1 + 1 is 2'}\n\n\n\nllm_chain.invoke({\"input_prompt\": \"What is my name?\"})\n\nllama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading \"&lt;s&gt;\" in prompt, this will likely reduce response quality, consider removing it...\n  inputs = [input]\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     840.73 ms\nllama_print_timings:      sample time =       6.22 ms /    16 runs   (    0.39 ms per token,  2571.93 tokens per second)\nllama_print_timings: prompt eval time =     346.93 ms /    36 tokens (    9.64 ms per token,   103.77 tokens per second)\nllama_print_timings:        eval time =    1359.76 ms /    15 runs   (   90.65 ms per token,    11.03 tokens per second)\nllama_print_timings:       total time =    1721.41 ms /    51 tokens\n\n\n{'input_prompt': 'What is my name?',\n 'chat_history': 'Human: Hi! My name is Gurp. What is 1 + 1?\\nAI:  Hello, Gurp! The answer to 1 + 1 is \\nHuman: What is my name?\\nAI:  Hi Gurp! The answer to 1 + 1 is 2',\n 'text': \" Hi Gurp! You're asking your own name; I am an\"}\n\n\n\n\n3.1.2 Windowed Conversation Buffer\nAs the conversation goes on, the size of the chat history grows until eventually it may exceed the token limit.\nOne approach to work around this is to only hold the last \\(k\\) conversations in memory rather than the entire history.\n\nfrom langchain.memory import ConversationBufferWindowMemory \n\n# Retain only the last 2 conversations in memory \nmemory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\") \n\n# Chain the LLM, prompt, and memory together \nllm_chain = LLMChain(prompt=prompt, llm=llm, memory=memory)\n\n/var/folders/8k/8jqhnfbd1t99blb07r1hs5440000gn/T/ipykernel_47761/3936398832.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n  memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n\n\nThis approach is not ideal for longer conversations. An alternative is to summarise the chat history to fit in the token limit, rather than truncating it."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter7/lesson.html#conversation-summary",
    "href": "posts/ml/hands_on_llm/chapter7/lesson.html#conversation-summary",
    "title": "Hands-On LLMs: Advanced Text Generation Techniques and Tools",
    "section": "3.2. Conversation Summary",
    "text": "3.2. Conversation Summary\nThis approach uses an LLM to summarise the main points of the history so far to reduce the number of tokens required to pass to the main LLM.\nThe “summary LLM” can be a different model to our “main LLM”. We may want to use a smaller LLM for the “easier” task of summarisation to speed up computation.\nThere will now be two LLM calls per invocation: the user prompt and the summarisation prompt.\n\n\n\nConversation summary diagram\n\n\n\n# Create a summary prompt template \nsummary_prompt_template = \"\"\"\n    &lt;s&gt;&lt;|user|&gt;Summarize the conversations and update with the new lines. \n    Current summary: {summary} new lines of conversation: {new_lines} \n    New summary:&lt;|end|&gt; &lt;|assistant|&gt;\n\"\"\"\nsummary_prompt = PromptTemplate(\n    input_variables=[\"new_lines\", \"summary\"],\n    template=summary_prompt_template\n)\n\nIn this example, we’ll pass both calls to the same LLM, but in general we don’t have to.\n\nfrom langchain.memory import ConversationSummaryMemory \n\n# Define the type of memory we will use \nmemory = ConversationSummaryMemory(\n    llm=llm,\n    memory_key=\"chat_history\",\n    prompt=summary_prompt\n) \n\n# Chain the LLM, prompt, and memory together \nllm_chain = LLMChain(prompt=prompt, llm=llm, memory=memory)\n\n/var/folders/8k/8jqhnfbd1t99blb07r1hs5440000gn/T/ipykernel_47761/178162952.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n  memory = ConversationSummaryMemory(\n\n\nWe can try this out by having a short conversation with the LLM and checking it has retained previous information.\n\n# Generate a conversation and ask for the name \nllm_chain.invoke({\"input_prompt\": \"Hi! My name is Gurp. What is 1 + 1?\"}) \nllm_chain.invoke({\"input_prompt\": \"What is my name?\"})\n\nllama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading \"&lt;s&gt;\" in prompt, this will likely reduce response quality, consider removing it...\n  inputs = [input]\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     840.73 ms\nllama_print_timings:      sample time =       1.33 ms /    16 runs   (    0.08 ms per token, 12066.37 tokens per second)\nllama_print_timings: prompt eval time =     273.75 ms /    19 tokens (   14.41 ms per token,    69.41 tokens per second)\nllama_print_timings:        eval time =    1438.78 ms /    15 runs   (   95.92 ms per token,    10.43 tokens per second)\nllama_print_timings:       total time =    1717.89 ms /    34 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     840.73 ms\nllama_print_timings:      sample time =       5.37 ms /    16 runs   (    0.34 ms per token,  2977.85 tokens per second)\nllama_print_timings: prompt eval time =     291.94 ms /    77 tokens (    3.79 ms per token,   263.75 tokens per second)\nllama_print_timings:        eval time =    1356.44 ms /    15 runs   (   90.43 ms per token,    11.06 tokens per second)\nllama_print_timings:       total time =    1661.81 ms /    92 tokens\nllama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading \"&lt;s&gt;\" in prompt, this will likely reduce response quality, consider removing it...\n  inputs = [input]\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     840.73 ms\nllama_print_timings:      sample time =       1.40 ms /    16 runs   (    0.09 ms per token, 11428.57 tokens per second)\nllama_print_timings: prompt eval time =     133.14 ms /    28 tokens (    4.76 ms per token,   210.30 tokens per second)\nllama_print_timings:        eval time =    1437.07 ms /    15 runs   (   95.80 ms per token,    10.44 tokens per second)\nllama_print_timings:       total time =    1575.77 ms /    43 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     840.73 ms\nllama_print_timings:      sample time =       4.67 ms /    16 runs   (    0.29 ms per token,  3427.59 tokens per second)\nllama_print_timings: prompt eval time =     294.58 ms /    81 tokens (    3.64 ms per token,   274.96 tokens per second)\nllama_print_timings:        eval time =    1346.13 ms /    15 runs   (   89.74 ms per token,    11.14 tokens per second)\nllama_print_timings:       total time =    1654.38 ms /    96 tokens\n\n\n{'input_prompt': 'What is my name?',\n 'chat_history': ' Gurp initiated a conversation by introducing himself and asking for the sum',\n 'text': ' It seems there may have been a misunderstanding. In our current conversation, you'}\n\n\n\n# Check whether it has summarized everything thus far \nllm_chain.invoke({\"input_prompt\": \"What was the first question I asked?\"})\n\nllama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading \"&lt;s&gt;\" in prompt, this will likely reduce response quality, consider removing it...\n  inputs = [input]\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     840.73 ms\nllama_print_timings:      sample time =       3.82 ms /    16 runs   (    0.24 ms per token,  4187.39 tokens per second)\nllama_print_timings: prompt eval time =     274.15 ms /    31 tokens (    8.84 ms per token,   113.08 tokens per second)\nllama_print_timings:        eval time =    1342.18 ms /    15 runs   (   89.48 ms per token,    11.18 tokens per second)\nllama_print_timings:       total time =    1625.96 ms /    46 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     840.73 ms\nllama_print_timings:      sample time =       1.95 ms /    16 runs   (    0.12 ms per token,  8200.92 tokens per second)\nllama_print_timings: prompt eval time =     300.16 ms /    84 tokens (    3.57 ms per token,   279.85 tokens per second)\nllama_print_timings:        eval time =    1455.62 ms /    15 runs   (   97.04 ms per token,    10.30 tokens per second)\nllama_print_timings:       total time =    1761.25 ms /    99 tokens\n\n\n{'input_prompt': 'What was the first question I asked?',\n 'chat_history': ' Gurp introduced himself to the human and inquired about their name. The',\n 'text': ' The first question you asked could be, \"Nice to meet you, G'}\n\n\n\n# Check what the summary is thus far \nmemory.load_memory_variables({})\n\n{'chat_history': ' Gurp introduced himself to the human and inquired about their name, while'}\n\n\nThe conversation summary approach reduces the tokens required, but it does risk losing information depending on the quality of the summary."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter7/lesson.html#comparison-of-memory-approaches",
    "href": "posts/ml/hands_on_llm/chapter7/lesson.html#comparison-of-memory-approaches",
    "title": "Hands-On LLMs: Advanced Text Generation Techniques and Tools",
    "section": "3.3. Comparison of Memory Approaches",
    "text": "3.3. Comparison of Memory Approaches\nConversation buffer\nPros:\n\nEasiest to implement\nEnsures no loss of info (as long as conversation fits in congtext window)\n\nCons:\n\nSlower generation (more tokens needed)\nOnly suitable for LLMs with large context windows\nHandles larger chat histories poorly\n\nWindowed conversation buffer\nPros:\n\nCan use LLMs with smaller context windows\nGood for shorter chats; no information loss over the last k interactions\n\nCons:\n\nOnly captures k interactions\nNo compression, so can still require a large context window if k is large\n\nConversation summary\nPros:\n\nCaptures full history\nEnables long chats\nReduces required tokens\n\nCons:\n\nRequires an additional LLM call per interaction\nQuality of response depends on LLM’s summarisation quality"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter7/lesson.html#react",
    "href": "posts/ml/hands_on_llm/chapter7/lesson.html#react",
    "title": "Hands-On LLMs: Advanced Text Generation Techniques and Tools",
    "section": "4.1. ReAct",
    "text": "4.1. ReAct\nMany agent-based systems rely on the ReAct framework, which standard for Reasoning and Acting.\nWe can give the LLM the ability to use tools, but it can only generate text, so it needs to generate the right text to interact with tools. For example, if we let it use a weather forecasting API, it needs to provide a request in the correct format.\nReAct merges the concepts of reasoning and acting as they are essentially two sides of the same coin: we want reasonong to afect actions and actions to affect reasoning. It does this by iteratively following these three steps:\n\nThought\nAction\nObservation\n\nWe incorporate this into a prompt template like so:\n\n\n\nReAct prompt template\n\n\nWe ask it to create a thought about the prompt, then trigger an action based on the thought, then observe the output, i.e. whatever it retrieved from an external tool.\nAn example of this is using an LLM to use a calculator.\n\nimport os \nfrom langchain_openai import ChatOpenAI \n\n# Load OpenAI's LLMs with LangChain \nos.environ[\"OPENAI_API_KEY\"] = \"MY_KEY\" \nopenai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n\n\n# Create the ReAct template \n\nreact_template = \"\"\"\n    Answer the following questions as best you can. \n    You have access to the following tools: {tools} \n    \n    Use the following format: \n    \n    Question: the input question you must answer \n    Thought: you should always think about what to do \n    Action: the action to take, should be one of [{tool_names}] \n    Action Input: the input to the action \n    Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) \n    Thought: I now know the final answer \n    Final Answer: the final answer to the original input question\n    \n    Begin! \n    \n    Question: {input} \n    Thought:{agent_scratchpad}\n\"\"\" \n    \nprompt = PromptTemplate(\n    template=react_template,\n    input_variables=[\"tools\", \"tool_names\", \"input\", \"agent_scratchpad\"]\n)\n\nNext we need to define the tools it can use to interact with the outside world.\n\nfrom langchain.agents import load_tools, Tool \nfrom langchain.tools import DuckDuckGoSearchResults \n\n# You can create the tool to pass to an agent \nsearch = DuckDuckGoSearchResults() \nsearch_tool = Tool(\n    name=\"duckduck\",\n    description=\"A web search engine. Use this to as a search engine for general queries.\",\n    func=search.run\n)\n\n# Prepare tools \ntools = load_tools([\"llm-math\"], llm=llm)   # Calculator tool is included by default \ntools.append(search_tool)\n\nFinally we can create the ReAct agent and pass it to the AgentExecutor which handles the execution steps.\n\nfrom langchain.agents import AgentExecutor, create_react_agen\n\n# Construct the ReAct agent \nagent = create_react_agent(openai_llm, tools, prompt) \nagent_executor = AgentExecutor(\n    agent=agent,\n    tools=tools,\n    verbose=True,\n    handle_parsing_errors=True\n)\n\nNow we can invoke the LLM to find the price of an item and convert the currency. It will choose the appropriate tools to use for this.\n\n# What is the price of a MacBook Pro? \nagent_executor.invoke({\n    \"input\": \"What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD.\"\n})\n\nThe agent generates intermediate steps during execution which we can use to follow its train of thought.\nThe important thing to consider when using agents is that there is no human in the loop; it will generate an answer but there is no guarantee that it is the correct answer.\nWe can make some tweaks to help ourselves debug this. For exampe, asking the agent to return the website’s URL that it retrieved prices from to make manual verification easier."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter2/lesson.html#tokenization-in-action",
    "href": "posts/ml/hands_on_llm/chapter2/lesson.html#tokenization-in-action",
    "title": "Hands-On LLMs: Tokens and Embeddings",
    "section": "1.1. Tokenization in Action",
    "text": "1.1. Tokenization in Action\nWe can load a model to see what tokenization looks like practice.\nWe’ll load a smaller open-source model and its corresponding tokenizer.\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nif torch.cuda.is_available():\n    device = \"cuda\"\n    device_map = \"cuda\"\nelif torch.backends.mps.is_available():\n    device = \"mps\"\n    device_map=None\nelse:\n    device = \"cpu\"\n    device_map = \"cpu\"\n\n\n1.1.1. Load the model\nThis can take a few minutes depedning on internet connection.\n\nmodel_name = \"microsoft/Phi-3-mini-4k-instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=device_map,\n    torch_dtype=\"auto\",\n    trust_remote_code=True\n)\nmodel.to(device)\n\n\n\n\n\n\n\nA new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n- configuration_phi3.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n- modeling_phi3.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\nCurrent `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPhi3ForCausalLM(\n  (model): Phi3Model(\n    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n    (embed_dropout): Dropout(p=0.0, inplace=False)\n    (layers): ModuleList(\n      (0-31): 32 x Phi3DecoderLayer(\n        (self_attn): Phi3Attention(\n          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n          (rotary_emb): Phi3RotaryEmbedding()\n        )\n        (mlp): Phi3MLP(\n          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n          (activation_fn): SiLU()\n        )\n        (input_layernorm): Phi3RMSNorm()\n        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n        (post_attention_layernorm): Phi3RMSNorm()\n      )\n    )\n    (norm): Phi3RMSNorm()\n  )\n  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n)\n\n\n\n\n1.1.2. Load the tokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\n\n\n\n1.1.3. Use the model to generate text\nFirst we tokenize the input promt. Then we pass this to the model. We can peek in at each step to see what’s actually being passed around.\nWe’ll start with the following input prompt:\n\ninput_prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.&lt;|assistant|&gt;\"\n\nThe tokenizer converts this text to a list of integers. These are the input IDs that are passed to the model.\n\n# Tokenize the input prompt \ninput_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids.to(device)\n\n\nprint(input_ids)\n\ntensor([[14350,   385,  4876, 27746,  5281,   304, 19235,   363,   278, 25305,\n           293, 16423,   292,   286,   728,   481, 29889, 12027,  7420,   920,\n           372,  9559, 29889, 32001]], device='mps:0')\n\n\nWe can “decode” these input IDs, converting them back to text, to see how the tokenizer splits the text. It uses sub-word tokens, so mishap is split into m, ish, ap. Punctuation is its own token and there is a special token for &lt;|assistant|&gt; Spaces are implicit; parital tokens have a special hidden character preceding them and tokens without that character are assumed to have a space before them.\n\nfor id in input_ids[0]: \n    print(tokenizer.decode(id))\n\nWrite\nan\nemail\napolog\nizing\nto\nSarah\nfor\nthe\ntrag\nic\ngarden\ning\nm\nish\nap\n.\nExp\nlain\nhow\nit\nhappened\n.\n&lt;|assistant|&gt;\n\n\nWe can now pass this tokenized input to the model to generate new tokens.\n\n# Due to a quirk of Macs, we need to explicitly pass it an attention mask as it cannot be inferred\nif device == 'mps':\n    model_kwargs = {'attention_mask': (input_ids != tokenizer.pad_token_id).long()}\nelse:\n    model_kwargs = {}\n\n# Generate the text \ngeneration_output = model.generate(input_ids=input_ids, max_new_tokens=100, **model_kwargs)\n\n\nThe output of the generation appends tokens to the input.\n\ngeneration_output\n\ntensor([[14350,   385,  4876, 27746,  5281,   304, 19235,   363,   278, 25305,\n           293, 16423,   292,   286,   728,   481, 29889, 12027,  7420,   920,\n           372,  9559, 29889, 32001,  3323,   622, 29901, 17778, 29888,  2152,\n          6225, 11763,   363,   278, 19906,   292,   341,   728,   481,    13,\n            13,    13, 29928,   799, 19235, 29892,    13,    13,    13, 29902,\n          4966,   445,  2643, 14061,   366,  1532, 29889,   306,   626,  5007,\n           304,  4653,   590,  6483,   342,  3095, 11763,   363,   278,   443,\n          6477,   403, 15134,   393, 10761,   297,   596, 16423, 22600, 29889,\n            13,    13,    13,  2887,   366,  1073, 29892,   306,   505,  2337,\n          7336,  2859,   278, 15409,   322, 22024,   339,  1793,   310,   596,\n         16423, 29889,   739,   756,  1063,   263,  2752,   310,  8681, 12232,\n           363,   592, 29892,   322,   306,   471,  1468, 24455,   304,   505,\n           278, 15130,   304,  1371]], device='mps:0')\n\n\nAgain, we can decode this to see the output text\n\n# Print the output \nprint(tokenizer.decode(generation_output[0]))\n\nWrite an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.&lt;|assistant|&gt; Subject: Heartfelt Apologies for the Gardening Mishap\n\n\nDear Sarah,\n\n\nI hope this message finds you well. I am writing to express my deepest apologies for the unfortunate incident that occurred in your garden yesterday.\n\n\nAs you know, I have always admired the beauty and tranquility of your garden. It has been a source of inspiration for me, and I was thrilled to have the opportunity to help"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter2/lesson.html#tokenizer-design",
    "href": "posts/ml/hands_on_llm/chapter2/lesson.html#tokenizer-design",
    "title": "Hands-On LLMs: Tokens and Embeddings",
    "section": "1.2. Tokenizer Design",
    "text": "1.2. Tokenizer Design\nThere are three primary decisions that determine how the tokenizer splits a given prompt:\n\nTokenization method: byte pair encoding (BPE), WordPiece\nTokenizer parameters: vocabulary size, choice of special tokens\nTraining data set: a tokenizer trained on English text will give different results to one trained on Punjabi text or Python code, etc.\n\nTokenizers are used on the input (to encode text -&gt; numbers) and on the output (to decode numbers -&gt; text).\n\n1.2.1 Tokenization Methods\nThere are four promininent tokenization schemes:\n\nWord tokens.\n\nPros: Simple to implement and understand; can fit more text in a given context window\nCons: Unable to handle unseen words; vocab has lots of tokens for almost identical words (e.g. write, writing, written, wrote)\n\nSub-word tokens.\n\nPros: Can represent new words by breaking down into other known tokens\nCons: Choice of partial words dictionary requires careful design\n\nCharacter tokens.\n\nPros: Can represent any new word\nCons: Modeling is more difficult; can’t fit as much text in a context window\n\nByte tokens. Breaks tokens down into the individual unicode character bytes. This is also called “tokenization-free representation”.\n\nPros: Can represent text of different alphabets, useful for multilingual models\n\n\nSome tokenizers employ a hybrid approach. For example, GPT-2 uses sub-word tokenization and falls back to byte tokens for other characters.\nParticular cases of interest that distinguish tokenization (and model) performance are the way the tokenizer handles:\n\nCapitalization\nOther languages\nEmojis\nCode - keywords and whitespace. Some models have different tokens for one space, two spaces, three spaces, four spaces etc.\nNumbers and digits - does it encode each digit as a separate number or as a whole? E.g. 420 vs 4,2,0. Separate seems to perform maths better.\nSpecial tokens - beginning of text, end of text, user/system/assistant tags, separator token used to separate two text inputs in similarity models.\n\n\n\n1.2.2. Tokenizer Parameters\nThe LLM designer makes decisions about the paramters of the tokenizer:\n\nVocabulary size: \\(\\approx 50k\\) is typical currently\nSpecial tokens: Particular use cases may warrant special tokens, e.g. coding, research citations, etc\nCapitalisation: Treat upper case and lower case as separate tokens? Or convert all to lower case?\nTraining data domain"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter2/lesson.html#sentencedocument-embeddings",
    "href": "posts/ml/hands_on_llm/chapter2/lesson.html#sentencedocument-embeddings",
    "title": "Hands-On LLMs: Tokens and Embeddings",
    "section": "2.2. Sentence/Document Embeddings",
    "text": "2.2. Sentence/Document Embeddings\nSome models operate on sentences or entire documents.\nA simple approach is to take the word embeddings for each word in the document, then average them. Some LLMs produce “text embeddings” which represent the whole text as an embedding vector directly.\n\nfrom sentence_transformers import SentenceTransformer \n\n# Load model \nmodel = SentenceTransformer(\"sentence-transformers/all-mpnetbase-v2\") \n\n# Convert text to text embeddings \nvector = model.encode(\"Best movie ever!\")"
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter2/lesson.html#non-llm-based-embeddings",
    "href": "posts/ml/hands_on_llm/chapter2/lesson.html#non-llm-based-embeddings",
    "title": "Hands-On LLMs: Tokens and Embeddings",
    "section": "2.3. Non-LLM-based Embeddings",
    "text": "2.3. Non-LLM-based Embeddings\nEmbeddings are useful in NLP more generally, and some techniques, such as Word2Vec and GloVe, predate LLMs.\nThese can be useful to apply NLP to non-text applications, such as music recommendations.\nSay we have a data set of songs belonging to playlists. This can help us learn which songs are similar, because similar songs are likely to be neighbouring on playlists, just as similar words are likely to be neighbouring in a sentence.\nSo we can convert each song to an ID, and treat a playlist like a sentence, i.e. it is just a sequence of tokens. Then we can train a Word2Vec model on it to get embedding vectors for each song.\nThen, if we have a song we like, we can look at its embedding vector and find similar songs by finding the songs with the closest embeddings."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter4/lesson.html#the-task",
    "href": "posts/ml/hands_on_llm/chapter4/lesson.html#the-task",
    "title": "Hands-On LLMs: Text Classification",
    "section": "1.1. The task",
    "text": "1.1. The task\nThe goal is to assign a label to some input text. This has applications in sentiment analysis, entity recognition and detecting language.\nText classification can be achieve using representation models i.e. encoder-only, or generative models i.e. decoder-only.\nThere are, of course, classical NLP approaches to this that do not rely on LLMs. For example, representing text using TF-IDF and training a logistic classifier on this.\nOne caveat to be aware of when using any pre-trained models is that we often don’t know the training data used; the weights are open-source but the training data isn’t.\nThis makes evaluation trickier, as we can’t be sure if our test data is truly out-of-sample."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter4/lesson.html#using-a-pre-trained-classifier",
    "href": "posts/ml/hands_on_llm/chapter4/lesson.html#using-a-pre-trained-classifier",
    "title": "Hands-On LLMs: Text Classification",
    "section": "2.1. Using a pre-trained classifier",
    "text": "2.1. Using a pre-trained classifier\nWe can load a pre-trained classification model from HuggingFace:\n\nfrom transformers import pipeline \n\n\nmodel_path = \"cardiffnlp/twitter-roberta-base-sentiment-latest\" \npipe = pipeline(model=model_path, tokenizer=model_path, return_all_scores=True, device=device)\n\n\n\n\n\n\n\nSome weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\nThen we use it to infer the sentiment of our test data:\n\nimport numpy as np \nfrom tqdm import tqdm \nfrom transformers.pipelines.pt_utils import KeyDataset \n\n\n# Run inference for each test instance\ny_pred = [] \nfor output in tqdm(pipe(KeyDataset(data[\"test\"], \"text\")), total=len(data[\"test\"])):\n    negative_score = output[0][\"score\"] \n    positive_score = output[2][\"score\"] \n    assignment = np.argmax([negative_score, positive_score])\n    y_pred.append(assignment)\n\n100%|██████████| 1066/1066 [00:33&lt;00:00, 32.03it/s]\n\n\nWe can then evaluate the performance by looking at classification metrics based on the confusion matrix:\n\nfrom sklearn.metrics import classification_report \n\n\nperformance = classification_report(\n    data[\"test\"][\"label\"],\n    y_pred,\n    target_names=[\"Negative Review\", \"Positive Review\"]\n)\nprint(performance)\n\n                 precision    recall  f1-score   support\n\nNegative Review       0.76      0.88      0.81       533\nPositive Review       0.86      0.72      0.78       533\n\n       accuracy                           0.80      1066\n      macro avg       0.81      0.80      0.80      1066\n   weighted avg       0.81      0.80      0.80      1066\n\n\n\nThis is a pretty good result considering we’ve just used a generic off-the-shelf model with no training and not specific to our domain! We get 80% accuracy and an F1 score of 0.81."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter4/lesson.html#using-a-pre-trained-embedding-model",
    "href": "posts/ml/hands_on_llm/chapter4/lesson.html#using-a-pre-trained-embedding-model",
    "title": "Hands-On LLMs: Text Classification",
    "section": "2.2. Using a pre-trained embedding model",
    "text": "2.2. Using a pre-trained embedding model\nThis approach is helpful when we can not find a “similar enough” classification model that has been trained on a similar task.\nWe use an embedding model to generate embedding vectors for our text data. Think of this like the feature engineering step of a classical ML classification task.\nWe can then feed these embeddings / features to train a lightweight classifier. This can be any classifier of choice; there is nothing NLP-specific to the problem at this point.\nWe can load a pre-trained embedding model and use it to generate embeddings for our training and test data. This embedding model is kept frozen.\n\nfrom sentence_transformers import SentenceTransformer \n\n\nmodel = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_embeddings = model.encode(data[\"train\"][\"text\"], show_progress_bar=True) \ntest_embeddings = model.encode(data[\"test\"][\"text\"], show_progress_bar=True)\n\n\n\n\n\n\n\nNext we can use these embeddings to train a classifier. In this case, we just use a simple logistic regression, but this can be any classifier.\n\nfrom sklearn.linear_model import LogisticRegression\n\n\nclf = LogisticRegression(random_state=42)\nclf.fit(train_embeddings, data[\"train\"][\"label\"])\n\nLogisticRegression(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression(random_state=42) \n\n\nNow we can evaluate the model as before:\n\ny_pred = clf.predict(test_embeddings)\nperformance = classification_report(data[\"test\"][\"label\"], y_pred, target_names=[\"Negative Review\", \"Positive Review\"])\n\nprint(performance)\n\n                 precision    recall  f1-score   support\n\nNegative Review       0.85      0.86      0.85       533\nPositive Review       0.86      0.85      0.85       533\n\n       accuracy                           0.85      1066\n      macro avg       0.85      0.85      0.85      1066\n   weighted avg       0.85      0.85      0.85      1066\n\n\n\nEven better! With a pre-trained embedding model and training our own lightweight classifier in a few seconds, we get improved accuracy of 85%."
  },
  {
    "objectID": "posts/ml/hands_on_llm/chapter4/lesson.html#zero-shot-classification-using-and-embedding-model",
    "href": "posts/ml/hands_on_llm/chapter4/lesson.html#zero-shot-classification-using-and-embedding-model",
    "title": "Hands-On LLMs: Text Classification",
    "section": "2.3. Zero-shot classification using and embedding model",
    "text": "2.3. Zero-shot classification using and embedding model\nGetting labelled data is expensive and time-consuming.\nWe can use zero-shot classification in the absence of labels. This can be useful in cases where we want to assess the feasibility of a task as a first step to determine whether it’s worth the effort of collecting labelled data.\nWe do this by:\n\nDescribe the label, then create the embedding vector of this description. This acts as the baseline vector for the label.\nWe can then compare the embedding vector of any text, using cosine similarity, to get a measure of how well our label matches our text.\n\n\n\n\nimage.png\n\n\nWe create embedding vectors for our labels:\n\nlabel_embeddings = model.encode([\"A negative review\", \"A positive review\"])\n\nThen we can classify our text data by calculating the cosine similarity with each of our labels, and assigning the more similar label.\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\nsim_matrix = cosine_similarity(test_embeddings, label_embeddings)\ny_pred = np.argmax(sim_matrix, axis=1)\n\nAgain, the model evaluation is the same:\n\nperformance = classification_report(data[\"test\"][\"label\"], y_pred, target_names=[\"Negative Review\", \"Positive Review\"])\n\nprint(performance)\n\n                 precision    recall  f1-score   support\n\nNegative Review       0.78      0.77      0.78       533\nPositive Review       0.77      0.79      0.78       533\n\n       accuracy                           0.78      1066\n      macro avg       0.78      0.78      0.78      1066\n   weighted avg       0.78      0.78      0.78      1066\n\n\n\nThe accuracy is 78%. Pretty good considering we have no labelled data and did no training!"
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html",
    "href": "posts/ml/meta_learning/meta_learning.html",
    "title": "Meta Learning",
    "section": "",
    "text": "These are notes on the book Meta Learning by Radek Osmulski."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#learning-to-program",
    "href": "posts/ml/meta_learning/meta_learning.html#learning-to-program",
    "title": "Meta Learning",
    "section": "1. Learning to Program",
    "text": "1. Learning to Program\nYou must become a developer before you can be a deep learning expert. Start by tinkering with things you enjoy. Don’t exert yourself too much so that you can stay consistent above all else.\n\n1.1. CS Foundations\nStart with a programming MOOC like Harvard CS50 if coming into this fresh. Don’t get bogged down in tutorial hell, just get familiar enough with high-level concepts to be able to google the rest.\n“It doesn’t make sense to invest all your time into learnng calligraphy if you have nothing to write about”.\n\n\n1.2. IDE\nLearn to use an IDE effectively. If stuck, just start with VSCode.\n\n\n1.3. Version control\nLearn to use git and GitHub.\n\n\n1.4. DevOps\nLearn how to use your computer in the context of writing code: spin up a cloud VM, ssh into it, move data around, etc. A good resource is The Missing Semester.\n\n\n1.5. Start Learning Deep Learning the Right Way\nThe above 4 steps are to get to a stage where you can take the Practical Deep Learning for Coders which is the best starting foundation to get a high-level grounding in ML.\nUse the top-down approach to learning championed by fastai:\n\nWatch a lecture.\nLook through the associated notebook - run the code and understand the inputs and outputs of each cell.\nStart with a new notebook and try to work through the same steps from scratch as an open-book exercise. Also read the documentation as you go along to fill any gaps.\nFind a similar dataset (or make one) and try the same techniques you’ve just learned. Creating a dataset is a great way to think about feature engineering and choices of labels.\n\nLearn the whole game then play out of town."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#theory-vs-practice",
    "href": "posts/ml/meta_learning/meta_learning.html#theory-vs-practice",
    "title": "Meta Learning",
    "section": "2. Theory vs Practice",
    "text": "2. Theory vs Practice\nStarting from an “elements-first” approach can feel like a never ending struggle. You want to learn ML, but then need to study calculus, but then you need to study real analysis before that, but then you need to study set theory before that…\nThe problem with theory: theory follows practice!\nBecome a great practitioner first and the theory will make more sense afterwards, once you have some intuition. Practical problems give you an incredibly useful feedback loop for your learning that you don’t get from following a linear theoretical curriculum.\nFor best results, combine practice and theory, in that order."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#programming-is-about-what-you-have-to-say",
    "href": "posts/ml/meta_learning/meta_learning.html#programming-is-about-what-you-have-to-say",
    "title": "Meta Learning",
    "section": "3. Programming is About What You Have to Say",
    "text": "3. Programming is About What You Have to Say\nYour ability as a developer is measured by the utility of things you can do in your language of choice. What you have to say is the most important thing!\nThe key to getting started is to read and write a lot of code. Start with 100-200 line projects, anything under 1000 lines should be possible to follow. Then graduate to larger problems.\nDomain knowledge comes first. Once you know what you are trying to achieve and broadly how you can achieve it, you can worry about best practices to write clean, maintainable code later.\nThe fastest way to learn to program is to learn to say something useful."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#the-secret-of-good-developers",
    "href": "posts/ml/meta_learning/meta_learning.html#the-secret-of-good-developers",
    "title": "Meta Learning",
    "section": "4. The Secret of Good Developers",
    "text": "4. The Secret of Good Developers\nBecoming familiar with a codebase or problem requires holding multiple things in your head: the layout of the code, how it is tested, how you intend to change it, other places that might be affected by your change, etc. This requires uninterrupted focus. When you are interrupted, you drop those things you were holding in your head, and you might not always pick them up again when you switch back.\nThe price of context switching is extremely high!\nLong, uninterrupted sessions - “deep work” - are the key to progress."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#the-best-way-to-improve-as-a-developer",
    "href": "posts/ml/meta_learning/meta_learning.html#the-best-way-to-improve-as-a-developer",
    "title": "Meta Learning",
    "section": "5. The Best Way to Improve as a Developer",
    "text": "5. The Best Way to Improve as a Developer\nYou sharpen your skills with practice. To get better at a thing, do the thing!\nThe key is to read and write code. Everything else, like MOOCs, books, articles etc are nice as a side dish but they are not the main course."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#achieving-flow",
    "href": "posts/ml/meta_learning/meta_learning.html#achieving-flow",
    "title": "Meta Learning",
    "section": "6. Achieving Flow",
    "text": "6. Achieving Flow\nFlow is difficult to achieve, but we can help ourselves by removing any obvious obstacles.\nLearn your IDE inside out, and know all of the keyboard shortcuts so that you don’t interrupt your flow by switching to your mouse or searching through settings.\nLikewise, address easy things like making sure you’ve charged your laptop, keyboard, mouse etc and you don’t spend time battling connectivity issues or just plugging things in.\nFlow is a spectrum. Don’t think of it as “in flow” or “not in flow”. Rather, “how much are you flowing?”. Take small actions to increase it.\nJust work on what you want to work on. Don’t overthink it and talk yourself out of doing something because you’re not a “real” developer/author/startup founder etc."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#genuine-work-compounds",
    "href": "posts/ml/meta_learning/meta_learning.html#genuine-work-compounds",
    "title": "Meta Learning",
    "section": "7. Genuine Work Compounds",
    "text": "7. Genuine Work Compounds\n\n“Reading a book without taking notes is like discovering a new territory without making a map.”\n\nDoing &gt; thinking\nThinking about something without writing notes or doing more work on it is like running on a treadmill when your goal is to get from A to B. Just do a little bit: write some notes one day, then outline the project then next, then start the first component, etc. Before long, you will have made more progress than you expected.\nThe more “atoms you move” the more feeback you can get, so the more you can reflect and learn."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#how-to-structure-an-ml-project",
    "href": "posts/ml/meta_learning/meta_learning.html#how-to-structure-an-ml-project",
    "title": "Meta Learning",
    "section": "8. How to Structure an ML Project",
    "text": "8. How to Structure an ML Project\n\n“The only way to maintain your sanity in the long run is to be paranoid in the short run.”\n\nThe key is a good train-validation-test set split.\nThen construct a baseline. The smaller and simpler, the better.\nThis helps us know we are moving in the right direction when we iterate. It also gives an idea of what shape our reeal results should be.\nStart broad. Explore different models and architectures first, rather than diving straight in to tuning hyperparameters.\nMake changes incrementally, then run the model and check that your output is the correct shape and not all zeros. You can’t write a complex model all in one sweep! This requires running the entire pipelines regularly. This could be a big time sink to run on the whole dataset, so just train on 5% or less to keep iterations fast.\nTime is a key component of success. You might get a decent solution quite quickly. But going from good to great is a creative endeavour, which requires time to think and mull over in your subconscious."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#how-to-win-at-kaggle",
    "href": "posts/ml/meta_learning/meta_learning.html#how-to-win-at-kaggle",
    "title": "Meta Learning",
    "section": "9. How to Win at Kaggle",
    "text": "9. How to Win at Kaggle\n\nJoin a competition early\n\nDownload the dataset, understand the schema of inputs and outputs\nStart writing code immediately. A good starting point is to just download the data and make a submission (of random numbers or all zeros) to make sure you have all of the mechanics in place before working on your model.\nMore time for more iteration loops\nMore time to mull the problem and think creatively\n\nRead Kaggle forums (daily)\nMake small improvements (daily)\n\nSmall tweaks compound into big results\nInitial exploration should cover as much ground as possible, so try multiple architectures rather than focusing on tuning one specific model in the early stages.\n\nFind an appropriate validation split that mirrors the leaderboard\n\nIs random sampling appropriate or does the split require more thought?\nTrain two models and submit them. The leaderboard results should reflect what you saw locally (i.e. which was the better model). If not, you might have problems with your validation split.\n\nPosts by top Kagglers will take you 80% of the way\nPapers, blogs and creativity will take you the remaining 20%\n\nWhen reading papers, you don’t need to understand every paragraph, and trying to do so would be overly time-consuming and counter-productive. Scan the paper to understand the general idea and whether it is relevant to your problem.\nBlog posts are often more accessible and quicker to implement.\n\nEnsemble your results\n\nCross-validation is a related concept and also essential."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#hardware-for-deep-learning",
    "href": "posts/ml/meta_learning/meta_learning.html#hardware-for-deep-learning",
    "title": "Meta Learning",
    "section": "10. Hardware for Deep Learning",
    "text": "10. Hardware for Deep Learning\nExplore hardware only to the extent that you find it interesting. Otherwise it’s a false economy: you are paying with your time learning about concepts that might save a few $ here and there.\nStart with a cloud VM, colab or Kaggle kernels.\nOnce you know you are serious, a home rig is the most cost effective option. Get a GPU with the most RAM you can afford. This first rig should just get you to a stage where you know what you like to work on and what the bottlenecks worth improving are: more RAM, better CPU, more storage (and how fast does the storage need to be).\nDebugging and profiling your code (use %%timeit in Jupyter) is essential."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#sharing-your-work",
    "href": "posts/ml/meta_learning/meta_learning.html#sharing-your-work",
    "title": "Meta Learning",
    "section": "11. Sharing Your Work",
    "text": "11. Sharing Your Work\nA resume is of limited use nowadays. Meet people who influence hiring decisions where they hang out: conferences, social media, meetups, blogs etc.\nCredibility is key. Helping others and showing your work builds credibility.\nIf you are looking for work, say so! On Twitter, Linkedin etc, and reach out to people you know.\nThe deep learning community is active on Twitter. But keep your time on Twitter limited and focused. Your goal isn’t to become a content creator.\nShare your work. This builds a personal brand. It also gives you a milestone to work towards and defines when it is “done”. The sooner you start sharing your work, the better. There are fewer, if any, of the negative consequences you might anticipate when sharing work online. Failure is just “cheap feedback”; embrace it!\nMost people’s biggest regret when learning ML is not enough time spent doing and too much time spent learning theory.\nThere’s one guaranteed way to fail: stop trying. Learning compounds; you need to give it time before you see exponential results.\nA general technique that works for all aspects of life: oberseve whether you are getting the desired results. If not, change your approach.\nFind a mentor. This might not necessarily be someone you know or interact with directly, or even someone who’s alive. If you follow them (on Twitter) and learn from what they have to say, they are a mentor.\nThe “permissionless apprenticeship” approach to finding a mentor means to give value first before you receive value."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#references",
    "href": "posts/ml/meta_learning/meta_learning.html#references",
    "title": "Meta Learning",
    "section": "References",
    "text": "References\n\nThe Missing Semester\nValidation sets\nPersonal brand"
  },
  {
    "objectID": "react-series.html",
    "href": "react-series.html",
    "title": "Series: React",
    "section": "",
    "text": "React: TypeScript Essentials\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 7: I don’t got no type, bad code is the only thing that I like\n\n\n\n\n\nMar 21, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: Testing\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 6: Testing my patience\n\n\n\n\n\nMar 20, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: Deployment\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 5: Deploying React Apps\n\n\n\n\n\nMar 18, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: Debugging\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 4: A Bug’s Life\n\n\n\n\n\nMar 17, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: Styling\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 3: Styling it Out\n\n\n\n\n\nMar 16, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: JavaScript Essentials\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 2: WTF is JSX\n\n\n\n\n\nMar 14, 2024\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: A Gentle Introduction\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 1: Getting Started with React\n\n\n\n\n\nMar 12, 2024\n\n\n12 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "This site contains my personal software and AI projects.\nFor projects that I’ve worked on, see my projects section. For notes on various topics that I’ve made, see my notes section.\nIf you’re interested in any of this stuff, my socials are at the bottom of this page - get in touch!"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "About",
    "section": "Bio",
    "text": "Bio\nI’m a data scientist with experience at big hedge funds and plucky start-ups. I spent a few years at Man Group, both as a discretionary long-short equity analyst in GLG and on the systematic trading side as a quant in AHL, where I focused on equities and futures.\nI currently work at a fintech startup called BMLL Technologies, where I lead projects doing interesting things with massive amounts of financial order book data."
  },
  {
    "objectID": "fastai-series.html",
    "href": "fastai-series.html",
    "title": "Series: FastAI course",
    "section": "",
    "text": "FastAI Lesson 8: Convolutions\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 8\n\n\n\n\n\nMar 5, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 7: Collaborative Filtering\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 7\n\n\n\n\n\nMar 1, 2024\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 6.5: Road to the Top\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 6.5\n\n\n\n\n\nFeb 13, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 6: Random Forests\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 6\n\n\n\n\n\nOct 9, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 5: Natural Language Processing\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 5\n\n\n\n\n\nSep 23, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 4: Natural Language Processing\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 4\n\n\n\n\n\nSep 7, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 3: How Does a Neural Network Learn?\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 3\n\n\n\n\n\nSep 1, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 2: Deployment\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 2\n\n\n\n\n\nAug 30, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 1: Image Classification Models\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 1\n\n\n\n\n\nAug 23, 2023\n\n\n3 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "gen-deep-learning-series.html",
    "href": "gen-deep-learning-series.html",
    "title": "Series: Generative AI",
    "section": "",
    "text": "Generative AI: GANs\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\nGAN\n\n\n\nPart 4: Generative Adversarial Networks\n\n\n\n\n\nApr 10, 2024\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI: VAEs\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\nVAE\n\n\n\nPart 3: Variational Autoencoders\n\n\n\n\n\nMar 6, 2024\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI: Deep Learning Foundations\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\n\nPart 2: The Building Blocks for Generative AI\n\n\n\n\n\nFeb 26, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI: Intro\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\n\nPart 1: Introduction to Generative AI\n\n\n\n\n\nFeb 19, 2024\n\n\n5 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  }
]