[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Notes",
    "section": "",
    "text": "Series\n\nfastai-series\nThis series contains notes from following the fastai “Practical Deep Learning for Coders” course. See the course page and book.\n\n\ngenerative-ai-series\nThis series contains notes from the book “Generative Deep Learning” by David Foster.\n\n\n\nAll Posts\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nGenerative AI: Chapter 2\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\n\nIntro to deep learning\n\n\n\n\n\nFeb 26, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI: Chapter 1\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\n\nIntro to generative AI\n\n\n\n\n\nFeb 19, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 7: Collaborative Filtering\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 7\n\n\n\n\n\nFeb 14, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 6.5: Road to the Top\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 6.5\n\n\n\n\n\nFeb 13, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 6: Random Forests\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 6\n\n\n\n\n\nOct 9, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 5: Natural Language Processing\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 5\n\n\n\n\n\nSep 23, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 4: Natural Language Processing\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 4\n\n\n\n\n\nSep 7, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 3: How Does a Neural Network Learn?\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 3\n\n\n\n\n\nSep 1, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 2: Deployment\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 2\n\n\n\n\n\nAug 30, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 1: Image Classification Models\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 1\n\n\n\n\n\nAug 23, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nSystem Design Notes\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\n\nNotes on System Design\n\n\n\n\n\nAug 14, 2023\n\n\n24 min\n\n\n\n\n\n\n\n\n\n\n\n\nPitching Notes\n\n\n\n\n\n\nBusiness\n\n\n\nNotes on Pitching\n\n\n\n\n\nAug 4, 2023\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nKalman Filter Notes\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\n\nNotes on Kalman filters\n\n\n\n\n\nJul 23, 2023\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nPublic Speaking Notes\n\n\n\n\n\n\nBusiness\n\n\n\nNotes on Public Speaking\n\n\n\n\n\nJul 20, 2023\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nMarketing Notes\n\n\n\n\n\n\nBusiness\n\n\n\nNotes on Marketing\n\n\n\n\n\nJul 18, 2023\n\n\n18 min\n\n\n\n\n\n\n\n\n\n\n\n\nSoftware Architecture Notes\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\n\nNotes on Software Architecture\n\n\n\n\n\nJun 23, 2023\n\n\n22 min\n\n\n\n\n\n\n\n\n\n\n\n\nTensorflow Notes\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\n\nNotes on Tensorflow\n\n\n\n\n\nFeb 23, 2023\n\n\n21 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "fastai-series.html",
    "href": "fastai-series.html",
    "title": "Series: FastAI course",
    "section": "",
    "text": "FastAI Lesson 7: Collaborative Filtering\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 7\n\n\n\n\n\nFeb 14, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 6.5: Road to the Top\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 6.5\n\n\n\n\n\nFeb 13, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 6: Random Forests\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 6\n\n\n\n\n\nOct 9, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 5: Natural Language Processing\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 5\n\n\n\n\n\nSep 23, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 4: Natural Language Processing\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 4\n\n\n\n\n\nSep 7, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 3: How Does a Neural Network Learn?\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 3\n\n\n\n\n\nSep 1, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 2: Deployment\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 2\n\n\n\n\n\nAug 30, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 1: Image Classification Models\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 1\n\n\n\n\n\nAug 23, 2023\n\n\n3 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "This site contains my personal software and AI projects.\nFor projects that I’ve worked on, see my projects section. For notes on various topics that I’ve made, see my notes section.\nIf you’re interested in any of this stuff, my socials are at the bottom of this page - get in touch!"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "About",
    "section": "Bio",
    "text": "Bio\nI’m a data scientist with experience at big hedge funds and plucky start-ups. I spent a few years at Man Group, both as a discretionary long-short equity analyst in GLG and on the systematic trading side as a quant in AHL, where I focused on equities and futures.\nI currently work at a fintech startup called BMLL Technologies, where I lead projects doing interesting things with massive amounts of financial order book data."
  },
  {
    "objectID": "posts/ml/tensorflow/tensorflow.html",
    "href": "posts/ml/tensorflow/tensorflow.html",
    "title": "Tensorflow Notes",
    "section": "",
    "text": "These are notes taken primarily from the Complete Tensorflow and Keras Udemy course\n\n\n\nData sourcing - train_test_split\nExploratory data analysis - plot distributions, correlations\nData cleaning - drop redundant columns, handle missing data (drop or impute)\nFeature engineering - one hot encoding categories, scaling/normalising numerical values, combining columns, extracting info from columns (e.g. zip code from address)\nModel selection and training - model and hyperparameters\nModel tuning and evaluation metrics - classification: classification_report, confusion matrix, accuracy, recall, F1. Regression: error (RMSE, MAE, etc)\nPredictions\n\n\n\n\n\nSupervised learning\n\nLinear regression\nSVM\nNaive Bayes\nKNN\nRandom forests\n\nUnsupervised learning\n\nDimensionality reduction\nClustering\n\nReinforcement Learning\n\nSupervised learning tasks can be broadly broken into:\n\nRegression\nClassification\n\nSingle class\nMulti class\n\n\n\n\n\n\n\nGeneral idea of a neural network that can then be extended to specific cases, e.g. CNNs and RNNs.\n\nPerceptron: a weighted average of inputs passed through some activation gate function to arrive at an output decision\nNetwork of perceptrons\nActivation function: a non-linear transfer function f(wx+b)\nCost function and gradient descent: convex optimisation of a loss function using sub-gradient descent. The optimizer can be set in the compile method of the model.\nBackpropagation: use chain rule to determine partial gradients of each weight and bias. This means we only need a single forward pass followed by a single backward pass. Contrast this to if we perturbed each weight or bias to determine each partial gradient: in that case, for each epoch we would need to run a forward pass per weight/bias in the network, which is potentially millions!\nDropout: a technique to avoid overfitting by randomly dropping neurons in each epoch.\n\nGeneral structure:\n\nInput layer\nHidden layer(s)\nOutput layer\n\nInput and output layers are determined by the problem:\n\nInput size: number of features in the data\nOutput size number of targets to predict, i.e. one for single class classification or single target regression, or multiple for multiclass (one per class)\nOutput layer activation determined by problem. For single class classification activation='sigmoid', for multiclass classification activation='softmax'\nLoss function determined by problem. For single class classification loss='binary_crossentropy', for multiclass classification loss='categorical_crossentropy'\n\nHidden layers are less well-defined. Some heuristics here.\nVanishing gradients can be an issue for lower layers in particular, where the partial gradients of individual layers can be very small, so when multiplied together in chain rule the gradient is vanishingly small. Exploding gradients are a similar issue but where gradients get increasingly large when multiplied together. Some techniques to rectify vanishing/exploding gradients:\n\nUse different activation functions with larger gradients close to 0 and 1, e.g. leaky ReLU or ELU (exponential linear unit)\nBatch normalisation: scale each gradient by the mean and standard deviation of the batch\nDifferent weight initialisation methods, e.g. Xavier initialisation\n\nExample model outline\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\n\nmodel = Sequential()\n\n# Input layer\nmodel.add(Dense(78, activation='relu'))  # Number of input features\nmodel.add(Dropout(0.2))  # Optional dropout layer after each layer. Omitted for next layers for clarity\n\n# Hidden layers\nmodel.add(Dense(39, activation='relu'))\nmodel.add(Dense(19, activation='relu'))\n\n# Output layer\nmodel.add(Dense(1, activation='sigmoid'))  # Number of target classes (single class in this case)\n\n# Problem setup\nmodel.compile(loss='binary_crossentropy', optimizer='adam')  # Type of problem (single class classification in this case)\n\n# Training\nmodel.fit(\n    x=X_train,\n    y=y_train,\n    batch_size=256,\n    epochs=30,\n    validation_data=(X_test, y_test)\n)\n\n# Prediction\nmodel.predict(new_input)  # The new input needs to be shaped and scaled the sane as the training data\n\n\n\nThese are used for image classification problems where convolutional filters are useful for extracting features from input arrays.\n\nImage kernels/filters\n\nGrayscale 2D arrays\nRGB 3D tensors\n\nConvolutional layers\nPooling layers\n\nGeneral structure:\n\nInput layer\nConvolutional layer\nPooling layer\n(Optionally more pairs of convolutional and pooling layers)\nFlattening layer\nDense hidden layer(s)\nOutput layer\n\nExample model outline\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nmodel = Sequential()\n\n# Convolutional layer followed by pooling. Deeper networks may have multiple pairs of these.\nmodel.add(Conv2D(filters=32, kernel_size=(4,4),input_shape=(28, 28, 1), activation='relu',))  # Input shape determined by input data size\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\n# Flatten _images from 2D to 1D before final layer\nmodel.add(Flatten())\n\n# Dense hidden layer\nmodel.add(Dense(128, activation='relu'))\n\n# Output layer\nmodel.add(Dense(10, activation='softmax'))  # Size and activation determined by problem; multiclass classification in this case\n\n# Problem setup\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')  # Loss determined by problem, multiclass classification in this case\n\n# Training (with optional early stopping)\nearly_stop = EarlyStopping(monitor='val_loss',patience=2)\nmodel.fit(x_train,y_cat_train,epochs=10,validation_data=(x_test,y_cat_test),callbacks=[early_stop])\n\n# Prediction\nmodel.predict(new_input)  # The new input needs to be shaped and scaled the sane as the training data\n\n\n\nThese are used for modelling sequences with variable lengths of inputs and outputs.\nRecurrent neurons take as their input the current data AND the previous epoch’s output. We could try to pass EVERY previous epoch’s output as an input, but would run into issues of vanushing gradients.\nLSTMs take this idea a step further by incorporating the previous epochs input AND some longer lookback of epoch where some old outputs are “forgotten”.\nA basic RNN:\n\n            --------------------------------\n            |                              |\n H_t-1 ---&gt; |                              |---&gt; H_t\n            |                              |\n X_t   ---&gt; |                              |\n            |                              |\n            --------------------------------\n\nwhere:\nH_t is the neuron's output at current epoch t\nH_t-1 is the neuron's output from the previous epoch t-1\nX_t is the input data at current epoch t\n\nH_t = tanh(W[H_t-1, X_t] + b)\nAn LSTM (Long Short Term Memory) unit:\n\n            --------------------------------\n            |                              |\n H_t-1 ---&gt; |                              |---&gt; H_t\n            |                              |\n C_t-1 ---&gt; |                              |---&gt; C_t\n            |                              |\n X_t   ---&gt; |                              |\n            |                              |\n            --------------------------------\n            \nThe `forget gate` determines which part of the old short-term memory and current input is \"forgotten\" by the new long-term memory.\nThese are a set of weights (between 0 and 1) that get applied to the old long-term memory to downweight it.\nF_t = sigmoid(W_F[H_t-1, X_t] + b_F)\n\nThe `input gate` i_t similarly gates the input and old short-term memory.\nThis will later (in the update gate) get combined  with a candidate value for the new long-term memory `C_candidate_t`\nI_t = sigmoid(W_I[H_t-1, X_t] + b_I)\nC_candidate_t = tanh(W_C_cand[H_t-1, X_t] + b_C_cand) \n\nThe `update gate` for the new long-term memory `C_t` is then calculated as a sum of forgotten old memory and input-weighted candidate memory:\nC_t = F_t*C_t-1 + I_t*C_candidate_t \n\nThe `output gate` O_t is a combination of the old short-term memory and latest input data.\nThis is then combined with the latest long-term memory to produce the output of the recurrent neuron, which is also the updated short-term memory H_t:\nO_t = sigmoid(W_O[H_t-1, X_t] + b_O)\nH_t = O_t * tanh(C_t) \n\nwhere:\nH_t is short-term memory at epoch t\nC_t is long-term memory at epoch t\nX_t is the input data at current epoch t\n\nsigmoid is a sigmoid  activation function\nF_t is an intermediate forget gate weight\nI_t is an intermediate input gate weight\nO_t is an intermediate output gate weight\nThere are several variants of RNNs. RNNs with peepholes\nThis leaks long-term memory into the forget, input and output gates. Note that the forget gate and input gate each get the OLD long-term memory, whereas the output gate gets the NEW long-term memory.\nF_t = sigmoid(W_F[C_t-1, H_t-1, X_t] + b_F)\nI_t = sigmoid(W_I[C_t-1, H_t-1, X_t] + b_I)\nO_t = sigmoid(W_O[C_t, H_t-1, X_t] + b_O)\nGated Recurrent Unit (GRU)\nThis combines the forget and input gates into a single gate. It also has some other changes. This is simpler than a typical LSTM model as it has fewer parameters. This makes it more computationally efficient, and in practice they can have similar performance.\nz_t = sigmoid(W_z[H_t-1, X_t])\nr_t = sigmoid(W_r[H_t-1, X_t])\nH_candidate_t = tanh(W_H_candidate[r_t*h_t-1, x_t])\nH_t = (1 - z_t) * H_t-1 + z_t * H_candidate_t\nThe sequences modelled with RNNs can be:\n\nOne-to-many\nMany-to-many\nMany-to-one\n\nConsiderations for choosing the input sequence length:\n\nIt should be long enough to capture any patterns or seasonality in the data\nThe validation set and test set each need to have a size at least (input_length + output_length), so the longer the input length, the more data is required to be set aside.\n\nThe validation loss for RNNs/LSTMs can be volatile, so allow a higher patience when using early stopping.\nExample model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,LSTM\n\n# Generators for helper functions to chunk up the input series into an array of pairs of (input_batch, target_output)\ntrain_generator = TimeseriesGenerator(scaled_train, scaled_train, length=batch_length, batch_size=1)\nvalidation_generator = TimeseriesGenerator(scaled_test, scaled_test, length=batch_length, batch_size=1)\n\n# Create an LSTM model\nmodel = Sequential()\nmodel.add(LSTM(100, activation='relu', input_shape=(batch_length, n_features)))\nmodel.add(Dense(1))  # Output layer\nmodel.compile(optimizer='adam', loss='mse')  # Problem setup for regression\n\n# Fit the model\nearly_stop = EarlyStopping(monitor='val_loss', patience=10)  # RNNs can be volatile so if use a higher patience with an early stopping callback\nmodel.fit(train_generator, epochs=20, validation_data=validation_generator, callbacks=[early_stop])\n\n# Evaluate test data\ntest_predictions = []\nfinal_batch = scaled_train[-batch_length:, :]\ncurrent_batch = final_batch.reshape((1, batch_length, n_features))\n\nfor test_val in scaled_test:\n    pred_val = model.predict(current_batch)  # These will later need to use the scaler.inverse_transform of the scaler originally used on the training data\n    test_predictions.append(pred_val[0])\n    current_batch = np.append(current_batch[:,1:,:], pred_val.reshape(1,1,1), axis=1)\n\n\n\nCharacter-based generative NLP: given an input string, e.g. [‘h’, ‘e’, ‘l’, ‘l’], predict the sequence shifted by one character, e.g. [‘e’, ‘l’, ‘l’, ‘o’]\nThe embedding, GRU and dense layers work on the sequence in the following way: \nSteps:\n\nRead in text data\nText processing and vectorisation - one-hot encoding\nCreate batches\nCreate the model\nTrain the model\nGenerate new text\n\nStep 1: Read in text data\n\nA corpus of &gt;1 million characters is a good size\nThe more distinct the style of your corpus, the more obvious it will be if your model has worked.\n\nGiven some structured input_text string, we can get the one-hot encoded vocab list of unique characters.\nvocab_list = sorted(set(input_text))\nStep 2: Text processing\nVectorise the text - create a mapping between character and integer. This is the encoding dictionary used to vectorise the corpus.\n# Mappings back and forth between characters and their encoded integers\nchar_to_ind = {char: ind for ind, char in enumerate(vocab_list)}\nind_to_char = {ind: char for ind, char in enumerate(vocab_list)}\n\nencoded_text = np.array([char_to_ind[char] for char in input_text])\nStep 3: Create batches\nThe sequence length should be long enough to capture structure, e.g. for poetry with rhyming couplets it should be the number of characters in 3 lines to capture the rhyme and non-rhyme. Too long a sequence makes the model take longer to train and captures too much historical noise.\nCreate a shuffled dataset where:\n\ninput sequence is the first n characters\noutput sequence is n characters lagged by one\n\nWe want to shuffle these sequence pairs into a random order so the model doesn’t overfit to any section of the text, but can instead generate characters given any seed text.\nseq_len = 120\ntotal_num_seq = len(input_text) // (seq_len + 1)\n\n# Create Training Sequences\nchar_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\nsequences = char_dataset.batch(seq_len + 1, drop_remainder=True)  # drop_remainder drops the last incomplete sequence\n\n# Create tuples of input and output sequences\ndef create_seq_targets(seq):\n    input_seq = seq[:-1]\n    output_seq = seq[-1:]\n    return input_seq, output_seq\ndataset = sequences.map(create_seq_targets)\n\n# Generate training batches\nbatch_size = 128\nbuffer_size = 10000  # Shuffle this amount of batches rather than shuffling the entire dataset in memory\ndataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\nStep 4: Create the model\nSet up the loss function and layers.\nThis model uses 3 layers:\n\nEmbedding\n\nEmbed positive integers, e.g. “a”&lt;-&gt;1, as dense vectors of fixed size. This embedding size is a hyperparameter for the user to decide.\nNumber of layers should be smaller but a similar scale to the vocab size; typically use the power of 2 that is just smaller than the vocab size.\n\nGRU\n\nThis is the main thing to vary in the model: number of hidden layers and number of neurons in each, types of neurons (RNN, LSTM, GRU), etc. Typically use lots of layers here.\n\nDense\n\nOne neuron per character (one-hot encoded), so this produces a probability per character. The user can vary the “temperature”, choosing less probable characters more/less often.\n\n\nLoss function:\n\nSparse categorical cross-entropy\nUse sparse categorical cross-entropy because the classes are mutually exclusive. Use regular categorical cross-entropy when one smaple can have multiple classes, or the labels are soft probabilities.\nSee link in NLP appendix for discussion.\nUse logits=True because vocab input is one hot encoded.\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, GRU\nfrom tensorflow.keras.losses import sparse_categorical_crossentropy\n\n# Hyperparameters\nvocab_size = len(vocab_list)\nembed_dim = 64\nrnn_neurons = 1026\n\n# Create model \nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embed_dim, batch_input_shape=[batch_size, None]))\nmodel.add(GRU(rnn_neurons, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'))\nmodel.add(Dense(vocab_size))\nsparse_cat_loss = lambda y_true,y_pred: sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)  # We want a custom loss function with logits=True\nmodel.compile(optimizer='adam', loss=sparse_cat_loss) \nStep 5: Train the model\nepochs = 30\nmodel.fit(dataset,epochs=epochs)\nStep 6: Generate text\nAllow the model to accept a batch size of 1 to allow us to give an input seed text from which to generate text from.\nWe format the seed text so it can be fed into the network, then loop through generating one character at a time and feeding that back into the model.\nmodel.build(tf.TensorShape([1, None]))\n\ndef generate_text(model, input_seed_text, gen_size=100, temperature=1.0):\n    \"\"\"\n    model: tensorflow.model\n    input_seed_text: str\n    gen_size: int\n    temperature: float\n    \"\"\"\n    generated_text_result = []\n    vectorised_seed = tf.expand_dims([char_to_ind[s] for s in input_seed_text], 0)\n    model.reset_states()\n    \n    for k in range(gen_size):\n        raw_predictions = model(vectorised_seed)\n        predictions = tf.squeeze(raw_predictions, 0) / temperature\n        predicted_index = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n        generated_text_result.append(ind_to_char[predicted_index])\n        \n        # Set a new vectorised seed to generate the next character in the loop\n        vectorised_seed = tf.expand_dims([predicted_index], 0)\n        \n    return (input_seed_text + ''.join(generated_text_result))\n\n\n\nThis is an unsupervised learning problem, therefore evaluation metrics are different. Sometimes called semi-supervised as labels may be used in training the model, but not available when it’s used to predict new values.\nApplications:\n\nDimensionality reduction\nNoise removal\n\nDesigned to reproduce its input in the output layer. Number of input neurons is the same as the number of output layers. The encoder reduces dimensionality to the hidden layer. The hidden layer should contain the most relevant information required to reconstruct the input. The decoder reconstructs a high dimensional output from a low dimensional input.\n\nEncoder: Input layer -&gt; Hidden layer\nDecoder: Hidden layer -&gt; Output\n\nStacked autoencoders include multiple hidden layers.\nAutoencoder for dimensionality reduction\nLet’s try to reduce an input dataset from 3 dimensions to 2. We want to train an autoencoder to go from 3 -&gt; 2 -&gt; 3.\nWhen we scale data, we fit and transform on the entire dataset rather than a train/test split, because there is no ground truth for the “correct” dimensionality reduction.\nUsing SGD in Autoencoders allows the learning rate to be varied as a hyperparameter to affect how well the hidden layer learns. Larger values will train quicker but potentially perform worse.\nTo retrieve the lower dimensionality representation, use just the encoder half of the trained autoencoder to predict the input.\n# Scale data\nscaler = MinMaxScaler()\nscaled_data = scaler.fit_transform(input_features)\n\n# Create model\nencoder = Sequential()\nencoder.add(Dense(units=2, acivation='relu', input_shape=[3]))\n\ndecoder = Sequential()\ndecoder.add(Dense(units=3, acivation='relu', input_shape=[2]))\n\nautoencoder = Sequential([encoder, decoder])\nautoencoder.compile(loss='mse', optimizer=SGD(lr=1.5))\n\n# Train the full autoencoder model\nautoencoder.fit(scaled_data, scaled_data, epochs=5)\n\n# Reduce dimensionality by using just the encoder part of the trained model\nencoded_2dim = encoder.predict(scaled_data)\nAutoencoder for noise removal\nStarting with clean images, we want to train an autoencoder to learn to remove noise. We create a noisy dataset by adding noise to our clean images. We then try to reconstruct that input, and compare to the original clean images as the ground truth.\nStarting with 28x28 images (e.g. MNIST dataset), this gives 784 input features when flattened. Use multiple hidden layers as there are a large number of input features that would be difficult to learn in one layer. The numbers of layers and number of neurons in each are hyperparameters to tune; decreasing in powers of 2 is a good rule of thumb.\nAdd a Gaussian noise layer to train the model to remove noise. Without the Gaussian layer, the model would be used for dimensionality reduction rather than noise removal.\nThe final layer uses a sigmoid activation because it is a binary classification problem - does the output match the input?\nencoder = Sequential()\nencoder.add(Flatten(input_shape=[28, 28]))\nencoder.add(Gaussian(0.2))  # Optional layer if training for noise removal rather than dimensionality reduction\nencoder.add(Dense(400, activation='relu'))\nencoder.add(Dense(200, activation='relu'))\nencoder.add(Dense(100, activation='relu'))\nencoder.add(Dense(50, activation='relu'))\nencoder.add(Dense(25, activation='relu'))\n\ndecoder = Sequential()\ndecoder.add(Dense(50, input_shape=[25], activation='relu'))\ndecoder.add(Dense(100, activation='relu'))\ndecoder.add(Dense(200, activation='relu'))\ndecoder.add(Dense(400, activation='relu'))\ndecoder.add(Dense(28*28, activation='sigmoid'))\n\nautoencoder = Sequential([encoder, decoder])\nautoencoder.compile(loss='binary_crossentropy', optimizer=SGD(lr=1.5), metrics=['accuracy'])\nautoencoder.fit(X_train, X_train, epochs=5)\n\n\n\nUse 2 networks competing against each other to generate data - counterfeiter (generator) vs detective (discriminator).\n\nGenerator: recieves random noise\nDiscriminator: receives data set containing real data and fake generated data, and attempts to calssify real vs fake (binary classificaiton).\n\nTwo training phases, each only training one of the networks:\n\nTrain discriminator - real images (labeled 1) and generated images (labeled 0) are fed to the discriminator network.\nTrain generator - only feed generated images to discriminator, ALL labeled 1.\n\nThe generator never sees real images, it creates images based only off of the gradients flowing back from the discriminator.\nDifficulties with GANs:\n\nTraining resources - GPUs generally required\nMode collapse - generator learns an image that fools the discriminator, then only produces this image. Deep convolutional GANs and mini-batch discrimination are two solution to this problem.\nInstability - True performance is hard to measure; just because the discriminator was fooled, it doesn’t mean that the generated image was actually realistic.\n\nCreating the model\nWe create the generator and discriminator as separate models, then join them into a GAN object. This is analogous to how we joined an encoder and decoder into an autoencoder.\nThe discriminator is trained on a binary classification problem, real or fake, so uses a binary cross entropy loss function. Backpropagation only alters the weights of the discriminator in the first phase, not the generator.\ndiscriminator = Sequential()\n\n# Flatten the image\ndiscriminator.add(Flatten(input_shape=[28,28]))\n\n# Some hidden layers\ndiscriminator.add(Dense(150,activation='relu'))\ndiscriminator.add(Dense(100,activation='relu'))\n\n# Binary classification - real vs fake\ndiscriminator.add(Dense(1,activation=\"sigmoid\"))\ndiscriminator.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\nThe generator is trained in the second phase.\nWe set a codings_size, which is the size of the latent representation from which the generator will create a full image. For example, if we want to create a 28x28 image (784 pixels), we may want to generate this from a 100 element input. This is analogous to the decoder part of an autoencoder.\nThe codings_size should be smaller than the total number of features but large enough so that there is something to learn from.\nThe generator is NOT compiled at this step, it is compiled in the combined GAN later so that it is only trained at that step.\ncodings_size = 100\ngenerator = Sequential()\n\n# Input shape of first layer is the codings_size\ngenerator.add(Dense(150, activation=\"relu\", input_shape=[codings_size]))\n\n# Some hidden layers\ngenerator.add(Dense(200,activation='relu'))\n\n# Output is the size of the image we want to create\ngenerator.add(Dense(784, activation=\"sigmoid\"))\ngenerator.add(Reshape([28,28]))\nThe GAN combines the generator and discriminator.\nThe discriminator is only trained in the first phase, not the second phase.\nGAN = Sequential([generator, discriminator])\ndiscriminator.trainable = False\nGAN.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\nTraining the model\nWe first create batches of images from our input data, similarly to previous models.\nbatch_size = 32  # Size of training input batches\nbuffer_size = 1000  # Don't load the entire dataset into memory\nepochs = 1\n\nraw_dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(buffer_size=buffer_size)\nbatched_dataset = raw_dataset.batch(batch_size, drop_remainder=True).prefetch(1)\nThe training loop for each epoch trains the discriminator to distinguish real vs fake images (binary classification). Then it trains the generator to generate fake images using ONLY the gradients learned in the discriminator training step; the generator NEVER sees any real images.\ngenerator, discriminator = GAN.layers\n\nfor epoch in range(epochs):\n    for index, X_batch in enumerate(batched_dataset):\n        # Phase 1: Train the discriminator\n        noise = tf.random.normal(shape=[batch_size, codings_size])\n        generated_images = generator(noise)\n        real_images = tf.dtypes.cast(X_batch,tf.float32)\n        X_train_discriminator = tf.concat([generated_images, real_images], axis=0)\n        y_train_discriminator = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)  # Targets set to zero for fake _images and 1 for real _images\n        discriminator.trainable = True\n        discriminator.train_on_batch(X_train_discriminator, y_train_discriminator)\n\n        # Phase 2: Train the generator\n        noise = tf.random.normal(shape=[batch_size, codings_size])\n        y_train_generator = tf.constant([[1.]] * batch_size)  # We want discriminator to believe that fake _images are real\n        discriminator.trainable = False\n        GAN.train_on_batch(noise, y_train_generator)    \nDeep convolutional GANs\nThese are GANs which use convolutional layers inside the discriminator and generator models.\nThe models are almost identical to the models above, just with additional Conv2D, Conv2DTranspose and BatchNormalization layers.\nChanges compared to regular GANs:\n\nAdditional convolutional layers in model.\nReshape the training set to match the images.\nRescale the training data to be between -1 and 1 so that tanh activation function works.\nActivation of discriminator output layer is tanh rather than sigmoid.\n\n\n\n\n\n\n\n\nComplete Tensorflow and Keras Udemy course\nIntuition behind neural networks\nThe deep learning bible (with lectures)\nHeuristics for choosing hidden layers\nAlternatives to the deprecated model predict for different classification problems\nMIT course\n\n\n\n\n\nImage kernels explained\nChoosing CNN layers\n\n\n\n\n\nOverview of RNNs\nWikipedia page contains the equations of LSTMs and peepholes\nLSTMs vs GRUs\nWorked example of LSTM\nRNN cheatsheet\n\n\n\n\n\nThe unreasonable effectiveness of RNNs - essay on RNNs applied to NLP\nSparse vs dense categorical crossentropy loss function\n\n\n\n\n\nbuffer_size argument in tensorflow prefetch and shuffle\nMode collapse"
  },
  {
    "objectID": "posts/ml/tensorflow/tensorflow.html#end-end-process",
    "href": "posts/ml/tensorflow/tensorflow.html#end-end-process",
    "title": "Tensorflow Notes",
    "section": "",
    "text": "Data sourcing - train_test_split\nExploratory data analysis - plot distributions, correlations\nData cleaning - drop redundant columns, handle missing data (drop or impute)\nFeature engineering - one hot encoding categories, scaling/normalising numerical values, combining columns, extracting info from columns (e.g. zip code from address)\nModel selection and training - model and hyperparameters\nModel tuning and evaluation metrics - classification: classification_report, confusion matrix, accuracy, recall, F1. Regression: error (RMSE, MAE, etc)\nPredictions"
  },
  {
    "objectID": "posts/ml/tensorflow/tensorflow.html#machine-learning-categories",
    "href": "posts/ml/tensorflow/tensorflow.html#machine-learning-categories",
    "title": "Tensorflow Notes",
    "section": "",
    "text": "Supervised learning\n\nLinear regression\nSVM\nNaive Bayes\nKNN\nRandom forests\n\nUnsupervised learning\n\nDimensionality reduction\nClustering\n\nReinforcement Learning\n\nSupervised learning tasks can be broadly broken into:\n\nRegression\nClassification\n\nSingle class\nMulti class"
  },
  {
    "objectID": "posts/ml/tensorflow/tensorflow.html#deep-learning-models",
    "href": "posts/ml/tensorflow/tensorflow.html#deep-learning-models",
    "title": "Tensorflow Notes",
    "section": "",
    "text": "General idea of a neural network that can then be extended to specific cases, e.g. CNNs and RNNs.\n\nPerceptron: a weighted average of inputs passed through some activation gate function to arrive at an output decision\nNetwork of perceptrons\nActivation function: a non-linear transfer function f(wx+b)\nCost function and gradient descent: convex optimisation of a loss function using sub-gradient descent. The optimizer can be set in the compile method of the model.\nBackpropagation: use chain rule to determine partial gradients of each weight and bias. This means we only need a single forward pass followed by a single backward pass. Contrast this to if we perturbed each weight or bias to determine each partial gradient: in that case, for each epoch we would need to run a forward pass per weight/bias in the network, which is potentially millions!\nDropout: a technique to avoid overfitting by randomly dropping neurons in each epoch.\n\nGeneral structure:\n\nInput layer\nHidden layer(s)\nOutput layer\n\nInput and output layers are determined by the problem:\n\nInput size: number of features in the data\nOutput size number of targets to predict, i.e. one for single class classification or single target regression, or multiple for multiclass (one per class)\nOutput layer activation determined by problem. For single class classification activation='sigmoid', for multiclass classification activation='softmax'\nLoss function determined by problem. For single class classification loss='binary_crossentropy', for multiclass classification loss='categorical_crossentropy'\n\nHidden layers are less well-defined. Some heuristics here.\nVanishing gradients can be an issue for lower layers in particular, where the partial gradients of individual layers can be very small, so when multiplied together in chain rule the gradient is vanishingly small. Exploding gradients are a similar issue but where gradients get increasingly large when multiplied together. Some techniques to rectify vanishing/exploding gradients:\n\nUse different activation functions with larger gradients close to 0 and 1, e.g. leaky ReLU or ELU (exponential linear unit)\nBatch normalisation: scale each gradient by the mean and standard deviation of the batch\nDifferent weight initialisation methods, e.g. Xavier initialisation\n\nExample model outline\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\n\nmodel = Sequential()\n\n# Input layer\nmodel.add(Dense(78, activation='relu'))  # Number of input features\nmodel.add(Dropout(0.2))  # Optional dropout layer after each layer. Omitted for next layers for clarity\n\n# Hidden layers\nmodel.add(Dense(39, activation='relu'))\nmodel.add(Dense(19, activation='relu'))\n\n# Output layer\nmodel.add(Dense(1, activation='sigmoid'))  # Number of target classes (single class in this case)\n\n# Problem setup\nmodel.compile(loss='binary_crossentropy', optimizer='adam')  # Type of problem (single class classification in this case)\n\n# Training\nmodel.fit(\n    x=X_train,\n    y=y_train,\n    batch_size=256,\n    epochs=30,\n    validation_data=(X_test, y_test)\n)\n\n# Prediction\nmodel.predict(new_input)  # The new input needs to be shaped and scaled the sane as the training data\n\n\n\nThese are used for image classification problems where convolutional filters are useful for extracting features from input arrays.\n\nImage kernels/filters\n\nGrayscale 2D arrays\nRGB 3D tensors\n\nConvolutional layers\nPooling layers\n\nGeneral structure:\n\nInput layer\nConvolutional layer\nPooling layer\n(Optionally more pairs of convolutional and pooling layers)\nFlattening layer\nDense hidden layer(s)\nOutput layer\n\nExample model outline\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nmodel = Sequential()\n\n# Convolutional layer followed by pooling. Deeper networks may have multiple pairs of these.\nmodel.add(Conv2D(filters=32, kernel_size=(4,4),input_shape=(28, 28, 1), activation='relu',))  # Input shape determined by input data size\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\n# Flatten _images from 2D to 1D before final layer\nmodel.add(Flatten())\n\n# Dense hidden layer\nmodel.add(Dense(128, activation='relu'))\n\n# Output layer\nmodel.add(Dense(10, activation='softmax'))  # Size and activation determined by problem; multiclass classification in this case\n\n# Problem setup\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')  # Loss determined by problem, multiclass classification in this case\n\n# Training (with optional early stopping)\nearly_stop = EarlyStopping(monitor='val_loss',patience=2)\nmodel.fit(x_train,y_cat_train,epochs=10,validation_data=(x_test,y_cat_test),callbacks=[early_stop])\n\n# Prediction\nmodel.predict(new_input)  # The new input needs to be shaped and scaled the sane as the training data\n\n\n\nThese are used for modelling sequences with variable lengths of inputs and outputs.\nRecurrent neurons take as their input the current data AND the previous epoch’s output. We could try to pass EVERY previous epoch’s output as an input, but would run into issues of vanushing gradients.\nLSTMs take this idea a step further by incorporating the previous epochs input AND some longer lookback of epoch where some old outputs are “forgotten”.\nA basic RNN:\n\n            --------------------------------\n            |                              |\n H_t-1 ---&gt; |                              |---&gt; H_t\n            |                              |\n X_t   ---&gt; |                              |\n            |                              |\n            --------------------------------\n\nwhere:\nH_t is the neuron's output at current epoch t\nH_t-1 is the neuron's output from the previous epoch t-1\nX_t is the input data at current epoch t\n\nH_t = tanh(W[H_t-1, X_t] + b)\nAn LSTM (Long Short Term Memory) unit:\n\n            --------------------------------\n            |                              |\n H_t-1 ---&gt; |                              |---&gt; H_t\n            |                              |\n C_t-1 ---&gt; |                              |---&gt; C_t\n            |                              |\n X_t   ---&gt; |                              |\n            |                              |\n            --------------------------------\n            \nThe `forget gate` determines which part of the old short-term memory and current input is \"forgotten\" by the new long-term memory.\nThese are a set of weights (between 0 and 1) that get applied to the old long-term memory to downweight it.\nF_t = sigmoid(W_F[H_t-1, X_t] + b_F)\n\nThe `input gate` i_t similarly gates the input and old short-term memory.\nThis will later (in the update gate) get combined  with a candidate value for the new long-term memory `C_candidate_t`\nI_t = sigmoid(W_I[H_t-1, X_t] + b_I)\nC_candidate_t = tanh(W_C_cand[H_t-1, X_t] + b_C_cand) \n\nThe `update gate` for the new long-term memory `C_t` is then calculated as a sum of forgotten old memory and input-weighted candidate memory:\nC_t = F_t*C_t-1 + I_t*C_candidate_t \n\nThe `output gate` O_t is a combination of the old short-term memory and latest input data.\nThis is then combined with the latest long-term memory to produce the output of the recurrent neuron, which is also the updated short-term memory H_t:\nO_t = sigmoid(W_O[H_t-1, X_t] + b_O)\nH_t = O_t * tanh(C_t) \n\nwhere:\nH_t is short-term memory at epoch t\nC_t is long-term memory at epoch t\nX_t is the input data at current epoch t\n\nsigmoid is a sigmoid  activation function\nF_t is an intermediate forget gate weight\nI_t is an intermediate input gate weight\nO_t is an intermediate output gate weight\nThere are several variants of RNNs. RNNs with peepholes\nThis leaks long-term memory into the forget, input and output gates. Note that the forget gate and input gate each get the OLD long-term memory, whereas the output gate gets the NEW long-term memory.\nF_t = sigmoid(W_F[C_t-1, H_t-1, X_t] + b_F)\nI_t = sigmoid(W_I[C_t-1, H_t-1, X_t] + b_I)\nO_t = sigmoid(W_O[C_t, H_t-1, X_t] + b_O)\nGated Recurrent Unit (GRU)\nThis combines the forget and input gates into a single gate. It also has some other changes. This is simpler than a typical LSTM model as it has fewer parameters. This makes it more computationally efficient, and in practice they can have similar performance.\nz_t = sigmoid(W_z[H_t-1, X_t])\nr_t = sigmoid(W_r[H_t-1, X_t])\nH_candidate_t = tanh(W_H_candidate[r_t*h_t-1, x_t])\nH_t = (1 - z_t) * H_t-1 + z_t * H_candidate_t\nThe sequences modelled with RNNs can be:\n\nOne-to-many\nMany-to-many\nMany-to-one\n\nConsiderations for choosing the input sequence length:\n\nIt should be long enough to capture any patterns or seasonality in the data\nThe validation set and test set each need to have a size at least (input_length + output_length), so the longer the input length, the more data is required to be set aside.\n\nThe validation loss for RNNs/LSTMs can be volatile, so allow a higher patience when using early stopping.\nExample model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,LSTM\n\n# Generators for helper functions to chunk up the input series into an array of pairs of (input_batch, target_output)\ntrain_generator = TimeseriesGenerator(scaled_train, scaled_train, length=batch_length, batch_size=1)\nvalidation_generator = TimeseriesGenerator(scaled_test, scaled_test, length=batch_length, batch_size=1)\n\n# Create an LSTM model\nmodel = Sequential()\nmodel.add(LSTM(100, activation='relu', input_shape=(batch_length, n_features)))\nmodel.add(Dense(1))  # Output layer\nmodel.compile(optimizer='adam', loss='mse')  # Problem setup for regression\n\n# Fit the model\nearly_stop = EarlyStopping(monitor='val_loss', patience=10)  # RNNs can be volatile so if use a higher patience with an early stopping callback\nmodel.fit(train_generator, epochs=20, validation_data=validation_generator, callbacks=[early_stop])\n\n# Evaluate test data\ntest_predictions = []\nfinal_batch = scaled_train[-batch_length:, :]\ncurrent_batch = final_batch.reshape((1, batch_length, n_features))\n\nfor test_val in scaled_test:\n    pred_val = model.predict(current_batch)  # These will later need to use the scaler.inverse_transform of the scaler originally used on the training data\n    test_predictions.append(pred_val[0])\n    current_batch = np.append(current_batch[:,1:,:], pred_val.reshape(1,1,1), axis=1)\n\n\n\nCharacter-based generative NLP: given an input string, e.g. [‘h’, ‘e’, ‘l’, ‘l’], predict the sequence shifted by one character, e.g. [‘e’, ‘l’, ‘l’, ‘o’]\nThe embedding, GRU and dense layers work on the sequence in the following way: \nSteps:\n\nRead in text data\nText processing and vectorisation - one-hot encoding\nCreate batches\nCreate the model\nTrain the model\nGenerate new text\n\nStep 1: Read in text data\n\nA corpus of &gt;1 million characters is a good size\nThe more distinct the style of your corpus, the more obvious it will be if your model has worked.\n\nGiven some structured input_text string, we can get the one-hot encoded vocab list of unique characters.\nvocab_list = sorted(set(input_text))\nStep 2: Text processing\nVectorise the text - create a mapping between character and integer. This is the encoding dictionary used to vectorise the corpus.\n# Mappings back and forth between characters and their encoded integers\nchar_to_ind = {char: ind for ind, char in enumerate(vocab_list)}\nind_to_char = {ind: char for ind, char in enumerate(vocab_list)}\n\nencoded_text = np.array([char_to_ind[char] for char in input_text])\nStep 3: Create batches\nThe sequence length should be long enough to capture structure, e.g. for poetry with rhyming couplets it should be the number of characters in 3 lines to capture the rhyme and non-rhyme. Too long a sequence makes the model take longer to train and captures too much historical noise.\nCreate a shuffled dataset where:\n\ninput sequence is the first n characters\noutput sequence is n characters lagged by one\n\nWe want to shuffle these sequence pairs into a random order so the model doesn’t overfit to any section of the text, but can instead generate characters given any seed text.\nseq_len = 120\ntotal_num_seq = len(input_text) // (seq_len + 1)\n\n# Create Training Sequences\nchar_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\nsequences = char_dataset.batch(seq_len + 1, drop_remainder=True)  # drop_remainder drops the last incomplete sequence\n\n# Create tuples of input and output sequences\ndef create_seq_targets(seq):\n    input_seq = seq[:-1]\n    output_seq = seq[-1:]\n    return input_seq, output_seq\ndataset = sequences.map(create_seq_targets)\n\n# Generate training batches\nbatch_size = 128\nbuffer_size = 10000  # Shuffle this amount of batches rather than shuffling the entire dataset in memory\ndataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\nStep 4: Create the model\nSet up the loss function and layers.\nThis model uses 3 layers:\n\nEmbedding\n\nEmbed positive integers, e.g. “a”&lt;-&gt;1, as dense vectors of fixed size. This embedding size is a hyperparameter for the user to decide.\nNumber of layers should be smaller but a similar scale to the vocab size; typically use the power of 2 that is just smaller than the vocab size.\n\nGRU\n\nThis is the main thing to vary in the model: number of hidden layers and number of neurons in each, types of neurons (RNN, LSTM, GRU), etc. Typically use lots of layers here.\n\nDense\n\nOne neuron per character (one-hot encoded), so this produces a probability per character. The user can vary the “temperature”, choosing less probable characters more/less often.\n\n\nLoss function:\n\nSparse categorical cross-entropy\nUse sparse categorical cross-entropy because the classes are mutually exclusive. Use regular categorical cross-entropy when one smaple can have multiple classes, or the labels are soft probabilities.\nSee link in NLP appendix for discussion.\nUse logits=True because vocab input is one hot encoded.\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, GRU\nfrom tensorflow.keras.losses import sparse_categorical_crossentropy\n\n# Hyperparameters\nvocab_size = len(vocab_list)\nembed_dim = 64\nrnn_neurons = 1026\n\n# Create model \nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embed_dim, batch_input_shape=[batch_size, None]))\nmodel.add(GRU(rnn_neurons, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'))\nmodel.add(Dense(vocab_size))\nsparse_cat_loss = lambda y_true,y_pred: sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)  # We want a custom loss function with logits=True\nmodel.compile(optimizer='adam', loss=sparse_cat_loss) \nStep 5: Train the model\nepochs = 30\nmodel.fit(dataset,epochs=epochs)\nStep 6: Generate text\nAllow the model to accept a batch size of 1 to allow us to give an input seed text from which to generate text from.\nWe format the seed text so it can be fed into the network, then loop through generating one character at a time and feeding that back into the model.\nmodel.build(tf.TensorShape([1, None]))\n\ndef generate_text(model, input_seed_text, gen_size=100, temperature=1.0):\n    \"\"\"\n    model: tensorflow.model\n    input_seed_text: str\n    gen_size: int\n    temperature: float\n    \"\"\"\n    generated_text_result = []\n    vectorised_seed = tf.expand_dims([char_to_ind[s] for s in input_seed_text], 0)\n    model.reset_states()\n    \n    for k in range(gen_size):\n        raw_predictions = model(vectorised_seed)\n        predictions = tf.squeeze(raw_predictions, 0) / temperature\n        predicted_index = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n        generated_text_result.append(ind_to_char[predicted_index])\n        \n        # Set a new vectorised seed to generate the next character in the loop\n        vectorised_seed = tf.expand_dims([predicted_index], 0)\n        \n    return (input_seed_text + ''.join(generated_text_result))\n\n\n\nThis is an unsupervised learning problem, therefore evaluation metrics are different. Sometimes called semi-supervised as labels may be used in training the model, but not available when it’s used to predict new values.\nApplications:\n\nDimensionality reduction\nNoise removal\n\nDesigned to reproduce its input in the output layer. Number of input neurons is the same as the number of output layers. The encoder reduces dimensionality to the hidden layer. The hidden layer should contain the most relevant information required to reconstruct the input. The decoder reconstructs a high dimensional output from a low dimensional input.\n\nEncoder: Input layer -&gt; Hidden layer\nDecoder: Hidden layer -&gt; Output\n\nStacked autoencoders include multiple hidden layers.\nAutoencoder for dimensionality reduction\nLet’s try to reduce an input dataset from 3 dimensions to 2. We want to train an autoencoder to go from 3 -&gt; 2 -&gt; 3.\nWhen we scale data, we fit and transform on the entire dataset rather than a train/test split, because there is no ground truth for the “correct” dimensionality reduction.\nUsing SGD in Autoencoders allows the learning rate to be varied as a hyperparameter to affect how well the hidden layer learns. Larger values will train quicker but potentially perform worse.\nTo retrieve the lower dimensionality representation, use just the encoder half of the trained autoencoder to predict the input.\n# Scale data\nscaler = MinMaxScaler()\nscaled_data = scaler.fit_transform(input_features)\n\n# Create model\nencoder = Sequential()\nencoder.add(Dense(units=2, acivation='relu', input_shape=[3]))\n\ndecoder = Sequential()\ndecoder.add(Dense(units=3, acivation='relu', input_shape=[2]))\n\nautoencoder = Sequential([encoder, decoder])\nautoencoder.compile(loss='mse', optimizer=SGD(lr=1.5))\n\n# Train the full autoencoder model\nautoencoder.fit(scaled_data, scaled_data, epochs=5)\n\n# Reduce dimensionality by using just the encoder part of the trained model\nencoded_2dim = encoder.predict(scaled_data)\nAutoencoder for noise removal\nStarting with clean images, we want to train an autoencoder to learn to remove noise. We create a noisy dataset by adding noise to our clean images. We then try to reconstruct that input, and compare to the original clean images as the ground truth.\nStarting with 28x28 images (e.g. MNIST dataset), this gives 784 input features when flattened. Use multiple hidden layers as there are a large number of input features that would be difficult to learn in one layer. The numbers of layers and number of neurons in each are hyperparameters to tune; decreasing in powers of 2 is a good rule of thumb.\nAdd a Gaussian noise layer to train the model to remove noise. Without the Gaussian layer, the model would be used for dimensionality reduction rather than noise removal.\nThe final layer uses a sigmoid activation because it is a binary classification problem - does the output match the input?\nencoder = Sequential()\nencoder.add(Flatten(input_shape=[28, 28]))\nencoder.add(Gaussian(0.2))  # Optional layer if training for noise removal rather than dimensionality reduction\nencoder.add(Dense(400, activation='relu'))\nencoder.add(Dense(200, activation='relu'))\nencoder.add(Dense(100, activation='relu'))\nencoder.add(Dense(50, activation='relu'))\nencoder.add(Dense(25, activation='relu'))\n\ndecoder = Sequential()\ndecoder.add(Dense(50, input_shape=[25], activation='relu'))\ndecoder.add(Dense(100, activation='relu'))\ndecoder.add(Dense(200, activation='relu'))\ndecoder.add(Dense(400, activation='relu'))\ndecoder.add(Dense(28*28, activation='sigmoid'))\n\nautoencoder = Sequential([encoder, decoder])\nautoencoder.compile(loss='binary_crossentropy', optimizer=SGD(lr=1.5), metrics=['accuracy'])\nautoencoder.fit(X_train, X_train, epochs=5)\n\n\n\nUse 2 networks competing against each other to generate data - counterfeiter (generator) vs detective (discriminator).\n\nGenerator: recieves random noise\nDiscriminator: receives data set containing real data and fake generated data, and attempts to calssify real vs fake (binary classificaiton).\n\nTwo training phases, each only training one of the networks:\n\nTrain discriminator - real images (labeled 1) and generated images (labeled 0) are fed to the discriminator network.\nTrain generator - only feed generated images to discriminator, ALL labeled 1.\n\nThe generator never sees real images, it creates images based only off of the gradients flowing back from the discriminator.\nDifficulties with GANs:\n\nTraining resources - GPUs generally required\nMode collapse - generator learns an image that fools the discriminator, then only produces this image. Deep convolutional GANs and mini-batch discrimination are two solution to this problem.\nInstability - True performance is hard to measure; just because the discriminator was fooled, it doesn’t mean that the generated image was actually realistic.\n\nCreating the model\nWe create the generator and discriminator as separate models, then join them into a GAN object. This is analogous to how we joined an encoder and decoder into an autoencoder.\nThe discriminator is trained on a binary classification problem, real or fake, so uses a binary cross entropy loss function. Backpropagation only alters the weights of the discriminator in the first phase, not the generator.\ndiscriminator = Sequential()\n\n# Flatten the image\ndiscriminator.add(Flatten(input_shape=[28,28]))\n\n# Some hidden layers\ndiscriminator.add(Dense(150,activation='relu'))\ndiscriminator.add(Dense(100,activation='relu'))\n\n# Binary classification - real vs fake\ndiscriminator.add(Dense(1,activation=\"sigmoid\"))\ndiscriminator.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\nThe generator is trained in the second phase.\nWe set a codings_size, which is the size of the latent representation from which the generator will create a full image. For example, if we want to create a 28x28 image (784 pixels), we may want to generate this from a 100 element input. This is analogous to the decoder part of an autoencoder.\nThe codings_size should be smaller than the total number of features but large enough so that there is something to learn from.\nThe generator is NOT compiled at this step, it is compiled in the combined GAN later so that it is only trained at that step.\ncodings_size = 100\ngenerator = Sequential()\n\n# Input shape of first layer is the codings_size\ngenerator.add(Dense(150, activation=\"relu\", input_shape=[codings_size]))\n\n# Some hidden layers\ngenerator.add(Dense(200,activation='relu'))\n\n# Output is the size of the image we want to create\ngenerator.add(Dense(784, activation=\"sigmoid\"))\ngenerator.add(Reshape([28,28]))\nThe GAN combines the generator and discriminator.\nThe discriminator is only trained in the first phase, not the second phase.\nGAN = Sequential([generator, discriminator])\ndiscriminator.trainable = False\nGAN.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\nTraining the model\nWe first create batches of images from our input data, similarly to previous models.\nbatch_size = 32  # Size of training input batches\nbuffer_size = 1000  # Don't load the entire dataset into memory\nepochs = 1\n\nraw_dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(buffer_size=buffer_size)\nbatched_dataset = raw_dataset.batch(batch_size, drop_remainder=True).prefetch(1)\nThe training loop for each epoch trains the discriminator to distinguish real vs fake images (binary classification). Then it trains the generator to generate fake images using ONLY the gradients learned in the discriminator training step; the generator NEVER sees any real images.\ngenerator, discriminator = GAN.layers\n\nfor epoch in range(epochs):\n    for index, X_batch in enumerate(batched_dataset):\n        # Phase 1: Train the discriminator\n        noise = tf.random.normal(shape=[batch_size, codings_size])\n        generated_images = generator(noise)\n        real_images = tf.dtypes.cast(X_batch,tf.float32)\n        X_train_discriminator = tf.concat([generated_images, real_images], axis=0)\n        y_train_discriminator = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)  # Targets set to zero for fake _images and 1 for real _images\n        discriminator.trainable = True\n        discriminator.train_on_batch(X_train_discriminator, y_train_discriminator)\n\n        # Phase 2: Train the generator\n        noise = tf.random.normal(shape=[batch_size, codings_size])\n        y_train_generator = tf.constant([[1.]] * batch_size)  # We want discriminator to believe that fake _images are real\n        discriminator.trainable = False\n        GAN.train_on_batch(noise, y_train_generator)    \nDeep convolutional GANs\nThese are GANs which use convolutional layers inside the discriminator and generator models.\nThe models are almost identical to the models above, just with additional Conv2D, Conv2DTranspose and BatchNormalization layers.\nChanges compared to regular GANs:\n\nAdditional convolutional layers in model.\nReshape the training set to match the images.\nRescale the training data to be between -1 and 1 so that tanh activation function works.\nActivation of discriminator output layer is tanh rather than sigmoid."
  },
  {
    "objectID": "posts/ml/tensorflow/tensorflow.html#a.-appendix",
    "href": "posts/ml/tensorflow/tensorflow.html#a.-appendix",
    "title": "Tensorflow Notes",
    "section": "",
    "text": "Complete Tensorflow and Keras Udemy course\nIntuition behind neural networks\nThe deep learning bible (with lectures)\nHeuristics for choosing hidden layers\nAlternatives to the deprecated model predict for different classification problems\nMIT course\n\n\n\n\n\nImage kernels explained\nChoosing CNN layers\n\n\n\n\n\nOverview of RNNs\nWikipedia page contains the equations of LSTMs and peepholes\nLSTMs vs GRUs\nWorked example of LSTM\nRNN cheatsheet\n\n\n\n\n\nThe unreasonable effectiveness of RNNs - essay on RNNs applied to NLP\nSparse vs dense categorical crossentropy loss function\n\n\n\n\n\nbuffer_size argument in tensorflow prefetch and shuffle\nMode collapse"
  },
  {
    "objectID": "posts/ml/fastai/lesson1/lesson.html",
    "href": "posts/ml/fastai/lesson1/lesson.html",
    "title": "FastAI Lesson 1: Image Classification Models",
    "section": "",
    "text": "These are notes from lesson 1 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\nTrain an image classifier: see car classification notebook\n\n\n\n\nThere is a companion course focusing on AI ethics here\nThoughts on education: play the whole game first to get an idea of the big picture. Don’t become afflicted with “elementitis” getting too bogged down in details too early.\n\nColoured cups - green, amber, red - for each student to indicate how well they are following\nMeta Learning by Radek Osmulski - a book based on the author’s experience of learning about deep learning (via the fastai course)\nMathematician’s Lament by Paul Lockhart - a book about the state of mathematics education\nMaking Learning Whole by David Perkins - a book about apporaches to holistic learning\n\nRISE is a jupyter notebook extension to turn notebooks into slides. Jeremy uses notebooks for: source code, book, blogging, CI/CD.\n\n\n\nBefore deep learning, the approach to machine learning was to enlist many domain experts to handcraft features and feed this into a constrained linear model (e.g. ridge regression). This is time-consuming, expensive and requires many domain experts.\nNeural networks learn these features. Looking inside a CNN, for example, shows that these learned features match interpretable features that an expert might handcraft. An illustration of the features learned is given in this paper by Zeiler and Fergus.\nFor image classifiers, you don’t need particularly large images as inputs. GPUs are so quick now that if you use large images, most of the time is spent on opening the file rather than computations. So often we resize images down to 400x400 pixels or smaller.\nFor most use cases, there are pre-trained models and sensible default values that we can use. In practice, most of the time is spent on the input layer and output layer. For most models the middle layers are identical.\n\n\n\nData blocks structure the input to learners. An overview of the DataBlock class:\n\nblocks determines the input and output type as a tuple. For multi-target classification this tuple can be arbitrary length.\nget_items is a function that returns a list of all the inputs.\nsplitter defines how to split the training/validation set.\nget_y is a function that returns the label of a given input image.\nitem_tfms defines what transforms to apply to the inputs before training, e.g. resize.\ndataloaders is a method that parallelises loading the data.\n\nA learner combines the model (e.g. resnet or something from timm library) and the data to run that model on (the dataloaders from the DataBlock).\nThe fine_tune method starts with a pretrained model weights rather than randomised weights, and only needs to learn the differences between your data and the original model.\nOther image problems that can utilise deep learning:\n\nImage classification\nImage segmentation\n\nOther problem types use the same process, just with different DataBlock blocks types and the rest is the same. For example, tabular data, collaborative filtering.\n\n\n\n\nTraditional computer programs are essentially:\n\n\n\n\n\n\nFigure 1: Traditional computer programs\n\n\n\nDeep learning models are:\n\n\n\n\n\n\nFigure 2: Deep learning model\n\n\n\n\n\n\n\nCourse lesson page\nVisualizing and Understanding Convolutional Networks\nTIMM library\nStatistical Modeling: The Two Cultures\n\n\n\n\nFigure 1: Traditional computer programs\nFigure 2: Deep learning model"
  },
  {
    "objectID": "posts/ml/fastai/lesson1/lesson.html#notes-on-learning",
    "href": "posts/ml/fastai/lesson1/lesson.html#notes-on-learning",
    "title": "FastAI Lesson 1: Image Classification Models",
    "section": "",
    "text": "There is a companion course focusing on AI ethics here\nThoughts on education: play the whole game first to get an idea of the big picture. Don’t become afflicted with “elementitis” getting too bogged down in details too early.\n\nColoured cups - green, amber, red - for each student to indicate how well they are following\nMeta Learning by Radek Osmulski - a book based on the author’s experience of learning about deep learning (via the fastai course)\nMathematician’s Lament by Paul Lockhart - a book about the state of mathematics education\nMaking Learning Whole by David Perkins - a book about apporaches to holistic learning\n\nRISE is a jupyter notebook extension to turn notebooks into slides. Jeremy uses notebooks for: source code, book, blogging, CI/CD."
  },
  {
    "objectID": "posts/ml/fastai/lesson1/lesson.html#background-on-deep-learning-and-image-classification",
    "href": "posts/ml/fastai/lesson1/lesson.html#background-on-deep-learning-and-image-classification",
    "title": "FastAI Lesson 1: Image Classification Models",
    "section": "",
    "text": "Before deep learning, the approach to machine learning was to enlist many domain experts to handcraft features and feed this into a constrained linear model (e.g. ridge regression). This is time-consuming, expensive and requires many domain experts.\nNeural networks learn these features. Looking inside a CNN, for example, shows that these learned features match interpretable features that an expert might handcraft. An illustration of the features learned is given in this paper by Zeiler and Fergus.\nFor image classifiers, you don’t need particularly large images as inputs. GPUs are so quick now that if you use large images, most of the time is spent on opening the file rather than computations. So often we resize images down to 400x400 pixels or smaller.\nFor most use cases, there are pre-trained models and sensible default values that we can use. In practice, most of the time is spent on the input layer and output layer. For most models the middle layers are identical."
  },
  {
    "objectID": "posts/ml/fastai/lesson1/lesson.html#overview-of-the-fastai-learner",
    "href": "posts/ml/fastai/lesson1/lesson.html#overview-of-the-fastai-learner",
    "title": "FastAI Lesson 1: Image Classification Models",
    "section": "",
    "text": "Data blocks structure the input to learners. An overview of the DataBlock class:\n\nblocks determines the input and output type as a tuple. For multi-target classification this tuple can be arbitrary length.\nget_items is a function that returns a list of all the inputs.\nsplitter defines how to split the training/validation set.\nget_y is a function that returns the label of a given input image.\nitem_tfms defines what transforms to apply to the inputs before training, e.g. resize.\ndataloaders is a method that parallelises loading the data.\n\nA learner combines the model (e.g. resnet or something from timm library) and the data to run that model on (the dataloaders from the DataBlock).\nThe fine_tune method starts with a pretrained model weights rather than randomised weights, and only needs to learn the differences between your data and the original model.\nOther image problems that can utilise deep learning:\n\nImage classification\nImage segmentation\n\nOther problem types use the same process, just with different DataBlock blocks types and the rest is the same. For example, tabular data, collaborative filtering."
  },
  {
    "objectID": "posts/ml/fastai/lesson1/lesson.html#deep-learning-vs-traditional-computer-programs",
    "href": "posts/ml/fastai/lesson1/lesson.html#deep-learning-vs-traditional-computer-programs",
    "title": "FastAI Lesson 1: Image Classification Models",
    "section": "",
    "text": "Traditional computer programs are essentially:\n\n\n\n\n\n\nFigure 1: Traditional computer programs\n\n\n\nDeep learning models are:\n\n\n\n\n\n\nFigure 2: Deep learning model"
  },
  {
    "objectID": "posts/ml/fastai/lesson1/lesson.html#references",
    "href": "posts/ml/fastai/lesson1/lesson.html#references",
    "title": "FastAI Lesson 1: Image Classification Models",
    "section": "",
    "text": "Course lesson page\nVisualizing and Understanding Convolutional Networks\nTIMM library\nStatistical Modeling: The Two Cultures\n\n\n\n\nFigure 1: Traditional computer programs\nFigure 2: Deep learning model"
  },
  {
    "objectID": "posts/ml/fastai/lesson6_1/lesson.html",
    "href": "posts/ml/fastai/lesson6_1/lesson.html",
    "title": "FastAI Lesson 6.5: Road to the Top",
    "section": "",
    "text": "These are notes on the “Road to the Top” notebooks that span lessons 6 and 7 of Fast AI Practical Deep Learning for Coders. I’ve separated these from the main topics of those lectires to keep the posts focused.\n\n\n\n\n\n\nHomework Task\n\n\n\n\nRecreate the softmax and cross-entropy loss spreadsheet example\nRead the “Road to the Top” notebook series - parts 1, 2 and 3\nRead “Things that confused me about cross entropy” by Chris Said.\n\n\n\n\n\nThe focus should be:\n\nCreate an effective validation set\nIterate quickly to find changes which improve the validation set\n\nTrain a simple model straight away, get a result and submit it. You can iterate and improve later.\nData augmentation, in particular test-time augmentation (TTA), can improve results.\n\n\n\nRules of thumb on model selection by problem type:\n\nFor computer vision use CNNs, fine-tune a pre-trained model. See comparison of pre-trained models here.\n\nIf unsure which model to choose, start with (small) convnext.\nDifferent models can have big differences in accuracy so try a few small models from different families.\nOnce you are happy with the model, try a size up in that family.\nResizing images to a square is a good, easy compromise to accomodate many different image sizes, orientations and aspect ratios. A more involved approach that may improve results is to batch images together and resize them to the median rectangle size.\n\nFor tabular data, random forests will give a reasonably good result.\n\nGBMs will give a better result eventually, but with more effort required to get a good model.\nWorth running a hyperparameter grid search for GBM because it’s fiddly.\n\n\nRules of thumb on hardware:\n\nYou generally want ~8 physical CPUs per GPU\nIf model training is CPU bound, it can help to resize images to be smaller. The file I/O on the CPU may be taking too long.\nIf model training is CPU bound and GPU usage is low, you might as well go up to a “better” (i.e. bigger) model with no increase of overall execution time.\n\n\n\n\nThis is a technique that allows you to run larger batch sizes without running out of GPU memory.\nRather than needing to fit a whole batch (say, 64 images) in memory, we can split this up into sub-batches. Let’s say we use an accumulation factor of 4; this means each sub-batch would have 16 images.\nWe calculate the gradient for each sub-batch but don’t immediately subtract it from the weights. We also keep a running count of the number of images we have seen. Once we have seen 64 images, i.e. 4 batches, then apply the total accumulated gradient and reset the count. Remember that in Pytorch, if we calculate another gradient without zeroing in between, it will add the new gradient to the old one, so this is in effect the “default behaviour”.\nGradient accumulation gives an identical result unless using batch normalisation, since the moving averages will be more volatile. Most big models use layer normalisation rather than batch normalisation so often this is not an issue.\nHow do we pick a batch size? Just pick the largest you can without running out of GPU memory.\nWhy not just reduce the batch size? If using a pretrained model, the other parameters such as learning rate work for that batch size and might not work for others. So we’d have to perform a hyperparameter search again to find a good combination for the new batch size. Gradient accumulation sidesteps this issue.\n\n\n\nConsider the case where we want to train a model with two different predicted labels, say plant species and disease type. If we have 10 plant species and 10 diseases, we create a model with 20 output neurons.\nHow does the model know what each should do? Through the loss function. We make one loss function for disease which takes the first 10 columns of the input to the final layer, and a second loss function for variety which takes the last 10 columns of the input to the final layer. The overall loss function is the sum of the two.\nIf you train it for longer, the multi target model can get better at predicting one target, say disease, than a single target model trained on just disease alone.\n\nThe early layers that are useful for variety may also be useful for disease\nKnowing the variety may be useful in predicting disease if there are confounding factors between disease and variety. For example, certain diseases affect certain varieties.\n\n\n\n\n\nCourse lesson page\n“Road to the Top” notebook\n“Things that confused me about cross entropy” by Chris Said."
  },
  {
    "objectID": "posts/ml/fastai/lesson6_1/lesson.html#general-points",
    "href": "posts/ml/fastai/lesson6_1/lesson.html#general-points",
    "title": "FastAI Lesson 6.5: Road to the Top",
    "section": "",
    "text": "The focus should be:\n\nCreate an effective validation set\nIterate quickly to find changes which improve the validation set\n\nTrain a simple model straight away, get a result and submit it. You can iterate and improve later.\nData augmentation, in particular test-time augmentation (TTA), can improve results."
  },
  {
    "objectID": "posts/ml/fastai/lesson6_1/lesson.html#some-rules-of-thumb",
    "href": "posts/ml/fastai/lesson6_1/lesson.html#some-rules-of-thumb",
    "title": "FastAI Lesson 6.5: Road to the Top",
    "section": "",
    "text": "Rules of thumb on model selection by problem type:\n\nFor computer vision use CNNs, fine-tune a pre-trained model. See comparison of pre-trained models here.\n\nIf unsure which model to choose, start with (small) convnext.\nDifferent models can have big differences in accuracy so try a few small models from different families.\nOnce you are happy with the model, try a size up in that family.\nResizing images to a square is a good, easy compromise to accomodate many different image sizes, orientations and aspect ratios. A more involved approach that may improve results is to batch images together and resize them to the median rectangle size.\n\nFor tabular data, random forests will give a reasonably good result.\n\nGBMs will give a better result eventually, but with more effort required to get a good model.\nWorth running a hyperparameter grid search for GBM because it’s fiddly.\n\n\nRules of thumb on hardware:\n\nYou generally want ~8 physical CPUs per GPU\nIf model training is CPU bound, it can help to resize images to be smaller. The file I/O on the CPU may be taking too long.\nIf model training is CPU bound and GPU usage is low, you might as well go up to a “better” (i.e. bigger) model with no increase of overall execution time."
  },
  {
    "objectID": "posts/ml/fastai/lesson6_1/lesson.html#gradient-accumulation",
    "href": "posts/ml/fastai/lesson6_1/lesson.html#gradient-accumulation",
    "title": "FastAI Lesson 6.5: Road to the Top",
    "section": "",
    "text": "This is a technique that allows you to run larger batch sizes without running out of GPU memory.\nRather than needing to fit a whole batch (say, 64 images) in memory, we can split this up into sub-batches. Let’s say we use an accumulation factor of 4; this means each sub-batch would have 16 images.\nWe calculate the gradient for each sub-batch but don’t immediately subtract it from the weights. We also keep a running count of the number of images we have seen. Once we have seen 64 images, i.e. 4 batches, then apply the total accumulated gradient and reset the count. Remember that in Pytorch, if we calculate another gradient without zeroing in between, it will add the new gradient to the old one, so this is in effect the “default behaviour”.\nGradient accumulation gives an identical result unless using batch normalisation, since the moving averages will be more volatile. Most big models use layer normalisation rather than batch normalisation so often this is not an issue.\nHow do we pick a batch size? Just pick the largest you can without running out of GPU memory.\nWhy not just reduce the batch size? If using a pretrained model, the other parameters such as learning rate work for that batch size and might not work for others. So we’d have to perform a hyperparameter search again to find a good combination for the new batch size. Gradient accumulation sidesteps this issue."
  },
  {
    "objectID": "posts/ml/fastai/lesson6_1/lesson.html#multi-target-classification",
    "href": "posts/ml/fastai/lesson6_1/lesson.html#multi-target-classification",
    "title": "FastAI Lesson 6.5: Road to the Top",
    "section": "",
    "text": "Consider the case where we want to train a model with two different predicted labels, say plant species and disease type. If we have 10 plant species and 10 diseases, we create a model with 20 output neurons.\nHow does the model know what each should do? Through the loss function. We make one loss function for disease which takes the first 10 columns of the input to the final layer, and a second loss function for variety which takes the last 10 columns of the input to the final layer. The overall loss function is the sum of the two.\nIf you train it for longer, the multi target model can get better at predicting one target, say disease, than a single target model trained on just disease alone.\n\nThe early layers that are useful for variety may also be useful for disease\nKnowing the variety may be useful in predicting disease if there are confounding factors between disease and variety. For example, certain diseases affect certain varieties."
  },
  {
    "objectID": "posts/ml/fastai/lesson6_1/lesson.html#references",
    "href": "posts/ml/fastai/lesson6_1/lesson.html#references",
    "title": "FastAI Lesson 6.5: Road to the Top",
    "section": "",
    "text": "Course lesson page\n“Road to the Top” notebook\n“Things that confused me about cross entropy” by Chris Said."
  },
  {
    "objectID": "posts/ml/fastai/lesson5/lesson.html",
    "href": "posts/ml/fastai/lesson5/lesson.html",
    "title": "FastAI Lesson 5: Natural Language Processing",
    "section": "",
    "text": "These are notes from lesson 5 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\n\nRecreate the Jupyter notebook to train a linear model and a neural network from scratch - see from scratch notebook\nThen repeat the exercise using the fastai framework (it’s much easier!) - see framework notebook\nRead numpy broadcasting rules\n\n\n\n\n\nTrain a linear model from scratch in Python using on the Titanic dataset. Then train a neural network from scratch in a similar way. This is an extension of the spreadsheet approach to make a neural network from scratch in lesson 3.\n\n\nNever throw away data.\nAn easy way of imputing missing values is to fill them with the mode. This is good enough for a first pass at creating a model.\n\n\n\nNumeric values that can grow exponentially like prices or population sizes often have long-tailed distributions.\nAn easy way to scale is to take log(x+1). The +1 term is just to avoid taking log of 0.\n\n\n\nOne-hot encode any categorical variables.\nWe should include an “other” category for each in case the validation or test set contains a category we didn’t encounter in the training set.\nIf there are categories with ony a small number of observations we can group them into an “other” category too.\n\n\n\nBroadcasting arrays together avoids boilerplate code to make dimensions match.\nFor reference, see numpy’s broadcasting rules and try APL\n\n\n\nFor a binary classification model, the outputs should be between 0 and 1. If you train a linear model, it might result in negative values or values &gt;1.\nThis means we can improve the loss by just clipping to ensure they stay between 0 and 1.\nThis is the idea behind the sigmoid function for output layers: smaller values will tend to 0 and larger values will tend to 1. In general, for any binary classification model, we should always have a sigmoid as the final layer. If the model isn’t training well, it’s worth checking that the final activation function is a sigmoid.\nThe sigmoid function is defined as: \\[ y = \\frac{1}{1+e^{-x}} \\]\n\n\n\nIn general, the middle layers of a neural network are similar between different problems.\nThe input layer will depend on the data for our specific problem. The output will depend on the target for our specific problem.\nSo we spend most of our time thinking about the correct input and output layers, and the hidden layers are less important.\n\n\n\n\nWhen creating the models from scratch, there was a lot of boilerplate code to:\n\nImpute missing values using the “obvious” method (fill with mode)\nNormalise continuous variables to be between 0 and 1\nOne-hot encode categorical variables\nRepeat all of these steps in the same order for the test set\n\nThe benefits of using a framework like fastai:\n\nLess boilerplate, so the obvious things are done automatically unless you say otherwise\nRepeating all of the feature engineering steps on the output is trivial with DataLoaders\n\n\n\n\nCreating independent models and taking the mean of their predictions improves the accuracy. There are a few different approaches to ensembling a categorical variable:\n\nTake the mean of the predictions (binary 0 or 1)\nTake the mean of the probabilities (continuous between 0 and 1) then threshold the result\nTake the mode of the predictions (binary 0 or 1)\n\nIn general the mean approaches work better but there’s no rule as to why, so try all of them.\n\n\n\n\nCourse lesson page\nNumpy broadcasting rules\nAPL"
  },
  {
    "objectID": "posts/ml/fastai/lesson5/lesson.html#training-a-model-from-scratch",
    "href": "posts/ml/fastai/lesson5/lesson.html#training-a-model-from-scratch",
    "title": "FastAI Lesson 5: Natural Language Processing",
    "section": "",
    "text": "Train a linear model from scratch in Python using on the Titanic dataset. Then train a neural network from scratch in a similar way. This is an extension of the spreadsheet approach to make a neural network from scratch in lesson 3.\n\n\nNever throw away data.\nAn easy way of imputing missing values is to fill them with the mode. This is good enough for a first pass at creating a model.\n\n\n\nNumeric values that can grow exponentially like prices or population sizes often have long-tailed distributions.\nAn easy way to scale is to take log(x+1). The +1 term is just to avoid taking log of 0.\n\n\n\nOne-hot encode any categorical variables.\nWe should include an “other” category for each in case the validation or test set contains a category we didn’t encounter in the training set.\nIf there are categories with ony a small number of observations we can group them into an “other” category too.\n\n\n\nBroadcasting arrays together avoids boilerplate code to make dimensions match.\nFor reference, see numpy’s broadcasting rules and try APL\n\n\n\nFor a binary classification model, the outputs should be between 0 and 1. If you train a linear model, it might result in negative values or values &gt;1.\nThis means we can improve the loss by just clipping to ensure they stay between 0 and 1.\nThis is the idea behind the sigmoid function for output layers: smaller values will tend to 0 and larger values will tend to 1. In general, for any binary classification model, we should always have a sigmoid as the final layer. If the model isn’t training well, it’s worth checking that the final activation function is a sigmoid.\nThe sigmoid function is defined as: \\[ y = \\frac{1}{1+e^{-x}} \\]\n\n\n\nIn general, the middle layers of a neural network are similar between different problems.\nThe input layer will depend on the data for our specific problem. The output will depend on the target for our specific problem.\nSo we spend most of our time thinking about the correct input and output layers, and the hidden layers are less important."
  },
  {
    "objectID": "posts/ml/fastai/lesson5/lesson.html#using-a-framework",
    "href": "posts/ml/fastai/lesson5/lesson.html#using-a-framework",
    "title": "FastAI Lesson 5: Natural Language Processing",
    "section": "",
    "text": "When creating the models from scratch, there was a lot of boilerplate code to:\n\nImpute missing values using the “obvious” method (fill with mode)\nNormalise continuous variables to be between 0 and 1\nOne-hot encode categorical variables\nRepeat all of these steps in the same order for the test set\n\nThe benefits of using a framework like fastai:\n\nLess boilerplate, so the obvious things are done automatically unless you say otherwise\nRepeating all of the feature engineering steps on the output is trivial with DataLoaders"
  },
  {
    "objectID": "posts/ml/fastai/lesson5/lesson.html#ensembles",
    "href": "posts/ml/fastai/lesson5/lesson.html#ensembles",
    "title": "FastAI Lesson 5: Natural Language Processing",
    "section": "",
    "text": "Creating independent models and taking the mean of their predictions improves the accuracy. There are a few different approaches to ensembling a categorical variable:\n\nTake the mean of the predictions (binary 0 or 1)\nTake the mean of the probabilities (continuous between 0 and 1) then threshold the result\nTake the mode of the predictions (binary 0 or 1)\n\nIn general the mean approaches work better but there’s no rule as to why, so try all of them."
  },
  {
    "objectID": "posts/ml/fastai/lesson5/lesson.html#references",
    "href": "posts/ml/fastai/lesson5/lesson.html#references",
    "title": "FastAI Lesson 5: Natural Language Processing",
    "section": "",
    "text": "Course lesson page\nNumpy broadcasting rules\nAPL"
  },
  {
    "objectID": "posts/ml/fastai/lesson3/lesson.html",
    "href": "posts/ml/fastai/lesson3/lesson.html",
    "title": "FastAI Lesson 3: How Does a Neural Network Learn?",
    "section": "",
    "text": "These are notes from lesson 3 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\nRecreate the spreadsheet to train a linear model and a neural network from scratch: see spreadsheet\n\n\n\n\nSome options for cloud environments: Kaggle, Colab, Paperspace.\nA comparison of performance vs training time for different image models useful for model selection is provided here. Resnet and convnext are generally good to start with.\nThe best practice is to start with a “good enough” model with a quick training time, so you can iterate quickly. Then as you get further on with your research and need a better model, you can move to a slower, more accurate model.\nThe criteria we generally care about are:\n\nHow fast are they\nHow much memory do they use\nHow accurate are they\n\nA learner object contains the pre-processing steps and the model itself.\n\n\n\nHow do we fit a function to data? An ML model is just a function fitted to data. Deep learning is just fitting an infinitely flexible model to data.\nIn this notebook there is an interactive cell to fit a quadratic function to some noisy data: y = ax^2 +bx + c\nWe can vary a, b and c to get a better fit by eye. We can make this more rigorous by defining a loss function to quantify how good the fit is.\nIn this case, use the mean absolute error: mae = mean(abs(actual - preds)) We can then use stochastic gradient descent to autome the tweaking of a, b and c to minimise the loss.\nWe can store the parameters as a tensor, then pytorch will calculate gradients of the loss function based on that tensor.\nabc = torch.tensor([1.1,1.1,1.1]) \nabc.requires_grad_()  # Modifies abc in place so that it will include gradient calculations on anything which uses abc downstream\nloss = quad_mae(abc)  # `loss` uses abc so pytorch will give the gradient of loss too\nloss.backward()  # Back-propagate the gradients\nabc.grad  # Returns a tensor of the loss gradients\nWe can then take a “small step” in the direction that will decrease the gradient. The size of the step should be proportional to the size of the gradient. We define a learning rate hyperparameter to determine how much to scale the gradients by.\nlearning_rate = 0.01\nabc -= abc.grad * learning_rate\nloss = quad_mae(abc)  # The loss should now be smaller\nWe can repeat this process to take multiple steps to minimise the gradient.\nfor step in range(10):\n    loss = quad_mae(abc)\n    loss.backward()\n    with torch.no_grad():\n        abc -= abc.grad * learning_rate\n    print(f'step={step}; loss={loss:.2f}')\nThe learning rate should decrease as we get closer to the minimum to ensure we don’t overshoot the minimum and increase the loss. A learning rate schedule can be specified to do this.\n\n\n\nFor deep learning, the premise is the same but instead of quadratic functions, we fit ReLUs and other non-linear functions.\nUniversal approximation theorem states that this is infinitely expressive if enough ReLUs (or other non-linear units) are combined. This means we can learn any computable function.\nA ReLU is essentially a linear function with the negative values clipped to 0.\ndef relu(m,c,x):\n    y = m * x + c\n    return np.clip(y, 0.)\nThis is all that deep learning is! All we need is:\n\nA model - a bunch of ReLUs combined will be flexible\nA loss function - mean absolute error between the actual data values and the values predicted by the model\nAn optimiser - stochastic gradient descent can start from random weights to incrementally improve the loss until we get a “good enough” fit\n\nWe just need enough time and data. There are a few hacks to decrease the time and data required:\n\nData augmentation\nRunning on GPUs to parallelise matrix multiplications\nConvolutions to skip over values to reduce the number of matrix multiplications required\nTransfer learning - initialise with parameters from another pre-trained model instead of random weights.\n\nThis spreadsheet from the homework task is a worked example of manually training a multivariate linear model, then extending that to a neural network summing two ReLUs.\n\n\n\n\nCourse lesson page\nComparison of computer vision models\n“How does a neural network really work?” notebook"
  },
  {
    "objectID": "posts/ml/fastai/lesson3/lesson.html#model-selection",
    "href": "posts/ml/fastai/lesson3/lesson.html#model-selection",
    "title": "FastAI Lesson 3: How Does a Neural Network Learn?",
    "section": "",
    "text": "Some options for cloud environments: Kaggle, Colab, Paperspace.\nA comparison of performance vs training time for different image models useful for model selection is provided here. Resnet and convnext are generally good to start with.\nThe best practice is to start with a “good enough” model with a quick training time, so you can iterate quickly. Then as you get further on with your research and need a better model, you can move to a slower, more accurate model.\nThe criteria we generally care about are:\n\nHow fast are they\nHow much memory do they use\nHow accurate are they\n\nA learner object contains the pre-processing steps and the model itself."
  },
  {
    "objectID": "posts/ml/fastai/lesson3/lesson.html#fitting-a-quadratic-function",
    "href": "posts/ml/fastai/lesson3/lesson.html#fitting-a-quadratic-function",
    "title": "FastAI Lesson 3: How Does a Neural Network Learn?",
    "section": "",
    "text": "How do we fit a function to data? An ML model is just a function fitted to data. Deep learning is just fitting an infinitely flexible model to data.\nIn this notebook there is an interactive cell to fit a quadratic function to some noisy data: y = ax^2 +bx + c\nWe can vary a, b and c to get a better fit by eye. We can make this more rigorous by defining a loss function to quantify how good the fit is.\nIn this case, use the mean absolute error: mae = mean(abs(actual - preds)) We can then use stochastic gradient descent to autome the tweaking of a, b and c to minimise the loss.\nWe can store the parameters as a tensor, then pytorch will calculate gradients of the loss function based on that tensor.\nabc = torch.tensor([1.1,1.1,1.1]) \nabc.requires_grad_()  # Modifies abc in place so that it will include gradient calculations on anything which uses abc downstream\nloss = quad_mae(abc)  # `loss` uses abc so pytorch will give the gradient of loss too\nloss.backward()  # Back-propagate the gradients\nabc.grad  # Returns a tensor of the loss gradients\nWe can then take a “small step” in the direction that will decrease the gradient. The size of the step should be proportional to the size of the gradient. We define a learning rate hyperparameter to determine how much to scale the gradients by.\nlearning_rate = 0.01\nabc -= abc.grad * learning_rate\nloss = quad_mae(abc)  # The loss should now be smaller\nWe can repeat this process to take multiple steps to minimise the gradient.\nfor step in range(10):\n    loss = quad_mae(abc)\n    loss.backward()\n    with torch.no_grad():\n        abc -= abc.grad * learning_rate\n    print(f'step={step}; loss={loss:.2f}')\nThe learning rate should decrease as we get closer to the minimum to ensure we don’t overshoot the minimum and increase the loss. A learning rate schedule can be specified to do this."
  },
  {
    "objectID": "posts/ml/fastai/lesson3/lesson.html#fit-a-deep-learning-model",
    "href": "posts/ml/fastai/lesson3/lesson.html#fit-a-deep-learning-model",
    "title": "FastAI Lesson 3: How Does a Neural Network Learn?",
    "section": "",
    "text": "For deep learning, the premise is the same but instead of quadratic functions, we fit ReLUs and other non-linear functions.\nUniversal approximation theorem states that this is infinitely expressive if enough ReLUs (or other non-linear units) are combined. This means we can learn any computable function.\nA ReLU is essentially a linear function with the negative values clipped to 0.\ndef relu(m,c,x):\n    y = m * x + c\n    return np.clip(y, 0.)\nThis is all that deep learning is! All we need is:\n\nA model - a bunch of ReLUs combined will be flexible\nA loss function - mean absolute error between the actual data values and the values predicted by the model\nAn optimiser - stochastic gradient descent can start from random weights to incrementally improve the loss until we get a “good enough” fit\n\nWe just need enough time and data. There are a few hacks to decrease the time and data required:\n\nData augmentation\nRunning on GPUs to parallelise matrix multiplications\nConvolutions to skip over values to reduce the number of matrix multiplications required\nTransfer learning - initialise with parameters from another pre-trained model instead of random weights.\n\nThis spreadsheet from the homework task is a worked example of manually training a multivariate linear model, then extending that to a neural network summing two ReLUs."
  },
  {
    "objectID": "posts/ml/fastai/lesson3/lesson.html#references",
    "href": "posts/ml/fastai/lesson3/lesson.html#references",
    "title": "FastAI Lesson 3: How Does a Neural Network Learn?",
    "section": "",
    "text": "Course lesson page\nComparison of computer vision models\n“How does a neural network really work?” notebook"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html",
    "href": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html",
    "title": "Generative AI: Chapter 2",
    "section": "",
    "text": "These are notes from chapter 2 of Generative Deep Learning by David Foster.\n\n\n\nDeep learning is a class of ML algorithms that use multiple stacked layers of processing units to learn high-level representations from unstructured data.\n\nStructured data is naturally arranged into columns of features. Think of relational database tables.\nBy contrast, unstructured data refers to any data that is not structured in tabular format. E.g. images, audio, text. Individual pixels – or frequencies or words – are not informative alone, but higher-level representations are, so we aim to learn these.\n\n\n\nMost deep learning systems in practice are ANNs,so deep learning has become synonymous with deep deural networks, vut there are other forms such as deep belief networks.\nA neural network where all adjacent layers are fully connected is called a Multilayer Perceptron (MLP) for historical reasons.\nBatches of inputs (e.g. images) are passed through and the predicted outputs are compared to the ground truth. This gives a loss function.\nThe prediction error is then back propagated through the network to adjust the weights incrementally.\nThis allows the model to learn features without manual feature engineering. Earlier layers learn low-level features (like edges) and these are combined in intermediate leayers to learn features like teeth, through to later layers which learning high-level representations like smiling.\n\n\n\nWe will implement an MLP for a supervised (read: not generative) learning problem to classify images.\nThe following uses the CIFAR-10 dataset.\n\n\nLoad the dataset, then scale pixel values to be in the 0 to 1 range and one-hot encode the labels.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras import layers, models, optimizers, utils, datasets\n\n\n# Load the data set\n(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n\n# Scale pixel values to be between 0 and 1.\nx_train = x_train.astype(\"float32\") / 255.\nx_test = x_test.astype(\"float32\") / 255.\n\n# One-hot encode the labels. The CIFAR-10 dataset contains 10 categories.\nNUM_CLASSES = 10\ny_train = utils.to_categorical(y_train, NUM_CLASSES)\ny_test = utils.to_categorical(y_test, NUM_CLASSES)\n\n\nDownloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n170498071/170498071 [==============================] - 64s 0us/step\n\n\n\nplt.imshow(x_train[1])\n\n\n\n\n\n\n\n\n\nprint(y_train[:5])\n\n[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n\n\n\n\n\nThe model is a series of fully connected dense layers.\n\n\nCode\nINPUT_SHAPE = (32, 32, 3,)\n\ninput_layer = layers.Input(INPUT_SHAPE)\nx = layers.Flatten()(input_layer)\nx = layers.Dense(units=200, activation='relu')(x)\nx = layers.Dense(units=150, activation='relu')(x)\noutput_layer = layers.Dense(units=10, activation='softmax')(x)\n\nmodel = models.Model(input_layer, output_layer)\n\n\nThe nonlinear activation function is critical, as this ensures the model is able to learn complex functions of the input rather than simply a linear combination of inputs.\nEach unit within a given layer has a bias term that alwways output 1, which ensures that the output of a unit can be non-zero even if the inputs were all 0. This helps prevent vanishing gradients.\nThe number of parameters in the 200-unit Dense layer is thus: \\(200 * (3072 + 1) = 614600\\)\n\nmodel.summary()\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n                                                                 \n flatten (Flatten)           (None, 3072)              0         \n                                                                 \n dense (Dense)               (None, 200)               614600    \n                                                                 \n dense_1 (Dense)             (None, 150)               30150     \n                                                                 \n dense_2 (Dense)             (None, 10)                1510      \n                                                                 \n=================================================================\nTotal params: 646260 (2.47 MB)\nTrainable params: 646260 (2.47 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\nTo train the model, we require an optimizer and a loss function.\n\n\nThis is used by the network to compare predicted outputs \\(p_i\\) with ground truth labels \\(y_i\\).\nFor regression tasks use mean-squared error: \\[\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - p_i)^2\n\\]\nFor classification tasks with exclusive categories uses categorical cross-entropy: \\[\nCCE = - \\sum_{i=1}^{n} y_i \\log(p_i)\n\\]\nFor classification tasks with binary one output unit or if an observation can belong to multiple classes simultaneously use binary cross-entropy: \\[\nBCE = - \\frac{1}{n} \\sum_{i=1}^{n} (y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) )\n\\]\n\n\n\nThis is the algorithm that updates the weights of the model based on the loss function.\nAdaptive Moment Estimation (ADAM) or Root Mean Squared Propagation (RMSProp) are commonly used.\nThe learning rate is the hyperparameter that is most likely to need tweaking.\n\n\n\nThis determines gow many images are shown to the model in each training step.\nThe larger the batch size, the more stable the gradient calculation but at the expense of a slower training step.\n\nIt is recommend to increase the batch size as training progresses\n\n\n\nCode\n# WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n# opt = optimizers.Adam(learning_rate=0.0005)\nopt = optimizers.legacy.Adam(learning_rate=0.0005)\n\nmodel.compile(\n    loss=\"categorical_crossentropy\",\n    optimizer=opt,\n    metrics=[\"accuracy\"]\n)\n\n\n\n\nCode\nmodel.fit(x_train, y_train, batch_size=32, epochs=10, shuffle=True)\n\n\nEpoch 1/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.8519 - accuracy: 0.3320\nEpoch 2/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.6692 - accuracy: 0.4044\nEpoch 3/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.5840 - accuracy: 0.4360\nEpoch 4/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.5353 - accuracy: 0.4532\nEpoch 5/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4925 - accuracy: 0.4711\nEpoch 6/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4675 - accuracy: 0.4764\nEpoch 7/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4386 - accuracy: 0.4878\nEpoch 8/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4146 - accuracy: 0.4971\nEpoch 9/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.3917 - accuracy: 0.5055\nEpoch 10/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.3691 - accuracy: 0.5124\n\n\n&lt;keras.src.callbacks.History at 0x149e41610&gt;\n\n\n\n\n\n\nUse the model to predict the classification of unseen images from the test set.\nEvaluate the model loss and accuracy over the test set:\n\n\nCode\nmodel.evaluate(x_test, y_test)\n\n\n313/313 [==============================] - 0s 560us/step - loss: 1.4571 - accuracy: 0.4813\n\n\n[1.4570728540420532, 0.4812999963760376]\n\n\n\n\nCode\n# Look at the predictions and ground truth labels from the test set\nCLASSES = np.array(\n    [\n        \"airplane\",\n        \"automobile\",\n        \"bird\",\n        \"cat\",\n        \"deer\",\n        \"dog\",\n        \"frog\",\n        \"horse\",\n        \"ship\",\n        \"truck\",\n    ]\n)\n\ny_pred = model.predict(x_test)\ny_pred_single = CLASSES[np.argmax(y_pred, axis=1)]\ny_test_single = CLASSES[np.argmax(y_test, axis=1)]\n\n\n313/313 [==============================] - 0s 493us/step\n\n\nPlot a comparison of a random selection of test images.\n\n\nCode\nN_IMAGES = 10\nindices = np.random.choice(range(len(x_test)), N_IMAGES)\n\nfig = plt.figure(figsize=(15, 3))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i, idx in enumerate(indices):\n    img = x_test[idx]\n    ax = fig.add_subplot(1, N_IMAGES, i + 1)\n    ax.axis(\"off\")\n    ax.text(\n        0.5,\n        -0.35,\n        \"pred = \" + str(y_pred_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.text(\n        0.5,\n        -0.7,\n        \"act = \" + str(y_test_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.imshow(img)\n\n\n\n\n\n\n\n\n\n\n\n\n\nA simple MLP does not take into account the spatial structure of images, so we can improve performance by using a convolutional layer.\nThis slides a kernel or filter (say, a 3x3 window) over the image and multiply the values elementwise.\nA convolutional layer is a collection of kernels where the (initially random) weights are learned through training.\n\n\nThis is the step size used when moving the kernel window across the image.\nThis determines the output size of the layer. For example, a stride=2 layer will half the height and width of the input image.\n\n\n\nWhen stride=1, we may still end up with a smaller images as the kernel does not overhang the edges of the image.\nUsing padding='same' adds 0s around the border to ensure that the output size is the same as the input size in this case (i.e. it lets the kernel overhang the edges of the original image). This helps to keep track of the size of the tensor.\n\n\n\nThe output of a convolutional layer is another 4-dimensional tensor with shape (batch_size, height, width, num_kernels).\nSo we can stack convolutional layers in series to learn higher-level representations.\n\n\n\nThe exploding gradient problem is common in deep learning: when errors are back-propagated, the gradient values in earlier layers can grow exponentially large.\nIn practice, this can cause overflow errors resulting in the loss function returning NaN.\nThe input layer is scaled, so we may naively expect the activations of all future layers to be well-scaled too. This may initially be true, but as training moves the weights further from their initial values the assumption of outputs being well-scaled breaks down. This is called covariate shift.\nBatch normalization z-scores each input channel, i.e. it subtract the mean and divides by the standard deviation of each mini-batch \\(B\\): \\[\n\\hat{x_i} = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_{B}^2 + \\epsilon}}\n\\]\nWe then learn two parameters, scale \\(\\gamma\\) and shift \\(\\beta\\). Then the output of the BatchNormalization layer is: \\[\ny_i = \\gamma \\hat{x_i} + \\beta\n\\]\nAt test time we may predict only a single image, so the batch normalization value would not be meaningful. Instead, the BatchNormalization layer also stores the moving average of \\(\\gamma\\) and \\(\\beta\\) as tow additional (derived therefore non-trainable) parameters. These moving average values are used at test time.\nBatch normalization has the added benefit of reducing overfitting in practice.\nUsually, batch normalization layers are placed before activation layers, although this is a matter of preference. The acronym BAD can be helpful to remember the general order: Batchnorm, Activation, Dropout.\n\n\n\nWe want to ensure that the model generalises well to unseen data, so it cannot rely on any single layer or neuron too heavily. Dropout acts as a regularization technique to prevent overfitting.\nEach Dropout layer chooses a random set of units from the previous layer and sets their value to 0\nAt test time the Dropout layer does not drop any connections, utilising the full network.\n\n\n\n\n\n\nThis is the same data set as the MLP case so we can re-use the same steps.\n\n\n\nThis is a sequence of stacks containing Conv2D layers with nonlinear acitvation functions.\n\n\nCode\ninput_layer = layers.Input((32, 32, 3))\n\n# Stack 1\nx = layers.Conv2D(filters=32, kernel_size=3, strides=1, padding=\"same\")(\n    input_layer\n)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Stack 2\nx = layers.Conv2D(filters=32, kernel_size=3, strides=2, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Stack 3\nx = layers.Conv2D(filters=64, kernel_size=3, strides=1, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Stack 4\nx = layers.Conv2D(filters=64, kernel_size=3, strides=2, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Dense final layers\nx = layers.Flatten()(x)\nx = layers.Dense(128)(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\nx = layers.Dropout(rate=0.5)(x)\nx = layers.Dense(NUM_CLASSES)(x)\n\n# Output\noutput_layer = layers.Activation(\"softmax\")(x)\nmodel = models.Model(input_layer, output_layer)\n\n\n\nmodel.summary()\n\nModel: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_4 (InputLayer)        [(None, 32, 32, 3)]       0         \n                                                                 \n conv2d_4 (Conv2D)           (None, 32, 32, 32)        896       \n                                                                 \n batch_normalization_5 (Bat  (None, 32, 32, 32)        128       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_5 (LeakyReLU)   (None, 32, 32, 32)        0         \n                                                                 \n conv2d_5 (Conv2D)           (None, 16, 16, 32)        9248      \n                                                                 \n batch_normalization_6 (Bat  (None, 16, 16, 32)        128       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_6 (LeakyReLU)   (None, 16, 16, 32)        0         \n                                                                 \n conv2d_6 (Conv2D)           (None, 16, 16, 64)        18496     \n                                                                 \n batch_normalization_7 (Bat  (None, 16, 16, 64)        256       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_7 (LeakyReLU)   (None, 16, 16, 64)        0         \n                                                                 \n conv2d_7 (Conv2D)           (None, 8, 8, 64)          36928     \n                                                                 \n batch_normalization_8 (Bat  (None, 8, 8, 64)          256       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_8 (LeakyReLU)   (None, 8, 8, 64)          0         \n                                                                 \n flatten_2 (Flatten)         (None, 4096)              0         \n                                                                 \n dense_5 (Dense)             (None, 128)               524416    \n                                                                 \n batch_normalization_9 (Bat  (None, 128)               512       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_9 (LeakyReLU)   (None, 128)               0         \n                                                                 \n dropout_1 (Dropout)         (None, 128)               0         \n                                                                 \n dense_6 (Dense)             (None, 10)                1290      \n                                                                 \n activation_1 (Activation)   (None, 10)                0         \n                                                                 \n=================================================================\nTotal params: 592554 (2.26 MB)\nTrainable params: 591914 (2.26 MB)\nNon-trainable params: 640 (2.50 KB)\n_________________________________________________________________\n\n\n\n\n\nThis is identical to the previous MLP model\n\n\nCode\n# WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n# opt = optimizers.Adam(learning_rate=0.0005)\nopt = optimizers.legacy.Adam(learning_rate=0.0005)\n\nmodel.compile(\n    loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"]\n)\n\n\n\n\nCode\nmodel.fit(\n    x_train,\n    y_train,\n    batch_size=32,\n    epochs=10,\n    shuffle=True,\n    validation_data=(x_test, y_test),\n)\n\n\nEpoch 1/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 1.5393 - accuracy: 0.4599 - val_loss: 1.5656 - val_accuracy: 0.4825\nEpoch 2/10\n1563/1563 [==============================] - 31s 20ms/step - loss: 1.1390 - accuracy: 0.5980 - val_loss: 1.2575 - val_accuracy: 0.5584\nEpoch 3/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 0.9980 - accuracy: 0.6515 - val_loss: 0.9970 - val_accuracy: 0.6513\nEpoch 4/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 0.9146 - accuracy: 0.6798 - val_loss: 0.9783 - val_accuracy: 0.6613\nEpoch 5/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.8515 - accuracy: 0.7044 - val_loss: 0.8269 - val_accuracy: 0.7094\nEpoch 6/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 0.7940 - accuracy: 0.7230 - val_loss: 0.8417 - val_accuracy: 0.7102\nEpoch 7/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.7511 - accuracy: 0.7362 - val_loss: 0.8542 - val_accuracy: 0.7044\nEpoch 8/10\n1563/1563 [==============================] - 29s 18ms/step - loss: 0.7167 - accuracy: 0.7495 - val_loss: 0.8554 - val_accuracy: 0.7083\nEpoch 9/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.6833 - accuracy: 0.7622 - val_loss: 1.0474 - val_accuracy: 0.6651\nEpoch 10/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.6510 - accuracy: 0.7726 - val_loss: 0.8449 - val_accuracy: 0.7020\n\n\n&lt;keras.src.callbacks.History at 0x14aa81d50&gt;\n\n\n\n\n\nAs before, we use the model to predict the classification of unseen images from the test set.\nEvaluate the model loss and accuracy over the test set:\n\n\nCode\nmodel.evaluate(x_test, y_test)\n\n\n313/313 [==============================] - 2s 5ms/step - loss: 0.8449 - accuracy: 0.7020\n\n\n[0.8448793888092041, 0.7020000219345093]\n\n\n\n\nCode\ny_pred = model.predict(x_test)\ny_pred_single = CLASSES[np.argmax(y_pred, axis=1)]\ny_test_single = CLASSES[np.argmax(y_test, axis=1)]\n\n\n313/313 [==============================] - 2s 5ms/step\n\n\nPlot a comparison of a random selection of test images.\n\n\nCode\nN_IMAGES = 10\nindices = np.random.choice(range(len(x_test)), N_IMAGES)\n\nfig = plt.figure(figsize=(15, 3))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i, idx in enumerate(indices):\n    img = x_test[idx]\n    ax = fig.add_subplot(1, N_IMAGES, i + 1)\n    ax.axis(\"off\")\n    ax.text(\n        0.5,\n        -0.35,\n        \"pred = \" + str(y_pred_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.text(\n        0.5,\n        -0.7,\n        \"act = \" + str(y_test_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.imshow(img)"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#what-is-deep-learning",
    "href": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#what-is-deep-learning",
    "title": "Generative AI: Chapter 2",
    "section": "",
    "text": "Deep learning is a class of ML algorithms that use multiple stacked layers of processing units to learn high-level representations from unstructured data.\n\nStructured data is naturally arranged into columns of features. Think of relational database tables.\nBy contrast, unstructured data refers to any data that is not structured in tabular format. E.g. images, audio, text. Individual pixels – or frequencies or words – are not informative alone, but higher-level representations are, so we aim to learn these."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#deep-neural-networks",
    "href": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#deep-neural-networks",
    "title": "Generative AI: Chapter 2",
    "section": "",
    "text": "Most deep learning systems in practice are ANNs,so deep learning has become synonymous with deep deural networks, vut there are other forms such as deep belief networks.\nA neural network where all adjacent layers are fully connected is called a Multilayer Perceptron (MLP) for historical reasons.\nBatches of inputs (e.g. images) are passed through and the predicted outputs are compared to the ground truth. This gives a loss function.\nThe prediction error is then back propagated through the network to adjust the weights incrementally.\nThis allows the model to learn features without manual feature engineering. Earlier layers learn low-level features (like edges) and these are combined in intermediate leayers to learn features like teeth, through to later layers which learning high-level representations like smiling."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#building-a-multilayer-perceptron",
    "href": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#building-a-multilayer-perceptron",
    "title": "Generative AI: Chapter 2",
    "section": "",
    "text": "We will implement an MLP for a supervised (read: not generative) learning problem to classify images.\nThe following uses the CIFAR-10 dataset.\n\n\nLoad the dataset, then scale pixel values to be in the 0 to 1 range and one-hot encode the labels.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras import layers, models, optimizers, utils, datasets\n\n\n# Load the data set\n(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n\n# Scale pixel values to be between 0 and 1.\nx_train = x_train.astype(\"float32\") / 255.\nx_test = x_test.astype(\"float32\") / 255.\n\n# One-hot encode the labels. The CIFAR-10 dataset contains 10 categories.\nNUM_CLASSES = 10\ny_train = utils.to_categorical(y_train, NUM_CLASSES)\ny_test = utils.to_categorical(y_test, NUM_CLASSES)\n\n\nDownloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n170498071/170498071 [==============================] - 64s 0us/step\n\n\n\nplt.imshow(x_train[1])\n\n\n\n\n\n\n\n\n\nprint(y_train[:5])\n\n[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n\n\n\n\n\nThe model is a series of fully connected dense layers.\n\n\nCode\nINPUT_SHAPE = (32, 32, 3,)\n\ninput_layer = layers.Input(INPUT_SHAPE)\nx = layers.Flatten()(input_layer)\nx = layers.Dense(units=200, activation='relu')(x)\nx = layers.Dense(units=150, activation='relu')(x)\noutput_layer = layers.Dense(units=10, activation='softmax')(x)\n\nmodel = models.Model(input_layer, output_layer)\n\n\nThe nonlinear activation function is critical, as this ensures the model is able to learn complex functions of the input rather than simply a linear combination of inputs.\nEach unit within a given layer has a bias term that alwways output 1, which ensures that the output of a unit can be non-zero even if the inputs were all 0. This helps prevent vanishing gradients.\nThe number of parameters in the 200-unit Dense layer is thus: \\(200 * (3072 + 1) = 614600\\)\n\nmodel.summary()\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n                                                                 \n flatten (Flatten)           (None, 3072)              0         \n                                                                 \n dense (Dense)               (None, 200)               614600    \n                                                                 \n dense_1 (Dense)             (None, 150)               30150     \n                                                                 \n dense_2 (Dense)             (None, 10)                1510      \n                                                                 \n=================================================================\nTotal params: 646260 (2.47 MB)\nTrainable params: 646260 (2.47 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\nTo train the model, we require an optimizer and a loss function.\n\n\nThis is used by the network to compare predicted outputs \\(p_i\\) with ground truth labels \\(y_i\\).\nFor regression tasks use mean-squared error: \\[\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - p_i)^2\n\\]\nFor classification tasks with exclusive categories uses categorical cross-entropy: \\[\nCCE = - \\sum_{i=1}^{n} y_i \\log(p_i)\n\\]\nFor classification tasks with binary one output unit or if an observation can belong to multiple classes simultaneously use binary cross-entropy: \\[\nBCE = - \\frac{1}{n} \\sum_{i=1}^{n} (y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) )\n\\]\n\n\n\nThis is the algorithm that updates the weights of the model based on the loss function.\nAdaptive Moment Estimation (ADAM) or Root Mean Squared Propagation (RMSProp) are commonly used.\nThe learning rate is the hyperparameter that is most likely to need tweaking.\n\n\n\nThis determines gow many images are shown to the model in each training step.\nThe larger the batch size, the more stable the gradient calculation but at the expense of a slower training step.\n\nIt is recommend to increase the batch size as training progresses\n\n\n\nCode\n# WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n# opt = optimizers.Adam(learning_rate=0.0005)\nopt = optimizers.legacy.Adam(learning_rate=0.0005)\n\nmodel.compile(\n    loss=\"categorical_crossentropy\",\n    optimizer=opt,\n    metrics=[\"accuracy\"]\n)\n\n\n\n\nCode\nmodel.fit(x_train, y_train, batch_size=32, epochs=10, shuffle=True)\n\n\nEpoch 1/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.8519 - accuracy: 0.3320\nEpoch 2/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.6692 - accuracy: 0.4044\nEpoch 3/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.5840 - accuracy: 0.4360\nEpoch 4/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.5353 - accuracy: 0.4532\nEpoch 5/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4925 - accuracy: 0.4711\nEpoch 6/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4675 - accuracy: 0.4764\nEpoch 7/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4386 - accuracy: 0.4878\nEpoch 8/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4146 - accuracy: 0.4971\nEpoch 9/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.3917 - accuracy: 0.5055\nEpoch 10/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.3691 - accuracy: 0.5124\n\n\n&lt;keras.src.callbacks.History at 0x149e41610&gt;\n\n\n\n\n\n\nUse the model to predict the classification of unseen images from the test set.\nEvaluate the model loss and accuracy over the test set:\n\n\nCode\nmodel.evaluate(x_test, y_test)\n\n\n313/313 [==============================] - 0s 560us/step - loss: 1.4571 - accuracy: 0.4813\n\n\n[1.4570728540420532, 0.4812999963760376]\n\n\n\n\nCode\n# Look at the predictions and ground truth labels from the test set\nCLASSES = np.array(\n    [\n        \"airplane\",\n        \"automobile\",\n        \"bird\",\n        \"cat\",\n        \"deer\",\n        \"dog\",\n        \"frog\",\n        \"horse\",\n        \"ship\",\n        \"truck\",\n    ]\n)\n\ny_pred = model.predict(x_test)\ny_pred_single = CLASSES[np.argmax(y_pred, axis=1)]\ny_test_single = CLASSES[np.argmax(y_test, axis=1)]\n\n\n313/313 [==============================] - 0s 493us/step\n\n\nPlot a comparison of a random selection of test images.\n\n\nCode\nN_IMAGES = 10\nindices = np.random.choice(range(len(x_test)), N_IMAGES)\n\nfig = plt.figure(figsize=(15, 3))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i, idx in enumerate(indices):\n    img = x_test[idx]\n    ax = fig.add_subplot(1, N_IMAGES, i + 1)\n    ax.axis(\"off\")\n    ax.text(\n        0.5,\n        -0.35,\n        \"pred = \" + str(y_pred_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.text(\n        0.5,\n        -0.7,\n        \"act = \" + str(y_test_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.imshow(img)"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#cnn-concepts-and-layers",
    "href": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#cnn-concepts-and-layers",
    "title": "Generative AI: Chapter 2",
    "section": "",
    "text": "A simple MLP does not take into account the spatial structure of images, so we can improve performance by using a convolutional layer.\nThis slides a kernel or filter (say, a 3x3 window) over the image and multiply the values elementwise.\nA convolutional layer is a collection of kernels where the (initially random) weights are learned through training.\n\n\nThis is the step size used when moving the kernel window across the image.\nThis determines the output size of the layer. For example, a stride=2 layer will half the height and width of the input image.\n\n\n\nWhen stride=1, we may still end up with a smaller images as the kernel does not overhang the edges of the image.\nUsing padding='same' adds 0s around the border to ensure that the output size is the same as the input size in this case (i.e. it lets the kernel overhang the edges of the original image). This helps to keep track of the size of the tensor.\n\n\n\nThe output of a convolutional layer is another 4-dimensional tensor with shape (batch_size, height, width, num_kernels).\nSo we can stack convolutional layers in series to learn higher-level representations.\n\n\n\nThe exploding gradient problem is common in deep learning: when errors are back-propagated, the gradient values in earlier layers can grow exponentially large.\nIn practice, this can cause overflow errors resulting in the loss function returning NaN.\nThe input layer is scaled, so we may naively expect the activations of all future layers to be well-scaled too. This may initially be true, but as training moves the weights further from their initial values the assumption of outputs being well-scaled breaks down. This is called covariate shift.\nBatch normalization z-scores each input channel, i.e. it subtract the mean and divides by the standard deviation of each mini-batch \\(B\\): \\[\n\\hat{x_i} = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_{B}^2 + \\epsilon}}\n\\]\nWe then learn two parameters, scale \\(\\gamma\\) and shift \\(\\beta\\). Then the output of the BatchNormalization layer is: \\[\ny_i = \\gamma \\hat{x_i} + \\beta\n\\]\nAt test time we may predict only a single image, so the batch normalization value would not be meaningful. Instead, the BatchNormalization layer also stores the moving average of \\(\\gamma\\) and \\(\\beta\\) as tow additional (derived therefore non-trainable) parameters. These moving average values are used at test time.\nBatch normalization has the added benefit of reducing overfitting in practice.\nUsually, batch normalization layers are placed before activation layers, although this is a matter of preference. The acronym BAD can be helpful to remember the general order: Batchnorm, Activation, Dropout.\n\n\n\nWe want to ensure that the model generalises well to unseen data, so it cannot rely on any single layer or neuron too heavily. Dropout acts as a regularization technique to prevent overfitting.\nEach Dropout layer chooses a random set of units from the previous layer and sets their value to 0\nAt test time the Dropout layer does not drop any connections, utilising the full network."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#building-a-convolutional-neural-network",
    "href": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#building-a-convolutional-neural-network",
    "title": "Generative AI: Chapter 2",
    "section": "",
    "text": "This is the same data set as the MLP case so we can re-use the same steps.\n\n\n\nThis is a sequence of stacks containing Conv2D layers with nonlinear acitvation functions.\n\n\nCode\ninput_layer = layers.Input((32, 32, 3))\n\n# Stack 1\nx = layers.Conv2D(filters=32, kernel_size=3, strides=1, padding=\"same\")(\n    input_layer\n)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Stack 2\nx = layers.Conv2D(filters=32, kernel_size=3, strides=2, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Stack 3\nx = layers.Conv2D(filters=64, kernel_size=3, strides=1, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Stack 4\nx = layers.Conv2D(filters=64, kernel_size=3, strides=2, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Dense final layers\nx = layers.Flatten()(x)\nx = layers.Dense(128)(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\nx = layers.Dropout(rate=0.5)(x)\nx = layers.Dense(NUM_CLASSES)(x)\n\n# Output\noutput_layer = layers.Activation(\"softmax\")(x)\nmodel = models.Model(input_layer, output_layer)\n\n\n\nmodel.summary()\n\nModel: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_4 (InputLayer)        [(None, 32, 32, 3)]       0         \n                                                                 \n conv2d_4 (Conv2D)           (None, 32, 32, 32)        896       \n                                                                 \n batch_normalization_5 (Bat  (None, 32, 32, 32)        128       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_5 (LeakyReLU)   (None, 32, 32, 32)        0         \n                                                                 \n conv2d_5 (Conv2D)           (None, 16, 16, 32)        9248      \n                                                                 \n batch_normalization_6 (Bat  (None, 16, 16, 32)        128       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_6 (LeakyReLU)   (None, 16, 16, 32)        0         \n                                                                 \n conv2d_6 (Conv2D)           (None, 16, 16, 64)        18496     \n                                                                 \n batch_normalization_7 (Bat  (None, 16, 16, 64)        256       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_7 (LeakyReLU)   (None, 16, 16, 64)        0         \n                                                                 \n conv2d_7 (Conv2D)           (None, 8, 8, 64)          36928     \n                                                                 \n batch_normalization_8 (Bat  (None, 8, 8, 64)          256       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_8 (LeakyReLU)   (None, 8, 8, 64)          0         \n                                                                 \n flatten_2 (Flatten)         (None, 4096)              0         \n                                                                 \n dense_5 (Dense)             (None, 128)               524416    \n                                                                 \n batch_normalization_9 (Bat  (None, 128)               512       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_9 (LeakyReLU)   (None, 128)               0         \n                                                                 \n dropout_1 (Dropout)         (None, 128)               0         \n                                                                 \n dense_6 (Dense)             (None, 10)                1290      \n                                                                 \n activation_1 (Activation)   (None, 10)                0         \n                                                                 \n=================================================================\nTotal params: 592554 (2.26 MB)\nTrainable params: 591914 (2.26 MB)\nNon-trainable params: 640 (2.50 KB)\n_________________________________________________________________\n\n\n\n\n\nThis is identical to the previous MLP model\n\n\nCode\n# WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n# opt = optimizers.Adam(learning_rate=0.0005)\nopt = optimizers.legacy.Adam(learning_rate=0.0005)\n\nmodel.compile(\n    loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"]\n)\n\n\n\n\nCode\nmodel.fit(\n    x_train,\n    y_train,\n    batch_size=32,\n    epochs=10,\n    shuffle=True,\n    validation_data=(x_test, y_test),\n)\n\n\nEpoch 1/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 1.5393 - accuracy: 0.4599 - val_loss: 1.5656 - val_accuracy: 0.4825\nEpoch 2/10\n1563/1563 [==============================] - 31s 20ms/step - loss: 1.1390 - accuracy: 0.5980 - val_loss: 1.2575 - val_accuracy: 0.5584\nEpoch 3/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 0.9980 - accuracy: 0.6515 - val_loss: 0.9970 - val_accuracy: 0.6513\nEpoch 4/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 0.9146 - accuracy: 0.6798 - val_loss: 0.9783 - val_accuracy: 0.6613\nEpoch 5/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.8515 - accuracy: 0.7044 - val_loss: 0.8269 - val_accuracy: 0.7094\nEpoch 6/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 0.7940 - accuracy: 0.7230 - val_loss: 0.8417 - val_accuracy: 0.7102\nEpoch 7/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.7511 - accuracy: 0.7362 - val_loss: 0.8542 - val_accuracy: 0.7044\nEpoch 8/10\n1563/1563 [==============================] - 29s 18ms/step - loss: 0.7167 - accuracy: 0.7495 - val_loss: 0.8554 - val_accuracy: 0.7083\nEpoch 9/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.6833 - accuracy: 0.7622 - val_loss: 1.0474 - val_accuracy: 0.6651\nEpoch 10/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.6510 - accuracy: 0.7726 - val_loss: 0.8449 - val_accuracy: 0.7020\n\n\n&lt;keras.src.callbacks.History at 0x14aa81d50&gt;\n\n\n\n\n\nAs before, we use the model to predict the classification of unseen images from the test set.\nEvaluate the model loss and accuracy over the test set:\n\n\nCode\nmodel.evaluate(x_test, y_test)\n\n\n313/313 [==============================] - 2s 5ms/step - loss: 0.8449 - accuracy: 0.7020\n\n\n[0.8448793888092041, 0.7020000219345093]\n\n\n\n\nCode\ny_pred = model.predict(x_test)\ny_pred_single = CLASSES[np.argmax(y_pred, axis=1)]\ny_test_single = CLASSES[np.argmax(y_test, axis=1)]\n\n\n313/313 [==============================] - 2s 5ms/step\n\n\nPlot a comparison of a random selection of test images.\n\n\nCode\nN_IMAGES = 10\nindices = np.random.choice(range(len(x_test)), N_IMAGES)\n\nfig = plt.figure(figsize=(15, 3))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i, idx in enumerate(indices):\n    img = x_test[idx]\n    ax = fig.add_subplot(1, N_IMAGES, i + 1)\n    ax.axis(\"off\")\n    ax.text(\n        0.5,\n        -0.35,\n        \"pred = \" + str(y_pred_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.text(\n        0.5,\n        -0.7,\n        \"act = \" + str(y_test_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.imshow(img)"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html",
    "href": "posts/software/system_design/system_design_notes.html",
    "title": "System Design Notes",
    "section": "",
    "text": "Steps:\n\nRequirements engineering\nCapacity estimation\nData modeling\nAPI design\nSystem design\nDesign discussion\n\n\n\nFunctional and non-functional requirements. Scale of the system.\n\n\nWhat the system should do.\n\nWhich problem does the system solve\nWhich features are essential to solve these problems\n\nCore features: bare minimum required to solve the user’s problem. Support features: features that help the user solve their problem more conveniently.\n\n\n\nWhat the system should handle.\n\n\n\n\n\n\nInterview Tips\n\n\n\n\nIdentify the non-functional requirements the interviewer cares most about\nShow awareness of system attributes, trade-offs and user experience\n\n\n\nSystem analysis:\n\nRead or write heavy?\n\nImpacts requirements for database scaling, redundancy of services, effectiveness of caching.\nSystem priorities - is it worse if the system fails to read or to write?\n\nMonolithic or distributed architecture?\n\nScale is the key consideration. Large scale applications must be distributed, smaller scale can be single server.\n\nData availability vs consistency\n\nCAP trilemma - A system can have only two of: Consistency, Availability, Partition tolerance\nChoice determines the impact of a network failure\n\n\nNon-functional requirements:\n\nAvailability - How long is the system up and running per year?\n\nAvailability of a system is the product of its components’ availability\nReliability, redundancy, fault tolerance\n\nConsistency - Data appears the same on all nodes regardless of the user and location.\n\nLinearizability\n\nScalability - Ability of a system to handle fast-growing user base and temporary load spikes\n\nVertical scaling - single server that we add more CPU/RAM to\n\nPros: fast inter-process communication, data consistency\nCons: single point of failure, hardware limits on maximum scale\n\nHorizontal scaling - cluster of servers in parallel\n\nPros: redundancy improves availability, linear cost of scaling\nCons: complex architecture, data inconsistency\n\n\nLatency - Amount of time taken for a single message to be delivered\n\nExperienced latency = network latency + system latency\nDatabase and data model choices affect latency\nCaching can be effective\n\nCompatibility - Ability of a system to operate seamlessly with other software, hardware and systems\nSecurity\n\nBasic security measures: TLS encrypted network traffic, API keys for rate limiting\n\n\nNon-functional requirements depend heavily on expected scale. The following help quantify expected scale, and relate to capacity estimation in the next step.\n\nDaily active users (DAU)\nPeak active users\nInteractions per user - including read/write ratio\nRequest size\nReplication factor - determines the storage requirement (typically 3x)\n\n\n\n\n\n\n\n\n\n\n\nInterview Tips\n\n\n\n\nSimplify wherever you can - simple assumptions and round heavily\nConvert all numbers to scientific notation\nKnow powers of 2\n\n\n\n\n\nRequests per second (RPS) is the key measure.\nRequests per day = Daily active users * Activities per user\nRequests per second = RPD / 10^5\nPeak load = RPS * Peak load factor\nRead/write ratio is important to get the total requests. Usually, information on either reads or writes is missing and must be inferred using the other.\nTotal RPS = Read RPS + Write RPS\n\n\n\nThe amount of data that can be transmitted between two nodes in a fixed period of time. Bits per second.\nThroughput, bandwidth and latency are all related and can be thought of with the motorway analogy.\n\nThroughput is the total number of vehicles that must pass through that road per day.\nBandwidth is the number of lanes.\nLatency is the time taken to get from one end of the road to another.\n\nThe bandwidth must be large enough to support the required throughput.\nBandwidth = Total RPS * Request size\nEven high bandwidth can result in low latency if there is an unexpectedly high peak throughput. Rush hour traffic jam.\nRequest size can be varied according to bandwidth, e.g. YouTube lowers the resolution if the connection is slow.\n\n\n\nMeasured in bits per second using the Write RPS. Long term storage (assuming the same user base) is also helpful.\nStorage capacity per second = Write RPS * Request size * Replication factor\nStorage capacity in 5 years = Storage capacity per second * 2*10^8 \n\n\n\n\nKey concepts are: entities, attributes, relations.\nOptimisation discussions are anchored by the data model.\nRelational databases: denormalization, SQL tuning, sharding, federation\nNon-relational databases: indexing, caching\nThe data model is not a full-blown data schema (which only applies to relational databases) but a more informal, high-level version.\nMust haves:\n\nDerive entities and attributes.\nDraw connections between entities. Then:\nSelect a database based on the requirements\nExtend data model to a proper data schema\nIdentify bottlenecks and apply quick fixes Later:\nDefer any detailed discussions to the design discussion section.\n\n\n\nEntities are “things” with a distinct and independent existence, e.g. users, files, tweets, posts, channels. Create a unique table for each entity.\nAttributes describe properties of an entity, e.g. a user’s name, date of birth. These are the fields in the table of that entity.\n\n\n\n\n\n\nInterview Tips\n\n\n\n\nDo not focus on irrelevant details\nDo not miss any critical functionality\nDouble check all tables after you extract all info from requirements\n\n\n\n\n\n\nExtract connections between entities from the requirements. Relationships can be one-to-one, one-to-many or many-to-many.\n\n\n\n\nSpecify the API endpoints with:\n\nFunction signature\nParameters\nResponse and status codes\n\nThis specifies a binding contract between the client and the application server. There can be APIs between every system component, but just focus on the client/server interface.\nProcess:\n\nRevisit the functional requirements\nDerive the goal of each endpoint\nDerive the signature from the goal and the inputs from the data model\nDefine the response outputs\n\nNaming conventions. Use HTTP request types in the call signature where appropriate: GET, POST, PUT, PATCH, DELETE\n\n\n\n\n\n\nInterview Tips\n\n\n\n\nAvoid vague inputs\nDon’t add extra parameters that are out of scope\nAvoid redundant parameters\nDon’t miss any parameters\nConfirm the output repsonse satisfies the requirements\nIdentify inefficiencies of the data structure\n\n\n\n\n\n\nSystem components:\n\nRepresentation layer\n\nWeb app\nMobile app\nConsole\n\nService (and load balancer)\nData store\n\nRelational database\n\nPros: linked tables reduce duplicate data; flexible queries\n\nKey-value store\n\nUseful for lots of small continuous reads and writes\nPros: Fast access, lightweight, highly scalable\nCons: Cannot query by value, no standard query language so data access in written in the application layer no data normalisation\n\n\n\nScaling:\n\nScale services by having multiple instances of the service with a load balancer in front\nScale database with federation (functional partitioning) - split up data by function\n\nSystem diagrams: arrows point in direction of user flow (not data flow)\n\n\n\nTypes of questions:\n\nNFR questions\n\nHow to achieve scalability\n\nIdentify each bottleneck and remedy it\n\nHow to improve resiliency\n\nJustification question\n\nChoice of components, e.g. what would change if you changed block storage for object storage or relational database\nAdditional components, e.g. is there a use case for a message queue and where\n\nExtension questions\n\nAdditional functional requirements not in the original scope\n\n\n\n\n\n\n\nThere are two parameters we can vary:\n\nLength of the key - should be limited so the URL is short enough\nRange of allowed characters - should be URL-safe\n\nUsing Base-64 encoding as the character range, then a 6-digit key length gives enough unique URLs; 64^6 = 68 billion\nEncoding options: 1. MD5 hash: not collision resistant and too long 2. Encoded counter, each URL is just an index int, and its value is the base-64 encoding of that int: length is not fixed and increases over time 3. Key range, generate all unique keys beforehand: no collisions, but storage intensive 4. Key range modified, generate 5-digit keys beforehand, then add the 6th digit on the fly.\n\n\n\n\n\n4 main operations (CRUD): Create, Read, Update, Delete.\nTransactions - group several operations together\n\nEither the entire transaction succeeds or it fails and rolls back to the initial state. Commit or abort on error.\nWithout transactions, if an individual operation fails, it can be complex to unwind the related transactions to return to the initial state.\nError handling is simpler as manual rollback of operations is not required.\nTransactions provide guarantees so we can reason about the database state before and after the transaction.\n\nTransactions enforce “ACID” guarantees:\n\nAtomicity - Transactions cannot be broken down into smaller parts.\nConsistency - All data points within the database must align to be properly read and accepted. Raise a consistency error if this is not the case. Database consistency is different from system consistency which states the data should be the same across all nodes.\nIsolation - Concurrently executing transactions are isolated from each other. Avoids race conditions if multiple clients are accessing the same database record.\nDurability - Once a transaction is committed to the database, it is permanently preserved. Backups and transaction logs can restore committed transactions in the case of a failure.\n\nRelational databases are a good fit when:\n\nData is well-structured\nUse case requires complex querying\nData consistency is important\n\nLimitations of relational databases:\n\nHorizontal scaling is complex\nDistributed databases are more complex to keep transaction guarantees\n\nTwo phase commit protocol (2PC):\n\nPrepare - Ask each node if it;s able to promise to carry out the transaction\nCommit - Block the nodes and do the commit\n\nBlocking causes problems on distributed systems where it may cause unexpected consequences if the database is unavailable for this short time.\n\n\n\n\n\nExamples of non-relational databases:\n\nKey-value store\nDocument store\nWide-column store\nGraph store\n\nThese vary a lot, and in general NoSQL simply means “no ACID guarantees”.\nTransactions were seen as the enemy of scalability, so needed to be abandoned completely for performance and scalability. ACID enforces consistency for relational databases; for non-relational databases there is eventual consistency.\nBASE:\n\nBasically Available - Always possible to read and write data even though it may not be consistent. E.g. reads may not reflect latest changes, writes may not be persisted.\nSoft state - Lack of consistency guarantees mean data state may change without any interactions with the application as the database reaches eventual consistency.\nEventual consistency - Data will eventually become consistent once inputs stops.\n\nBenefits:\n\nWithout atomicity constraint, overheads like 2PC are not required\nWithout consistency constraint, horizontal scaling is trivial\nWithout isolation constraint, no blocking is required which improves availability.\n\nNon-relational databases are a good fit when:\n\nLarge data volume that isn’t tabular\nHigh availability requirement\nLack of consistency across nodes is acceptable\n\nLimitations:\n\nConsistency is necessary for some use cases\nLack of standardisation\n\n\n\n\n\nProblem: if we have large files with small changes, we don’t want to have to re-upload and re-download the whole file every time something changes.\nSolution: identify the changes and only push/pull these. Similar to git.\nThe rsync algorithm makes it easier to compare changes by:\n\nSplit the file into blocks\nCalculate a checksum for each block (using MD5 hashing algorithm)\nTo compare files between the client and the server, we only need to send the hashes back and forth\nFor the mismatching hashes, transfer the corresponding blocks\n\n\n\n\nOne-way communication that broadcasts file updates one-to-many.\n\n\nThese are synchronous.\nPolling is an example of a pull API. We periodically query the server to see if there are any updates. If not, the server responds immediately saying there is no new data. This may result in delayed updates and can overwhelm the server with too many requests.\n\n\n\n\n\n\nFigure 1: Polling\n\n\n\n\n\n\nThese are asynchronous.\nLong-polling. The client connects to the server and makes a request. Instead of replying immediately, the server waits until there is an update and then sends the response. If there is no update for a long time, the connection times out and the client must reconnect. Server resources are tied up until the connection is closed, even if there is no update.\n\n\n\n\n\n\nFigure 2: LongPolling\n\n\n\nWebsockets. The client establishes a connection using an HTTP request and response. This establishes a TCP/IP connection for 2-way communication. This allows for real-time applications without long-polling.\n\n\n\n\n\n\nFigure 3: Web Sockets\n\n\n\nServer sent events. Event-based approach. EventSource API supported by all browsers. The connection is 1-directional; the client can only pass data to the server at the point of connection. After that, only the server can send data to the client. The server doesn’t know if the client loses connection.\n\n\n\n\n\n\nFigure 4: SSE\n\n\n\n\n\n\n\nA system component that implements asynchronous communication between services, decoupling them. An independent server which producers and consumers connect to. AKA message queues.\nMessage brokers persist the messages until there are consumers ready to receive them.\nMessage brokers are similar in principle to databases since they persist data. The differences are they: automatically delete messages after delivery, do not support queries, typically have a small working set, and notify clients about changes.\nRabbitMQ, Kafka and Redis are open source message brokers. Amazon, GCP and Azure have managed service implementations.\n\n\nA one-to-one relationship between sender and receiver. Each message is consumed by exactly one receiver.\nThe message broker serves as an abstraction layer. The producer only sends to the broker. The receiver only receives from the broker. The services do not need to know about one another. The broker also guarantees delivery, so the message does not get lost if the consumer is unavailable, it will retry.\n\n\n\n\n\n\nFigure 5: Point-to-Point\n\n\n\n\n\n\nA one-to-many relationship between sender and receiver. The publisher publishes to a certain topic. Any consumer who is interested can then subscribe to receive that message.\n\n\n\n\n\n\nFigure 6: Pub-Sub\n\n\n\n\n\n\n\n\n\nFiles are stored in folders, with metadata about creation time and modification time.\nBenefits:\n\nSimplicity - simple and well-known pattern\nCompatibility - works with most applications and OSes\nLimitations:\nPerformance degradation - as size increases, the resource demands increase.\nExpensive - although cheap in principle, they can get expensive when trying to work around the performance issues.\n\nUse cases: data protection, local archiving, data retrieval done by users.\n\n\n\nA system component that manages data as objects in a flat structure. An object contains the file and its metadata.\nBenefits:\n\nHorizontal scalability\n\nLimitations:\n\nObjects must be edited as a unit - degrades performance if only a small part of the file needs to be updated.\n\nUse cases: large amounts of unstructured data.\n\n\n\nA system component that breaks up and then stores data as fixed-size blocks, each with a unique identifier.\nBenefits:\n\nHighly structured - easy to index, search and retrieve blocks\nLow data transfer overheads\nSupports frequent data writes without performance degradation\nLow latency\n\nLimitations:\n\nNo metadata, so metadata must be handled manually\nExpensive\n\n\n\n\nA subclass of key-value stores, which hold computer-readable documents, like XML or JSON objects.\n\n\n\n\nIssues:\n\nMassive files: 4TB for 4 hours of 4K video\n\nHow can we reliably upload very large files?\n\nLots of different end-user clients/devices/connection speeds\n\nHow can we process and store a range of versions/resolutions?\n\n\nProcessing pipeline:\n\n\n\n\n\n\nFigure 7: Video Processing Pipeline\n\n\n\n\nFile chunker\n\nSame principle as the file-sharing problem.\nSplit the file into chunks, then use checksums to determine that files are present and correct.\nIf there is a network outage, we only need to send the remaining chunks.\n\nContent filter\n\nCheck if the video complies with the platform’s content policy wrt copyright, piracy, NSFW\n\nTranscoder\n\nData is decoded to an uncompressed format\nThis will fan out in the next step to create multiple optimised versions of the file\n\nQuality conversion\n\nConvert the uncompressed file into multiple different resolution options.\n\n\nArchitecture considerations:\n\nWait for the full file to upload before processing? Or process each chunk as it is uploaded?\nAn upload service persists chunks to a database. Object store is a good choice as in the file-sharing example.\nOnce a chunk is ready to be processed, it can be read and placed in the message queue for the processing pipeline.\nThe processing pipeline handles fixed-size chunks, so the hardware requirements are known ahead of time. A chunk can be processed as soon as a hardware unit becomes available.\n\n\n\n\n\n\n\nFigure 8: Video Upload Architecture\n\n\n\n\n\n\nThe challenges are similar to that of file-sharing, as this is essentially large files being transferred. Latency and processing power of the end-user device.\nTwo main transfer protocols:\n\nUDP\nTCP\n\n\n\nA connection-less protocol. Nodes do NOT establish a stable end-to-end connection before sending data. Instead, each packet has its destination address attached so the network can route it to the correct place.\nAdvantages:\n\nFast\nPackets are routed independently so if some are lost the others can still reach their destination\n\nDisadvantages:\n\nNo guarantee that data will arrive intact.\n\nUse cases:\n\nLow-latency applications like video conferencing or gaming.\nNot suitable for movie or audio streaming, as missing bits cannot be tolerated.\n\n\n\n\nA connection is established between 2 nodes. The nodes agree on certain parameters before any data is transferred. Parameters: IP addresses of source and destination, port numbers of source and destination.\nThree-way handshake establishes the connection:\n\nSender transfers their sequence number to the Receiver.\nReceiver acknowledges and sends its own sequence number.\nSender acknowledges.\n\nAdvantages:\n\nReliability; guarantees delivery and receives acknowledgements before further packets are sent.\nReceiver acknowledgements ensure the receiver is able to process/buffer the data in time before more packets are sent.\n\nDisadvantages:\n\nSlower due to error checking and resending lost packets.\nRequires three-way handshake to establish connection which is slower.\n\nUse cases:\n\nMedia streaming (HTTP live-streaming or MPEG-DASH)\n\nAdaptive bitrate streaming\n\nTCP adjusts transmission speed in response to network conditions\nWhen packet loss is detected, smaller chunks are sent at a lower transmission speed. The lower resolution file can be sent in this case.\n\n\n\n\n\nPerceived latency = Network latency + System latency\n\n\nCaching is the process of storing copies of frequently accessed data in a temporary storage location.\nCaches utilise the difference in read performance between memory and storage. Times to fetch 1MB from:\n\nHDD: 3 ms\nSSD: 0.2 ms\nRAM: 0.01 ms\n\nWhen a user requests data it first requests from the cache\n\nCache HIT: If the data is in the cache, return it\nCache MISS: If the data is not in the cache, pass the request to the database, update the cache and return the data\n\nA cache can grow stale over time. There are 2 common approaches to mitigate this:\n\nSet up a time-to-live (TTL) policy within the cache\n\n\nTrade off between short TTL (fresh data but worse performance) vs long TTL (data can be stale)\n\n\nImplement active cache invalidation mechanism\n\n\nComplicated to implement. Requires an “invalidation service” component to monitor and read from the database, then update the cache when necessary.\n\nApplication-level caching. Insert caching logic into the application’s source code to temporarily store data in memory.\nTwo approaches to application-level caching:\n\nQuery-based implementation\n\nHash the query as a key and store the value against the key in a key-value store.\nLimitations: hard to delete cached result with a complex query; if a cell changes then all cached query which might include it need updating.\n\nObject-based implementation\n\nThe result of a query is stored as an object\nBenefits: only one serialisation/deserialisation overhead when reading/writing; complexity of object doesn’t matter, serialized objects in cache can be used by multiple applications.\n\n\nPopular implementations: Redis, Memcache, Hazlecast\n\n\n\nA network of caching layer in different locations (Points of Presence, PoPs). Full data is stored on SSD and most frequently accessed data is stored in RAM.\nDon’t cache dynamic or time-sensitive data.\nInvalidating the cache. You can manually update the CDN, but there may be additional caches at the ISP- and browser-level which would still serve stale data.\n\n\n\n\n\n\nFigure 9: CDN Invalidation\n\n\n\nApproaches to invalidating cache:\n\nCaching headers - set a time when an object should be cleared, e.g. max-age=86400 caches for a day.\nCache busting - change the link to all files and assets, so the CDN treats them like new files.\n\n\n\n\n\nThese are specialised NoSQL databases with the following benefits:\n\nCan match even with typos or non-exact matches\nFull-text search means you can suggest autocomplete results and related queries as the user types\nIndexing allows faster search performance on big data.\n\nThe database is structures as a hashmap where the inverted index points at documents.\n\nDocument - Represents a specific entity, like a row in a relational database\nInverted index - Maps from content to the documents that include it. The inverted index splits each document into individual search terms and makes each point to the document itself.\nRanking algorithm - Produces a list of possibly relevant documents. Additional factors may impact this, like a user’s previous search history.\n\nThe database is a dictionary of {search_term_id: [document_ids, …]}\n\n\n\n\n\n\nFigure 10: Inverted Index\n\n\n\nPopular implementations: Lucene, Elastic search, Apache solr, Atas search (MongoDB), Redis search.\nBenefits of search engine databases:\n\nScalable - NoSQL so scale horizontally\nSchemaless\nWorks out of the box - just need ot decide upfront what attributes should be searchable.\n\nLimitations:\n\nNo ACID guarantees.\nNot efficient at reading/writing data, so requires another database to manage state.\n\nMaintaining consistency between the main database and the search engine database can be tricky.\n\nSome SQL databases have full-text search so may not require a dedicated search engine database.\n\n\n\n\n\n\n\nAn app that lets a user add and delete items from a todo list.\n\nRepresentation layer is a web app\nMicroservices handle the todo CRUD operations and the user details separately\n\nEach service is scaled horizontally, so a load balancer is placed in front of it\n\nRelational database stores data\n\nThere are two databases, one per service, which is an example of functional partitioning. This means todo service I/O does not interfere with user service I/O and vice versa.\n\n\n\n\n\n\n\n\nFigure 11: To-do App System Design\n\n\n\n\n\n\nTake an input URL and return a unique, shortened URL that redirects to the original.\nPlanned system architecture:\n\nPre-generate all 5 digit keys (1 billion entries rather than 64 billion for 6 digits)\nSystem retrieves 5-difit keys from database\nSystem appends 1 out of 64 characters\nThe system is extendable because if we run out of keys we can append 2 more characters rather than 1\n\n\n\nSystem analysis:\n\nSystem is read heavy - Once a short URL is created it will be read multiple times\nDistributed system as this has to scale\nAvailability &gt; Consistency - not so much of a concern if a link isn’t available to all users at the same time, but they must be unique and lead to their destination.\n\nRequirements engineering:-\nCore feature:\n\nA user can input a URL of arbitrary length and receive a unique short URL of fixed size.\nA user can navigate to a short link and be redirected to the original URL.\n\nSupport features:\n\nA user can see their link history of all created short URLs.\nLifecycle policy - links should expire after a default time span.\n\nNon-functional requirements:\n\nAvailability - the system should be available 99% of the time.\nScalability - the system should support billions of short URLs, and thousands of concurrent users.\nLatency - the system should return redirect from a short URL to the original in under 1 second.\n\nQuestions to capture scale:\n\nDaily active users, and how often do users interact per day\n\n100 million, 1 interaction per day\n\nPeak active users - are there events that lead to traffic spikes?\n\nNo spikes\n\nRead/write ratio\n\n10 to 1\n\nRequest size - how long are the originals URLs typically\n\n200 characters =&gt; 200 Bytes\n\nReplication factor\n\n1x - ignore replication\n\n\nCapacity estimation:-\n\nRequests per second\n\nReads per second = DAU * interactions per day / seconds in day = 10^8 * 1 / 10^5 = 1000 reads/s\nWrites per second = Reads per second / read write ratio = 1000 / 10 = 100 writes/s\nRequests per second = Reads per second + Writes per second = 1000 + 100 = 1100 requests/s\nNo peak loads to consider\n\nBandwidth\n\nBandwidth = Requests per second * Message size\nRead bandwidth = 1000 * 200 Bytes = 200kB/s\nWrite bandwidth = 100 * 200 Bytes = 20 kB/s\n\nStorage\n\nStorage per year = Write bandwidth * seconds per year * Replication factor = 20000 Bytes * (360024365) * 1 = 630 GB\n\n\nData model:-\nIdentify entities, attributes and relationships.\nEntities and attributes:\n\nLinks: Key (used to create short URL), Original URL, Expiry date\nUsers: UserID, Links\nKey ranges: Key range, In use (bool)\n\nRelationships:\n\nUsers own Links\nLinks belong to Key ranges\n\nData stores:\n\nUsers\n\nUser data is typically relational and we rarely want it all returned at once.\nConsistency is important as we want the user to have the same experience regardless of which server handles their log in, and don’t want userIds to clash.\nRelational database.\n\nLinks\n\nNon-functional requirements of low latency and high availability.\nData and relationships are not complex.\nKey-value store.\n\nKey ranges\n\nFavour data consistency as we don’t want to accidentally reuse the same key range.\nFiltering keys by status would help to find available key ranges.\nRelational database.\n\n\nAPI design:-\nEndpoints:\n\ncreateUrl(originalUrl: str) -&gt; shortUrl\n\nresponse code 200, {shortURL: str}\n\ngetLinkHistory(userId: str, sorting: {‘asc’, ‘desc’})\n\nresponse code 200, {links: [str], order: ‘asc’}\n\nredirectURL(shortUrl: str) -&gt; originalUrl\n\nresponse code 200, {originalUrl: str}\n\n\nSystem design:-\nUser flows for each of the required functional requirements.\n\n\n\n\n\n\nFigure 12: URL Shortener System Design\n\n\n\n\n\n\n\nRequirements engineering\nRequirements gathering:\n\nRead-heavy\nDistributed\nData consistency is more important than availability - files should be consistent for all users\n\nCore functional requirements:\n\nA user can upload a file to the server\nA user can download their files from the server\n\nSecondary functional requirements:\n\nA user can see a history of files uploaded and downloaded\nA user can see who has downloaded a specific file and when\n\nNon-functional requirements:\n\nConsistency - data should be consistent across users and devices\nResilience - customer data should never be lost\nMinimal latency\nCompatibility across different devices\nSecurity - multi-tenancy; customer data should be separate of one another\n\nQuestions to determine scale:\n\nDaily active users\nPeak active users - what events lead to a spike?\nInteractions per user - how many file syncs, how many files does a user store\nRequest size - how large are files?\nRead/write ratio\nReplication factor\n\nCapacity estimation\nThroughput\n\nPeak write RPS = 2 peak load ratio * 10^8 users * 2 files/day / 10^5 seconds = 4000 RPS\nPeak read RPS = 10 read/write ration * 4000 write RPS = 40000 RPS\nPeak total RPS = 44000 RPS\n\nBandwidth\n\nWrite bandwidth = 4000 Write RPS * 100 kB Request size = 400 MB/s\nRead bandwidth = 40000 Read RPS * 100 kB request size = 4 GB/s\nTotal bandwidth = Write bandwidth + Read bandwidth = 4.4 GB/s\n\nStorage\n\nStorage capacity = 5 * 10^8 Total users * 100 files per user * 1MB File Size * 3 Replication factor = 15*10^10 MB = 15000 TB\n\nData model\nUsers: Store Ids for the files that belong to them\nFiles: Metadata on the owner, file edit history, filename, size, chunks that comprise the file, version history\nAPI Design\nEndpoints:\n\ncompareHashes(fileId: str, chunkHashes: list)\n\nTakes a file ID and a list of chunk hashes and returns a list of the hashes that need resyncing .\nresponse code 200, {syncChunks: [Hash,…]}\n\nuploadChange(fileId: str, chunks: list)\n\nUpload the chunks to resync the file\nresponse code 200, {message: “Chunks uploaded successfully”}\n\nrequestUpdate(fileId: str, chunkHashes: list)\n\nUpdate the local version of the file to match that on the server.\nresponse code 200, {chunks: [Chunk,…]}\n\n\nSystem Design\n\n\n\n\n\n\nFigure 13: Dropbox System Design\n\n\n\nDesign Discussion\n\nWhat can we do to achieve a scalable design?\n\nIdentify bottlenecks and remedy each.\n\nWhat can we do to achieve resiliency?\n\n\n\n\n\n\nUdemy course\nExcalidraw session\nCapacity estimation cheat sheet\n“Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems” by Martin Kleppmann\nRsync algorithm\nSearch Engines Information Retrieval in Practice book\n\n\n\n\nFigure 1: Polling\nFigure 2: LongPolling\nFigure 3: Web Sockets\nFigure 4: SSE\nFigure 5: Point-to-Point\nFigure 6: Pub-Sub\nFigure 7: Video Processing Pipeline\nFigure 8: Video Upload Architecture\nFigure 9: CDN Invalidation\nFigure 10: Inverted Index\nFigure 11: To-do App System Design\nFigure 12: URL Shortener System Design\nFigure 13: Dropbox System Design"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#requirements-engineering",
    "href": "posts/software/system_design/system_design_notes.html#requirements-engineering",
    "title": "System Design Notes",
    "section": "",
    "text": "Functional and non-functional requirements. Scale of the system.\n\n\nWhat the system should do.\n\nWhich problem does the system solve\nWhich features are essential to solve these problems\n\nCore features: bare minimum required to solve the user’s problem. Support features: features that help the user solve their problem more conveniently.\n\n\n\nWhat the system should handle.\n\n\n\n\n\n\nInterview Tips\n\n\n\n\nIdentify the non-functional requirements the interviewer cares most about\nShow awareness of system attributes, trade-offs and user experience\n\n\n\nSystem analysis:\n\nRead or write heavy?\n\nImpacts requirements for database scaling, redundancy of services, effectiveness of caching.\nSystem priorities - is it worse if the system fails to read or to write?\n\nMonolithic or distributed architecture?\n\nScale is the key consideration. Large scale applications must be distributed, smaller scale can be single server.\n\nData availability vs consistency\n\nCAP trilemma - A system can have only two of: Consistency, Availability, Partition tolerance\nChoice determines the impact of a network failure\n\n\nNon-functional requirements:\n\nAvailability - How long is the system up and running per year?\n\nAvailability of a system is the product of its components’ availability\nReliability, redundancy, fault tolerance\n\nConsistency - Data appears the same on all nodes regardless of the user and location.\n\nLinearizability\n\nScalability - Ability of a system to handle fast-growing user base and temporary load spikes\n\nVertical scaling - single server that we add more CPU/RAM to\n\nPros: fast inter-process communication, data consistency\nCons: single point of failure, hardware limits on maximum scale\n\nHorizontal scaling - cluster of servers in parallel\n\nPros: redundancy improves availability, linear cost of scaling\nCons: complex architecture, data inconsistency\n\n\nLatency - Amount of time taken for a single message to be delivered\n\nExperienced latency = network latency + system latency\nDatabase and data model choices affect latency\nCaching can be effective\n\nCompatibility - Ability of a system to operate seamlessly with other software, hardware and systems\nSecurity\n\nBasic security measures: TLS encrypted network traffic, API keys for rate limiting\n\n\nNon-functional requirements depend heavily on expected scale. The following help quantify expected scale, and relate to capacity estimation in the next step.\n\nDaily active users (DAU)\nPeak active users\nInteractions per user - including read/write ratio\nRequest size\nReplication factor - determines the storage requirement (typically 3x)"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#capacity-estimation",
    "href": "posts/software/system_design/system_design_notes.html#capacity-estimation",
    "title": "System Design Notes",
    "section": "",
    "text": "Interview Tips\n\n\n\n\nSimplify wherever you can - simple assumptions and round heavily\nConvert all numbers to scientific notation\nKnow powers of 2\n\n\n\n\n\nRequests per second (RPS) is the key measure.\nRequests per day = Daily active users * Activities per user\nRequests per second = RPD / 10^5\nPeak load = RPS * Peak load factor\nRead/write ratio is important to get the total requests. Usually, information on either reads or writes is missing and must be inferred using the other.\nTotal RPS = Read RPS + Write RPS\n\n\n\nThe amount of data that can be transmitted between two nodes in a fixed period of time. Bits per second.\nThroughput, bandwidth and latency are all related and can be thought of with the motorway analogy.\n\nThroughput is the total number of vehicles that must pass through that road per day.\nBandwidth is the number of lanes.\nLatency is the time taken to get from one end of the road to another.\n\nThe bandwidth must be large enough to support the required throughput.\nBandwidth = Total RPS * Request size\nEven high bandwidth can result in low latency if there is an unexpectedly high peak throughput. Rush hour traffic jam.\nRequest size can be varied according to bandwidth, e.g. YouTube lowers the resolution if the connection is slow.\n\n\n\nMeasured in bits per second using the Write RPS. Long term storage (assuming the same user base) is also helpful.\nStorage capacity per second = Write RPS * Request size * Replication factor\nStorage capacity in 5 years = Storage capacity per second * 2*10^8"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#data-modeling",
    "href": "posts/software/system_design/system_design_notes.html#data-modeling",
    "title": "System Design Notes",
    "section": "",
    "text": "Key concepts are: entities, attributes, relations.\nOptimisation discussions are anchored by the data model.\nRelational databases: denormalization, SQL tuning, sharding, federation\nNon-relational databases: indexing, caching\nThe data model is not a full-blown data schema (which only applies to relational databases) but a more informal, high-level version.\nMust haves:\n\nDerive entities and attributes.\nDraw connections between entities. Then:\nSelect a database based on the requirements\nExtend data model to a proper data schema\nIdentify bottlenecks and apply quick fixes Later:\nDefer any detailed discussions to the design discussion section.\n\n\n\nEntities are “things” with a distinct and independent existence, e.g. users, files, tweets, posts, channels. Create a unique table for each entity.\nAttributes describe properties of an entity, e.g. a user’s name, date of birth. These are the fields in the table of that entity.\n\n\n\n\n\n\nInterview Tips\n\n\n\n\nDo not focus on irrelevant details\nDo not miss any critical functionality\nDouble check all tables after you extract all info from requirements\n\n\n\n\n\n\nExtract connections between entities from the requirements. Relationships can be one-to-one, one-to-many or many-to-many."
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#api-design",
    "href": "posts/software/system_design/system_design_notes.html#api-design",
    "title": "System Design Notes",
    "section": "",
    "text": "Specify the API endpoints with:\n\nFunction signature\nParameters\nResponse and status codes\n\nThis specifies a binding contract between the client and the application server. There can be APIs between every system component, but just focus on the client/server interface.\nProcess:\n\nRevisit the functional requirements\nDerive the goal of each endpoint\nDerive the signature from the goal and the inputs from the data model\nDefine the response outputs\n\nNaming conventions. Use HTTP request types in the call signature where appropriate: GET, POST, PUT, PATCH, DELETE\n\n\n\n\n\n\nInterview Tips\n\n\n\n\nAvoid vague inputs\nDon’t add extra parameters that are out of scope\nAvoid redundant parameters\nDon’t miss any parameters\nConfirm the output repsonse satisfies the requirements\nIdentify inefficiencies of the data structure"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#system-design-1",
    "href": "posts/software/system_design/system_design_notes.html#system-design-1",
    "title": "System Design Notes",
    "section": "",
    "text": "System components:\n\nRepresentation layer\n\nWeb app\nMobile app\nConsole\n\nService (and load balancer)\nData store\n\nRelational database\n\nPros: linked tables reduce duplicate data; flexible queries\n\nKey-value store\n\nUseful for lots of small continuous reads and writes\nPros: Fast access, lightweight, highly scalable\nCons: Cannot query by value, no standard query language so data access in written in the application layer no data normalisation\n\n\n\nScaling:\n\nScale services by having multiple instances of the service with a load balancer in front\nScale database with federation (functional partitioning) - split up data by function\n\nSystem diagrams: arrows point in direction of user flow (not data flow)"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#design-discussion",
    "href": "posts/software/system_design/system_design_notes.html#design-discussion",
    "title": "System Design Notes",
    "section": "",
    "text": "Types of questions:\n\nNFR questions\n\nHow to achieve scalability\n\nIdentify each bottleneck and remedy it\n\nHow to improve resiliency\n\nJustification question\n\nChoice of components, e.g. what would change if you changed block storage for object storage or relational database\nAdditional components, e.g. is there a use case for a message queue and where\n\nExtension questions\n\nAdditional functional requirements not in the original scope"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#system-components-deep-dive",
    "href": "posts/software/system_design/system_design_notes.html#system-components-deep-dive",
    "title": "System Design Notes",
    "section": "",
    "text": "There are two parameters we can vary:\n\nLength of the key - should be limited so the URL is short enough\nRange of allowed characters - should be URL-safe\n\nUsing Base-64 encoding as the character range, then a 6-digit key length gives enough unique URLs; 64^6 = 68 billion\nEncoding options: 1. MD5 hash: not collision resistant and too long 2. Encoded counter, each URL is just an index int, and its value is the base-64 encoding of that int: length is not fixed and increases over time 3. Key range, generate all unique keys beforehand: no collisions, but storage intensive 4. Key range modified, generate 5-digit keys beforehand, then add the 6th digit on the fly.\n\n\n\n\n\n4 main operations (CRUD): Create, Read, Update, Delete.\nTransactions - group several operations together\n\nEither the entire transaction succeeds or it fails and rolls back to the initial state. Commit or abort on error.\nWithout transactions, if an individual operation fails, it can be complex to unwind the related transactions to return to the initial state.\nError handling is simpler as manual rollback of operations is not required.\nTransactions provide guarantees so we can reason about the database state before and after the transaction.\n\nTransactions enforce “ACID” guarantees:\n\nAtomicity - Transactions cannot be broken down into smaller parts.\nConsistency - All data points within the database must align to be properly read and accepted. Raise a consistency error if this is not the case. Database consistency is different from system consistency which states the data should be the same across all nodes.\nIsolation - Concurrently executing transactions are isolated from each other. Avoids race conditions if multiple clients are accessing the same database record.\nDurability - Once a transaction is committed to the database, it is permanently preserved. Backups and transaction logs can restore committed transactions in the case of a failure.\n\nRelational databases are a good fit when:\n\nData is well-structured\nUse case requires complex querying\nData consistency is important\n\nLimitations of relational databases:\n\nHorizontal scaling is complex\nDistributed databases are more complex to keep transaction guarantees\n\nTwo phase commit protocol (2PC):\n\nPrepare - Ask each node if it;s able to promise to carry out the transaction\nCommit - Block the nodes and do the commit\n\nBlocking causes problems on distributed systems where it may cause unexpected consequences if the database is unavailable for this short time.\n\n\n\n\n\nExamples of non-relational databases:\n\nKey-value store\nDocument store\nWide-column store\nGraph store\n\nThese vary a lot, and in general NoSQL simply means “no ACID guarantees”.\nTransactions were seen as the enemy of scalability, so needed to be abandoned completely for performance and scalability. ACID enforces consistency for relational databases; for non-relational databases there is eventual consistency.\nBASE:\n\nBasically Available - Always possible to read and write data even though it may not be consistent. E.g. reads may not reflect latest changes, writes may not be persisted.\nSoft state - Lack of consistency guarantees mean data state may change without any interactions with the application as the database reaches eventual consistency.\nEventual consistency - Data will eventually become consistent once inputs stops.\n\nBenefits:\n\nWithout atomicity constraint, overheads like 2PC are not required\nWithout consistency constraint, horizontal scaling is trivial\nWithout isolation constraint, no blocking is required which improves availability.\n\nNon-relational databases are a good fit when:\n\nLarge data volume that isn’t tabular\nHigh availability requirement\nLack of consistency across nodes is acceptable\n\nLimitations:\n\nConsistency is necessary for some use cases\nLack of standardisation\n\n\n\n\n\nProblem: if we have large files with small changes, we don’t want to have to re-upload and re-download the whole file every time something changes.\nSolution: identify the changes and only push/pull these. Similar to git.\nThe rsync algorithm makes it easier to compare changes by:\n\nSplit the file into blocks\nCalculate a checksum for each block (using MD5 hashing algorithm)\nTo compare files between the client and the server, we only need to send the hashes back and forth\nFor the mismatching hashes, transfer the corresponding blocks\n\n\n\n\nOne-way communication that broadcasts file updates one-to-many.\n\n\nThese are synchronous.\nPolling is an example of a pull API. We periodically query the server to see if there are any updates. If not, the server responds immediately saying there is no new data. This may result in delayed updates and can overwhelm the server with too many requests.\n\n\n\n\n\n\nFigure 1: Polling\n\n\n\n\n\n\nThese are asynchronous.\nLong-polling. The client connects to the server and makes a request. Instead of replying immediately, the server waits until there is an update and then sends the response. If there is no update for a long time, the connection times out and the client must reconnect. Server resources are tied up until the connection is closed, even if there is no update.\n\n\n\n\n\n\nFigure 2: LongPolling\n\n\n\nWebsockets. The client establishes a connection using an HTTP request and response. This establishes a TCP/IP connection for 2-way communication. This allows for real-time applications without long-polling.\n\n\n\n\n\n\nFigure 3: Web Sockets\n\n\n\nServer sent events. Event-based approach. EventSource API supported by all browsers. The connection is 1-directional; the client can only pass data to the server at the point of connection. After that, only the server can send data to the client. The server doesn’t know if the client loses connection.\n\n\n\n\n\n\nFigure 4: SSE\n\n\n\n\n\n\n\nA system component that implements asynchronous communication between services, decoupling them. An independent server which producers and consumers connect to. AKA message queues.\nMessage brokers persist the messages until there are consumers ready to receive them.\nMessage brokers are similar in principle to databases since they persist data. The differences are they: automatically delete messages after delivery, do not support queries, typically have a small working set, and notify clients about changes.\nRabbitMQ, Kafka and Redis are open source message brokers. Amazon, GCP and Azure have managed service implementations.\n\n\nA one-to-one relationship between sender and receiver. Each message is consumed by exactly one receiver.\nThe message broker serves as an abstraction layer. The producer only sends to the broker. The receiver only receives from the broker. The services do not need to know about one another. The broker also guarantees delivery, so the message does not get lost if the consumer is unavailable, it will retry.\n\n\n\n\n\n\nFigure 5: Point-to-Point\n\n\n\n\n\n\nA one-to-many relationship between sender and receiver. The publisher publishes to a certain topic. Any consumer who is interested can then subscribe to receive that message.\n\n\n\n\n\n\nFigure 6: Pub-Sub\n\n\n\n\n\n\n\n\n\nFiles are stored in folders, with metadata about creation time and modification time.\nBenefits:\n\nSimplicity - simple and well-known pattern\nCompatibility - works with most applications and OSes\nLimitations:\nPerformance degradation - as size increases, the resource demands increase.\nExpensive - although cheap in principle, they can get expensive when trying to work around the performance issues.\n\nUse cases: data protection, local archiving, data retrieval done by users.\n\n\n\nA system component that manages data as objects in a flat structure. An object contains the file and its metadata.\nBenefits:\n\nHorizontal scalability\n\nLimitations:\n\nObjects must be edited as a unit - degrades performance if only a small part of the file needs to be updated.\n\nUse cases: large amounts of unstructured data.\n\n\n\nA system component that breaks up and then stores data as fixed-size blocks, each with a unique identifier.\nBenefits:\n\nHighly structured - easy to index, search and retrieve blocks\nLow data transfer overheads\nSupports frequent data writes without performance degradation\nLow latency\n\nLimitations:\n\nNo metadata, so metadata must be handled manually\nExpensive\n\n\n\n\nA subclass of key-value stores, which hold computer-readable documents, like XML or JSON objects.\n\n\n\n\nIssues:\n\nMassive files: 4TB for 4 hours of 4K video\n\nHow can we reliably upload very large files?\n\nLots of different end-user clients/devices/connection speeds\n\nHow can we process and store a range of versions/resolutions?\n\n\nProcessing pipeline:\n\n\n\n\n\n\nFigure 7: Video Processing Pipeline\n\n\n\n\nFile chunker\n\nSame principle as the file-sharing problem.\nSplit the file into chunks, then use checksums to determine that files are present and correct.\nIf there is a network outage, we only need to send the remaining chunks.\n\nContent filter\n\nCheck if the video complies with the platform’s content policy wrt copyright, piracy, NSFW\n\nTranscoder\n\nData is decoded to an uncompressed format\nThis will fan out in the next step to create multiple optimised versions of the file\n\nQuality conversion\n\nConvert the uncompressed file into multiple different resolution options.\n\n\nArchitecture considerations:\n\nWait for the full file to upload before processing? Or process each chunk as it is uploaded?\nAn upload service persists chunks to a database. Object store is a good choice as in the file-sharing example.\nOnce a chunk is ready to be processed, it can be read and placed in the message queue for the processing pipeline.\nThe processing pipeline handles fixed-size chunks, so the hardware requirements are known ahead of time. A chunk can be processed as soon as a hardware unit becomes available.\n\n\n\n\n\n\n\nFigure 8: Video Upload Architecture\n\n\n\n\n\n\nThe challenges are similar to that of file-sharing, as this is essentially large files being transferred. Latency and processing power of the end-user device.\nTwo main transfer protocols:\n\nUDP\nTCP\n\n\n\nA connection-less protocol. Nodes do NOT establish a stable end-to-end connection before sending data. Instead, each packet has its destination address attached so the network can route it to the correct place.\nAdvantages:\n\nFast\nPackets are routed independently so if some are lost the others can still reach their destination\n\nDisadvantages:\n\nNo guarantee that data will arrive intact.\n\nUse cases:\n\nLow-latency applications like video conferencing or gaming.\nNot suitable for movie or audio streaming, as missing bits cannot be tolerated.\n\n\n\n\nA connection is established between 2 nodes. The nodes agree on certain parameters before any data is transferred. Parameters: IP addresses of source and destination, port numbers of source and destination.\nThree-way handshake establishes the connection:\n\nSender transfers their sequence number to the Receiver.\nReceiver acknowledges and sends its own sequence number.\nSender acknowledges.\n\nAdvantages:\n\nReliability; guarantees delivery and receives acknowledgements before further packets are sent.\nReceiver acknowledgements ensure the receiver is able to process/buffer the data in time before more packets are sent.\n\nDisadvantages:\n\nSlower due to error checking and resending lost packets.\nRequires three-way handshake to establish connection which is slower.\n\nUse cases:\n\nMedia streaming (HTTP live-streaming or MPEG-DASH)\n\nAdaptive bitrate streaming\n\nTCP adjusts transmission speed in response to network conditions\nWhen packet loss is detected, smaller chunks are sent at a lower transmission speed. The lower resolution file can be sent in this case.\n\n\n\n\n\nPerceived latency = Network latency + System latency\n\n\nCaching is the process of storing copies of frequently accessed data in a temporary storage location.\nCaches utilise the difference in read performance between memory and storage. Times to fetch 1MB from:\n\nHDD: 3 ms\nSSD: 0.2 ms\nRAM: 0.01 ms\n\nWhen a user requests data it first requests from the cache\n\nCache HIT: If the data is in the cache, return it\nCache MISS: If the data is not in the cache, pass the request to the database, update the cache and return the data\n\nA cache can grow stale over time. There are 2 common approaches to mitigate this:\n\nSet up a time-to-live (TTL) policy within the cache\n\n\nTrade off between short TTL (fresh data but worse performance) vs long TTL (data can be stale)\n\n\nImplement active cache invalidation mechanism\n\n\nComplicated to implement. Requires an “invalidation service” component to monitor and read from the database, then update the cache when necessary.\n\nApplication-level caching. Insert caching logic into the application’s source code to temporarily store data in memory.\nTwo approaches to application-level caching:\n\nQuery-based implementation\n\nHash the query as a key and store the value against the key in a key-value store.\nLimitations: hard to delete cached result with a complex query; if a cell changes then all cached query which might include it need updating.\n\nObject-based implementation\n\nThe result of a query is stored as an object\nBenefits: only one serialisation/deserialisation overhead when reading/writing; complexity of object doesn’t matter, serialized objects in cache can be used by multiple applications.\n\n\nPopular implementations: Redis, Memcache, Hazlecast\n\n\n\nA network of caching layer in different locations (Points of Presence, PoPs). Full data is stored on SSD and most frequently accessed data is stored in RAM.\nDon’t cache dynamic or time-sensitive data.\nInvalidating the cache. You can manually update the CDN, but there may be additional caches at the ISP- and browser-level which would still serve stale data.\n\n\n\n\n\n\nFigure 9: CDN Invalidation\n\n\n\nApproaches to invalidating cache:\n\nCaching headers - set a time when an object should be cleared, e.g. max-age=86400 caches for a day.\nCache busting - change the link to all files and assets, so the CDN treats them like new files.\n\n\n\n\n\nThese are specialised NoSQL databases with the following benefits:\n\nCan match even with typos or non-exact matches\nFull-text search means you can suggest autocomplete results and related queries as the user types\nIndexing allows faster search performance on big data.\n\nThe database is structures as a hashmap where the inverted index points at documents.\n\nDocument - Represents a specific entity, like a row in a relational database\nInverted index - Maps from content to the documents that include it. The inverted index splits each document into individual search terms and makes each point to the document itself.\nRanking algorithm - Produces a list of possibly relevant documents. Additional factors may impact this, like a user’s previous search history.\n\nThe database is a dictionary of {search_term_id: [document_ids, …]}\n\n\n\n\n\n\nFigure 10: Inverted Index\n\n\n\nPopular implementations: Lucene, Elastic search, Apache solr, Atas search (MongoDB), Redis search.\nBenefits of search engine databases:\n\nScalable - NoSQL so scale horizontally\nSchemaless\nWorks out of the box - just need ot decide upfront what attributes should be searchable.\n\nLimitations:\n\nNo ACID guarantees.\nNot efficient at reading/writing data, so requires another database to manage state.\n\nMaintaining consistency between the main database and the search engine database can be tricky.\n\nSome SQL databases have full-text search so may not require a dedicated search engine database."
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#system-design-examples-and-discussion",
    "href": "posts/software/system_design/system_design_notes.html#system-design-examples-and-discussion",
    "title": "System Design Notes",
    "section": "",
    "text": "An app that lets a user add and delete items from a todo list.\n\nRepresentation layer is a web app\nMicroservices handle the todo CRUD operations and the user details separately\n\nEach service is scaled horizontally, so a load balancer is placed in front of it\n\nRelational database stores data\n\nThere are two databases, one per service, which is an example of functional partitioning. This means todo service I/O does not interfere with user service I/O and vice versa.\n\n\n\n\n\n\n\n\nFigure 11: To-do App System Design\n\n\n\n\n\n\nTake an input URL and return a unique, shortened URL that redirects to the original.\nPlanned system architecture:\n\nPre-generate all 5 digit keys (1 billion entries rather than 64 billion for 6 digits)\nSystem retrieves 5-difit keys from database\nSystem appends 1 out of 64 characters\nThe system is extendable because if we run out of keys we can append 2 more characters rather than 1\n\n\n\nSystem analysis:\n\nSystem is read heavy - Once a short URL is created it will be read multiple times\nDistributed system as this has to scale\nAvailability &gt; Consistency - not so much of a concern if a link isn’t available to all users at the same time, but they must be unique and lead to their destination.\n\nRequirements engineering:-\nCore feature:\n\nA user can input a URL of arbitrary length and receive a unique short URL of fixed size.\nA user can navigate to a short link and be redirected to the original URL.\n\nSupport features:\n\nA user can see their link history of all created short URLs.\nLifecycle policy - links should expire after a default time span.\n\nNon-functional requirements:\n\nAvailability - the system should be available 99% of the time.\nScalability - the system should support billions of short URLs, and thousands of concurrent users.\nLatency - the system should return redirect from a short URL to the original in under 1 second.\n\nQuestions to capture scale:\n\nDaily active users, and how often do users interact per day\n\n100 million, 1 interaction per day\n\nPeak active users - are there events that lead to traffic spikes?\n\nNo spikes\n\nRead/write ratio\n\n10 to 1\n\nRequest size - how long are the originals URLs typically\n\n200 characters =&gt; 200 Bytes\n\nReplication factor\n\n1x - ignore replication\n\n\nCapacity estimation:-\n\nRequests per second\n\nReads per second = DAU * interactions per day / seconds in day = 10^8 * 1 / 10^5 = 1000 reads/s\nWrites per second = Reads per second / read write ratio = 1000 / 10 = 100 writes/s\nRequests per second = Reads per second + Writes per second = 1000 + 100 = 1100 requests/s\nNo peak loads to consider\n\nBandwidth\n\nBandwidth = Requests per second * Message size\nRead bandwidth = 1000 * 200 Bytes = 200kB/s\nWrite bandwidth = 100 * 200 Bytes = 20 kB/s\n\nStorage\n\nStorage per year = Write bandwidth * seconds per year * Replication factor = 20000 Bytes * (360024365) * 1 = 630 GB\n\n\nData model:-\nIdentify entities, attributes and relationships.\nEntities and attributes:\n\nLinks: Key (used to create short URL), Original URL, Expiry date\nUsers: UserID, Links\nKey ranges: Key range, In use (bool)\n\nRelationships:\n\nUsers own Links\nLinks belong to Key ranges\n\nData stores:\n\nUsers\n\nUser data is typically relational and we rarely want it all returned at once.\nConsistency is important as we want the user to have the same experience regardless of which server handles their log in, and don’t want userIds to clash.\nRelational database.\n\nLinks\n\nNon-functional requirements of low latency and high availability.\nData and relationships are not complex.\nKey-value store.\n\nKey ranges\n\nFavour data consistency as we don’t want to accidentally reuse the same key range.\nFiltering keys by status would help to find available key ranges.\nRelational database.\n\n\nAPI design:-\nEndpoints:\n\ncreateUrl(originalUrl: str) -&gt; shortUrl\n\nresponse code 200, {shortURL: str}\n\ngetLinkHistory(userId: str, sorting: {‘asc’, ‘desc’})\n\nresponse code 200, {links: [str], order: ‘asc’}\n\nredirectURL(shortUrl: str) -&gt; originalUrl\n\nresponse code 200, {originalUrl: str}\n\n\nSystem design:-\nUser flows for each of the required functional requirements.\n\n\n\n\n\n\nFigure 12: URL Shortener System Design\n\n\n\n\n\n\n\nRequirements engineering\nRequirements gathering:\n\nRead-heavy\nDistributed\nData consistency is more important than availability - files should be consistent for all users\n\nCore functional requirements:\n\nA user can upload a file to the server\nA user can download their files from the server\n\nSecondary functional requirements:\n\nA user can see a history of files uploaded and downloaded\nA user can see who has downloaded a specific file and when\n\nNon-functional requirements:\n\nConsistency - data should be consistent across users and devices\nResilience - customer data should never be lost\nMinimal latency\nCompatibility across different devices\nSecurity - multi-tenancy; customer data should be separate of one another\n\nQuestions to determine scale:\n\nDaily active users\nPeak active users - what events lead to a spike?\nInteractions per user - how many file syncs, how many files does a user store\nRequest size - how large are files?\nRead/write ratio\nReplication factor\n\nCapacity estimation\nThroughput\n\nPeak write RPS = 2 peak load ratio * 10^8 users * 2 files/day / 10^5 seconds = 4000 RPS\nPeak read RPS = 10 read/write ration * 4000 write RPS = 40000 RPS\nPeak total RPS = 44000 RPS\n\nBandwidth\n\nWrite bandwidth = 4000 Write RPS * 100 kB Request size = 400 MB/s\nRead bandwidth = 40000 Read RPS * 100 kB request size = 4 GB/s\nTotal bandwidth = Write bandwidth + Read bandwidth = 4.4 GB/s\n\nStorage\n\nStorage capacity = 5 * 10^8 Total users * 100 files per user * 1MB File Size * 3 Replication factor = 15*10^10 MB = 15000 TB\n\nData model\nUsers: Store Ids for the files that belong to them\nFiles: Metadata on the owner, file edit history, filename, size, chunks that comprise the file, version history\nAPI Design\nEndpoints:\n\ncompareHashes(fileId: str, chunkHashes: list)\n\nTakes a file ID and a list of chunk hashes and returns a list of the hashes that need resyncing .\nresponse code 200, {syncChunks: [Hash,…]}\n\nuploadChange(fileId: str, chunks: list)\n\nUpload the chunks to resync the file\nresponse code 200, {message: “Chunks uploaded successfully”}\n\nrequestUpdate(fileId: str, chunkHashes: list)\n\nUpdate the local version of the file to match that on the server.\nresponse code 200, {chunks: [Chunk,…]}\n\n\nSystem Design\n\n\n\n\n\n\nFigure 13: Dropbox System Design\n\n\n\nDesign Discussion\n\nWhat can we do to achieve a scalable design?\n\nIdentify bottlenecks and remedy each.\n\nWhat can we do to achieve resiliency?"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#references",
    "href": "posts/software/system_design/system_design_notes.html#references",
    "title": "System Design Notes",
    "section": "",
    "text": "Udemy course\nExcalidraw session\nCapacity estimation cheat sheet\n“Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems” by Martin Kleppmann\nRsync algorithm\nSearch Engines Information Retrieval in Practice book\n\n\n\n\nFigure 1: Polling\nFigure 2: LongPolling\nFigure 3: Web Sockets\nFigure 4: SSE\nFigure 5: Point-to-Point\nFigure 6: Pub-Sub\nFigure 7: Video Processing Pipeline\nFigure 8: Video Upload Architecture\nFigure 9: CDN Invalidation\nFigure 10: Inverted Index\nFigure 11: To-do App System Design\nFigure 12: URL Shortener System Design\nFigure 13: Dropbox System Design"
  },
  {
    "objectID": "posts/business/pitching/pitching.html",
    "href": "posts/business/pitching/pitching.html",
    "title": "Pitching Notes",
    "section": "",
    "text": "Notes from “Pitch Anything” by Oren Klaff.\nSome of the book comes across as a bit incel sigma vibes, talking about alphas and betas. I don’t agree with it, but summarising it here for the parts that are interesting.\n\n\nFrames, AKA perspective AKA point of view, describe a person’s mental model of controlling the world. The person who owns the frame owns the conversation.\nInformation must penetrate through the following layers of the brain in order.\n\nSurvival - “crocodile brain”. Ignore anything that isn’t new, exciting, dangerous, unexpected. Needs big picture, high contrast points.\nSocial\nReason\n\nLogic won’t reach the “reason” part of the brain until the croc brain and social brain are satiated. Data is perceived as a threat by layer 1. the croc brain needs unexpected information that is concrete not abstract.\nSTRONG process:\n\nSet the frame\nTell the story\nReveal the intrigue\nOffer the prize\nNail the hook point\nGet the deal\n\n\n\n\nFrames are the mental structure that shape how you see the world. When frames collide, the stronger frame absorbs the weaker frame. If you have to explain your authority, power, position, leverage then you have the weaker frame.\nOpposing frames and how to counter them.\n\nPower frame - fuelled by status and ego\n\nDon’t react to our strengthen their status\nActs of denial/defiance - establish your own rituals of power rather than abiding by theirs.\nMildly shocking but not unfriendly. Humour is key.\n\nPrize frame\n\nMake yourself the prize. Make them qualify themselves to you. The money has to earn you, not the other way around.\nMake statements not “trial close” questions\n\nTime frame - occurs later in the interaction\n\nStop as soon as you are done OR have lost their attention. Continuing further signals desperation.\n\nAnalyst frame - stats, technical details\n\nIntrigue frame counters this\nThe brain can’t perform cold cognitions (analysis) at the same time as hot cognitions (excitement)\nTell a brief, relevant, intriguing story with tension involving you at the centre of it to break the cold cognition.\n\n\n\n\n\nGlobal status is a function of wealth, power and status.\nBeta traps enforce your lower status. E.g. making you wait, big dogs not showing up to meetings, small talk before meetings.\nSituational status is the temporary shift of status based on the situation. Create local star power to increase your situational status.\n\nIgnore power rituals, avoid beta traps, ignore their global status\nFrame control - small, friendly acts of defiance\nLocal star power - shift conversation to a topic where you are the domain expert\nPrize frame - position yourself as the reward, e.g. leave after a hard cut off time\nConfirm your status - make them say you’re the alpha\n\nShock -&gt; frame control -&gt; local star power -&gt; hook point -&gt; leave\n\n\n\nEvery pitch should tell a story.\nFour sections of the pitch:\n\nIntroduce yourself and the big idea (5 mins)\nExplain the budget and secret sauce (10 mins)\nOffer the deal (2 mins)\nStack hot cognition frames (3 mins)\n\n\n\nKeep the pitch short - 20 mins.\n\nAnnounce the time constraint at the start to appease the croc brain. Otherwise people don’t know how long they are trapped for and panic.\nDNA presentation by Crick and Watson was 5 mins\n\nIntro should be highlights of successes NOT a life story.\n\nThe impression you leave is based the average not the sum, so don’t dilute it.\n\nThe “why now?” frame. 3 market forces pattern:\n\nEconomic\nSocial\nTechnology\n\nTell the story of how your idea came to be.\n\nMovement is key to overcoming change blindness. Don’t just show 2 states, describe the transition between them.\nTrends, impacts of those trends, how have these opened a (temporary) market window (time frame).\nThis story places you as the hero at the centre of solving this problem (prize frame).\n\nIdea introduction pattern:\nFor [target customers]\n\nWho are dissatisfied with [current offerings in the market]\n \nMy idea/product is a [new idea or product category]\n \nThat provides [key problem/solution features]\n \nUnlike [the competing product]\n \nMy idea/product is [describe key features]\n\n\n\nTune the message to the croc brain of the target.\n\nSimplicity can come across as naive, but focus on describing concrete things like relationships between people. This is high-level enough that it will not engage cold cognitions or threaten the croc brain.\n\nAttention = Desire (anticipation of reward) + Tension (fear of loss, introduce consequences)\n\nAttention is high when novelty is high.\nPush/pull patterns to increase tension and desire\nThe pitch narrative is a series of tension loops.\nWhen the tension is released and attention is lost the pitch is over.\nDesire eventually becomes fear, so keep the pitch short.\n\nMust haves:\n\nNumbers and projections\n\nThis should demonstrate your skill at budgeting (a difficult, precise skill), not your skill at projecting (a simple, inaccurate skill).\n\nCompetition\n\nHow easy is it for new competitors to enter?\nHow easy is it for customers to switch TO you and AWAY from you?\n\nSecret sauce\n\nYour competitive advantage.\nPhrasing it this way frames you as the prize.\n\n\n\n\n\nDescribe what they get if they do business with you\n\nWhat you will deliver, when, and how.\nWhat are their roles, responsibilities and next steps.\n\n\n\n\n\nPropose something concrete and actionable in such a compelling way that the target wants to chase it. Don’t wait for the target to evaluate you on the spot at the end of the pitch as that triggers cold cognitions. Pile up the hot cognitions instead.\nA hot cognition is emotional rather than analytical.\n\nYou decide that you like something before you understand it.\nData is used to justify decisions after the fact.\nHot cognitions are generally unavoidable; you may be able to control the expression of emotion, but the initial experience is still there.\nHot cognitions tend to be instant and enduring.\nHot vs cold cognitions can be characterised by spinach vs chocolate - you know one is good for you but you want the other anyway.\nHot cognitions are “knowing” something through feeling it, cold cognitions are “knowing something through evaluating it.”\n\nStacking frames.\n\nWe want to stack frames to create “wanting”, an emotional response\nThis is not the same as getting the target to “like” us, as that is a slow, intellectual cold cognition.\nAppealing to the analytical brain can only trigger a “like”; we want to trigger a “want” in the croc brain.\n\nThe 4 frame stack of hot cognitions:\n\nThe intrigue frame\n\nA story where most important things are who it happened to and how the characters reacted to their situation.\nThe events don’t need to be extreme but the characters’ reactions should be.\nA narrative that feels correct in time with convey a strong sense of truth and accuracy.\n“Man in the jungle” formula - Put a man in the jungle; have beasts attack him; will he get to safety?; get the man to the edge of the jungle but don’t get him out of it (cliffhanger)\n\nThe prize frame\n\nPosition yourself as the most important party in the deal. Conviction is key.\n“I am the prize. You are trying to impress me. You are trying to win my approval.”\n\nThe time frame\n\nThis creates a scarcity bias that triggers fear that triggers action.\nExtreme time pressure feels forced and cutrate. There is a balance between pressure and fairness; there should be a real-world time constraint.\n\nThe moral authority frame\n\nOnce the frame stacking is complete, you have the target’s attention for about 30 seconds.\n\n\n\nValidation-seeking behaviour (i.e. neediness) is the number one deal killer.\n\nNeediness is a threat signal, it triggers fear and uncertainty in the target which triggers self-protection.\n\nAvoid “soft closes” aiming to seek validation, e.g. “So what do you think so far?”, “Do you still think this is a good deal?”\nCauses of neediness:\n\nAnxiety and insecurity turn to fear, so you resort to acceptance-seeking behaviour to confirm that you’re still “part of the pack”\nResponse to disappointment is to seek validation\nWe want something only the target can give us\nWe need cooperation from the target, and the lack of cooperation causes anxiety\nWe believe the target can make us feel good by accepting our offer\n\nCounteracting validation-seeking behaviours:\n\nWant nothing\nFocus only on things you do well\nAnnounce your intention to leave - time frame. When you finish the pitch, withdraw and deny your audience; this creates a prize frame.\n\nMantra: \"I don't need these people, they need me. I am the prize\".\nExample:\n\nTime frame - “The deal will be fully subscribed in the next 10 days”.\nPower frame - “We don’t need VC money but want a big name on cap sheet for later IPO”\nPrize frame - “Are you really the right investor for this? We need to know more about the relationships and value you can bring.”\n\n\n\n\nYou are not pushing, you are interacting with people using basic rules of social dynamics.\nCroc brain:\n\nBoring: Ignore it\nDangerous? Fight it or run\nComplicated? Radically summarise\n\nThe presenter’s problem boils down to deciding which information to include. It is not like an engineering problem where more information is better.\nThree key insights:\n\nAppeal to the croc brain\nControl frames\nUse humour - this is not to relieve tension but to signal that you are so confident that you’re able to play around a little\n\nIf the pitch stops being fun, you’re doing it wrong.\nSteps to learning the method:\n\nRecognise beta traps; anything designed to control your behaviour\nStep around beta traps\nIdentify social frames\nInitiate frame collisions with safe targets - always with good humour and “a twinkle in your eye”\nPractice the push/pull of small acts of defiance and denial\n\nThe only rule is that you make the rules that others follow."
  },
  {
    "objectID": "posts/business/pitching/pitching.html#the-method",
    "href": "posts/business/pitching/pitching.html#the-method",
    "title": "Pitching Notes",
    "section": "",
    "text": "Frames, AKA perspective AKA point of view, describe a person’s mental model of controlling the world. The person who owns the frame owns the conversation.\nInformation must penetrate through the following layers of the brain in order.\n\nSurvival - “crocodile brain”. Ignore anything that isn’t new, exciting, dangerous, unexpected. Needs big picture, high contrast points.\nSocial\nReason\n\nLogic won’t reach the “reason” part of the brain until the croc brain and social brain are satiated. Data is perceived as a threat by layer 1. the croc brain needs unexpected information that is concrete not abstract.\nSTRONG process:\n\nSet the frame\nTell the story\nReveal the intrigue\nOffer the prize\nNail the hook point\nGet the deal"
  },
  {
    "objectID": "posts/business/pitching/pitching.html#frame-control",
    "href": "posts/business/pitching/pitching.html#frame-control",
    "title": "Pitching Notes",
    "section": "",
    "text": "Frames are the mental structure that shape how you see the world. When frames collide, the stronger frame absorbs the weaker frame. If you have to explain your authority, power, position, leverage then you have the weaker frame.\nOpposing frames and how to counter them.\n\nPower frame - fuelled by status and ego\n\nDon’t react to our strengthen their status\nActs of denial/defiance - establish your own rituals of power rather than abiding by theirs.\nMildly shocking but not unfriendly. Humour is key.\n\nPrize frame\n\nMake yourself the prize. Make them qualify themselves to you. The money has to earn you, not the other way around.\nMake statements not “trial close” questions\n\nTime frame - occurs later in the interaction\n\nStop as soon as you are done OR have lost their attention. Continuing further signals desperation.\n\nAnalyst frame - stats, technical details\n\nIntrigue frame counters this\nThe brain can’t perform cold cognitions (analysis) at the same time as hot cognitions (excitement)\nTell a brief, relevant, intriguing story with tension involving you at the centre of it to break the cold cognition."
  },
  {
    "objectID": "posts/business/pitching/pitching.html#status",
    "href": "posts/business/pitching/pitching.html#status",
    "title": "Pitching Notes",
    "section": "",
    "text": "Global status is a function of wealth, power and status.\nBeta traps enforce your lower status. E.g. making you wait, big dogs not showing up to meetings, small talk before meetings.\nSituational status is the temporary shift of status based on the situation. Create local star power to increase your situational status.\n\nIgnore power rituals, avoid beta traps, ignore their global status\nFrame control - small, friendly acts of defiance\nLocal star power - shift conversation to a topic where you are the domain expert\nPrize frame - position yourself as the reward, e.g. leave after a hard cut off time\nConfirm your status - make them say you’re the alpha\n\nShock -&gt; frame control -&gt; local star power -&gt; hook point -&gt; leave"
  },
  {
    "objectID": "posts/business/pitching/pitching.html#pitching-the-big-idea",
    "href": "posts/business/pitching/pitching.html#pitching-the-big-idea",
    "title": "Pitching Notes",
    "section": "",
    "text": "Every pitch should tell a story.\nFour sections of the pitch:\n\nIntroduce yourself and the big idea (5 mins)\nExplain the budget and secret sauce (10 mins)\nOffer the deal (2 mins)\nStack hot cognition frames (3 mins)\n\n\n\nKeep the pitch short - 20 mins.\n\nAnnounce the time constraint at the start to appease the croc brain. Otherwise people don’t know how long they are trapped for and panic.\nDNA presentation by Crick and Watson was 5 mins\n\nIntro should be highlights of successes NOT a life story.\n\nThe impression you leave is based the average not the sum, so don’t dilute it.\n\nThe “why now?” frame. 3 market forces pattern:\n\nEconomic\nSocial\nTechnology\n\nTell the story of how your idea came to be.\n\nMovement is key to overcoming change blindness. Don’t just show 2 states, describe the transition between them.\nTrends, impacts of those trends, how have these opened a (temporary) market window (time frame).\nThis story places you as the hero at the centre of solving this problem (prize frame).\n\nIdea introduction pattern:\nFor [target customers]\n\nWho are dissatisfied with [current offerings in the market]\n \nMy idea/product is a [new idea or product category]\n \nThat provides [key problem/solution features]\n \nUnlike [the competing product]\n \nMy idea/product is [describe key features]\n\n\n\nTune the message to the croc brain of the target.\n\nSimplicity can come across as naive, but focus on describing concrete things like relationships between people. This is high-level enough that it will not engage cold cognitions or threaten the croc brain.\n\nAttention = Desire (anticipation of reward) + Tension (fear of loss, introduce consequences)\n\nAttention is high when novelty is high.\nPush/pull patterns to increase tension and desire\nThe pitch narrative is a series of tension loops.\nWhen the tension is released and attention is lost the pitch is over.\nDesire eventually becomes fear, so keep the pitch short.\n\nMust haves:\n\nNumbers and projections\n\nThis should demonstrate your skill at budgeting (a difficult, precise skill), not your skill at projecting (a simple, inaccurate skill).\n\nCompetition\n\nHow easy is it for new competitors to enter?\nHow easy is it for customers to switch TO you and AWAY from you?\n\nSecret sauce\n\nYour competitive advantage.\nPhrasing it this way frames you as the prize.\n\n\n\n\n\nDescribe what they get if they do business with you\n\nWhat you will deliver, when, and how.\nWhat are their roles, responsibilities and next steps."
  },
  {
    "objectID": "posts/business/pitching/pitching.html#stack-hot-cognition-frames-3-mins",
    "href": "posts/business/pitching/pitching.html#stack-hot-cognition-frames-3-mins",
    "title": "Pitching Notes",
    "section": "",
    "text": "Propose something concrete and actionable in such a compelling way that the target wants to chase it. Don’t wait for the target to evaluate you on the spot at the end of the pitch as that triggers cold cognitions. Pile up the hot cognitions instead.\nA hot cognition is emotional rather than analytical.\n\nYou decide that you like something before you understand it.\nData is used to justify decisions after the fact.\nHot cognitions are generally unavoidable; you may be able to control the expression of emotion, but the initial experience is still there.\nHot cognitions tend to be instant and enduring.\nHot vs cold cognitions can be characterised by spinach vs chocolate - you know one is good for you but you want the other anyway.\nHot cognitions are “knowing” something through feeling it, cold cognitions are “knowing something through evaluating it.”\n\nStacking frames.\n\nWe want to stack frames to create “wanting”, an emotional response\nThis is not the same as getting the target to “like” us, as that is a slow, intellectual cold cognition.\nAppealing to the analytical brain can only trigger a “like”; we want to trigger a “want” in the croc brain.\n\nThe 4 frame stack of hot cognitions:\n\nThe intrigue frame\n\nA story where most important things are who it happened to and how the characters reacted to their situation.\nThe events don’t need to be extreme but the characters’ reactions should be.\nA narrative that feels correct in time with convey a strong sense of truth and accuracy.\n“Man in the jungle” formula - Put a man in the jungle; have beasts attack him; will he get to safety?; get the man to the edge of the jungle but don’t get him out of it (cliffhanger)\n\nThe prize frame\n\nPosition yourself as the most important party in the deal. Conviction is key.\n“I am the prize. You are trying to impress me. You are trying to win my approval.”\n\nThe time frame\n\nThis creates a scarcity bias that triggers fear that triggers action.\nExtreme time pressure feels forced and cutrate. There is a balance between pressure and fairness; there should be a real-world time constraint.\n\nThe moral authority frame\n\nOnce the frame stacking is complete, you have the target’s attention for about 30 seconds."
  },
  {
    "objectID": "posts/business/pitching/pitching.html#eradicating-neediness",
    "href": "posts/business/pitching/pitching.html#eradicating-neediness",
    "title": "Pitching Notes",
    "section": "",
    "text": "Validation-seeking behaviour (i.e. neediness) is the number one deal killer.\n\nNeediness is a threat signal, it triggers fear and uncertainty in the target which triggers self-protection.\n\nAvoid “soft closes” aiming to seek validation, e.g. “So what do you think so far?”, “Do you still think this is a good deal?”\nCauses of neediness:\n\nAnxiety and insecurity turn to fear, so you resort to acceptance-seeking behaviour to confirm that you’re still “part of the pack”\nResponse to disappointment is to seek validation\nWe want something only the target can give us\nWe need cooperation from the target, and the lack of cooperation causes anxiety\nWe believe the target can make us feel good by accepting our offer\n\nCounteracting validation-seeking behaviours:\n\nWant nothing\nFocus only on things you do well\nAnnounce your intention to leave - time frame. When you finish the pitch, withdraw and deny your audience; this creates a prize frame.\n\nMantra: \"I don't need these people, they need me. I am the prize\".\nExample:\n\nTime frame - “The deal will be fully subscribed in the next 10 days”.\nPower frame - “We don’t need VC money but want a big name on cap sheet for later IPO”\nPrize frame - “Are you really the right investor for this? We need to know more about the relationships and value you can bring.”"
  },
  {
    "objectID": "posts/business/pitching/pitching.html#closing-thoughts",
    "href": "posts/business/pitching/pitching.html#closing-thoughts",
    "title": "Pitching Notes",
    "section": "",
    "text": "You are not pushing, you are interacting with people using basic rules of social dynamics.\nCroc brain:\n\nBoring: Ignore it\nDangerous? Fight it or run\nComplicated? Radically summarise\n\nThe presenter’s problem boils down to deciding which information to include. It is not like an engineering problem where more information is better.\nThree key insights:\n\nAppeal to the croc brain\nControl frames\nUse humour - this is not to relieve tension but to signal that you are so confident that you’re able to play around a little\n\nIf the pitch stops being fun, you’re doing it wrong.\nSteps to learning the method:\n\nRecognise beta traps; anything designed to control your behaviour\nStep around beta traps\nIdentify social frames\nInitiate frame collisions with safe targets - always with good humour and “a twinkle in your eye”\nPractice the push/pull of small acts of defiance and denial\n\nThe only rule is that you make the rules that others follow."
  },
  {
    "objectID": "projects_section/xai_research/xai.html",
    "href": "projects_section/xai_research/xai.html",
    "title": "Explainable AI in Healthcare",
    "section": "",
    "text": "This is a research project I completed which aimed to quantify the confidence we should have in a trained model applied ot our data set.\nThe full paper is available here.\nSlides from a presentation I gave at the Nuffield Department of Orthopaedics, Rheumatology and Musculoskeletal Sciences (NDORMS) at the University of Oxford are available here.\n\n\n\n Back to top"
  },
  {
    "objectID": "projects_section/tradeintel/tradeintel.html",
    "href": "projects_section/tradeintel/tradeintel.html",
    "title": "TradeIntel",
    "section": "",
    "text": "This is a robo-advisor app to give tailored stock portfolio recommendations.\nThe user can input high-level preferences like their risk tolerance, industry preferences, and how closely they would like to follow the broader market. We then perform a portfolio optimisation process to recommend a robust portfolio based on those criteria.\nThis is available on the App Store\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html",
    "href": "posts/business/public_speaking/public_speaking.html",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "Notes from reading “Ted Talks: The offical TED guide to public speaking” by Chris Anderson.\n\n\n\n\nPresentation literacy is its own skill. Your goal is to be you, not a version of someone you like.\n\n\n\nIdeas are all that matter in the talk. This can be a novel idea, or a novel story or presentation of an existing idea.\nYour goal is to recreate the core idea inside your audience’s mind. The language you use must be shared by the speaker and listener for it to have this effect. Focus on what you can give to the audience.\nVisualise the talk as a journey that you guide the user through. Start where the audience starts. Don’t make any impossible leaps or unexpected changes of direction.\n\nWhat issues matter the most\nHow are they related?\nHow can they be easily explained?\nWhat are the key open questions? Controversies?\n\n\n\n\nFour talk styles to AVOID.\n\nSales Pitch\n\nYou should give not take.\nGenerosity evokes a response.\n\nRamble\n\nInsults the audience; suggests that their time doesn’t matter to you.\n\nOrganisation bore\n\nYour company’s structure is boring.\nFocus on the work and ideas, not the company, products or individuals.\n\nInspiration tryhard\n\nAll style and no substance.\nComes across as manipulative.\nInspiration has to be earned. “Like love you don’t get it by pursuing it”.\nInspiration is the response to authenticity, courage, wisdom, selflessness.\n\n\n\n\n\nThe throughline is a connecting theme that ties the narrative elements\n\nA focused, precise, unexpected idea\nNot the same as a theme\nYou should be able to encapsulate it in &lt;15 words\n\nTrace the path the journey will take. If the audience knows where you’re going, it will be easier to follow.\nCut down the talk to keep the throughline focused.\n\nDON’T just keep all the content but say it in less detail. Overstuffed = underexplained\nCut back the range of topics. Say less with more impact.\n“The power of the deleted word”.\nAn 18 minute time limit is short enough to keep an audience’s attention, long enough to cover enough ground and precise enough to be taken seriously.\n\nThe overall structure can be thought of in 3 parts: What? So what? What now? In more detail, this is:\n\nIntro - what will be covered\nContext - why this issue matters\nMain concepts\nPractical implications\n\nTalk about ideas not issues. Write for an audience of one.\nChecklist:\n\nIs this a topic I’m passionate about? Do I know enough to be worth the audience’s time? Do I have credibility?\nDoes it inspire curiosity?\nWill knowing this make a difference to the audience?\nIs the talk a gift or an ask?\nIs the information fresh?\nCan the topic be covered in enough detail with examples in the time slot?\nWhat are the 15 words that encapsulate the talk?\nWould those 15 words persuade someone they’re interested in hearing the talk?\n\n\n\n\n\n\n\nA human connection is required before you can inform, persuade or inspire. Knowledge has to be pulled in, not pushed in. Be AUTHENTIC.\nDos\n\nEye contact.\nVulnerability, but authentic.\nHumour signals to the group that you’ve all connected.\n\nDon’ts\n\nEgo breeds contempt.\nAvoid tribal topics/language, e.g. politics.\n\n\n\n\nStorytelling helps people to imagine and dream -&gt; empathise -&gt; care\nKey elements:\n\nCharacter\nTension\nRight level of detail\nSatisfying resolution\n\nIf you’re telling a story, know why you’re telling it. The goal is to give, not boost your ego.\nConnect the dots enough, but don’t force-feed the conclusion.\n\n\n\nSteps:\n\nStart where the audience is. No assumed knowledge or jargon.\nSpark curiosity. Create a knowledge gap that the listener wants to fill.\nIntroduce new concepts one-by-one. Name each concept.\nMetaphors and analogies. Reveal the “shape” of new concepts.\nExamples confirm understanding.\n\nKnowledge is built hierarchically. Explain the structure and where each concept fits. Convert a web of ideas into a string of words.\nCurse of knowledge. Revisit assumed knowledge.\nMake clear what the concept isn’t. An explanation is a small mental model in a large space of possibilities. Reduce the size of that space.\n\n\n\nDemolish the listener’s existing knowledge and rebuild it; replace it with something better. Genius is something that comes to you, not something you are.\nAn “intuition pump” is an example or metaphor that makes the conclusion more plausible. Not a rigorous argument, but nudges the listener in the right direction. Prime with emotion or story, then persuade with reason.\nTurn the audience into detectives. Tell a story. Start with a big mystery, traverse the possible solutions until only one plausible conclusion remains.\n\n\n\nThree approaches to revelatory talks:\n\nThe wonder walk\n\nReveal a succession of inspiration images\nObvious throughline with accessible language and silence\nThe message should be “how intriguing is this” not “look what I achieved”\n\nDemo\n\nInitial tease, background context, demo, implications\n\nDreamscape\n\nCreate a world that doesn’t exist but someday might\nPaint a bold picture of an alternative future\nMake others desire that future\nEmphasise human values not just clever tech, otherwise the audience might freak out at the possible negative implications of the technology\n\n\n\n\n\n\n\n\nSlides can distract. If you do use them, make sure they add something.\nVisuals should:\n\nReveal - set context, prime, BAM!\nExplain - one idea per slide with a clickbait headline not a summary. Highlight the point you’re making.\nDelight - show impressive things and let them speak for themselves. Don’t need to explain every image/slide.\n\nDon’t let the slides be spoilers for what you’re about to say. The message loses its impact, it’s not news anymore.\n\n\n\nThe goal is to have a set structure that you speak naturally and authentically about.\nBoth approaches have the same result:\n\nScript and memorise until natural, or\nFreestyle around bullet points and rehearse until structure is set\n\nBeing read to and being talked to are very different experiences\n\nDictating the speech rather than writing it can help make sure it comes across how you would speak rather than how you would write.\nDon’t end up in the uncanny valley between reading and speaking\nIn some cases, reading is powerful if it’s clear that this is a poetic, written piece. Abandon the script for an impactful finale.\n\nWhat you are saying matters more than the exact word choice.\nRehearse your impromptu remarks.\n\n\n\nPractice speaking by speaking!\nRehearse in phases:\n\nEditing and cutting\nPacing and timing\nDelivery\n\nAim to use &lt;90% of the time limit. results in tighter writing, and time to riff and enjoy the reaction.\n\n\n\nYou have the audience’s attention at the start, then it’s yours to lose. Opener has to capture attention and keep it. 2 stages:\n\n10 seconds to capture attention\n1 minute to hold it\n\nOpening techniques:\n\nDrama\nCuriosity - more specific questions are more intriguing\nCompelling slide/image - “the next image changed my life”\nTense but don’t give away - show where you’re going but save the reveal\n\nThe closer dictates how the whole talk will be remembered.\nClosing techniques:\n\nCamera pullback - big picture and implications\nCall to action\nPersonal commitment\nInspiring values/vision\nEncapsulation - neatly reframe the main idea\nNarrative symmetry - callback to opener\nLyrical inspiration - poetic conclusion\n\n\n\n\n\n\n\n\nChoose an outfit early\nDress like the audience but smarter\nDress for the people in the back row\n\n\n\n\n\nEmbrace the nerves and draw attention to it. Vulnerability humanises you.\nPick friendly faces in the audience and speak to them.\nBackup plan - have a story ready to fill in during unexpected technical issues\nFocus on the message - “THIS MATTERS”\n\n\n\n\nVisual barriers between the speaker and audience can create a sense of authority but at the expense of a human connection. If they can see you, you’re vulnerable. If you’re vulnerable, they can connect.\nReferring to notes is fine if done sparingly and unambiguously. It humanises you if done honestly. It appears sneaky and dishonest if you try to do it secretly.\n\n\n\nSpeaking style adds another stream of input parallel to the words themselves. It tells the listener how they should interpret the words.\nSix voice tools. Vary each of them depending on the meaning and emotion.\n\nVolume\nPitch\nPace\nTimbre\nTone\nProsody (singsong rise and fall)\n\nSpeed should be a conversational pace, approx 130-170 words per minute.\n\nPeople often worry about speaking too fast\nSpeaking too slow is a more common problem (overcorrection?)\nUnderstanding outpaces articulation, the listener is “dying of word starvation”\n\nSpeak, don’t orate.\nBody language\n\nStand and move intentionally.\nStop to make a point then walk to the next point.\n\n\n\n\nAdd too many ingredients and you risk losing attention. What you say can just be signposts for what you show - set it up, show it, shut up.\n\n\n\n\n\n\nKnowledge of specific facts is becoming more specialised and commoditised. Understanding how everything fits together is becoming broader, more unified.\nConcepts of specialised knowledge are outdated, stemming from industrial age thinknig. Modern thinking values:\n\nContextual knowledge\nCreating knowledge\nUnderstanding of humanity\n\nAs people become more interconnected, innovation becomes crowd-accelerated.\nHappiness is finding something bigger than you and dedicating your life to it. Nudge the world - give it questions to spark conversations. “The future is not yet written, we are all collectively in the process of writing it”"
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html#foundation",
    "href": "posts/business/public_speaking/public_speaking.html#foundation",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "Presentation literacy is its own skill. Your goal is to be you, not a version of someone you like.\n\n\n\nIdeas are all that matter in the talk. This can be a novel idea, or a novel story or presentation of an existing idea.\nYour goal is to recreate the core idea inside your audience’s mind. The language you use must be shared by the speaker and listener for it to have this effect. Focus on what you can give to the audience.\nVisualise the talk as a journey that you guide the user through. Start where the audience starts. Don’t make any impossible leaps or unexpected changes of direction.\n\nWhat issues matter the most\nHow are they related?\nHow can they be easily explained?\nWhat are the key open questions? Controversies?\n\n\n\n\nFour talk styles to AVOID.\n\nSales Pitch\n\nYou should give not take.\nGenerosity evokes a response.\n\nRamble\n\nInsults the audience; suggests that their time doesn’t matter to you.\n\nOrganisation bore\n\nYour company’s structure is boring.\nFocus on the work and ideas, not the company, products or individuals.\n\nInspiration tryhard\n\nAll style and no substance.\nComes across as manipulative.\nInspiration has to be earned. “Like love you don’t get it by pursuing it”.\nInspiration is the response to authenticity, courage, wisdom, selflessness.\n\n\n\n\n\nThe throughline is a connecting theme that ties the narrative elements\n\nA focused, precise, unexpected idea\nNot the same as a theme\nYou should be able to encapsulate it in &lt;15 words\n\nTrace the path the journey will take. If the audience knows where you’re going, it will be easier to follow.\nCut down the talk to keep the throughline focused.\n\nDON’T just keep all the content but say it in less detail. Overstuffed = underexplained\nCut back the range of topics. Say less with more impact.\n“The power of the deleted word”.\nAn 18 minute time limit is short enough to keep an audience’s attention, long enough to cover enough ground and precise enough to be taken seriously.\n\nThe overall structure can be thought of in 3 parts: What? So what? What now? In more detail, this is:\n\nIntro - what will be covered\nContext - why this issue matters\nMain concepts\nPractical implications\n\nTalk about ideas not issues. Write for an audience of one.\nChecklist:\n\nIs this a topic I’m passionate about? Do I know enough to be worth the audience’s time? Do I have credibility?\nDoes it inspire curiosity?\nWill knowing this make a difference to the audience?\nIs the talk a gift or an ask?\nIs the information fresh?\nCan the topic be covered in enough detail with examples in the time slot?\nWhat are the 15 words that encapsulate the talk?\nWould those 15 words persuade someone they’re interested in hearing the talk?"
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html#talk-tools",
    "href": "posts/business/public_speaking/public_speaking.html#talk-tools",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "A human connection is required before you can inform, persuade or inspire. Knowledge has to be pulled in, not pushed in. Be AUTHENTIC.\nDos\n\nEye contact.\nVulnerability, but authentic.\nHumour signals to the group that you’ve all connected.\n\nDon’ts\n\nEgo breeds contempt.\nAvoid tribal topics/language, e.g. politics.\n\n\n\n\nStorytelling helps people to imagine and dream -&gt; empathise -&gt; care\nKey elements:\n\nCharacter\nTension\nRight level of detail\nSatisfying resolution\n\nIf you’re telling a story, know why you’re telling it. The goal is to give, not boost your ego.\nConnect the dots enough, but don’t force-feed the conclusion.\n\n\n\nSteps:\n\nStart where the audience is. No assumed knowledge or jargon.\nSpark curiosity. Create a knowledge gap that the listener wants to fill.\nIntroduce new concepts one-by-one. Name each concept.\nMetaphors and analogies. Reveal the “shape” of new concepts.\nExamples confirm understanding.\n\nKnowledge is built hierarchically. Explain the structure and where each concept fits. Convert a web of ideas into a string of words.\nCurse of knowledge. Revisit assumed knowledge.\nMake clear what the concept isn’t. An explanation is a small mental model in a large space of possibilities. Reduce the size of that space.\n\n\n\nDemolish the listener’s existing knowledge and rebuild it; replace it with something better. Genius is something that comes to you, not something you are.\nAn “intuition pump” is an example or metaphor that makes the conclusion more plausible. Not a rigorous argument, but nudges the listener in the right direction. Prime with emotion or story, then persuade with reason.\nTurn the audience into detectives. Tell a story. Start with a big mystery, traverse the possible solutions until only one plausible conclusion remains.\n\n\n\nThree approaches to revelatory talks:\n\nThe wonder walk\n\nReveal a succession of inspiration images\nObvious throughline with accessible language and silence\nThe message should be “how intriguing is this” not “look what I achieved”\n\nDemo\n\nInitial tease, background context, demo, implications\n\nDreamscape\n\nCreate a world that doesn’t exist but someday might\nPaint a bold picture of an alternative future\nMake others desire that future\nEmphasise human values not just clever tech, otherwise the audience might freak out at the possible negative implications of the technology"
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html#preparation-process",
    "href": "posts/business/public_speaking/public_speaking.html#preparation-process",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "Slides can distract. If you do use them, make sure they add something.\nVisuals should:\n\nReveal - set context, prime, BAM!\nExplain - one idea per slide with a clickbait headline not a summary. Highlight the point you’re making.\nDelight - show impressive things and let them speak for themselves. Don’t need to explain every image/slide.\n\nDon’t let the slides be spoilers for what you’re about to say. The message loses its impact, it’s not news anymore.\n\n\n\nThe goal is to have a set structure that you speak naturally and authentically about.\nBoth approaches have the same result:\n\nScript and memorise until natural, or\nFreestyle around bullet points and rehearse until structure is set\n\nBeing read to and being talked to are very different experiences\n\nDictating the speech rather than writing it can help make sure it comes across how you would speak rather than how you would write.\nDon’t end up in the uncanny valley between reading and speaking\nIn some cases, reading is powerful if it’s clear that this is a poetic, written piece. Abandon the script for an impactful finale.\n\nWhat you are saying matters more than the exact word choice.\nRehearse your impromptu remarks.\n\n\n\nPractice speaking by speaking!\nRehearse in phases:\n\nEditing and cutting\nPacing and timing\nDelivery\n\nAim to use &lt;90% of the time limit. results in tighter writing, and time to riff and enjoy the reaction.\n\n\n\nYou have the audience’s attention at the start, then it’s yours to lose. Opener has to capture attention and keep it. 2 stages:\n\n10 seconds to capture attention\n1 minute to hold it\n\nOpening techniques:\n\nDrama\nCuriosity - more specific questions are more intriguing\nCompelling slide/image - “the next image changed my life”\nTense but don’t give away - show where you’re going but save the reveal\n\nThe closer dictates how the whole talk will be remembered.\nClosing techniques:\n\nCamera pullback - big picture and implications\nCall to action\nPersonal commitment\nInspiring values/vision\nEncapsulation - neatly reframe the main idea\nNarrative symmetry - callback to opener\nLyrical inspiration - poetic conclusion"
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html#on-stage",
    "href": "posts/business/public_speaking/public_speaking.html#on-stage",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "Choose an outfit early\nDress like the audience but smarter\nDress for the people in the back row\n\n\n\n\n\nEmbrace the nerves and draw attention to it. Vulnerability humanises you.\nPick friendly faces in the audience and speak to them.\nBackup plan - have a story ready to fill in during unexpected technical issues\nFocus on the message - “THIS MATTERS”\n\n\n\n\nVisual barriers between the speaker and audience can create a sense of authority but at the expense of a human connection. If they can see you, you’re vulnerable. If you’re vulnerable, they can connect.\nReferring to notes is fine if done sparingly and unambiguously. It humanises you if done honestly. It appears sneaky and dishonest if you try to do it secretly.\n\n\n\nSpeaking style adds another stream of input parallel to the words themselves. It tells the listener how they should interpret the words.\nSix voice tools. Vary each of them depending on the meaning and emotion.\n\nVolume\nPitch\nPace\nTimbre\nTone\nProsody (singsong rise and fall)\n\nSpeed should be a conversational pace, approx 130-170 words per minute.\n\nPeople often worry about speaking too fast\nSpeaking too slow is a more common problem (overcorrection?)\nUnderstanding outpaces articulation, the listener is “dying of word starvation”\n\nSpeak, don’t orate.\nBody language\n\nStand and move intentionally.\nStop to make a point then walk to the next point.\n\n\n\n\nAdd too many ingredients and you risk losing attention. What you say can just be signposts for what you show - set it up, show it, shut up."
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html#reflection",
    "href": "posts/business/public_speaking/public_speaking.html#reflection",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "Knowledge of specific facts is becoming more specialised and commoditised. Understanding how everything fits together is becoming broader, more unified.\nConcepts of specialised knowledge are outdated, stemming from industrial age thinknig. Modern thinking values:\n\nContextual knowledge\nCreating knowledge\nUnderstanding of humanity\n\nAs people become more interconnected, innovation becomes crowd-accelerated.\nHappiness is finding something bigger than you and dedicating your life to it. Nudge the world - give it questions to spark conversations. “The future is not yet written, we are all collectively in the process of writing it”"
  },
  {
    "objectID": "posts/business/marketing/marketing.html",
    "href": "posts/business/marketing/marketing.html",
    "title": "Marketing Notes",
    "section": "",
    "text": "Notes from “The 1-Page Marketing Plan” by Allan Dib.\n\n\n\n\nThe overall phases of the direct response marketing strategy are:\n\n\n\nPhase\nStatus\nGoal\n\n\n\n\nBefore\nProspect\nGet them to know you\n\n\nDuring\nLead\nGet them to like you\n\n\nAfter\nCustomer\nGet them to trust you\n\n\n\nDirect response marketing should be:\n\nSpecific\nTrackable\nInclude an offer\nFollow up\n\nA marketing plan encompasses:\n\n\n\n\n\n\n\nStage\nExample\n\n\n\n\nAdvertising\nSign for the circus\n\n\nPromotion\nPut the sign on an elephant and walk it into town\n\n\nPublicity\nElephant tramples the mayor’s flowers and it makes the news\n\n\nPR\nYou get the mayor to laugh it off\n\n\nSales\nPeople come to the circus and spend money\n\n\n\nStrategy changes with scale\n\nDon’t blindly copy large companies. “Branding” to “get your name out there” is expensive and ineffective.\nThe one and only objective of marketing for a small company is “make a profit” For large companies it may be: appease shareholders, board, managers, existing clients, win advertising awards. Making a profit is a low priority for them.\nStrategy without tactics = paralysis by analysis\nTactics without strategy = bright shiny object syndrome\n\nProduct quality is a customer retention tool, not a customer acquisition tool.\n\nPeople don’t know how good the product is until they’ve already bought it\n“If you build it, they will come” doesn’t work\n\nThe 80/20 rule (Pareto principle) states that 80% of the outputs come from 20% of the inputs. Applying the 80/20 rule to the 80/20 rule gives the 64/4 rule: 64% of the outputs come from 4% of the inputs. Identify this 4% and focus on it. Often this is marketing.\nSpend money to make time, not vice versa. Money is a renewable resource, time is finite.\n\n\n\nThe niche should be as specific as possible - an inch wide and a mile deep.\n\nToo broad a target market is useless as the message gets diluted\nMass marketing/branding can work for larger companies but not small. Large companies have enough firepower to fire arrows in all directions. Small companies need to fire in a specific direction.\n\nBecome a specialist in your field to make price irrelevant\n\nBecome an educator and build trust\nYou don’t want to be a commodity shopped on price. A customer won on price will be lost on price.\n\nUse the PVP index to identify the ideal niche. Assign values to each possible segment for:\n\nPersonal fulfillment\nValue to the marketplace\nProfitability\n\nQuestions for the target prospect:\n\nWhat keeps them up at night?\nWhat are they afraid of?\nWhat are they angry about? Who are they angry at?\nWhat trends affect them?\nWhat do they secretly desire most? What one thing do they crave above all else?\nHow do they make decisions? Analytical, emotional, social proof\nWhat language/jargon do they use?\nWhat magazines do they read? What websites do they visit?\nWhat is a typical day like?\nWhat is the single most dominant emotion this market feels?\n\nCreate an avatar for each prospect. Put a name and photo against each market segment and describe a specific person in detail. Describe their life, job, activities, relationships. Answer each of the questions above from their POV.\n\n\n\nThere are two key elements of the message:\n\nPurpose of the ad\nWhat it focuses on\n\nKeep the message focused: 1 ad = 1 objective. “Marketing on purpose” rather than “marketing by accident”. There should be a clear call to action. You should know the exact action you want the prospect to take next.\nThe following two questions help refine the USP:\n\nWhy should they buy?\nWhy should they buy from me?\n\nKey takeaways for USP:\n\nWhy should they buy from you rather than competitor\nIf you remove the company name and logo, would people still know it’s you?\nQuality and service are not USPs. The customer only finds these out after they buy from you. USPs should attract customers before they buy.\nLowest price is not a USP. There’s always someone willing to go out of business quicker than you.\n\nPeople rarely want what you’re selling, they want the result of what you’re selling. Sell the problem. People don’t know what they want until they’re presented with it. Purchasing is done with emotions and justified with logic after the fact. “If I had asked people what they wanted, they’d have said faster horses” - Henry Ford.\nWhen you confuse them, you lose them.\n\nExplain the product and its benefit in one sentence.\nThe biggest threat isn’t that someone buys from a competitor, it’s that they do nothing.\nElevator pitch of the form: “You know ? What we do is ? In fact .”\n\nBe remarkable\n\nTransform something ordinary into something that makes the customer smile.\nUnique is dangerous, you just need to be “different enough”.\n\nCraft the offer\n\nIf you don’t give a compelling reason to buy, customers will default to price, and you become a commodity.\nDon’t just offer a discount, think about which product will definitely solve their biggest problem.\nAdd value rather than discounting, through bundles and customisation.\nCreate “apples to oranges” comparisons to your competitors to get out of the commmodity game.\nTarget pain points. Sell the problem, not a feature list. Customers in pain are price insensitive.\n\nCreate the offer:\n\nValue to customer\nLanguage and jargon they use\nReason why this offer is here. People are skeptical if it is too good to be true.\nValue stacking. Add in high-margin bonuses.\nUpsells\nOutrageous guarantee\nCreate scarcity. People react more to fear of loss than prospect of gain.\n\nWriting copy:\n\nBe yourself and be authentic. “Professional” = boring.\n“The enemy in common”. People buy with emotions: fear, love, greed, guilt, pride.\nCopy should address the bad parts of the product, not just the good. Who is this not for?\n\nNaming:\n\nTitle=content\nChoose clarity over cleverness\n\n\n\n\n\n“Half the money I spend on advertising is wasted; the trouble is I don’t know which half” - John Wanamaker\n\nMeasure ROI of ad campaign\n\nIf it made more money that it lost, it was a success. The only metric that matters.\nCustomer acquisition cost vs lifetime value of customer\nOther metrics like response rate are only useful to the extent that they help you calculate the above.\nSpecific target market is key. The goal of the ad is to make they prospect say “Hey, that’s for me”.\nWhat gets measured gets managed. Conversely, what you don’t know will hurt you.\n\nLifetime value of customer = Frontend (initial purchase) + Backend (subsequent purchases)\n\nIdeally the frontend alone exceeds the customer acquisition cost to be sustainable. Then the cost of the ad is immediately recouped and the ad campaign can be reupped for another cycle.\n\nSuccessful marketing has three components:\n\nMarket\nMessage\nMedia\n\nSocial media is a media not a strategy and it’s not free. It’s only free if your time is worthless.\nEmail is preferable to social media because you own the mailing list. The customer database which is a valuable asset in its own right.\n\nEmail regularly - weekly at least\nGive value - 3 value-building emails for each offer email\nSnail mail complements email - less cluttered and less competitive because fewer people still use it.\n\nMarketing budget\n\nThe only time for a budget is during the testing phase when your finding what works (what delivers a positive ROI)\nOnce you have a campaign that works, go all in. It makes money, it’s not an expense so don’t limit it.\nIf you could buy £10 notes for £5 you’d buy as many as possible, you wouldn’t limit it with a budget. The same applies for a marketing campaign with a £5 cost of acquisition and £10 lifetime value of customer.\n\nAvoid single points of failure\n\nOne customer, supplier, ad medium, lead generation method.\nPaid media is reliable and allows ROI to be tracked and measured.\n\n\n\n\n\n\n\nTreat marketing like farming not hunting. Avoid “random acts of marketing” that hunters do. Build a CRM system and infrastructure for new leads, follow-ups, conversions, etc.\nThe first goal is to generate leads - find people who are interested.\n\nAvoid trying to sell from the ad immediately\nCapture all who could be interested, not just those ready to buy immediately\nBuild a customer database, then sell to them later\n\nThe market for your product might break down something like:\n\n3% - ready to buy now\n7% - open to buying\n30% - interested but not right now\n60% - not interested\n\nBy selling directly from the ad you limit yourself just to the 3%. Capture leads first and nurture them to appeal to the 40% who are interested.\nEducate customers to offer value, then you become a trusted advisor rather than a pest salesman.\n\n\n\nThe process of taking people from being vaguely interested to desiring it. The nurturing process means they should be predisposed to buying from you before you ever try to sell to them.\nStay in contact.\n\nMost stop after 1 or 2 follow-ups. The most effective salespeople do 10+ follow-ups.\nThe money is in the follow-up\n\nBuild trust and add value. Don’t pester to close.\n\nWhen the customer is ready to buy, you will be top of mind and they are predisposed to buy. It should be when they are ready to buy, not when you are ready to sell.\nDo not pressure prospects to close. Don’t be a pest, be a welcome guest.\nContact regularly - “market until they buy or die”.\nMost contact should be value add, with occasional pitch/offer emails. 3:1 ratio of value add:pitch.\n\nShock and awe package.\n\nPhysical parcel mailed to high probability prospects.\n“Lumpy” mail grabs attention - snail mail with something physical inside.\nNeed numbers on conversion probabilities and lifetime value of customer for this approach to be viable. Good marketing can’t beat bad math.\nHigh value contents should:\n\nProvide value\nPosition you as a trusted expert\nMove prospect further down the buying cycle.\n\n\nMake lots of offers. A/B test with small segments. Take risks, write compelling copy and make outrageous guarantees.\nThree major types of people:\n\n\n\nType\nRole\nResponsibility\n\n\n\n\nEntrepreneur\n“Make it up”\nVision\n\n\nSpecialist\n“Make it real”\nImplement\n\n\nManager\n“Make it recur”\nDaily BAU\n\n\n\nMarketing calendar.\n\nWhat needs to be done and when\nDaily, weekly, monthly, quarterly, annually\nEvent-triggers tasks, e.g. on signup, on complaint\n\nDelegate.\n\nIf someone else can do the job 80% as good as you, delegate it.\nSeparate majors and minors. Focus on the majors, dump or delegate the minors.\nDon’t mistake movement for achievement.\nDays are expensive. You can get more money but not more time.\n\n\n\n\nOnce you reach a certain level of expertise, the real profit is in how you market yourself. A concert violinist makes more money performing concerts than busking.\nTrust is key in sales.\n\nDifficult for unknown businesses.\nPeople are wary because of dodgy sellers. Assume every dog bites.\nEducate -&gt; Position as expert -&gt; Trust\nDelay the sale to show:\n\nYou are willing to give before you take\nYou are an educator, so can be trusted\n\nStop selling and start educating, consulting, advising.\nEntrepreneur is someone who solves people’s problems for profit.\n\nOutrageous guarantees.\n\nReverse the risk from the customer to you.\nGuarantee needs to be specific and address the prospect’s reservations directly. Not just a generic money-back guarantee.\nIf you’re an ethical operator, you’re already implicitly providing a guarantee, just not marketing it. Use it!\n\nPricing.\n\nThe marketing potential of price is often underutilised.\nDon’t just naively undercut the market leader or use cost-plus pricing.\nSmall number of options/tiers\n\nMore choice means you are more likely to pique interest but less likely to keep interest.\nFear of a suboptimal choice; the paradox of choice.\nStandard and premium tiers and a good rule of thumb.\n\n“Unlimited” option can often be profitable because customers overestimate how much they will use, so overpay for unlimited use.\nUltra high ticket item\n\nMakes other tiers seem more reasonably priced\n10% of customers would pay 10x more. 1% of customers would pay 100x more.\n\n\nOther price considerations:\n\nDon’t discount, unless specifically using a loss-leader strategy.\nTry before you buy (the puppy dog close)\n\nThe sale feels less permanent so more palatable for the customer.\nThe onus is on the customer to return - shifts the power balance.\n\nDon’t segregate departments - everyone can warm up a prospect.\nPayment plans - people are less attached to future money than present money.\n\n\n\n\n\n\n\nThe goal is to turn customers into a tribe of raving fans. Most companies stop marketing at the sell, but this is where the process begins. Most of the money is in the backend not the frontend.\nSell them what they want, give them what they need.\n\nThese don’t always overlap\nIf they buy a treadmill and don’t use it, they’ll blame you when they don’t lose weight. You need to make sure they use your product correctly after they buy it.\nSplit the process up for them to make it less daunting.\n\nCreate a sense of theatre\n\nPeople don’t just want ot be serviced, they want to be entertained.\nInnovate on: pricing, financing, packaging, support, delivery etc, not just product. For example, “will it blend” YouTube videos to sell ordinary blenders.\n\nUse technology to reduce friction. Treat tech like an employee. Why am I hiring it? What are its KPIs?\nBecome a voice of value to your tribe.\n\nEducation-based marketing leads to trust and becoming an expert in the field.\nTell your audience the effort that goes into the product. For example, shampoo bottle that says how many mint leaves went into it.\n\nCreate replicable systems\n\nSystems ensure you have a business, rather than you are the business\nOften overlooked because it is “back office” or “not urgent”\nBenefits: build valuable asset; leverage and scalability; consistency; lower labour costs\nAreas:\n\nMarketing (leads)\nSales (follow-up)\nFulfillment\nAdmin\n\nCreate an operations manual:\n\nImagine the business is 10x the size\nIdentify all roles\nIdentify all tasks - what does each role do\nChecklist for each task\n\n\nExit strategy. Start with the end in mind. If the goal is to sell for $50 million, any decision can be framed as “will this help me get to $50 million?”. Who will buy the company? Why? Customer base, revenue or IP?\n\n\n\nPeople are 21x more likely to buy from a company they’ve previously bought from. Increase revenue by selling more to existing customers.\n\nRaise prices\n\n\nPeople are less price sensitive than you’d expect.\nSome may leave but these were the lowest-value customers - this may be polluted revenue you want to fire anyway. A customer won on price can be lost on price.\nGive a reason why prices increased. Explain the benefits they’ve received and future innovations.\nGrandfather existing customers on old price as a loyalty bonus.\n\n\nUpselling\n\n\n“Contrast principle” - people are less price sensitive to add-ons. If they’ve already bought the suit then the shirt seems cheap.\n“Customers who bought X also bought Y” - people want to fit social norms.\nSell more when the client is “hot and heavy” in the buying mood. Some people naively think you should overwhelm them with more selling.\n\n\nAscension\n\n\nMove customers up to a higher-priced tier.\nProduct mix - standard, premium, ultra-high.\n\n\nFrequency of purchases\n\n\nReminders, subscriptions\nReasons to come back; voucher for next purchase.\n\n\nReactivation\n\n\nLure back lapsed customers with strong offer and call to action.\nAsk why they haven’t returned.\nApologise and make corrections if necessary. Make the feel special if they do return.\n\nKey numbers - what gets measured gets managed\n\nLeads\nConversion rate\nAverage transaction value\nBreak even point\n\nFor subscription models measure:\n\nMonthly recurring revenue\nChurn rate\nCustomer lifetime value\n\nPolluted revenue - dollars from toxic customers != dollars from raving fans. Types of customers:\n\nTribe\nChurners - can’t afford you so drop you on price or intro offer ending\nVampires - suck up all your time, you can’t afford them\nSnow leopard - big customer but not replicable\n\nNet promoter score\n\n“How likely are you to recommend us to a friend?”\nDetractors 0-6; Passive 7-8; Promoters 9-10\nNPS=+100 if everyone is a promoter, -100 if everyone is a detractor\n\nFire problem customers\n\n“The customer is always right” is naive. It should be “the right customer is always right”.\nGenuine customer complaints are valuable and may help retain other customers who had the same issue but didn’t speak up.\nLow-value, price-sensitive customers complain the most. They take up your time and energy at the expense of your tribe.\nFiring detractors creates scarcity and frees your time to focus on the tribe. With limited supply, customers have to play and pay by your rules.\n\n\n\n\nOrchestrating referrals is an active task, not needy or desperate.\n\nDon’t just deliver a good product and hope for the best\nPeople refer because it makes them feel good, helpful or knowledgeable, not to do you a favour.\nAppeal to their ego, offer them something valuable, give them a reason to refer.\n\nAsk (for a referral) and ye shall receive.\n\n“Law of 250” - most people know 250 people well enough to invite to a wedding or funeral. This is 250 potential referrals per customer.\nSet expectation of referrals beforehand - “We’re going to do a great job and we need your help… Most people refer 3 others.”\n\nBe specific about who you ask and what kind of referral you want. Conquer the bystander effect.\nMake referral profiles for each customer group in your database.\n\nWho do they know?\nHow can you make them remember you?\nWhat will make them look good?\nHow will they provide value to their referral target\n\nCustomer buying behaviour\n\nWho has your customers before you?\nJoint venture for referrals\nSell/exchange leads, resell complementary products, affiliate referral partner\n\nBrand = personality of the business. Think of the business as a person:\n\nName\nDesign (what they wear)\nPositioning (how they communicate)\nBrand promise (core values)\nTarget market (who they associate with)\nBrand awareness (how well known / popular)\n\nBest form of brand building is selling\n\nBranding is what you do after someone has bought from you.\nBrand equity - customers crossing the road to buy from you and walk past competitors.\n\n\n\n\n\nImplementation is crucial\n\nKnowing and not doing is the same as not knowing.\nCommon pitfalls:\n\nParalysis by analysis - 80% out the door is better than 100% in the drawer; money loves speed.\nFail to delegate\n“My business is different” - Spend your effort figuring out how to make it work rather than why it won’t work.\n\n\nTime is not money\n\nEntrepreneurs get paid for the value they create, not the time or effort they put in.\nMaking money is a side effect of creating value.\n\nThe value of a business is the eyeballs it has access to.\nQuestions to ponder:\n\nWhat business should I be in?\nWhat technologies will disrupt it?\nHow can I take advantage of tech rather than fight it?"
  },
  {
    "objectID": "posts/business/marketing/marketing.html#the-before-phase",
    "href": "posts/business/marketing/marketing.html#the-before-phase",
    "title": "Marketing Notes",
    "section": "",
    "text": "The overall phases of the direct response marketing strategy are:\n\n\n\nPhase\nStatus\nGoal\n\n\n\n\nBefore\nProspect\nGet them to know you\n\n\nDuring\nLead\nGet them to like you\n\n\nAfter\nCustomer\nGet them to trust you\n\n\n\nDirect response marketing should be:\n\nSpecific\nTrackable\nInclude an offer\nFollow up\n\nA marketing plan encompasses:\n\n\n\n\n\n\n\nStage\nExample\n\n\n\n\nAdvertising\nSign for the circus\n\n\nPromotion\nPut the sign on an elephant and walk it into town\n\n\nPublicity\nElephant tramples the mayor’s flowers and it makes the news\n\n\nPR\nYou get the mayor to laugh it off\n\n\nSales\nPeople come to the circus and spend money\n\n\n\nStrategy changes with scale\n\nDon’t blindly copy large companies. “Branding” to “get your name out there” is expensive and ineffective.\nThe one and only objective of marketing for a small company is “make a profit” For large companies it may be: appease shareholders, board, managers, existing clients, win advertising awards. Making a profit is a low priority for them.\nStrategy without tactics = paralysis by analysis\nTactics without strategy = bright shiny object syndrome\n\nProduct quality is a customer retention tool, not a customer acquisition tool.\n\nPeople don’t know how good the product is until they’ve already bought it\n“If you build it, they will come” doesn’t work\n\nThe 80/20 rule (Pareto principle) states that 80% of the outputs come from 20% of the inputs. Applying the 80/20 rule to the 80/20 rule gives the 64/4 rule: 64% of the outputs come from 4% of the inputs. Identify this 4% and focus on it. Often this is marketing.\nSpend money to make time, not vice versa. Money is a renewable resource, time is finite.\n\n\n\nThe niche should be as specific as possible - an inch wide and a mile deep.\n\nToo broad a target market is useless as the message gets diluted\nMass marketing/branding can work for larger companies but not small. Large companies have enough firepower to fire arrows in all directions. Small companies need to fire in a specific direction.\n\nBecome a specialist in your field to make price irrelevant\n\nBecome an educator and build trust\nYou don’t want to be a commodity shopped on price. A customer won on price will be lost on price.\n\nUse the PVP index to identify the ideal niche. Assign values to each possible segment for:\n\nPersonal fulfillment\nValue to the marketplace\nProfitability\n\nQuestions for the target prospect:\n\nWhat keeps them up at night?\nWhat are they afraid of?\nWhat are they angry about? Who are they angry at?\nWhat trends affect them?\nWhat do they secretly desire most? What one thing do they crave above all else?\nHow do they make decisions? Analytical, emotional, social proof\nWhat language/jargon do they use?\nWhat magazines do they read? What websites do they visit?\nWhat is a typical day like?\nWhat is the single most dominant emotion this market feels?\n\nCreate an avatar for each prospect. Put a name and photo against each market segment and describe a specific person in detail. Describe their life, job, activities, relationships. Answer each of the questions above from their POV.\n\n\n\nThere are two key elements of the message:\n\nPurpose of the ad\nWhat it focuses on\n\nKeep the message focused: 1 ad = 1 objective. “Marketing on purpose” rather than “marketing by accident”. There should be a clear call to action. You should know the exact action you want the prospect to take next.\nThe following two questions help refine the USP:\n\nWhy should they buy?\nWhy should they buy from me?\n\nKey takeaways for USP:\n\nWhy should they buy from you rather than competitor\nIf you remove the company name and logo, would people still know it’s you?\nQuality and service are not USPs. The customer only finds these out after they buy from you. USPs should attract customers before they buy.\nLowest price is not a USP. There’s always someone willing to go out of business quicker than you.\n\nPeople rarely want what you’re selling, they want the result of what you’re selling. Sell the problem. People don’t know what they want until they’re presented with it. Purchasing is done with emotions and justified with logic after the fact. “If I had asked people what they wanted, they’d have said faster horses” - Henry Ford.\nWhen you confuse them, you lose them.\n\nExplain the product and its benefit in one sentence.\nThe biggest threat isn’t that someone buys from a competitor, it’s that they do nothing.\nElevator pitch of the form: “You know ? What we do is ? In fact .”\n\nBe remarkable\n\nTransform something ordinary into something that makes the customer smile.\nUnique is dangerous, you just need to be “different enough”.\n\nCraft the offer\n\nIf you don’t give a compelling reason to buy, customers will default to price, and you become a commodity.\nDon’t just offer a discount, think about which product will definitely solve their biggest problem.\nAdd value rather than discounting, through bundles and customisation.\nCreate “apples to oranges” comparisons to your competitors to get out of the commmodity game.\nTarget pain points. Sell the problem, not a feature list. Customers in pain are price insensitive.\n\nCreate the offer:\n\nValue to customer\nLanguage and jargon they use\nReason why this offer is here. People are skeptical if it is too good to be true.\nValue stacking. Add in high-margin bonuses.\nUpsells\nOutrageous guarantee\nCreate scarcity. People react more to fear of loss than prospect of gain.\n\nWriting copy:\n\nBe yourself and be authentic. “Professional” = boring.\n“The enemy in common”. People buy with emotions: fear, love, greed, guilt, pride.\nCopy should address the bad parts of the product, not just the good. Who is this not for?\n\nNaming:\n\nTitle=content\nChoose clarity over cleverness\n\n\n\n\n\n“Half the money I spend on advertising is wasted; the trouble is I don’t know which half” - John Wanamaker\n\nMeasure ROI of ad campaign\n\nIf it made more money that it lost, it was a success. The only metric that matters.\nCustomer acquisition cost vs lifetime value of customer\nOther metrics like response rate are only useful to the extent that they help you calculate the above.\nSpecific target market is key. The goal of the ad is to make they prospect say “Hey, that’s for me”.\nWhat gets measured gets managed. Conversely, what you don’t know will hurt you.\n\nLifetime value of customer = Frontend (initial purchase) + Backend (subsequent purchases)\n\nIdeally the frontend alone exceeds the customer acquisition cost to be sustainable. Then the cost of the ad is immediately recouped and the ad campaign can be reupped for another cycle.\n\nSuccessful marketing has three components:\n\nMarket\nMessage\nMedia\n\nSocial media is a media not a strategy and it’s not free. It’s only free if your time is worthless.\nEmail is preferable to social media because you own the mailing list. The customer database which is a valuable asset in its own right.\n\nEmail regularly - weekly at least\nGive value - 3 value-building emails for each offer email\nSnail mail complements email - less cluttered and less competitive because fewer people still use it.\n\nMarketing budget\n\nThe only time for a budget is during the testing phase when your finding what works (what delivers a positive ROI)\nOnce you have a campaign that works, go all in. It makes money, it’s not an expense so don’t limit it.\nIf you could buy £10 notes for £5 you’d buy as many as possible, you wouldn’t limit it with a budget. The same applies for a marketing campaign with a £5 cost of acquisition and £10 lifetime value of customer.\n\nAvoid single points of failure\n\nOne customer, supplier, ad medium, lead generation method.\nPaid media is reliable and allows ROI to be tracked and measured."
  },
  {
    "objectID": "posts/business/marketing/marketing.html#the-during-phase",
    "href": "posts/business/marketing/marketing.html#the-during-phase",
    "title": "Marketing Notes",
    "section": "",
    "text": "Treat marketing like farming not hunting. Avoid “random acts of marketing” that hunters do. Build a CRM system and infrastructure for new leads, follow-ups, conversions, etc.\nThe first goal is to generate leads - find people who are interested.\n\nAvoid trying to sell from the ad immediately\nCapture all who could be interested, not just those ready to buy immediately\nBuild a customer database, then sell to them later\n\nThe market for your product might break down something like:\n\n3% - ready to buy now\n7% - open to buying\n30% - interested but not right now\n60% - not interested\n\nBy selling directly from the ad you limit yourself just to the 3%. Capture leads first and nurture them to appeal to the 40% who are interested.\nEducate customers to offer value, then you become a trusted advisor rather than a pest salesman.\n\n\n\nThe process of taking people from being vaguely interested to desiring it. The nurturing process means they should be predisposed to buying from you before you ever try to sell to them.\nStay in contact.\n\nMost stop after 1 or 2 follow-ups. The most effective salespeople do 10+ follow-ups.\nThe money is in the follow-up\n\nBuild trust and add value. Don’t pester to close.\n\nWhen the customer is ready to buy, you will be top of mind and they are predisposed to buy. It should be when they are ready to buy, not when you are ready to sell.\nDo not pressure prospects to close. Don’t be a pest, be a welcome guest.\nContact regularly - “market until they buy or die”.\nMost contact should be value add, with occasional pitch/offer emails. 3:1 ratio of value add:pitch.\n\nShock and awe package.\n\nPhysical parcel mailed to high probability prospects.\n“Lumpy” mail grabs attention - snail mail with something physical inside.\nNeed numbers on conversion probabilities and lifetime value of customer for this approach to be viable. Good marketing can’t beat bad math.\nHigh value contents should:\n\nProvide value\nPosition you as a trusted expert\nMove prospect further down the buying cycle.\n\n\nMake lots of offers. A/B test with small segments. Take risks, write compelling copy and make outrageous guarantees.\nThree major types of people:\n\n\n\nType\nRole\nResponsibility\n\n\n\n\nEntrepreneur\n“Make it up”\nVision\n\n\nSpecialist\n“Make it real”\nImplement\n\n\nManager\n“Make it recur”\nDaily BAU\n\n\n\nMarketing calendar.\n\nWhat needs to be done and when\nDaily, weekly, monthly, quarterly, annually\nEvent-triggers tasks, e.g. on signup, on complaint\n\nDelegate.\n\nIf someone else can do the job 80% as good as you, delegate it.\nSeparate majors and minors. Focus on the majors, dump or delegate the minors.\nDon’t mistake movement for achievement.\nDays are expensive. You can get more money but not more time.\n\n\n\n\nOnce you reach a certain level of expertise, the real profit is in how you market yourself. A concert violinist makes more money performing concerts than busking.\nTrust is key in sales.\n\nDifficult for unknown businesses.\nPeople are wary because of dodgy sellers. Assume every dog bites.\nEducate -&gt; Position as expert -&gt; Trust\nDelay the sale to show:\n\nYou are willing to give before you take\nYou are an educator, so can be trusted\n\nStop selling and start educating, consulting, advising.\nEntrepreneur is someone who solves people’s problems for profit.\n\nOutrageous guarantees.\n\nReverse the risk from the customer to you.\nGuarantee needs to be specific and address the prospect’s reservations directly. Not just a generic money-back guarantee.\nIf you’re an ethical operator, you’re already implicitly providing a guarantee, just not marketing it. Use it!\n\nPricing.\n\nThe marketing potential of price is often underutilised.\nDon’t just naively undercut the market leader or use cost-plus pricing.\nSmall number of options/tiers\n\nMore choice means you are more likely to pique interest but less likely to keep interest.\nFear of a suboptimal choice; the paradox of choice.\nStandard and premium tiers and a good rule of thumb.\n\n“Unlimited” option can often be profitable because customers overestimate how much they will use, so overpay for unlimited use.\nUltra high ticket item\n\nMakes other tiers seem more reasonably priced\n10% of customers would pay 10x more. 1% of customers would pay 100x more.\n\n\nOther price considerations:\n\nDon’t discount, unless specifically using a loss-leader strategy.\nTry before you buy (the puppy dog close)\n\nThe sale feels less permanent so more palatable for the customer.\nThe onus is on the customer to return - shifts the power balance.\n\nDon’t segregate departments - everyone can warm up a prospect.\nPayment plans - people are less attached to future money than present money."
  },
  {
    "objectID": "posts/business/marketing/marketing.html#the-after-phase",
    "href": "posts/business/marketing/marketing.html#the-after-phase",
    "title": "Marketing Notes",
    "section": "",
    "text": "The goal is to turn customers into a tribe of raving fans. Most companies stop marketing at the sell, but this is where the process begins. Most of the money is in the backend not the frontend.\nSell them what they want, give them what they need.\n\nThese don’t always overlap\nIf they buy a treadmill and don’t use it, they’ll blame you when they don’t lose weight. You need to make sure they use your product correctly after they buy it.\nSplit the process up for them to make it less daunting.\n\nCreate a sense of theatre\n\nPeople don’t just want ot be serviced, they want to be entertained.\nInnovate on: pricing, financing, packaging, support, delivery etc, not just product. For example, “will it blend” YouTube videos to sell ordinary blenders.\n\nUse technology to reduce friction. Treat tech like an employee. Why am I hiring it? What are its KPIs?\nBecome a voice of value to your tribe.\n\nEducation-based marketing leads to trust and becoming an expert in the field.\nTell your audience the effort that goes into the product. For example, shampoo bottle that says how many mint leaves went into it.\n\nCreate replicable systems\n\nSystems ensure you have a business, rather than you are the business\nOften overlooked because it is “back office” or “not urgent”\nBenefits: build valuable asset; leverage and scalability; consistency; lower labour costs\nAreas:\n\nMarketing (leads)\nSales (follow-up)\nFulfillment\nAdmin\n\nCreate an operations manual:\n\nImagine the business is 10x the size\nIdentify all roles\nIdentify all tasks - what does each role do\nChecklist for each task\n\n\nExit strategy. Start with the end in mind. If the goal is to sell for $50 million, any decision can be framed as “will this help me get to $50 million?”. Who will buy the company? Why? Customer base, revenue or IP?\n\n\n\nPeople are 21x more likely to buy from a company they’ve previously bought from. Increase revenue by selling more to existing customers.\n\nRaise prices\n\n\nPeople are less price sensitive than you’d expect.\nSome may leave but these were the lowest-value customers - this may be polluted revenue you want to fire anyway. A customer won on price can be lost on price.\nGive a reason why prices increased. Explain the benefits they’ve received and future innovations.\nGrandfather existing customers on old price as a loyalty bonus.\n\n\nUpselling\n\n\n“Contrast principle” - people are less price sensitive to add-ons. If they’ve already bought the suit then the shirt seems cheap.\n“Customers who bought X also bought Y” - people want to fit social norms.\nSell more when the client is “hot and heavy” in the buying mood. Some people naively think you should overwhelm them with more selling.\n\n\nAscension\n\n\nMove customers up to a higher-priced tier.\nProduct mix - standard, premium, ultra-high.\n\n\nFrequency of purchases\n\n\nReminders, subscriptions\nReasons to come back; voucher for next purchase.\n\n\nReactivation\n\n\nLure back lapsed customers with strong offer and call to action.\nAsk why they haven’t returned.\nApologise and make corrections if necessary. Make the feel special if they do return.\n\nKey numbers - what gets measured gets managed\n\nLeads\nConversion rate\nAverage transaction value\nBreak even point\n\nFor subscription models measure:\n\nMonthly recurring revenue\nChurn rate\nCustomer lifetime value\n\nPolluted revenue - dollars from toxic customers != dollars from raving fans. Types of customers:\n\nTribe\nChurners - can’t afford you so drop you on price or intro offer ending\nVampires - suck up all your time, you can’t afford them\nSnow leopard - big customer but not replicable\n\nNet promoter score\n\n“How likely are you to recommend us to a friend?”\nDetractors 0-6; Passive 7-8; Promoters 9-10\nNPS=+100 if everyone is a promoter, -100 if everyone is a detractor\n\nFire problem customers\n\n“The customer is always right” is naive. It should be “the right customer is always right”.\nGenuine customer complaints are valuable and may help retain other customers who had the same issue but didn’t speak up.\nLow-value, price-sensitive customers complain the most. They take up your time and energy at the expense of your tribe.\nFiring detractors creates scarcity and frees your time to focus on the tribe. With limited supply, customers have to play and pay by your rules.\n\n\n\n\nOrchestrating referrals is an active task, not needy or desperate.\n\nDon’t just deliver a good product and hope for the best\nPeople refer because it makes them feel good, helpful or knowledgeable, not to do you a favour.\nAppeal to their ego, offer them something valuable, give them a reason to refer.\n\nAsk (for a referral) and ye shall receive.\n\n“Law of 250” - most people know 250 people well enough to invite to a wedding or funeral. This is 250 potential referrals per customer.\nSet expectation of referrals beforehand - “We’re going to do a great job and we need your help… Most people refer 3 others.”\n\nBe specific about who you ask and what kind of referral you want. Conquer the bystander effect.\nMake referral profiles for each customer group in your database.\n\nWho do they know?\nHow can you make them remember you?\nWhat will make them look good?\nHow will they provide value to their referral target\n\nCustomer buying behaviour\n\nWho has your customers before you?\nJoint venture for referrals\nSell/exchange leads, resell complementary products, affiliate referral partner\n\nBrand = personality of the business. Think of the business as a person:\n\nName\nDesign (what they wear)\nPositioning (how they communicate)\nBrand promise (core values)\nTarget market (who they associate with)\nBrand awareness (how well known / popular)\n\nBest form of brand building is selling\n\nBranding is what you do after someone has bought from you.\nBrand equity - customers crossing the road to buy from you and walk past competitors."
  },
  {
    "objectID": "posts/business/marketing/marketing.html#conclusion",
    "href": "posts/business/marketing/marketing.html#conclusion",
    "title": "Marketing Notes",
    "section": "",
    "text": "Implementation is crucial\n\nKnowing and not doing is the same as not knowing.\nCommon pitfalls:\n\nParalysis by analysis - 80% out the door is better than 100% in the drawer; money loves speed.\nFail to delegate\n“My business is different” - Spend your effort figuring out how to make it work rather than why it won’t work.\n\n\nTime is not money\n\nEntrepreneurs get paid for the value they create, not the time or effort they put in.\nMaking money is a side effect of creating value.\n\nThe value of a business is the eyeballs it has access to.\nQuestions to ponder:\n\nWhat business should I be in?\nWhat technologies will disrupt it?\nHow can I take advantage of tech rather than fight it?"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html",
    "href": "posts/software/software_architecture/software_architect_notes.html",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "Notes from “The Complete Guide to Becoming a Software Architect” Udemy course\n\n\nA developer knows what can be done, an architect knows what should be done. How do we use technology to meet business requirements.\nGeneral system requirements:\n\nFast\nSecure\nReliable\nEasy to maintain\n\nArchitect’s need to know how to code for:\n\nArchitecture’s trustworthiness\nSupport developers\nRespect of developers\n\n\n\n\n\nUnderstand the business - Strengths, weaknesses, compettions, growth strategy.\nDefine the system’s goals - Goals are not requirements. Goals describe the effect on the organisation, requirements describe what the system should do.\nWork for your client’s clients - Prioritise the end user.\nTalk to the right people with the right language - What is the thing that really matters to the person I’m talking to?\n\nProject managers care about how things affect deadlines, developers care about the technologies used, CEOs care about business continuity and bottom line.\n\n\n\n\nUnderstand the system requirements - What the system should do.\nUnderstand the non-functional requirements - Technical and service level attributes, e.g. number of users, loads, volumes, performance.\nMap the components - Understand the system functionality and communicate this to your client. Completely non-technical at this point, no mention of specific technologies. A diagram of high level components helps.\nSelect the technology stack - Backend, frontend, data store.\nDesign the architecture\nWrite the architecture document\nSupport the team\n\nInclude developers in non-functional requirements and architecture design for two reasons:\n\nLearn about unknown scenarios early\nCreate ambassadors\n\n\n\n\nThe 2 types of requirements: functional and non-functional.\n\n\n“What the system should do”\n\nBusiness flows\nBusiness services\nUser interfaces\n\n\n\n\n“What the system should deal with”\n\nPerformance - latency and throughput\n\nLatency - How long does it take to perform a single task?\nThroughput - How many tasks can be performed in a given time unit?\n\nLoad - Quantity of work without crashing. Determines the availability of the system. Always look at peak (worst case) numbers.\nData volume - How much data will the system accumulate over time?\n\nData required on day one\nData growth (say, annually)\n\nConcurrent users - How many users will be using the system? This includes “dead times” which differentiates it from the load requirement. Rule of thumb is concurrent_users = load * 10\nSLA - Service Level Agreement for uptime.\n\nThe non-functional requirements are generally the more important in determining the architecture. The client will generally need guiding towards sensible values, otherwise they just want as much load as possible, as much uptime as possible etc.\n\n\n\n\nThe application type should be established early based on the use case and expected user interaction.\n\nWeb apps\n\nServe html pages.\nUI; user-initiated actions; large scale; short, focused actions.\nRequest-response model.\n\nWeb API\n\nServe data (often JSON).\nData retrieval and storage; client-initiated actions; large scale; short, focused actions.\nRequest-response model.\n\nMobile\n\nRequire user interaction; frontend for web API; location-based.\n\nConsole\n\nNo UI; limited interation; long-running processes; short actions for power users.\nRequire technical knowledge.\n\nService\n\nNo UI; managed by service manager; long-running processes.\n\nDesktop\n\nAll resources on the PC; UI; user-centric actions.\n\nFunction-as-a-service\n\nAWS lambda\n\n\n\n\n\nConsiderations:\n\nAppropriate for the task\nCommunity - e.g. stack overflow activity\nPopularity - google trends over 2 years\n\n\n\nCovers web app, web API, console, service.\nOptions: .NET, Java, node.js, PHP, Python\n\n\n\n\n\n\nFigure 1: Backend Tech\n\n\n\n\n\n\nCovers:\n\nWeb app - Angular, React\nMobile - Native (Swift, Java/Kotlin), Xamarin, React Native\nDesktop - depends on target OS\n\n\n\n\nSQL - small, structured data\n\nRelational tables\nTransactions\n\nAtomicity\nConsistency\nIsolation\nDurability\n\nQuerying language is universal\n\nNoSQL - huge, unstructured or semi-structured data\n\nEmphasis on scale and performance.\nSchema-less, with entities stored as JSON.\nEventual consistency - data can be temporarily inconsistent\nNo universal querying language\n\n\n\n\n\nQuality attributes that describe technical capabilities to fulfill the non-functional requirements. Non-functional requirements map to quality attributes, which are designed in the architecture.\n\nScalability - Adding computing resources without any interruption. Scale out (add more compute instances) is preferred over scale up (increase CPU, RAM on the existing instance). Scaling out introduces redundancy and is not limited by hardware.\nManageability - Know what’s going on and take action accordingly. The question “who reports the problems” determines if a system is manageable. The system should flag problems, not the suer.\nModularity - A system that is built from blcoks that can be changed or replaced without affecting the whole system.\nExtensibility - Functionality of the system can be extended without modifying existing code.\nTestability - How easy is it to test the application. Independent modules and methods and single responsibility principle aid testability.\n\n\n\n\nA software component is a piece of code that runs in a single process. Distributed systems are composed of independent software components deployed on separate processes/servers/containers.\nComponent architecture relates to the lower level details, whereas system architecture is the higher level view of how the components fit together to achieve the non-functional requirements.\n\n\nLayers represent horizontal functionality:\n\nUI/SI - user interface or service interface (i.e. an API), authentication\nBusiness logic - validation, enrichment, computation\nData access layer - connection handling, transaction handling, querying/saving data\n\nCode can flow downwards by one layer only. It can never skip a layer and can never go up a layer. This enforces modularity.\nA service might sit across layers, for example logging sits across the 3 layers described above. In this case, the logging service is called a “sross-cutting concern”.\nEach layer should be independent of the implementation of other layers. Layers should handle exceptions from those below them, log the error and then throw a more generic exception, so that there is no reference to other layers’ implementation.\nLayers are part of the same process. This is different from tiers, which are processes that are distributed across a network.\n\n\n\nAn interface declares the signature of an implementation. This means that each implementation must implement the methods described. The abstract base class in Python is an example of this.\nClose coupling should be avoided; “new is glue”.\n\n\n\n\nSingle responsibility principle: Each class, module or method should have exactly one responsibility.\nOpen/closed principle: Software should be open for extension but closed for modification. Can be implemented using class inheritance.\nLiskov substitution principle: If S is a subtype of T, then objects of type T can be replaced with objects of type S without altering the program. This looks similar to polymorphism but is stricter; not only should the classes implement the same methods, but the behaviour of those methods should be the same too, there should be no hidden functionality performed by S that is not performed by T or vice versa.\nInterface segregation principle: Many client-specific interfaces are preferable to one general purpose interface.\nDependency inversion principle: High-level modules should depend on abstractions rather than concrete implementations. Relies on dependency injection, where one object supplies the dependencies of another object. A factory method determines which class to load and returns an instance of that class. This also makes testing easier, as you can inject a mock class rather than having to mock out specific functionality.\n\n\n\n\n\nStructure - case, underscores\nContent - class names should be nouns, methods should be imperative verbs\n\n\n\n\nSome best practices:\n\nOnly catch exceptions if you are going to do something with it\nCatch specific exceptions\nUse try-catch on the smallest code fragments possible\nLayers should handle exceptions raised by layers below them, without exposing the specific implementation of the other layer. Catch, log, re-raise a more generic error.\n\nPurposes of logging:\n\nTrack errors\nGather data\n\n\n\n\n\nA collection of general, reusable solutions to common software design problems. Popularised in the book “Design patterns: elements of reusable object-oriented software”.\n\nThe factory pattern: Creating objects without specifying the exact class of the object. Avoids strong coupling between classes. Create an interface, then a factory method returns an instance of a class which implements that interface. If the implementation needs to change, we only need to change the factory method.\nThe repository pattern: Modules which don’t work with the data store should be oblivious to the type of data store. This is similar to the concept of the Data Access Layer. Layers are for architects, design patterns are for developers. They both seek to solve the same issue.\nThe facade pattern: Creating a layer of abstraction to mask complex actions. The facade does not create any new functionality, it is just a wrapper integrating existing modular functions together.\nThe command pattern: All the action’s information is encapsulated within an object.\n\n\n\n\nThe architecture design is the big picture that should answer the following:\n\nHow will the system work under heavy load?\nWhat will happen if the system crashes at a critical moment?\nHow complicated is it to update?\n\nThe architecture should:\n\nDefine components\nDefine how components communicate\nDefine the system’s quality attributes\n\nMake the correct choice as early as possible\n\n\nEnsuring the services are not strongly tied to other services. Avoid the “spiderweb” - when the graph of connections between services is densely connected.\nPrevents coupling of platforms and URLs.\nTo avoid URL coupling between services, two options are:\n\n“Yellow pages” directory\nGateway\n\nThe “yellow pages” contains a directory of all service URLs. If service A wants to query service D, A queries the yellow pages for D’s URL, then uses that URL to query D. This means that service A does not hardcode any information about service D. If D’s URLs change, then only the yellow pages need to be updated. Services only need to know the yellow pages directory’s URL.\n\n\n\n\n\n\nFigure 2: Yellow Pages\n\n\n\nThe gateway acts as a middleman. It holds a mapping table of all URLs Service A queries the gateway, which in turn queries service E. Service A doesn’t need to know about E or any other services. Services only need to know the gateway’s URL.\n\n\n\n\n\n\nFigure 3: Gateway\n\n\n\n\n\n\nThe application’s state is stored in only 2 places:\n\nThe data store\nThe user interface\n\nStateless architecture is preferred in virtually all circumstances.\nIn a stateful architecture, when a user logs in, the login service retrieves the user details from the database and stored them for future use. Data is stored in code.\nIf the login request was routed to server A, the user’s details are stored there. If the user later tried to add itemsto the cart and their cart service request is routed to server B, then their user details will not exist as those are stored on server A.\nDisadvantages of stateful:\n\nLack of scalability\nLack of redundancy\n\n\n\n\n\n\n\nFigure 4: Stateful\n\n\n\nIn a stateless architecture, no data is stored in the service itself. This means the behaviour will be the same regardless of which server a request was routed to.\n\n\n\n\n\n\nFigure 5: Stateless\n\n\n\n\n\n\nCaches store data locally to avoid retrieving the same data from the database multiple times. It trades reliability of data (it is stored in volatile memory) for improved performance.\nA cache should store data that is frequently accessed and rarely modified.\nTwo types of cache:\n\nIn-memory cache - Cache stored in memory on a single service\n\nPros:\n\nBest performance\nCan store any objects\nEasy to use\n\nCons:\n\nSize is limited by the process’s memory\nCan grow stale/inconsistent if the service is scaled out to multiple servers\n\n\nDistributed cache - Cache is independent of the services and can be accessed by all servers\n\nPros:\n\nSupports scaled out servers\nFailover capabilities\nStorage is unlimited\n\nCons:\n\nSetup is more complex\nOften only support primitive data types\nWorse performance than in-memory cache\n\n\n\n\n\n\nMessaging methods can be evaluated on these criteria:\n\nPerformance\nMessage size\nExecution model\nFeedback (handshaking) and reliability\nComplexity\n\nMessaging methods include:\n\nREST API\nHTTP Push\nQueue\nFile-based and database-based methods\n\n\n\nUniversal standard for HTTP-based systems.\nUseful for traditional web apps.\n\n\n\n\n\n\nFigure 6: REST API\n\n\n\n\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nVery fast\n\n\nMessage size\nSame as HTTP protocol limitations - GET 8KB, POST ~10MB\n\n\nExecution model\nRequest/response - ideal for quick, short actions\n\n\nFeedback\nImmediate feedback via response codes\n\n\nComplexity\nExtremely easy\n\n\n\n\n\n\nA client subscribes to the service waiting for an event. When that event occurs, the server notifies the client.\nFor real-time messaging, these often use web sockets to maintain the subscription connection, rather than a traditional request/response model.\nUseful for chat or monitoring.\n\n\n\n\n\n\nFigure 7: HTTP Push\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nVery fast\n\n\nMessage size\nLimited, few KB\n\n\nExecution model\nWeb socket connection / long polling\n\n\nFeedback\nNone (fire and forget)\n\n\nComplexity\nExtremely easy\n\n\n\n\n\n\nThe queue sits between two (or more) services. If service A wants to send a message to service B, A places the message in the queue and B periodically pulls from the queue.\nThis ensures messages will be handled exactly once and in the order received.\nUseful for complex systems with lots of data, when order and reliability are important.\n\n\n\n\n\n\nFigure 8: Queue\n\n\n\n\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nSlow - push/poll time and database persistence\n\n\nMessage size\nUnlimited but best practice to use small messages\n\n\nExecution model\nPolling\n\n\nFeedback\nVery reliable but feedback depends on the monitoring in place to ensure messages make it to the queue and get executed\n\n\nComplexity\nRequires training and setup to maintain a queue engine\n\n\n\n\n\n\nSimilar to queue, but rather han placing messages in a queue it places them in a file directory or database. There is no guarantee that messages are processed once and only once.\nIf multiple services are polling the same file folder, then when a new file is added we can get two issues:\n\nFile locked\nDuplicate processing\n\nSimilar use case to queues, but queues are generally preferred.\n\n\n\n\n\n\nFigure 9: File-based Messaging\n\n\n\n\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nSlow - push/poll time and database persistence\n\n\nMessage size\nUnlimited\n\n\nExecution model\nPolling\n\n\nFeedback\nVery reliable but feedback depends on the monitoring\n\n\nComplexity\nRequires training and setup to maintain the filestore or database\n\n\n\n\n\n\n\n\n\nCreate a central logging service that all other services call to write to a central database. This solves the problem where each microservice has its own log format, data and location. For example, some might be in files, SQL database, NoSQL database, etc.\nImplementation can be via an API or polling folders that each of the services write to.\n\n\n\n\n\n\nFigure 10: Central Logging Service\n\n\n\n\n\n\nCorrelation ID is an identifier attached to the beginning of a user flow and is attached to any action taken by that user, so that if there is an error at any point the user flow can be traced back through the different services’ logs.\n\n\n\n\n\nExternal considerations can affect architecture and design decisions.\n\nDeadlines\nDev team skills - New technologies can introduce uncertainty, delays and low quality\nIT support - Assign who will support the product from the outset. This should not be developers.\nCost - Build vs buy. Estimate cost vs value. Spend money to save time, don’t spend time to save money.\n\n\n\n\nThis should describe the basic elements of the system:\n\nTechnology stack\nComponents\nServices\nCommunication between components and services\n\nNo development should begin before the document is complete.\nGoals of the document:\n\nDescribe what should be developed and how\nList the functional and non-functional requirements\n\nAudience:\n\nEveryone involved with the system: project manager, CTO, QA leader, developers\nSections for management appear first as they are unlikely to read the whole document\nQA lead can begin preparing test infrastructure ahead of time\n\nFormat: UML (universal modeling language) is often used. It visualises the system’s design with concepts and diagrams. However, this assumes the audience is familiar with UML which is often not the case.\n\nKeep the contents as simple as possible\nUse plain English\nVisualise using whatever software is comfortable and appropriate\n\nStructure:\n\nBackground\nRequirements\nExecutive summary\nArchitecture overview\nComponents drill-down\n\n\n\nThis section validates your point of view and instils confidence that you understand the project.\nOne page for all team and management.\nDescribe the system from a business POV:\n\nThe system’s role\nReasons for replacing the old system\nExpected business impact\n\n\n\nFunctional and non-functional requirements - what should the system do and deal with? This section validates your understadning of the requirements. The requirements dictate the architecture so this section sets the scene for the chosen architecture.\nOne page for all team and management.\nThis should be a brief, bullet point list of requirements with no more than 3 lines on each.\n\n\n\n\nProvide a high-level view of the architecture.\nManagers will not read the whole document; &lt;3 pages for management.\n\nUse charts and diagrams\nWrite this after the rest of the document\nUse technical terms sparsely and only well known ones\nBe concise and don’t repeat yourself\n\n\n\n\nProvide a high-level view of the architecture in technical terms. Do not deep dive into specific components\n&lt;10 pages for developers and QA lead.\n\nGeneral description: type and major non-functional requirements\nHigh-level diagram of logical components, no specifics about hardware or technologies\nDiagram walkthrough: describe the various parts and their roles\nTechnology stack: iff there is a single stack include it here, otherwise include it in the components drill down section\n\n\n\n\nDetailed description of each component.\nUnlimited length, for developers and QA lead.\nFor each component:\n\nComponent’s role\nTechnology stack: Data store, backend, frontend\nComponent’s architecture: Describe the API (URL, logic, response code, comments). Describe the layers. Mention design patterns here.\nDevelopment instructions: Specific development guidelines\n\nBe specific and include rationale behind decisions.\n\n\n\n\nThese architecture patterns solve specific problems but add complexity to the system, so the costs and benefits should be compared.\n\n\nAn architecture in which functionalities are implemented as separate, loosely coupled services that interact with each other using a standard lightweight protocol.\nProblems with monolithic services:\n\nA single exception can crash the whole process\nUpdates impact all components\nLimited to one development platform/language\nUnoptimised compute resources\n\nWith microservices, each service is independent of others so can be updated separately, use a different platform, and be optimised separately.\nAn example of splitting a monolithic architecture into microservices:\n\n\n\n\n\n\nFigure 11: Monolith Example\n\n\n\nAs a microservice architecture, this becomes:\n\n\n\n\n\n\nFigure 12: Microservice Example\n\n\n\nProblems with microservices:\n\nComplex monitoring of all services and their interactions\nComplex architecture\nComplex testing\n\n\n\n\nStoring the deltas to each entity rather than updating it. The events can then be “rebuilt” from the start to give a view of the state at any given point in time.\nUse when history matters.\n\n\n\n\n\n\nFigure 13: Event Sourcing\n\n\n\nPros:\n\nTracing history\nSimple data model\nPerformance\nReporting\n\nCons:\n\nNo unified view - need to rebuild all events from the start to see the current state\nStorage usage\n\n\n\n\nCommand query responsibility segregation. Data storage and data retrieval are two separate databases, with a sync service between the two.\nThis integrates nicely with event sourcing, where events (deltas) are stored in one database and the current state is periodically built and stored in the retrieval database.\n\n\n\n\n\n\nFigure 14: CQRS\n\n\n\nPros:\n\nUseful with high-frequency updates that require near real-time querying\n\nCons:\n\nComplexity - need 2 databases, a sync service, ETL between the storage and retrieval database\n\n\n\n\n\nThe architect is often not the direct line manager of the developers implementing the software. You ultimately work with people, not software. Influence without authority is key.\n\nListening - assume you are not the smartest person in the room, collective wisdom is always better.\nDealing with criticism\n\nGenuine questioning: Provide facts and logic, be willing to go back and check again\nMocking: Don’t attack back, provide facts and logic\n\nBe smart not right\nOrganisational politics - be aware of it but don’t engage in it\nPublic speaking - define a goal, know your audience, be confident, don’t read, maintain eye contact\nLearning - blogs (DZone, InfoQ, O’Reilly), articles, conferences\n\n\n\n\n\n“The Complete Guide to Becoming a Software Architect” Udemy course\nDesign patterns\n\n\n\n\nFigure 1: Backend Tech\nFigure 2: Yellow Pages\nFigure 3: Gateway\nFigure 4: Stateful\nFigure 5: Stateless\nFigure 6: REST API\nFigure 7: HTTP Push\nFigure 8: Queue\nFigure 9: File-based Messaging\nFigure 10: Central Logging Service\nFigure 11: Monolith Example\nFigure 12: Microservice Example\nFigure 13: Event Sourcing\nFigure 14: CQRS"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#what-is-a-software-architect",
    "href": "posts/software/software_architecture/software_architect_notes.html#what-is-a-software-architect",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "A developer knows what can be done, an architect knows what should be done. How do we use technology to meet business requirements.\nGeneral system requirements:\n\nFast\nSecure\nReliable\nEasy to maintain\n\nArchitect’s need to know how to code for:\n\nArchitecture’s trustworthiness\nSupport developers\nRespect of developers"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#the-architects-mindset",
    "href": "posts/software/software_architecture/software_architect_notes.html#the-architects-mindset",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "Understand the business - Strengths, weaknesses, compettions, growth strategy.\nDefine the system’s goals - Goals are not requirements. Goals describe the effect on the organisation, requirements describe what the system should do.\nWork for your client’s clients - Prioritise the end user.\nTalk to the right people with the right language - What is the thing that really matters to the person I’m talking to?\n\nProject managers care about how things affect deadlines, developers care about the technologies used, CEOs care about business continuity and bottom line."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#the-architecture-process",
    "href": "posts/software/software_architecture/software_architect_notes.html#the-architecture-process",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "Understand the system requirements - What the system should do.\nUnderstand the non-functional requirements - Technical and service level attributes, e.g. number of users, loads, volumes, performance.\nMap the components - Understand the system functionality and communicate this to your client. Completely non-technical at this point, no mention of specific technologies. A diagram of high level components helps.\nSelect the technology stack - Backend, frontend, data store.\nDesign the architecture\nWrite the architecture document\nSupport the team\n\nInclude developers in non-functional requirements and architecture design for two reasons:\n\nLearn about unknown scenarios early\nCreate ambassadors"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#system-requirements",
    "href": "posts/software/software_architecture/software_architect_notes.html#system-requirements",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "The 2 types of requirements: functional and non-functional.\n\n\n“What the system should do”\n\nBusiness flows\nBusiness services\nUser interfaces\n\n\n\n\n“What the system should deal with”\n\nPerformance - latency and throughput\n\nLatency - How long does it take to perform a single task?\nThroughput - How many tasks can be performed in a given time unit?\n\nLoad - Quantity of work without crashing. Determines the availability of the system. Always look at peak (worst case) numbers.\nData volume - How much data will the system accumulate over time?\n\nData required on day one\nData growth (say, annually)\n\nConcurrent users - How many users will be using the system? This includes “dead times” which differentiates it from the load requirement. Rule of thumb is concurrent_users = load * 10\nSLA - Service Level Agreement for uptime.\n\nThe non-functional requirements are generally the more important in determining the architecture. The client will generally need guiding towards sensible values, otherwise they just want as much load as possible, as much uptime as possible etc."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#application-types",
    "href": "posts/software/software_architecture/software_architect_notes.html#application-types",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "The application type should be established early based on the use case and expected user interaction.\n\nWeb apps\n\nServe html pages.\nUI; user-initiated actions; large scale; short, focused actions.\nRequest-response model.\n\nWeb API\n\nServe data (often JSON).\nData retrieval and storage; client-initiated actions; large scale; short, focused actions.\nRequest-response model.\n\nMobile\n\nRequire user interaction; frontend for web API; location-based.\n\nConsole\n\nNo UI; limited interation; long-running processes; short actions for power users.\nRequire technical knowledge.\n\nService\n\nNo UI; managed by service manager; long-running processes.\n\nDesktop\n\nAll resources on the PC; UI; user-centric actions.\n\nFunction-as-a-service\n\nAWS lambda"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#select-technology-stack",
    "href": "posts/software/software_architecture/software_architect_notes.html#select-technology-stack",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "Considerations:\n\nAppropriate for the task\nCommunity - e.g. stack overflow activity\nPopularity - google trends over 2 years\n\n\n\nCovers web app, web API, console, service.\nOptions: .NET, Java, node.js, PHP, Python\n\n\n\n\n\n\nFigure 1: Backend Tech\n\n\n\n\n\n\nCovers:\n\nWeb app - Angular, React\nMobile - Native (Swift, Java/Kotlin), Xamarin, React Native\nDesktop - depends on target OS\n\n\n\n\nSQL - small, structured data\n\nRelational tables\nTransactions\n\nAtomicity\nConsistency\nIsolation\nDurability\n\nQuerying language is universal\n\nNoSQL - huge, unstructured or semi-structured data\n\nEmphasis on scale and performance.\nSchema-less, with entities stored as JSON.\nEventual consistency - data can be temporarily inconsistent\nNo universal querying language"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#the--ilities",
    "href": "posts/software/software_architecture/software_architect_notes.html#the--ilities",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "Quality attributes that describe technical capabilities to fulfill the non-functional requirements. Non-functional requirements map to quality attributes, which are designed in the architecture.\n\nScalability - Adding computing resources without any interruption. Scale out (add more compute instances) is preferred over scale up (increase CPU, RAM on the existing instance). Scaling out introduces redundancy and is not limited by hardware.\nManageability - Know what’s going on and take action accordingly. The question “who reports the problems” determines if a system is manageable. The system should flag problems, not the suer.\nModularity - A system that is built from blcoks that can be changed or replaced without affecting the whole system.\nExtensibility - Functionality of the system can be extended without modifying existing code.\nTestability - How easy is it to test the application. Independent modules and methods and single responsibility principle aid testability."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#components-architecture",
    "href": "posts/software/software_architecture/software_architect_notes.html#components-architecture",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "A software component is a piece of code that runs in a single process. Distributed systems are composed of independent software components deployed on separate processes/servers/containers.\nComponent architecture relates to the lower level details, whereas system architecture is the higher level view of how the components fit together to achieve the non-functional requirements.\n\n\nLayers represent horizontal functionality:\n\nUI/SI - user interface or service interface (i.e. an API), authentication\nBusiness logic - validation, enrichment, computation\nData access layer - connection handling, transaction handling, querying/saving data\n\nCode can flow downwards by one layer only. It can never skip a layer and can never go up a layer. This enforces modularity.\nA service might sit across layers, for example logging sits across the 3 layers described above. In this case, the logging service is called a “sross-cutting concern”.\nEach layer should be independent of the implementation of other layers. Layers should handle exceptions from those below them, log the error and then throw a more generic exception, so that there is no reference to other layers’ implementation.\nLayers are part of the same process. This is different from tiers, which are processes that are distributed across a network.\n\n\n\nAn interface declares the signature of an implementation. This means that each implementation must implement the methods described. The abstract base class in Python is an example of this.\nClose coupling should be avoided; “new is glue”.\n\n\n\n\nSingle responsibility principle: Each class, module or method should have exactly one responsibility.\nOpen/closed principle: Software should be open for extension but closed for modification. Can be implemented using class inheritance.\nLiskov substitution principle: If S is a subtype of T, then objects of type T can be replaced with objects of type S without altering the program. This looks similar to polymorphism but is stricter; not only should the classes implement the same methods, but the behaviour of those methods should be the same too, there should be no hidden functionality performed by S that is not performed by T or vice versa.\nInterface segregation principle: Many client-specific interfaces are preferable to one general purpose interface.\nDependency inversion principle: High-level modules should depend on abstractions rather than concrete implementations. Relies on dependency injection, where one object supplies the dependencies of another object. A factory method determines which class to load and returns an instance of that class. This also makes testing easier, as you can inject a mock class rather than having to mock out specific functionality.\n\n\n\n\n\nStructure - case, underscores\nContent - class names should be nouns, methods should be imperative verbs\n\n\n\n\nSome best practices:\n\nOnly catch exceptions if you are going to do something with it\nCatch specific exceptions\nUse try-catch on the smallest code fragments possible\nLayers should handle exceptions raised by layers below them, without exposing the specific implementation of the other layer. Catch, log, re-raise a more generic error.\n\nPurposes of logging:\n\nTrack errors\nGather data"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#design-patterns",
    "href": "posts/software/software_architecture/software_architect_notes.html#design-patterns",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "A collection of general, reusable solutions to common software design problems. Popularised in the book “Design patterns: elements of reusable object-oriented software”.\n\nThe factory pattern: Creating objects without specifying the exact class of the object. Avoids strong coupling between classes. Create an interface, then a factory method returns an instance of a class which implements that interface. If the implementation needs to change, we only need to change the factory method.\nThe repository pattern: Modules which don’t work with the data store should be oblivious to the type of data store. This is similar to the concept of the Data Access Layer. Layers are for architects, design patterns are for developers. They both seek to solve the same issue.\nThe facade pattern: Creating a layer of abstraction to mask complex actions. The facade does not create any new functionality, it is just a wrapper integrating existing modular functions together.\nThe command pattern: All the action’s information is encapsulated within an object."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#system-architecture",
    "href": "posts/software/software_architecture/software_architect_notes.html#system-architecture",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "The architecture design is the big picture that should answer the following:\n\nHow will the system work under heavy load?\nWhat will happen if the system crashes at a critical moment?\nHow complicated is it to update?\n\nThe architecture should:\n\nDefine components\nDefine how components communicate\nDefine the system’s quality attributes\n\nMake the correct choice as early as possible\n\n\nEnsuring the services are not strongly tied to other services. Avoid the “spiderweb” - when the graph of connections between services is densely connected.\nPrevents coupling of platforms and URLs.\nTo avoid URL coupling between services, two options are:\n\n“Yellow pages” directory\nGateway\n\nThe “yellow pages” contains a directory of all service URLs. If service A wants to query service D, A queries the yellow pages for D’s URL, then uses that URL to query D. This means that service A does not hardcode any information about service D. If D’s URLs change, then only the yellow pages need to be updated. Services only need to know the yellow pages directory’s URL.\n\n\n\n\n\n\nFigure 2: Yellow Pages\n\n\n\nThe gateway acts as a middleman. It holds a mapping table of all URLs Service A queries the gateway, which in turn queries service E. Service A doesn’t need to know about E or any other services. Services only need to know the gateway’s URL.\n\n\n\n\n\n\nFigure 3: Gateway\n\n\n\n\n\n\nThe application’s state is stored in only 2 places:\n\nThe data store\nThe user interface\n\nStateless architecture is preferred in virtually all circumstances.\nIn a stateful architecture, when a user logs in, the login service retrieves the user details from the database and stored them for future use. Data is stored in code.\nIf the login request was routed to server A, the user’s details are stored there. If the user later tried to add itemsto the cart and their cart service request is routed to server B, then their user details will not exist as those are stored on server A.\nDisadvantages of stateful:\n\nLack of scalability\nLack of redundancy\n\n\n\n\n\n\n\nFigure 4: Stateful\n\n\n\nIn a stateless architecture, no data is stored in the service itself. This means the behaviour will be the same regardless of which server a request was routed to.\n\n\n\n\n\n\nFigure 5: Stateless\n\n\n\n\n\n\nCaches store data locally to avoid retrieving the same data from the database multiple times. It trades reliability of data (it is stored in volatile memory) for improved performance.\nA cache should store data that is frequently accessed and rarely modified.\nTwo types of cache:\n\nIn-memory cache - Cache stored in memory on a single service\n\nPros:\n\nBest performance\nCan store any objects\nEasy to use\n\nCons:\n\nSize is limited by the process’s memory\nCan grow stale/inconsistent if the service is scaled out to multiple servers\n\n\nDistributed cache - Cache is independent of the services and can be accessed by all servers\n\nPros:\n\nSupports scaled out servers\nFailover capabilities\nStorage is unlimited\n\nCons:\n\nSetup is more complex\nOften only support primitive data types\nWorse performance than in-memory cache\n\n\n\n\n\n\nMessaging methods can be evaluated on these criteria:\n\nPerformance\nMessage size\nExecution model\nFeedback (handshaking) and reliability\nComplexity\n\nMessaging methods include:\n\nREST API\nHTTP Push\nQueue\nFile-based and database-based methods\n\n\n\nUniversal standard for HTTP-based systems.\nUseful for traditional web apps.\n\n\n\n\n\n\nFigure 6: REST API\n\n\n\n\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nVery fast\n\n\nMessage size\nSame as HTTP protocol limitations - GET 8KB, POST ~10MB\n\n\nExecution model\nRequest/response - ideal for quick, short actions\n\n\nFeedback\nImmediate feedback via response codes\n\n\nComplexity\nExtremely easy\n\n\n\n\n\n\nA client subscribes to the service waiting for an event. When that event occurs, the server notifies the client.\nFor real-time messaging, these often use web sockets to maintain the subscription connection, rather than a traditional request/response model.\nUseful for chat or monitoring.\n\n\n\n\n\n\nFigure 7: HTTP Push\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nVery fast\n\n\nMessage size\nLimited, few KB\n\n\nExecution model\nWeb socket connection / long polling\n\n\nFeedback\nNone (fire and forget)\n\n\nComplexity\nExtremely easy\n\n\n\n\n\n\nThe queue sits between two (or more) services. If service A wants to send a message to service B, A places the message in the queue and B periodically pulls from the queue.\nThis ensures messages will be handled exactly once and in the order received.\nUseful for complex systems with lots of data, when order and reliability are important.\n\n\n\n\n\n\nFigure 8: Queue\n\n\n\n\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nSlow - push/poll time and database persistence\n\n\nMessage size\nUnlimited but best practice to use small messages\n\n\nExecution model\nPolling\n\n\nFeedback\nVery reliable but feedback depends on the monitoring in place to ensure messages make it to the queue and get executed\n\n\nComplexity\nRequires training and setup to maintain a queue engine\n\n\n\n\n\n\nSimilar to queue, but rather han placing messages in a queue it places them in a file directory or database. There is no guarantee that messages are processed once and only once.\nIf multiple services are polling the same file folder, then when a new file is added we can get two issues:\n\nFile locked\nDuplicate processing\n\nSimilar use case to queues, but queues are generally preferred.\n\n\n\n\n\n\nFigure 9: File-based Messaging\n\n\n\n\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nSlow - push/poll time and database persistence\n\n\nMessage size\nUnlimited\n\n\nExecution model\nPolling\n\n\nFeedback\nVery reliable but feedback depends on the monitoring\n\n\nComplexity\nRequires training and setup to maintain the filestore or database\n\n\n\n\n\n\n\n\n\nCreate a central logging service that all other services call to write to a central database. This solves the problem where each microservice has its own log format, data and location. For example, some might be in files, SQL database, NoSQL database, etc.\nImplementation can be via an API or polling folders that each of the services write to.\n\n\n\n\n\n\nFigure 10: Central Logging Service\n\n\n\n\n\n\nCorrelation ID is an identifier attached to the beginning of a user flow and is attached to any action taken by that user, so that if there is an error at any point the user flow can be traced back through the different services’ logs."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#external-considerations",
    "href": "posts/software/software_architecture/software_architect_notes.html#external-considerations",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "External considerations can affect architecture and design decisions.\n\nDeadlines\nDev team skills - New technologies can introduce uncertainty, delays and low quality\nIT support - Assign who will support the product from the outset. This should not be developers.\nCost - Build vs buy. Estimate cost vs value. Spend money to save time, don’t spend time to save money."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#architecture-document",
    "href": "posts/software/software_architecture/software_architect_notes.html#architecture-document",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "This should describe the basic elements of the system:\n\nTechnology stack\nComponents\nServices\nCommunication between components and services\n\nNo development should begin before the document is complete.\nGoals of the document:\n\nDescribe what should be developed and how\nList the functional and non-functional requirements\n\nAudience:\n\nEveryone involved with the system: project manager, CTO, QA leader, developers\nSections for management appear first as they are unlikely to read the whole document\nQA lead can begin preparing test infrastructure ahead of time\n\nFormat: UML (universal modeling language) is often used. It visualises the system’s design with concepts and diagrams. However, this assumes the audience is familiar with UML which is often not the case.\n\nKeep the contents as simple as possible\nUse plain English\nVisualise using whatever software is comfortable and appropriate\n\nStructure:\n\nBackground\nRequirements\nExecutive summary\nArchitecture overview\nComponents drill-down\n\n\n\nThis section validates your point of view and instils confidence that you understand the project.\nOne page for all team and management.\nDescribe the system from a business POV:\n\nThe system’s role\nReasons for replacing the old system\nExpected business impact\n\n\n\nFunctional and non-functional requirements - what should the system do and deal with? This section validates your understadning of the requirements. The requirements dictate the architecture so this section sets the scene for the chosen architecture.\nOne page for all team and management.\nThis should be a brief, bullet point list of requirements with no more than 3 lines on each.\n\n\n\n\nProvide a high-level view of the architecture.\nManagers will not read the whole document; &lt;3 pages for management.\n\nUse charts and diagrams\nWrite this after the rest of the document\nUse technical terms sparsely and only well known ones\nBe concise and don’t repeat yourself\n\n\n\n\nProvide a high-level view of the architecture in technical terms. Do not deep dive into specific components\n&lt;10 pages for developers and QA lead.\n\nGeneral description: type and major non-functional requirements\nHigh-level diagram of logical components, no specifics about hardware or technologies\nDiagram walkthrough: describe the various parts and their roles\nTechnology stack: iff there is a single stack include it here, otherwise include it in the components drill down section\n\n\n\n\nDetailed description of each component.\nUnlimited length, for developers and QA lead.\nFor each component:\n\nComponent’s role\nTechnology stack: Data store, backend, frontend\nComponent’s architecture: Describe the API (URL, logic, response code, comments). Describe the layers. Mention design patterns here.\nDevelopment instructions: Specific development guidelines\n\nBe specific and include rationale behind decisions."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#advanced-architecture-topics",
    "href": "posts/software/software_architecture/software_architect_notes.html#advanced-architecture-topics",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "These architecture patterns solve specific problems but add complexity to the system, so the costs and benefits should be compared.\n\n\nAn architecture in which functionalities are implemented as separate, loosely coupled services that interact with each other using a standard lightweight protocol.\nProblems with monolithic services:\n\nA single exception can crash the whole process\nUpdates impact all components\nLimited to one development platform/language\nUnoptimised compute resources\n\nWith microservices, each service is independent of others so can be updated separately, use a different platform, and be optimised separately.\nAn example of splitting a monolithic architecture into microservices:\n\n\n\n\n\n\nFigure 11: Monolith Example\n\n\n\nAs a microservice architecture, this becomes:\n\n\n\n\n\n\nFigure 12: Microservice Example\n\n\n\nProblems with microservices:\n\nComplex monitoring of all services and their interactions\nComplex architecture\nComplex testing\n\n\n\n\nStoring the deltas to each entity rather than updating it. The events can then be “rebuilt” from the start to give a view of the state at any given point in time.\nUse when history matters.\n\n\n\n\n\n\nFigure 13: Event Sourcing\n\n\n\nPros:\n\nTracing history\nSimple data model\nPerformance\nReporting\n\nCons:\n\nNo unified view - need to rebuild all events from the start to see the current state\nStorage usage\n\n\n\n\nCommand query responsibility segregation. Data storage and data retrieval are two separate databases, with a sync service between the two.\nThis integrates nicely with event sourcing, where events (deltas) are stored in one database and the current state is periodically built and stored in the retrieval database.\n\n\n\n\n\n\nFigure 14: CQRS\n\n\n\nPros:\n\nUseful with high-frequency updates that require near real-time querying\n\nCons:\n\nComplexity - need 2 databases, a sync service, ETL between the storage and retrieval database"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#soft-skills",
    "href": "posts/software/software_architecture/software_architect_notes.html#soft-skills",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "The architect is often not the direct line manager of the developers implementing the software. You ultimately work with people, not software. Influence without authority is key.\n\nListening - assume you are not the smartest person in the room, collective wisdom is always better.\nDealing with criticism\n\nGenuine questioning: Provide facts and logic, be willing to go back and check again\nMocking: Don’t attack back, provide facts and logic\n\nBe smart not right\nOrganisational politics - be aware of it but don’t engage in it\nPublic speaking - define a goal, know your audience, be confident, don’t read, maintain eye contact\nLearning - blogs (DZone, InfoQ, O’Reilly), articles, conferences"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#references",
    "href": "posts/software/software_architecture/software_architect_notes.html#references",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "“The Complete Guide to Becoming a Software Architect” Udemy course\nDesign patterns\n\n\n\n\nFigure 1: Backend Tech\nFigure 2: Yellow Pages\nFigure 3: Gateway\nFigure 4: Stateful\nFigure 5: Stateless\nFigure 6: REST API\nFigure 7: HTTP Push\nFigure 8: Queue\nFigure 9: File-based Messaging\nFigure 10: Central Logging Service\nFigure 11: Monolith Example\nFigure 12: Microservice Example\nFigure 13: Event Sourcing\nFigure 14: CQRS"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson1/lesson.html",
    "href": "posts/ml/generative_deep_learning/lesson1/lesson.html",
    "title": "Generative AI: Chapter 1",
    "section": "",
    "text": "These are notes from chapter 1 of Generative Deep Learning by David Foster.\n\n\n\nGenerative modeling is a branch of machine learning that involves training a model to produce new data that is similar ot a given dataset.\n\nThis means we require some training data which we wish to imitate. The model should be probabilistic rather than deterministic, so that we can sample different variations of the output.\nA helpful way to define generative modeling is by contrasting it with its counterpart discriminative modeling:\n\nDiscriminative modeling estimates \\(p(y|x)\\) - the probability of a label \\(y\\) given some observation \\(x\\)\nGenerative modeling estimates \\(p(x)\\) - the probability of seeing an observation \\(x\\). By sampling from this distribution we can generate new observations.\n\nWe can also create a conditional generative model to estimate \\(p(x|y)\\), the probability of seeing an observation \\(x\\) with label \\(y\\). For ample, if we train a model to generate a given type of fruit.\n\n\n\n\n\nThe aim:\n\nWe have training data of observations \\(X\\)\nWe assume the training data has been generated by an unknown distribution \\(p_{data}\\)\nWe want to build a generative model \\(p_{model}\\) that mimics \\(p_{data}\\)\n\nIf we succeed, we can sample from \\(p_{model}\\) to generate synthetic observations\n\n\nDesirable properties of \\(p_{model}\\) are:\n\nAccuracy - If \\(p_{model}\\) is a high value it should look like a real observation, and likewise low values should look fake.\nGeneration - It should be easy to sample from it.\nRepresentation - It should be possible to understand how high-level features are represented by the model.\n\n\n\n\nWe want to be able to describe things in terms of high-level features. For example, when describing appearance, we might talk about hair colour, length, eye colour, etc. Not RGB values pixel by pixel…\nThis is the idea behind representation learning. We describe each training data observation using some lower-dimensional latent space. Then we learn a maaping function from the latent space to the original domain.\nEncoder-decoder techniques try to transform a high-dimensional nonlinear manifold into a simpler latent space.\n\n\n\n\n\nThe complete set of all values that an observation \\(x\\) can take.\n\n\n\nA function \\(p(x)\\) that maps a point \\(x\\) in the sample space to a number between 0 and 1. The integral over the sample space must equal 1 for it to be a well-define probability distribtution.\nThere is one true data distribution \\(p_{data}\\) but infinitely many approximations \\(p_{model}\\) that we can find.\n\n\n\nWe can structure our approach to finding \\(p_{model}\\) by restricting ourselves to a family of density functions \\(p_{\\theta}(x)\\) which can be described with a finite set of parameters \\(\\theta\\).\nFor example, restricting our search to a linear model \\(y = w*x + b\\) where we need to find the parameters \\(w\\) and \\(b\\).\n\n\n\nThe likelihood, \\(L(\\theta|x)\\), of a parameter set \\(\\theta\\) is a function which measures the plausability of the parameters having observed some data point \\(x\\). It is the probability of seeing the data \\(x\\) if the true data-generating distribution was the model parameterized by \\(\\theta\\).\nIt is defined as the parametric model density: \\[\nL(\\theta|x) = p_{\\theta}(x)\n\\]\nIf we have a dataset of independent observations \\(X\\) then the probabilities multiply: \\[\nL(\\theta|X) = \\prod_{x \\in X} p_{\\theta}(x)\n\\]\nThe product of a lot of decimals can get unwieldy, so we often take the log-likelihood \\(l\\) to turn the product into a sum: \\[\nl(\\theta|X) = \\sum_{x \\in X} \\log{p_{\\theta}(x) }\n\\]\nThe focus of parametric modeling is therefore to find the optimal parameter set \\(\\hat{\\theta}\\), which leads to…\n\n\n\nA technique to estimate \\(\\hat{\\theta}\\), the set of parameters that is most likely to explain the observed data \\(X\\): \\[\n\\hat{\\theta} = \\arg\\max_x l(\\theta|X)\n\\]\nNeural networks often work with a loss function, so this is equivalent to minimising the negative log-likelihood: \\[\n\\hat{\\theta} = \\arg\\min_\\theta -l(\\theta|X) = \\arg\\min_\\theta -\\log{p_{\\theta}(X)}\n\\]\n\n\n\nGenerative modeling is a form of MLE where the parameters are the neural network weights. We want to find the weights that maximise the likelihood of observing the training data.\nBut for high-dimensional problems \\(p_{\\theta}(X)\\) is intractable, it cannot be calculated directly.\nThere are different approaches to maing the problem tractable. These are summarised in the following section and explored in more detail in subsequent chapters.\n\n\n\n\nAll types of generative model are seeking to solve the same problem, but they take different approaches to modeling the intractable density function \\(p_{\\theta}(x)\\).\nThere are three general approaches:\n\nExplicitly model the density function but constrain the model in some way.\nExplicitly model a tractable approximation of the density function.\nImplicitly model the density function, using a stochastic process that generates the data directly.\n\n\n\n\n\n\nflowchart LR\n\n\n  A(Generative models) --&gt; B1(Explicit density)\n  A(Generative models) --&gt; B2(Implicit density)\n\n  B1 --&gt; C1(Approximate density)\n  B1 --&gt; C2(Tractable density)\n\n  C1 --&gt; D1(VAE)\n  C1 --&gt; D2(Energy-based model)\n  C1 --&gt; D3(Diffusion model)\n\n  C2 --&gt; D4(Autoregressive model)\n  C2 --&gt; D5(Normalizing flow model)\n\n  B2 --&gt; D6(GAN)\n\n\n\n\n\n\n\nImplicit density models don’t even try to estimate the probability density, instead focusing on producing a stochastic data generation process directly.\nTractable models place constraints on the model architecture (i.e. the parameters \\(\\theta\\))."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson1/lesson.html#what-is-generative-modeling",
    "href": "posts/ml/generative_deep_learning/lesson1/lesson.html#what-is-generative-modeling",
    "title": "Generative AI: Chapter 1",
    "section": "",
    "text": "Generative modeling is a branch of machine learning that involves training a model to produce new data that is similar ot a given dataset.\n\nThis means we require some training data which we wish to imitate. The model should be probabilistic rather than deterministic, so that we can sample different variations of the output.\nA helpful way to define generative modeling is by contrasting it with its counterpart discriminative modeling:\n\nDiscriminative modeling estimates \\(p(y|x)\\) - the probability of a label \\(y\\) given some observation \\(x\\)\nGenerative modeling estimates \\(p(x)\\) - the probability of seeing an observation \\(x\\). By sampling from this distribution we can generate new observations.\n\nWe can also create a conditional generative model to estimate \\(p(x|y)\\), the probability of seeing an observation \\(x\\) with label \\(y\\). For ample, if we train a model to generate a given type of fruit."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson1/lesson.html#the-generative-modeling-framework",
    "href": "posts/ml/generative_deep_learning/lesson1/lesson.html#the-generative-modeling-framework",
    "title": "Generative AI: Chapter 1",
    "section": "",
    "text": "The aim:\n\nWe have training data of observations \\(X\\)\nWe assume the training data has been generated by an unknown distribution \\(p_{data}\\)\nWe want to build a generative model \\(p_{model}\\) that mimics \\(p_{data}\\)\n\nIf we succeed, we can sample from \\(p_{model}\\) to generate synthetic observations\n\n\nDesirable properties of \\(p_{model}\\) are:\n\nAccuracy - If \\(p_{model}\\) is a high value it should look like a real observation, and likewise low values should look fake.\nGeneration - It should be easy to sample from it.\nRepresentation - It should be possible to understand how high-level features are represented by the model."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson1/lesson.html#representation-learning",
    "href": "posts/ml/generative_deep_learning/lesson1/lesson.html#representation-learning",
    "title": "Generative AI: Chapter 1",
    "section": "",
    "text": "We want to be able to describe things in terms of high-level features. For example, when describing appearance, we might talk about hair colour, length, eye colour, etc. Not RGB values pixel by pixel…\nThis is the idea behind representation learning. We describe each training data observation using some lower-dimensional latent space. Then we learn a maaping function from the latent space to the original domain.\nEncoder-decoder techniques try to transform a high-dimensional nonlinear manifold into a simpler latent space."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson1/lesson.html#core-probability-theory",
    "href": "posts/ml/generative_deep_learning/lesson1/lesson.html#core-probability-theory",
    "title": "Generative AI: Chapter 1",
    "section": "",
    "text": "The complete set of all values that an observation \\(x\\) can take.\n\n\n\nA function \\(p(x)\\) that maps a point \\(x\\) in the sample space to a number between 0 and 1. The integral over the sample space must equal 1 for it to be a well-define probability distribtution.\nThere is one true data distribution \\(p_{data}\\) but infinitely many approximations \\(p_{model}\\) that we can find.\n\n\n\nWe can structure our approach to finding \\(p_{model}\\) by restricting ourselves to a family of density functions \\(p_{\\theta}(x)\\) which can be described with a finite set of parameters \\(\\theta\\).\nFor example, restricting our search to a linear model \\(y = w*x + b\\) where we need to find the parameters \\(w\\) and \\(b\\).\n\n\n\nThe likelihood, \\(L(\\theta|x)\\), of a parameter set \\(\\theta\\) is a function which measures the plausability of the parameters having observed some data point \\(x\\). It is the probability of seeing the data \\(x\\) if the true data-generating distribution was the model parameterized by \\(\\theta\\).\nIt is defined as the parametric model density: \\[\nL(\\theta|x) = p_{\\theta}(x)\n\\]\nIf we have a dataset of independent observations \\(X\\) then the probabilities multiply: \\[\nL(\\theta|X) = \\prod_{x \\in X} p_{\\theta}(x)\n\\]\nThe product of a lot of decimals can get unwieldy, so we often take the log-likelihood \\(l\\) to turn the product into a sum: \\[\nl(\\theta|X) = \\sum_{x \\in X} \\log{p_{\\theta}(x) }\n\\]\nThe focus of parametric modeling is therefore to find the optimal parameter set \\(\\hat{\\theta}\\), which leads to…\n\n\n\nA technique to estimate \\(\\hat{\\theta}\\), the set of parameters that is most likely to explain the observed data \\(X\\): \\[\n\\hat{\\theta} = \\arg\\max_x l(\\theta|X)\n\\]\nNeural networks often work with a loss function, so this is equivalent to minimising the negative log-likelihood: \\[\n\\hat{\\theta} = \\arg\\min_\\theta -l(\\theta|X) = \\arg\\min_\\theta -\\log{p_{\\theta}(X)}\n\\]\n\n\n\nGenerative modeling is a form of MLE where the parameters are the neural network weights. We want to find the weights that maximise the likelihood of observing the training data.\nBut for high-dimensional problems \\(p_{\\theta}(X)\\) is intractable, it cannot be calculated directly.\nThere are different approaches to maing the problem tractable. These are summarised in the following section and explored in more detail in subsequent chapters."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson1/lesson.html#generative-model-taxonomy",
    "href": "posts/ml/generative_deep_learning/lesson1/lesson.html#generative-model-taxonomy",
    "title": "Generative AI: Chapter 1",
    "section": "",
    "text": "All types of generative model are seeking to solve the same problem, but they take different approaches to modeling the intractable density function \\(p_{\\theta}(x)\\).\nThere are three general approaches:\n\nExplicitly model the density function but constrain the model in some way.\nExplicitly model a tractable approximation of the density function.\nImplicitly model the density function, using a stochastic process that generates the data directly.\n\n\n\n\n\n\nflowchart LR\n\n\n  A(Generative models) --&gt; B1(Explicit density)\n  A(Generative models) --&gt; B2(Implicit density)\n\n  B1 --&gt; C1(Approximate density)\n  B1 --&gt; C2(Tractable density)\n\n  C1 --&gt; D1(VAE)\n  C1 --&gt; D2(Energy-based model)\n  C1 --&gt; D3(Diffusion model)\n\n  C2 --&gt; D4(Autoregressive model)\n  C2 --&gt; D5(Normalizing flow model)\n\n  B2 --&gt; D6(GAN)\n\n\n\n\n\n\n\nImplicit density models don’t even try to estimate the probability density, instead focusing on producing a stochastic data generation process directly.\nTractable models place constraints on the model architecture (i.e. the parameters \\(\\theta\\))."
  },
  {
    "objectID": "posts/ml/fastai/lesson4/lesson.html",
    "href": "posts/ml/fastai/lesson4/lesson.html",
    "title": "FastAI Lesson 4: Natural Language Processing",
    "section": "",
    "text": "These are notes from lesson 4 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\nKaggle NLP pattern similarity notebook: see notebook\n\n\n\n\nNLP applications: categorising documents, translation, text generation.\nWe are using the Huggingface transformers library for this lesson. It is now incorporated into the fastai library.\nULMFit is an algorithm which uses fine-tuning, in this example to train a positve/negative sentiment classifier in 3 steps: 1. Train an RNN on wikipedia to predict the next word. No labels required. 2. Fine-tune this for IMDb reviews to predict the next word of a movie review. Still no labels required. 3. Fine-tune this to classify the sentiment.\nTransformers have overtaken ULMFit as the state-of-the-art.\nLooking “inside” a CNN, the first layer contains elementary detectors like edge detectors, blob detectors, gradient detectors etc. These get combined in non-linear ways to make increasingly complex detectors. Layer 2 might combine vertical and horizontal edge detectors into a corner detector. By the later layers, it is detecting rich features like lizard eyes, dog faces etc. See Zeiler and Fegus.\nFor the fine-tuning process, the earlier layers are unlikely to need changing because they are more general. So we only need to fine-tune (i.e. re-train) the later layers.\n\n\n\nAs an example of NLP in practice, we walk through this notebook for U.S. Patent Phrase to Phrase Matching.\nReshape the input to fit a standard NLP task\n\nWe want to learn the similarity between two fields and are provided with similarity scores.\nWe concat the fields of interest (with identifiers in between). The identifiers themselves are not important, they just need to be consistent.\nThe NLP model is then a supervised regression task to predict the score given the concatendated string.\n\ndf['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor\n\n\nSplit the text into tokens (words). Tokens are, broadly speaking, words.\nThere are some caveats to that, as some languages like Chinese don’t fit nicely into that model. We don’t want the vocabulary to be too big. Alternative approaches use character tokenization rather than word tokenization.\nIn practice, we tokenize into subwords.\n\n\n\nMap each unique token to a number. One-hot encoding.\nThe choice of tokenization and numericalization depends on the model you use. Whoever trained the model chose a convention for tokenizing. We need to be consistent with that if we want the model to work correctly.\n\n\n\nThe Huggingface model hub contains thousands of pretrained models.\nFor NLP tasks, it is useful to choose a model that was trained on a similar corpus, so you can search the model hub. In this case, we search for “patent”.\nSome models are general purpose, e.g. deberta-v3 used in the lesson.\nULMFit handles large documents better as it can split up the document. Transformer approaches require loading the whole document into GPU memory, so struggle for larger documents.\n\n\n\nIf a model is too simple (i.e. not flexible enough) then it cannot fit the data and be biased, leading to underfitting.\nIf the model fits the data points too closely, it is overfitting.\nA good validation set, and monitoring validation error rather than training error as a metric, is key to avoiding overfitting. See this article for an in-depth discussion on the importance of choosing a good validation set.\nOften people will default to using a random train/test split. This is what scikit-learn uses. This is a BAD idea very often.\nFor time-series data, it’s easier to infer gaps than it is to predict a block in the future. The latter is the more common task but a random split simulates the former, giving unrealistically good performance.\nFor image data, there may be people, boats, etc that are in the training set but not the test set. By failing to have new people in the validation set, the model can learn things about specific people/boats that it can’t rely on in practice.\n\n\n\nMetrics are things that are human-understandable.\nLoss functions should be smooth and differentiable to aid in training.\nThese can sometimes be the same thing, but not in general. For example, accuracy is a good metric in image classification. We could tweak the weights in such a way that it improves the model slightly, but not so much that it now correctly classifies a previously incorrect image. This means the metric function is bumpy, therefore a bad loss function.\nThis article goes into more detail on the choice of metrics.\nAI can be particularly dangerous at confirming systematic biases, because it is so good at optimising metrics, so it will conform to any biases present in the training data. Making decisions based on the model then reinforces those biases.\n\nGoodhart’s law applies: If a metric becomes a target it’s no longer a good metric\n\n\n\n\nThe best way to understand a metric is not to look at the mathematical formula, but to plot some data for which the metric is high, medium and low, then see what that tells you.\nAfter that, look at the equation to see if your intuition matches the logic.\n\n\n\nFast AI has a function to find a good starting point. Otherwise, pick a small value and keep doubling it until it falls apart.\n\n\n\n\n\nCourse lesson page\nHuggingFace Transformers\nVisualizing and Understanding Convolutional Networks\nHuggingFace models\nValidation sets\nModel metrics"
  },
  {
    "objectID": "posts/ml/fastai/lesson4/lesson.html#nlp-background-and-transformers",
    "href": "posts/ml/fastai/lesson4/lesson.html#nlp-background-and-transformers",
    "title": "FastAI Lesson 4: Natural Language Processing",
    "section": "",
    "text": "NLP applications: categorising documents, translation, text generation.\nWe are using the Huggingface transformers library for this lesson. It is now incorporated into the fastai library.\nULMFit is an algorithm which uses fine-tuning, in this example to train a positve/negative sentiment classifier in 3 steps: 1. Train an RNN on wikipedia to predict the next word. No labels required. 2. Fine-tune this for IMDb reviews to predict the next word of a movie review. Still no labels required. 3. Fine-tune this to classify the sentiment.\nTransformers have overtaken ULMFit as the state-of-the-art.\nLooking “inside” a CNN, the first layer contains elementary detectors like edge detectors, blob detectors, gradient detectors etc. These get combined in non-linear ways to make increasingly complex detectors. Layer 2 might combine vertical and horizontal edge detectors into a corner detector. By the later layers, it is detecting rich features like lizard eyes, dog faces etc. See Zeiler and Fegus.\nFor the fine-tuning process, the earlier layers are unlikely to need changing because they are more general. So we only need to fine-tune (i.e. re-train) the later layers."
  },
  {
    "objectID": "posts/ml/fastai/lesson4/lesson.html#kaggle-competition-walkthrough",
    "href": "posts/ml/fastai/lesson4/lesson.html#kaggle-competition-walkthrough",
    "title": "FastAI Lesson 4: Natural Language Processing",
    "section": "",
    "text": "As an example of NLP in practice, we walk through this notebook for U.S. Patent Phrase to Phrase Matching.\nReshape the input to fit a standard NLP task\n\nWe want to learn the similarity between two fields and are provided with similarity scores.\nWe concat the fields of interest (with identifiers in between). The identifiers themselves are not important, they just need to be consistent.\nThe NLP model is then a supervised regression task to predict the score given the concatendated string.\n\ndf['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor\n\n\nSplit the text into tokens (words). Tokens are, broadly speaking, words.\nThere are some caveats to that, as some languages like Chinese don’t fit nicely into that model. We don’t want the vocabulary to be too big. Alternative approaches use character tokenization rather than word tokenization.\nIn practice, we tokenize into subwords.\n\n\n\nMap each unique token to a number. One-hot encoding.\nThe choice of tokenization and numericalization depends on the model you use. Whoever trained the model chose a convention for tokenizing. We need to be consistent with that if we want the model to work correctly.\n\n\n\nThe Huggingface model hub contains thousands of pretrained models.\nFor NLP tasks, it is useful to choose a model that was trained on a similar corpus, so you can search the model hub. In this case, we search for “patent”.\nSome models are general purpose, e.g. deberta-v3 used in the lesson.\nULMFit handles large documents better as it can split up the document. Transformer approaches require loading the whole document into GPU memory, so struggle for larger documents.\n\n\n\nIf a model is too simple (i.e. not flexible enough) then it cannot fit the data and be biased, leading to underfitting.\nIf the model fits the data points too closely, it is overfitting.\nA good validation set, and monitoring validation error rather than training error as a metric, is key to avoiding overfitting. See this article for an in-depth discussion on the importance of choosing a good validation set.\nOften people will default to using a random train/test split. This is what scikit-learn uses. This is a BAD idea very often.\nFor time-series data, it’s easier to infer gaps than it is to predict a block in the future. The latter is the more common task but a random split simulates the former, giving unrealistically good performance.\nFor image data, there may be people, boats, etc that are in the training set but not the test set. By failing to have new people in the validation set, the model can learn things about specific people/boats that it can’t rely on in practice.\n\n\n\nMetrics are things that are human-understandable.\nLoss functions should be smooth and differentiable to aid in training.\nThese can sometimes be the same thing, but not in general. For example, accuracy is a good metric in image classification. We could tweak the weights in such a way that it improves the model slightly, but not so much that it now correctly classifies a previously incorrect image. This means the metric function is bumpy, therefore a bad loss function.\nThis article goes into more detail on the choice of metrics.\nAI can be particularly dangerous at confirming systematic biases, because it is so good at optimising metrics, so it will conform to any biases present in the training data. Making decisions based on the model then reinforces those biases.\n\nGoodhart’s law applies: If a metric becomes a target it’s no longer a good metric\n\n\n\n\nThe best way to understand a metric is not to look at the mathematical formula, but to plot some data for which the metric is high, medium and low, then see what that tells you.\nAfter that, look at the equation to see if your intuition matches the logic.\n\n\n\nFast AI has a function to find a good starting point. Otherwise, pick a small value and keep doubling it until it falls apart."
  },
  {
    "objectID": "posts/ml/fastai/lesson4/lesson.html#references",
    "href": "posts/ml/fastai/lesson4/lesson.html#references",
    "title": "FastAI Lesson 4: Natural Language Processing",
    "section": "",
    "text": "Course lesson page\nHuggingFace Transformers\nVisualizing and Understanding Convolutional Networks\nHuggingFace models\nValidation sets\nModel metrics"
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html",
    "href": "posts/ml/fastai/lesson2/lesson.html",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "These are notes from lesson 2 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\n\nDeploy a model to Huggingface Spaces: see car classifier model\nDeploy a model to a Github Pages website: see car classifier website\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nA website for quizzes based on the book: AI quizzes\n\n\n\n\nIt can be useful to train a model on the data BEFORE you clean it\n\nCounterintuitive!\nThe confusion matrix output of the learner gives you a good intuition about which classifications are hard\nplot_top_losses shows which examples were hardest for the model to classify. This can find (1) when the model is correct but not confident, and (2) when the model was confident but incorrect\nImageClassifierCleaner shows the examples in the training and validation set ordered by loss, so we can choose to keep, reclassify or remove them\n\nFor image resizing, random resize crop can often be more effective.\n\nSquishing can result in weird, unrealistic images.\nPadding or mirroring can add false information that the model will erroneously learn.\nRandom crops give different sections of the image which acts as a form of data augmentation.\naug_transforms can be use for more sophisticated data augmentation like warping and recoloring images.\n\n\n\n\nOnce you are happy with the model you’ve trained, you can pickle the learner object and save it.\nlearn.export('model.pkl')\nThen you can add the saved model to the hugging face space. To use the model to make predictions, any external functions you used to create the model will need to be instantiated too.\nlearn = load_learner('model.pkl')\nlearn.predict(image)\n\n\n\nHugging face spaces hosts models with a choice of pre-canned interfaces to quickly deploy a model to the public. Gradio is used in the example in the lecture. Streamlit is an alternative to Gradio that is more flexible.\nGradio requires a dict of classes as keys and probabilities (as floats not tensors) as the values. To go from the Gradio prototype to a production app, you can view the Gradio API from the huggingface space which will show you the API. The API exposes an endpoint which you can then hit from your own frontend app.\nGithub pages is a free and simple way to host a public website. See this fastai repo as an example of a minimal example html website which issues GET requests to the Gradio API\n\n\n\nTo convert a notebook to a python script, you can add #|export to the top of any cells to include in the script, then use:\nfrom nbdev.export import notebook2script\nnotebook2script('name_of_output_file.py')\nUse #|default_exp app in the first cell of the notebook to set the default name of the exported python file.\n\n\n\nHow do we choose the number of epochs to train for? Whenever it is “good enough” for your use case.\nIf you need to train for longer, you may need to use data augmentation to prevent overfitting. Keep an eye on the validation error to check overfitting.\n\n\n\n\nCourse lesson page\nHuggingFace Spaces"
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html#data-pre-processing",
    "href": "posts/ml/fastai/lesson2/lesson.html#data-pre-processing",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "It can be useful to train a model on the data BEFORE you clean it\n\nCounterintuitive!\nThe confusion matrix output of the learner gives you a good intuition about which classifications are hard\nplot_top_losses shows which examples were hardest for the model to classify. This can find (1) when the model is correct but not confident, and (2) when the model was confident but incorrect\nImageClassifierCleaner shows the examples in the training and validation set ordered by loss, so we can choose to keep, reclassify or remove them\n\nFor image resizing, random resize crop can often be more effective.\n\nSquishing can result in weird, unrealistic images.\nPadding or mirroring can add false information that the model will erroneously learn.\nRandom crops give different sections of the image which acts as a form of data augmentation.\naug_transforms can be use for more sophisticated data augmentation like warping and recoloring images."
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html#saving-a-model",
    "href": "posts/ml/fastai/lesson2/lesson.html#saving-a-model",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "Once you are happy with the model you’ve trained, you can pickle the learner object and save it.\nlearn.export('model.pkl')\nThen you can add the saved model to the hugging face space. To use the model to make predictions, any external functions you used to create the model will need to be instantiated too.\nlearn = load_learner('model.pkl')\nlearn.predict(image)"
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html#hosting-a-model",
    "href": "posts/ml/fastai/lesson2/lesson.html#hosting-a-model",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "Hugging face spaces hosts models with a choice of pre-canned interfaces to quickly deploy a model to the public. Gradio is used in the example in the lecture. Streamlit is an alternative to Gradio that is more flexible.\nGradio requires a dict of classes as keys and probabilities (as floats not tensors) as the values. To go from the Gradio prototype to a production app, you can view the Gradio API from the huggingface space which will show you the API. The API exposes an endpoint which you can then hit from your own frontend app.\nGithub pages is a free and simple way to host a public website. See this fastai repo as an example of a minimal example html website which issues GET requests to the Gradio API"
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html#using-nbdev",
    "href": "posts/ml/fastai/lesson2/lesson.html#using-nbdev",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "To convert a notebook to a python script, you can add #|export to the top of any cells to include in the script, then use:\nfrom nbdev.export import notebook2script\nnotebook2script('name_of_output_file.py')\nUse #|default_exp app in the first cell of the notebook to set the default name of the exported python file."
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html#training-the-model-and-when-to-stop",
    "href": "posts/ml/fastai/lesson2/lesson.html#training-the-model-and-when-to-stop",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "How do we choose the number of epochs to train for? Whenever it is “good enough” for your use case.\nIf you need to train for longer, you may need to use data augmentation to prevent overfitting. Keep an eye on the validation error to check overfitting."
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html#references",
    "href": "posts/ml/fastai/lesson2/lesson.html#references",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "Course lesson page\nHuggingFace Spaces"
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html",
    "href": "posts/ml/fastai/lesson7/lesson.html",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "These are notes from lesson 7 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\n\nRecreate the collaborative filtering spreadsheet\n\n\n\n\n\nWe have users ratings of movies.\nSay we had “embeddings” of a set of categories for each. So for a given movie, we have a vector of [action, sci-fi, romance] and for a given user we have their preference for [action, sci-fi, romance]. Then we could do the dot product between user embedding and movie embedding to get the probability that the user likes that movie. That is, the predicted user rating.\nSo the problem boils down to:\n\nWhat are the embeddings? i.e. the salient factors ([action, sci-fi, romance] in the example above)\nHow do we get them?\n\nThe answer to both questions is: we just let the model learn them.\nLet’s just pick a randomised embedding for each movie and each user. Then we have a loss function which is the MAE between predicted user rating for a movie and actual rating. Now we can use SGD to optimise those embeddings to find the best values.\n\n\n\nHow should we choose the number of latent factors? (3 in the example above). Jeremy wrote down some ballpark values for models of different sizes in excel, then fit a function to it to get a heuristic measure. This is the default used by fast AI.\nAn embedding is just “look up in an array”.\nDoing a matrix multiply by a one hot encoded vector is the same as doing a lookup in an array, just in a more computationally efficient way. Recall the softmax example.\nPutting a sigmoid_range on the final layer to squish ratings to fit 0 to 5 means “the model doesn’t have to work as hard” to get movies in the right range. In practice we use 5.5 as the sigmoid scale value as a sigmoid can never hit 1, but we want ratings to be able to hit 5.\nAdding a user bias term and a movie bias term to the prediction call helps account for the fact that some users always rate high (4 or 5) but other users always rate low. And similarly for movies if everyone always rates it a 5 or a 1.\nWe want to avoid overfitting, but data augmentation isn’t possible here. We use weight decay AKA L2 regularisation. Add sum of weights squared to the loss function.\n\n\n\n\nCourse lesson page\nCollaborative filtering notebook"
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html#the-intuition-behind-collaborative-filtering",
    "href": "posts/ml/fastai/lesson7/lesson.html#the-intuition-behind-collaborative-filtering",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "We have users ratings of movies.\nSay we had “embeddings” of a set of categories for each. So for a given movie, we have a vector of [action, sci-fi, romance] and for a given user we have their preference for [action, sci-fi, romance]. Then we could do the dot product between user embedding and movie embedding to get the probability that the user likes that movie. That is, the predicted user rating.\nSo the problem boils down to:\n\nWhat are the embeddings? i.e. the salient factors ([action, sci-fi, romance] in the example above)\nHow do we get them?\n\nThe answer to both questions is: we just let the model learn them.\nLet’s just pick a randomised embedding for each movie and each user. Then we have a loss function which is the MAE between predicted user rating for a movie and actual rating. Now we can use SGD to optimise those embeddings to find the best values."
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html#assorted-notes-on-implementing-collaborative-filtering",
    "href": "posts/ml/fastai/lesson7/lesson.html#assorted-notes-on-implementing-collaborative-filtering",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "How should we choose the number of latent factors? (3 in the example above). Jeremy wrote down some ballpark values for models of different sizes in excel, then fit a function to it to get a heuristic measure. This is the default used by fast AI.\nAn embedding is just “look up in an array”.\nDoing a matrix multiply by a one hot encoded vector is the same as doing a lookup in an array, just in a more computationally efficient way. Recall the softmax example.\nPutting a sigmoid_range on the final layer to squish ratings to fit 0 to 5 means “the model doesn’t have to work as hard” to get movies in the right range. In practice we use 5.5 as the sigmoid scale value as a sigmoid can never hit 1, but we want ratings to be able to hit 5.\nAdding a user bias term and a movie bias term to the prediction call helps account for the fact that some users always rate high (4 or 5) but other users always rate low. And similarly for movies if everyone always rates it a 5 or a 1.\nWe want to avoid overfitting, but data augmentation isn’t possible here. We use weight decay AKA L2 regularisation. Add sum of weights squared to the loss function."
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html#references",
    "href": "posts/ml/fastai/lesson7/lesson.html#references",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "Course lesson page\nCollaborative filtering notebook"
  },
  {
    "objectID": "posts/ml/fastai/lesson6/lesson.html",
    "href": "posts/ml/fastai/lesson6/lesson.html",
    "title": "FastAI Lesson 6: Random Forests",
    "section": "",
    "text": "These are notes from lesson 6 of Fast AI Practical Deep Learning for Coders.\n\n\nIt’s worth starting with a random forest as a baseline model because “it’s very hard to screw up”. Decision trees only consider the ordering of data, so are not sensitive to outliers, skewed distributions, dummy variables etc.\nA random forest is an ensemble of trees. A tree is an ensemble of binary splits.\nbinary split -&gt; tree -&gt; forest\n\n\nPick a column of the data set and split the rows into two groups. Predict the outcome based just on that.\nFor example, in the titanic dataset, pick the Sex column. This splits into male vs female. If we predict that all female passengers survived and all male passengers died, that’s a reasonably accurate prediction.\nA model which iterates through the variables and finds the best binary split is called a “OneR” or one rule model.\nThis was actually the state-of-the-art in the 90s and performs reasonably well across a lot of problems.\n\n\n\nThis extends the “OneR” idea to two or more splits.\nFor example, if we first split by sex, then find the next best variable to split on to create a two layer tree.\nLeaf nodes are the number of terminating nodes in the tree. OneR creates 2 leaf nodes. If we split one side again, we’ll have 3 leaf nodes. If we split the other side, to create a balanced two layer tree, we’ll have 4 leaf nodes.\n\n\n\nThe idea behind bagging is that if we have many unbiased, uncorrelated models, we can average their predictions to get an aggregate model that is better than any of them.\nEach individual model will overfit, so either be too high or too low for a given point, a positive or negative error term respectively. By combining multiple uncorrelated models, the error will average to 0.\nAn easy way to get many uncorrelated models is to only use a random subset of the data each time. Then build a decision tree for each subset.\nThis is the idea behind random forests.\nThe error decreases with the number of trees, with diminishing returns.\n\nJeremy’s rule of thumb: improvements level off after about 30 trees and he doesn’t often use &gt;100.\n\n\n\n\n\n\n\nA nice side effect of using decision trees is that we get feature importance plots for free.\nGini is a measure of inequality, similar to the score used in the notebook to quantify how good a split was.\nIntuitively, this measures the likelihood that if you picked two samples from a group, how likely is it they’d be the same every time. If the group is all the same, the probability is 1. If every item is different, the probability is 0.\nThe idea is for each split of a decision tree, we can track the column used to split and the amount that the gini decreased by. If we loop through the tree and accumulate the “gini reduction” for each column, we have a metric of how important that column was in splitting the dataset.\nThis makes random forests a useful first model when tackling a new big dataset, as it can give an insight into how useful each column is.\n\n\n\nA comment on explainability, regarding feature importance compared to SHAP, LIME etc. These explain the model, so to be useful the model needs to be accurate.\nIf you use feature importance from a bad model then the columns it claims are important might not actually be. So the usefulness of explainability techniques boils down to what models can they explain and how accurate are those models.\n\n\n\nFor each tree, we trained on a subset of the rows. We can then see how each one performed on the held out data; this is the out-of-bag (OOB) error per tree.\nWe can average this over all of the trees to get an overall OOB error for the forest.\n\n\n\nThese are not specific to random forests and can be applied to any ML model including deep neural networks.\nIf we want to know how an independent variable affects the dependent variable, a naive approach would be to just plot them.\nBut this could include a confounding variable. For example, the price of bulldozers increases over time, but driven by the presence of air conditioning which also increased over time.\nA partial dependence plot takes each row in turn and sets the column(s) of interest to the first value, say, year=1950. Then we predict the target variable using our model. Then repeat this for year=1951, 1952, etc.\nWe can then average the target variable per year to get a view of how it depends on the independent variable, all else being equal.\n\n\n\n\nWe make multiple trees, but instead of fitting all to different data subsets, we fit to residuals.\nSo we fit a very small tree (OneR even) to the data to get a first prediction. Then we calculate the error term. Then we fit another tree to predict the error term. Then calculate the second order error term. Then fit a tree to this, etc.\nThen our prediction is the sum of these trees, rather than the average like with a random forest.\nThis is “boosting”; calculating an error term then fitting another model to it.\nContrast this with “bagging” which was when we calculate multiple models to different subsets of data and average them.\n\n\n\n\nCourse lesson page\nComparison of computer vision models for fine-tuning\nBagging predictors"
  },
  {
    "objectID": "posts/ml/fastai/lesson6/lesson.html#background-on-random-forests",
    "href": "posts/ml/fastai/lesson6/lesson.html#background-on-random-forests",
    "title": "FastAI Lesson 6: Random Forests",
    "section": "",
    "text": "It’s worth starting with a random forest as a baseline model because “it’s very hard to screw up”. Decision trees only consider the ordering of data, so are not sensitive to outliers, skewed distributions, dummy variables etc.\nA random forest is an ensemble of trees. A tree is an ensemble of binary splits.\nbinary split -&gt; tree -&gt; forest\n\n\nPick a column of the data set and split the rows into two groups. Predict the outcome based just on that.\nFor example, in the titanic dataset, pick the Sex column. This splits into male vs female. If we predict that all female passengers survived and all male passengers died, that’s a reasonably accurate prediction.\nA model which iterates through the variables and finds the best binary split is called a “OneR” or one rule model.\nThis was actually the state-of-the-art in the 90s and performs reasonably well across a lot of problems.\n\n\n\nThis extends the “OneR” idea to two or more splits.\nFor example, if we first split by sex, then find the next best variable to split on to create a two layer tree.\nLeaf nodes are the number of terminating nodes in the tree. OneR creates 2 leaf nodes. If we split one side again, we’ll have 3 leaf nodes. If we split the other side, to create a balanced two layer tree, we’ll have 4 leaf nodes.\n\n\n\nThe idea behind bagging is that if we have many unbiased, uncorrelated models, we can average their predictions to get an aggregate model that is better than any of them.\nEach individual model will overfit, so either be too high or too low for a given point, a positive or negative error term respectively. By combining multiple uncorrelated models, the error will average to 0.\nAn easy way to get many uncorrelated models is to only use a random subset of the data each time. Then build a decision tree for each subset.\nThis is the idea behind random forests.\nThe error decreases with the number of trees, with diminishing returns.\n\nJeremy’s rule of thumb: improvements level off after about 30 trees and he doesn’t often use &gt;100."
  },
  {
    "objectID": "posts/ml/fastai/lesson6/lesson.html#attributes-of-random-forests",
    "href": "posts/ml/fastai/lesson6/lesson.html#attributes-of-random-forests",
    "title": "FastAI Lesson 6: Random Forests",
    "section": "",
    "text": "A nice side effect of using decision trees is that we get feature importance plots for free.\nGini is a measure of inequality, similar to the score used in the notebook to quantify how good a split was.\nIntuitively, this measures the likelihood that if you picked two samples from a group, how likely is it they’d be the same every time. If the group is all the same, the probability is 1. If every item is different, the probability is 0.\nThe idea is for each split of a decision tree, we can track the column used to split and the amount that the gini decreased by. If we loop through the tree and accumulate the “gini reduction” for each column, we have a metric of how important that column was in splitting the dataset.\nThis makes random forests a useful first model when tackling a new big dataset, as it can give an insight into how useful each column is.\n\n\n\nA comment on explainability, regarding feature importance compared to SHAP, LIME etc. These explain the model, so to be useful the model needs to be accurate.\nIf you use feature importance from a bad model then the columns it claims are important might not actually be. So the usefulness of explainability techniques boils down to what models can they explain and how accurate are those models.\n\n\n\nFor each tree, we trained on a subset of the rows. We can then see how each one performed on the held out data; this is the out-of-bag (OOB) error per tree.\nWe can average this over all of the trees to get an overall OOB error for the forest.\n\n\n\nThese are not specific to random forests and can be applied to any ML model including deep neural networks.\nIf we want to know how an independent variable affects the dependent variable, a naive approach would be to just plot them.\nBut this could include a confounding variable. For example, the price of bulldozers increases over time, but driven by the presence of air conditioning which also increased over time.\nA partial dependence plot takes each row in turn and sets the column(s) of interest to the first value, say, year=1950. Then we predict the target variable using our model. Then repeat this for year=1951, 1952, etc.\nWe can then average the target variable per year to get a view of how it depends on the independent variable, all else being equal."
  },
  {
    "objectID": "posts/ml/fastai/lesson6/lesson.html#gradient-boosting-forests",
    "href": "posts/ml/fastai/lesson6/lesson.html#gradient-boosting-forests",
    "title": "FastAI Lesson 6: Random Forests",
    "section": "",
    "text": "We make multiple trees, but instead of fitting all to different data subsets, we fit to residuals.\nSo we fit a very small tree (OneR even) to the data to get a first prediction. Then we calculate the error term. Then we fit another tree to predict the error term. Then calculate the second order error term. Then fit a tree to this, etc.\nThen our prediction is the sum of these trees, rather than the average like with a random forest.\nThis is “boosting”; calculating an error term then fitting another model to it.\nContrast this with “bagging” which was when we calculate multiple models to different subsets of data and average them."
  },
  {
    "objectID": "posts/ml/fastai/lesson6/lesson.html#references",
    "href": "posts/ml/fastai/lesson6/lesson.html#references",
    "title": "FastAI Lesson 6: Random Forests",
    "section": "",
    "text": "Course lesson page\nComparison of computer vision models for fine-tuning\nBagging predictors"
  },
  {
    "objectID": "posts/ml/kalman_filter/kalman_filter.html",
    "href": "posts/ml/kalman_filter/kalman_filter.html",
    "title": "Kalman Filter Notes",
    "section": "",
    "text": "Notes from https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python\n\n\n\n\nWith measurements from 2 noisy sensors, we can combine them to get a much more accurate measurement. Never throw data away.\nIf we have a prediction model for how the world evolves (e.g. predicted daily weight gain) we can combine this with noisy measurements in the same way. Treat the prediction as a noisy sensor and combine as before, so that our estimate is some way between the prediction and the estimate. How close it is to one or the other depends on the relative accuracy of each, and this is a hyperparameter we can set.\nprediction =  x_est + dx*dt \n\nwhere:\n`x_est` is an initial constant that gets updated\n`dx` is a predicted daily weight gain rate\n`dt` is the time step\n\n\n\nWe can create a single parameter filter, where the parameter g corresponds to how strongly we trust the prediction over the measurement\nresidual = prediction - measurement\nestimate = measurement + g * residual\n\ng=0 =&gt; estimate=measurement\ng=1 =&gt; estimate=prediction\nfor other values of g, the estimate will be somewhere between the measurement and prediction\n\n\n\n\n\n\n\nFigure 1: g-h filter diagram\n\n\n\n\n\n\nWe can go one step further and use our noisy estimates of weight to refine our the daily weight gain w used in our prediction\ndx_t+1 = dx_t + h * residual / dt\nThis gives us the g-h filter - a generic filter which allows us to set a parameter g for the weight measurement confidence and h for the weight change measurement confidence.\nWe can then update the estimates for weight and weight change on every time step (or every measurement if time steps are irregular).\nThis insight forms the basis for Kalman filters, where we will set g and h dynamically on each time step.\n\n\n\nKey takeaways:\n\nMultiple data points are more accurate than one data point, so throw nothing away no matter how inaccurate it is.\nAlways choose a number part way between two data points to create a more accurate estimate.\nPredict the next measurement and rate of change based on the current estimate and how much we think it will change.\nThe new estimate is then chosen as part way between the prediction and next measurement scaled by how accurate each is.\nThe filter is only as good as the mathematical model used to express the system.\nFilters are designed, not selected ad hoc. No choice of g and h applies to all scenarios.\n\nTerminology:\n\nSystem - the object we want to estimate\nState, x - the current configuration of the system. Hidden.\nMeasurement, z - measured value of the state from a noisy sensor. Observable.\nState estimate - our filter’s estimate of the state.\nProcess model - the model we use to predict the next state based on the current state.\nSystem propagation - the predict step\nMeasurement update - the update step\nEpoch - one iteration of system propagation and measurement update.\n\n\n\n\n\n\n\nBayesian statistics treats probability as a belief about a single event. Frequentist statistics describes past events based on their frequency. It has no bearing on future events.\nIf I flip a coin 100 times and get 50 heads and 50 tails, frequentist statistics states the probability of heads was 50% for those cases. On the next coin flip, frequentist statistics has nothing to say about the probability. The state is simply unknown. Bayesian statistics incorporates these past events as a prior belief, so that we can say the next coin flip has a 50% chance of landing heads. “Belief” is a measure of the strength of our knowledge.\nWhen talking about the probability of something, we are implicitly saying “the probability that this event is true given past events”. This is a Bayesian approach. In practice, we may incorporate frequentist techniques too, as in the example above when the 100 previous coin tosses were used to inform our prior.\n\nPrior is the probability distribution before including the measurement’s information. This corresponds to the prediction in the Kalman filter.\nPosterior is a probability distribution after incorporating the measurement’s information. This corresponds to the estimated state in the Kalman filter.\nLikelihood is the joint probability of the observed data - how likely is each position given the measurement. This is not a probability distribution as it does not sum to 1.\n\nThe filter will use Bayes theorem:\nposterior = likelihood * prior / normalization\nIn even simpler filter terms:\nupdated knowledge = || likelihood of new knowledge * prior knowledge ||\nIf we have a prior distribution of positions and system model of the subsequent movement, we can convolve the two to calculate the posterior.\n\n\n\nThe discrete Bayes filter is a form of g-h filter. It is useful for multimodal, discrete problems.\nThe equations are:\nPredict step:\nx_bar = x * f_x(.)\nwhere:\n- x is the current state\n- f_x(.) is the state propagation function for x, i.e. the system model.\n\n\nUpdate step:\nx = ||L . x_bar||\nwhere:\n- L is the likely function\nIn pseudocode this is:\nInitialisation:\n1. Initialise our belief in the state.\n\nPredict:\n1. Predict state for the next time step using the system model.\n2. Adjust belief to account for uncertainty in prediction.\n\nUpdate:\n1. Get a measurement and belief about its accuracy (noise estimate).\n2. Compute likelihood of measurement matching each state.\n3. Update posterior state belief based on likelihood.\nAlgorithms of this form a called “predictor correctors”; we make a prediction then correct it. The predict step will always degrade our knowledge due to the uncertainty in the second step of the predict stage. But adding another measurement, even if noisy, improves our knowledge again. So we can converge on the most likely result.\n\n\n\nThe algorithm is trivial to implement, debug and understand.\nLimitations:\n\nScaling - Tracking i state variables results in O(n^i) runtime complexity.\nDiscrete - Most real-world examples are continuous. We can increase the granularity to get a discrete approximation of continuous measurements, but this increases the scale again.\nMultimodal - Sometimes you require a single output value.\nNeeds a state change measurement.\n\nThe Kalman filter is based on the same idea that we can use Bayesian reasoning to combine measurements and system models. The fundamental insight in this chapter is that we multiply (convolve) probabilities when we measure and shift probabilities when we update, which leads to a converging solution.\n\n\n\n\nGaussian distributions address some of the limitations of the discrete Bayes filter, as they are continuous and unimodal.\n\n\n\nRandom variables - the possible values and associated probabilities of a particular event. Denoted with capital letter, e.g. X\nSample space - the range of values a random variable can take. This is not unique, e.g. for a dice roll could be {1,2,3,4,5,6}, or {even,odd} or {d}\nProbability distribution - the probability for the random variable to take any value in the sample space. Probability distribution denoted with lower case p, and probability of a single event denoted with upper case P, e.g. P(X=1) = p(1)\nTotal probability - Probabilities are non-negative and sum/integrate to 1.\nSummary statistics - mean, median, mode\nExpected value is the probability-weighted mean of values. This equals the mean if the probability distribution is uniform. E[X] = \\sum{p_i x_i} mean = \\sum{x_i}/n\nVariance - var(X) = E[(x-mu)^2] = \\sum{(x_i - mu)^2}/n The square in the variance (rather han absolute value) is somewhat arbitrary, and reflects that the sign of the deviation shouldn’t matter and larger deviations are “worse” than smaller deviations. Precision is sometimes used which is the inverse of the variance.\n\n\n\n\nContinuous, unimodal probability distributions.\nf(x, mu, sigma) = (1 / sigma sqrt(2*pi)) * e^(-(x-mu)^2/2*sigma^2)\n\nwhich, removing constants, is proportional to e^-x^2\nThe probability of a single point is infinitesimally small. We can think of this in Bayesian terms or frequentist terms. As a Bayesian, if the thermometer reads exactly 22°C, then our belief is described by the Gaussian curve; our belief that the actual (system) temperature is near 22°C is very high, and our belief that the actual temperature is, say, near 18 is very low. As a frequentist we would say that if we took 1 billion temperature measurements of a system at exactly 22°C, then a histogram of the measurements would look like a Gaussian curve.\nGaussians are nice to work with because the sum of independent Gaussian random variables is another Gaussian random variable. The product of Gaussian distributions will be a Gaussian function, i.e. it is Gaussian but may not sum to 1 so will need to be scaled. Sum of two Gaussians:\n\nmu = mu_1 + mu_2\nvar = var_1 + var_2\n\nProduct of two Gaussians:\n\nmu = (var_1mu_2 + var_2mu_1) / (var_1 + var_2)\nvar = (var_1*var_2) / (var_1 + var_2)\n\nThey also mean we can summarise/approximate large datasets with only two numbers: mean and variance.\nBayes theorem applies to probability distribution functions (p) just like it does to individual events (P). From the general form of Bayes theorem:\np(A|B) = p(B|A)p(A)/p(B)\nRecasting this for the sensor problem, where A=x_i (the position) and B=z (sensor reading):\np(x_i|z) = p(z|x_i)p(x_i)/p(z)\n\np(z|x_i) is the likelihood - the probability that we get the sensor measurement z at each position x_i\np(x_i) is the prior - our belief before incorporating the measurement\np(x_i) is the evidence - in practice this is often an intractable integral which we can sidestep by treating this as a normalisation factor, as we know the posterior pdf must sum to 1.\np(x_i|z) is the posterior - this is typically a hard question to answer, but using Bayes we can substitute it with the easier question answered by the likelihood and prior. Rather than answering “what is the probability of cancer given this test result” (hard problem) we instead answer “what is the probability of this test result given someone has cancer” (easier problem)\n\n\n\n\nA limitation of using Gaussians to model physical systems is that it has infinitely long tails, but in practice many physical quantities have hard bounds, e.g. weight cannot be negative. Under a Gaussian assumption, model people’s weight as a Gaussian would suggest there is a miniscule chance that someone weights 1000000kg, which is obviously wrong.\nThis can impact the performance of Kalman filters as they rely on assumptions of Gaussian noise, which aren’t strictly true.\n\n\n\n\nA filter to track one state variable, position. A Kalman filter is essentially a Bayesian filter that uses Gaussians, so combines the previous two chapters.\n\n\n\n\nKalman filter repo\nArtificial Intelligence for Robotics course\nThink Stats ebook\n\n\n\n\nFigure 1: g-h filter diagram"
  },
  {
    "objectID": "posts/ml/kalman_filter/kalman_filter.html#g-h-filters",
    "href": "posts/ml/kalman_filter/kalman_filter.html#g-h-filters",
    "title": "Kalman Filter Notes",
    "section": "",
    "text": "With measurements from 2 noisy sensors, we can combine them to get a much more accurate measurement. Never throw data away.\nIf we have a prediction model for how the world evolves (e.g. predicted daily weight gain) we can combine this with noisy measurements in the same way. Treat the prediction as a noisy sensor and combine as before, so that our estimate is some way between the prediction and the estimate. How close it is to one or the other depends on the relative accuracy of each, and this is a hyperparameter we can set.\nprediction =  x_est + dx*dt \n\nwhere:\n`x_est` is an initial constant that gets updated\n`dx` is a predicted daily weight gain rate\n`dt` is the time step\n\n\n\nWe can create a single parameter filter, where the parameter g corresponds to how strongly we trust the prediction over the measurement\nresidual = prediction - measurement\nestimate = measurement + g * residual\n\ng=0 =&gt; estimate=measurement\ng=1 =&gt; estimate=prediction\nfor other values of g, the estimate will be somewhere between the measurement and prediction\n\n\n\n\n\n\n\nFigure 1: g-h filter diagram\n\n\n\n\n\n\nWe can go one step further and use our noisy estimates of weight to refine our the daily weight gain w used in our prediction\ndx_t+1 = dx_t + h * residual / dt\nThis gives us the g-h filter - a generic filter which allows us to set a parameter g for the weight measurement confidence and h for the weight change measurement confidence.\nWe can then update the estimates for weight and weight change on every time step (or every measurement if time steps are irregular).\nThis insight forms the basis for Kalman filters, where we will set g and h dynamically on each time step.\n\n\n\nKey takeaways:\n\nMultiple data points are more accurate than one data point, so throw nothing away no matter how inaccurate it is.\nAlways choose a number part way between two data points to create a more accurate estimate.\nPredict the next measurement and rate of change based on the current estimate and how much we think it will change.\nThe new estimate is then chosen as part way between the prediction and next measurement scaled by how accurate each is.\nThe filter is only as good as the mathematical model used to express the system.\nFilters are designed, not selected ad hoc. No choice of g and h applies to all scenarios.\n\nTerminology:\n\nSystem - the object we want to estimate\nState, x - the current configuration of the system. Hidden.\nMeasurement, z - measured value of the state from a noisy sensor. Observable.\nState estimate - our filter’s estimate of the state.\nProcess model - the model we use to predict the next state based on the current state.\nSystem propagation - the predict step\nMeasurement update - the update step\nEpoch - one iteration of system propagation and measurement update."
  },
  {
    "objectID": "posts/ml/kalman_filter/kalman_filter.html#discrete-bayes-filter",
    "href": "posts/ml/kalman_filter/kalman_filter.html#discrete-bayes-filter",
    "title": "Kalman Filter Notes",
    "section": "",
    "text": "Bayesian statistics treats probability as a belief about a single event. Frequentist statistics describes past events based on their frequency. It has no bearing on future events.\nIf I flip a coin 100 times and get 50 heads and 50 tails, frequentist statistics states the probability of heads was 50% for those cases. On the next coin flip, frequentist statistics has nothing to say about the probability. The state is simply unknown. Bayesian statistics incorporates these past events as a prior belief, so that we can say the next coin flip has a 50% chance of landing heads. “Belief” is a measure of the strength of our knowledge.\nWhen talking about the probability of something, we are implicitly saying “the probability that this event is true given past events”. This is a Bayesian approach. In practice, we may incorporate frequentist techniques too, as in the example above when the 100 previous coin tosses were used to inform our prior.\n\nPrior is the probability distribution before including the measurement’s information. This corresponds to the prediction in the Kalman filter.\nPosterior is a probability distribution after incorporating the measurement’s information. This corresponds to the estimated state in the Kalman filter.\nLikelihood is the joint probability of the observed data - how likely is each position given the measurement. This is not a probability distribution as it does not sum to 1.\n\nThe filter will use Bayes theorem:\nposterior = likelihood * prior / normalization\nIn even simpler filter terms:\nupdated knowledge = || likelihood of new knowledge * prior knowledge ||\nIf we have a prior distribution of positions and system model of the subsequent movement, we can convolve the two to calculate the posterior.\n\n\n\nThe discrete Bayes filter is a form of g-h filter. It is useful for multimodal, discrete problems.\nThe equations are:\nPredict step:\nx_bar = x * f_x(.)\nwhere:\n- x is the current state\n- f_x(.) is the state propagation function for x, i.e. the system model.\n\n\nUpdate step:\nx = ||L . x_bar||\nwhere:\n- L is the likely function\nIn pseudocode this is:\nInitialisation:\n1. Initialise our belief in the state.\n\nPredict:\n1. Predict state for the next time step using the system model.\n2. Adjust belief to account for uncertainty in prediction.\n\nUpdate:\n1. Get a measurement and belief about its accuracy (noise estimate).\n2. Compute likelihood of measurement matching each state.\n3. Update posterior state belief based on likelihood.\nAlgorithms of this form a called “predictor correctors”; we make a prediction then correct it. The predict step will always degrade our knowledge due to the uncertainty in the second step of the predict stage. But adding another measurement, even if noisy, improves our knowledge again. So we can converge on the most likely result.\n\n\n\nThe algorithm is trivial to implement, debug and understand.\nLimitations:\n\nScaling - Tracking i state variables results in O(n^i) runtime complexity.\nDiscrete - Most real-world examples are continuous. We can increase the granularity to get a discrete approximation of continuous measurements, but this increases the scale again.\nMultimodal - Sometimes you require a single output value.\nNeeds a state change measurement.\n\nThe Kalman filter is based on the same idea that we can use Bayesian reasoning to combine measurements and system models. The fundamental insight in this chapter is that we multiply (convolve) probabilities when we measure and shift probabilities when we update, which leads to a converging solution."
  },
  {
    "objectID": "posts/ml/kalman_filter/kalman_filter.html#probability-and-gaussians",
    "href": "posts/ml/kalman_filter/kalman_filter.html#probability-and-gaussians",
    "title": "Kalman Filter Notes",
    "section": "",
    "text": "Gaussian distributions address some of the limitations of the discrete Bayes filter, as they are continuous and unimodal.\n\n\n\nRandom variables - the possible values and associated probabilities of a particular event. Denoted with capital letter, e.g. X\nSample space - the range of values a random variable can take. This is not unique, e.g. for a dice roll could be {1,2,3,4,5,6}, or {even,odd} or {d}\nProbability distribution - the probability for the random variable to take any value in the sample space. Probability distribution denoted with lower case p, and probability of a single event denoted with upper case P, e.g. P(X=1) = p(1)\nTotal probability - Probabilities are non-negative and sum/integrate to 1.\nSummary statistics - mean, median, mode\nExpected value is the probability-weighted mean of values. This equals the mean if the probability distribution is uniform. E[X] = \\sum{p_i x_i} mean = \\sum{x_i}/n\nVariance - var(X) = E[(x-mu)^2] = \\sum{(x_i - mu)^2}/n The square in the variance (rather han absolute value) is somewhat arbitrary, and reflects that the sign of the deviation shouldn’t matter and larger deviations are “worse” than smaller deviations. Precision is sometimes used which is the inverse of the variance.\n\n\n\n\nContinuous, unimodal probability distributions.\nf(x, mu, sigma) = (1 / sigma sqrt(2*pi)) * e^(-(x-mu)^2/2*sigma^2)\n\nwhich, removing constants, is proportional to e^-x^2\nThe probability of a single point is infinitesimally small. We can think of this in Bayesian terms or frequentist terms. As a Bayesian, if the thermometer reads exactly 22°C, then our belief is described by the Gaussian curve; our belief that the actual (system) temperature is near 22°C is very high, and our belief that the actual temperature is, say, near 18 is very low. As a frequentist we would say that if we took 1 billion temperature measurements of a system at exactly 22°C, then a histogram of the measurements would look like a Gaussian curve.\nGaussians are nice to work with because the sum of independent Gaussian random variables is another Gaussian random variable. The product of Gaussian distributions will be a Gaussian function, i.e. it is Gaussian but may not sum to 1 so will need to be scaled. Sum of two Gaussians:\n\nmu = mu_1 + mu_2\nvar = var_1 + var_2\n\nProduct of two Gaussians:\n\nmu = (var_1mu_2 + var_2mu_1) / (var_1 + var_2)\nvar = (var_1*var_2) / (var_1 + var_2)\n\nThey also mean we can summarise/approximate large datasets with only two numbers: mean and variance.\nBayes theorem applies to probability distribution functions (p) just like it does to individual events (P). From the general form of Bayes theorem:\np(A|B) = p(B|A)p(A)/p(B)\nRecasting this for the sensor problem, where A=x_i (the position) and B=z (sensor reading):\np(x_i|z) = p(z|x_i)p(x_i)/p(z)\n\np(z|x_i) is the likelihood - the probability that we get the sensor measurement z at each position x_i\np(x_i) is the prior - our belief before incorporating the measurement\np(x_i) is the evidence - in practice this is often an intractable integral which we can sidestep by treating this as a normalisation factor, as we know the posterior pdf must sum to 1.\np(x_i|z) is the posterior - this is typically a hard question to answer, but using Bayes we can substitute it with the easier question answered by the likelihood and prior. Rather than answering “what is the probability of cancer given this test result” (hard problem) we instead answer “what is the probability of this test result given someone has cancer” (easier problem)\n\n\n\n\nA limitation of using Gaussians to model physical systems is that it has infinitely long tails, but in practice many physical quantities have hard bounds, e.g. weight cannot be negative. Under a Gaussian assumption, model people’s weight as a Gaussian would suggest there is a miniscule chance that someone weights 1000000kg, which is obviously wrong.\nThis can impact the performance of Kalman filters as they rely on assumptions of Gaussian noise, which aren’t strictly true."
  },
  {
    "objectID": "posts/ml/kalman_filter/kalman_filter.html#one-dimensional-kalman-filter",
    "href": "posts/ml/kalman_filter/kalman_filter.html#one-dimensional-kalman-filter",
    "title": "Kalman Filter Notes",
    "section": "",
    "text": "A filter to track one state variable, position. A Kalman filter is essentially a Bayesian filter that uses Gaussians, so combines the previous two chapters."
  },
  {
    "objectID": "posts/ml/kalman_filter/kalman_filter.html#references",
    "href": "posts/ml/kalman_filter/kalman_filter.html#references",
    "title": "Kalman Filter Notes",
    "section": "",
    "text": "Kalman filter repo\nArtificial Intelligence for Robotics course\nThink Stats ebook\n\n\n\n\nFigure 1: g-h filter diagram"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Projects\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplainable AI in Healthcare\n\n\n\nResearch\n\n\nAI\n\n\nHealthcare\n\n\n\nOpening the black box in medical AI\n\n\n\n\n\n\n\n\n\n\n\n\n\nTradeIntel\n\n\n\nTrading\n\n\nApp\n\n\n\nNext-generation robo advisor\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "plan.html",
    "href": "plan.html",
    "title": "Gurpreet Johl",
    "section": "",
    "text": "Fast AI\nSales\nAWS\nKalman filter"
  },
  {
    "objectID": "plan.html#current",
    "href": "plan.html#current",
    "title": "Gurpreet Johl",
    "section": "",
    "text": "Fast AI\nSales\nAWS\nKalman filter"
  },
  {
    "objectID": "plan.html#done",
    "href": "plan.html#done",
    "title": "Gurpreet Johl",
    "section": "Done",
    "text": "Done\n\nSQL\nDeep learning (Tensorflow)\nSoftware architecture\nMarketing\nPublic speaking\nPitching\nSaaS\nConsulting\nSystem design"
  },
  {
    "objectID": "plan.html#skills",
    "href": "plan.html#skills",
    "title": "Gurpreet Johl",
    "section": "Skills",
    "text": "Skills\n\nSoft skills\nDone: - Marketing - 1 page marketing plan; 22 immutable laws of marketing? - Public speaking - TED talk book. Steal the show by Michael Port? - Pitching - Pitch Anything by Oren Klaff - SaaS: Start small stay small; SaaS playbook - Consulting business book\nIn Progress: - Sales - Spin selling by Neil Rackham\nTo Do: - Strategy - startup bible book - Negotiating - never split the difference book and masterclass; influence the psychology of persuasion by Robert Cialdini - Sales - Founder Sales\nBacklog: - Operations - the goal by Eliyahu Goldratt - Leadership - teams of teams by Stanley mchrystal; Start with Why: How Great Leaders Inspire Everyone to Take Action by Simon Sinek - Networking - how to be a power connector by Judy Robinett - Fundraising - crack the funding code by Judy Robinett; venture deals by Brad Feld; the art of startup fundraising by Alejandro Cremades; the startup checklist by David Rose\n\n\nSoftware\nDone: - SQL - Udemy course - Software architecture - Udemy course - System design - Udemy course\nIn progress: - AWS solutions architect Udemy course\nTo Do: - Docker - Udemy course\nBacklog: - Game development - Unity udemy course - Cracking the coding interview - Data structures and algorithms https://allendowney.github.io/DSIRP/ - Devops - Continuous deployment book\n\n\nEngineering/maths\nDone: - Deep learning (Tensorflow) - Udemy course\nIn progress: - Fast AI course - Kalman filter book\nBacklog: - Computational linear algebra https://www.fast.ai/posts/2017-07-17-num-lin-alg.html - Timeseries forecasting: https://www.kaggle.com/learn/time-series?rvi=1 - RNNs? - https://greenteapress.com/wp/think-complexity-2e/\n\n\nStats\nBacklog: - Intro to probability book - Classical stats: t-stat, ANOVA https://www.udacity.com/course/intro-to-inferential-statistics–ud201 - Filters https://www.udacity.com/course/artificial-intelligence-for-robotics–cs373\n\n\nBig data\nBacklog: - https://www.tutorialspoint.com/pyspark/index.htm - https://www.udemy.com/course/best-hands-on-big-data-practices-and-use-cases-using-pyspark/ - https://www.udacity.com/course/learn-spark-at-udacity–ud2002\n\n\nInterview prep\nBacklog: - https://www.udacity.com/course/refresh-your-resume–ud243 - https://www.udacity.com/course/data-science-interview-prep–ud944 - https://www.udacity.com/course/machine-learning-interview-prep–ud1001"
  },
  {
    "objectID": "gen-deep-learning-series.html",
    "href": "gen-deep-learning-series.html",
    "title": "Series: Generative Deep Learning",
    "section": "",
    "text": "Generative AI: Chapter 2\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\n\nIntro to deep learning\n\n\n\n\n\nFeb 26, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI: Chapter 1\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\n\nIntro to generative AI\n\n\n\n\n\nFeb 19, 2024\n\n\n5 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  }
]