[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Posts",
    "section": "",
    "text": "Series\n\nfastai-series\nThis series contains notes from following the fastai “Practical Deep Learning for Coders” course. See the course page and book.\n\n\ngenerative-ai-series\nThis series contains notes from the book “Generative Deep Learning” by David Foster.\n\n\n\nAll Posts\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nGenerative AI: Chapter 1\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\n\nNotes from Generative Deep Learning book\n\n\n\n\n\nFeb 8, 2024\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 6: Random Forests\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 6\n\n\n\n\n\nOct 9, 2023\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 5: Natural Language Processing\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 5\n\n\n\n\n\nSep 23, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 4: Natural Language Processing\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 4\n\n\n\n\n\nSep 7, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 3: How Does a Neural Network Learn?\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 3\n\n\n\n\n\nSep 1, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 2: Deployment\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 2\n\n\n\n\n\nAug 30, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 1: Image Classification Models\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 1\n\n\n\n\n\nAug 23, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nSystem Design Notes\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\n\nNotes on System Design\n\n\n\n\n\nAug 14, 2023\n\n\n24 min\n\n\n\n\n\n\n\n\n\n\n\n\nPitching Notes\n\n\n\n\n\n\nBusiness\n\n\n\nNotes on Pitching\n\n\n\n\n\nAug 4, 2023\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nKalman Filter Notes\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\n\nNotes on Kalman filters\n\n\n\n\n\nJul 23, 2023\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nPublic Speaking Notes\n\n\n\n\n\n\nBusiness\n\n\n\nNotes on Public Speaking\n\n\n\n\n\nJul 20, 2023\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nMarketing Notes\n\n\n\n\n\n\nBusiness\n\n\n\nNotes on Marketing\n\n\n\n\n\nJul 18, 2023\n\n\n18 min\n\n\n\n\n\n\n\n\n\n\n\n\nSoftware Architecture Notes\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\n\nNotes on Software Architecture\n\n\n\n\n\nJun 23, 2023\n\n\n22 min\n\n\n\n\n\n\n\n\n\n\n\n\nTensorflow Notes\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\n\nNotes on Tensorflow\n\n\n\n\n\nFeb 23, 2023\n\n\n21 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "fastai-series.html",
    "href": "fastai-series.html",
    "title": "Series: FastAI course",
    "section": "",
    "text": "FastAI Lesson 6: Random Forests\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 6\n\n\n\n\n\nOct 9, 2023\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 5: Natural Language Processing\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 5\n\n\n\n\n\nSep 23, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 4: Natural Language Processing\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 4\n\n\n\n\n\nSep 7, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 3: How Does a Neural Network Learn?\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 3\n\n\n\n\n\nSep 1, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 2: Deployment\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 2\n\n\n\n\n\nAug 30, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 1: Image Classification Models\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 1\n\n\n\n\n\nAug 23, 2023\n\n\n3 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "This site contains (or will contain very soon) my personal software and AI projects…\n\n\n Back to top"
  },
  {
    "objectID": "posts/ml/tensorflow/tensorflow.html",
    "href": "posts/ml/tensorflow/tensorflow.html",
    "title": "Tensorflow Notes",
    "section": "",
    "text": "These are assorted notes on topics from various courses and projects.\n\n\n\nData sourcing - train_test_split\nExploratory data analysis - plot distributions, correlations\nData cleaning - drop redundant columns, handle missing data (drop or impute)\nFeature engineering - one hot encoding categories, scaling/normalising numerical values, combining columns, extracting info from columns (e.g. zip code from address)\nModel selection and training - model and hyperparameters\nModel tuning and evaluation metrics - classification: classification_report, confusion matrix, accuracy, recall, F1. Regression: error (RMSE, MAE, etc)\nPredictions\n\n\n\n\n\n\n\nSupervised learning\n\nSVM\n…\n\nUnsupervised learning\n\nDimensionality reduction\nClustering\n\nReinforcement Learning\n\nFor supervised learning: - Regression - Classification - Single class - Multi class\n\n\n\n\n\nGeneral idea of a neural network that can then be extended to specific cases, e.g. CNNs and RNNs.\n\nPerceptron: a weighted average of inputs passed through some activation gate function to arrive at an output decision\nNetwork of perceptrons\nActivation function: a non-linear transfer function f(wx+b)\nCost function and gradient descent: convex optimisation of a loss function using sub-gradient descent. The optimizer can be set in the compile method of the model.\nBackpropagation: use chain rule to determine partial gradients of each weight and bias. This means we only need a single forward pass followed by a single backward pass. Contrast this to if we perturbed each weight or bias to determine each partial gradient: in that case, for each epoch we would need to run a forward pass per weight/bias in the network, which is potentially millions!\nDropout: a technique to avoid overfitting by randomly dropping neurons in each epoch.\n\nGeneral structure: 1. Input layer 2. Hidden layer(s) 3. Output layer\nInput and output layers are determined by the problem: - Input size: number of features in the data - Output size number of targets to predict, i.e. one for single class classification or single target regression, or multiple for multiclass (one per class) - Output layer activation determined by problem. For single class classification activation='sigmoid', for multiclass classification activation='softmax' - Loss function determined by problem. For single class classification loss='binary_crossentropy', for multiclass classification loss='categorical_crossentropy'\nHidden layers are less well-defined. Some heuristics here: https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\nVanishing gradients can be an issue for lower layers in particular, where the partial gradients of individual layers can be very small, so when multiplied together in chain rule the gradient is vanishingly small. Exploding gradients are a similar issue but where gradients get increasingly large when multiplied together. Some techniques to rectify vanishing/exploding gradients: - Use different activation functions with larger gradients close to 0 and 1, e.g. leaky ReLU or ELU (exponential linear unit) - Batch normalisation: scale each gradient by the mean and standard deviation of the batch - Different weight initialisation methods, e.g. Xavier initialisation\nExample model outline\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\n\nmodel = Sequential()\n\n# Input layer\nmodel.add(Dense(78, activation='relu'))  # Number of input features\nmodel.add(Dropout(0.2))  # Optional dropout layer after each layer. Omitted for next layers for clarity\n\n# Hidden layers\nmodel.add(Dense(39, activation='relu'))\nmodel.add(Dense(19, activation='relu'))\n\n# Output layer\nmodel.add(Dense(1, activation='sigmoid'))  # Number of target classes (single class in this case)\n\n# Problem setup\nmodel.compile(loss='binary_crossentropy', optimizer='adam')  # Type of problem (single class classification in this case)\n\n# Training\nmodel.fit(\n    x=X_train,\n    y=y_train,\n    batch_size=256,\n    epochs=30,\n    validation_data=(X_test, y_test)\n)\n\n# Prediction\nmodel.predict(new_input)  # The new input needs to be shaped and scaled the sane as the training data\n\n\n\nThese are used for image classification problems where convolutional filters are useful for extracting features from input arrays.\n\nImage kernels/filters\n\nGrayscale 2D arrays\nRGB 3D tensors\n\nConvolutional layers\nPooling layers\n\nGeneral structure: 1. Input layer 2. Convolutional layer 3. Pooling layer 4. (Optionally more pairs of convolutional and pooling layers) 5. Flattening layer 6. Dense hidden layer(s) 7. Output layer\nExample model outline\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nmodel = Sequential()\n\n# Convolutional layer followed by pooling. Deeper networks may have multiple pairs of these.\nmodel.add(Conv2D(filters=32, kernel_size=(4,4),input_shape=(28, 28, 1), activation='relu',))  # Input shape determined by input data size\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\n# Flatten _images from 2D to 1D before final layer\nmodel.add(Flatten())\n\n# Dense hidden layer\nmodel.add(Dense(128, activation='relu'))\n\n# Output layer\nmodel.add(Dense(10, activation='softmax'))  # Size and activation determined by problem; multiclass classification in this case\n\n# Problem setup\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')  # Loss determined by problem, multiclass classification in this case\n\n# Training (with optional early stopping)\nearly_stop = EarlyStopping(monitor='val_loss',patience=2)\nmodel.fit(x_train,y_cat_train,epochs=10,validation_data=(x_test,y_cat_test),callbacks=[early_stop])\n\n# Prediction\nmodel.predict(new_input)  # The new input needs to be shaped and scaled the sane as the training data\n\n\n\nThese are used for modelling sequences with variable lengths of inputs and outputs.\nRecurrent neurons take as their input the current data AND the previous epoch’s output. We could try to pass EVERY previous epoch’s output as an input, but would run into issues of vanushing gradients.\nLSTMs take this idea a step further by incorporating the previous epochs input AND some longer lookback of epoch where some old outputs are “forgotten”.\nA basic RNN:\n\n            --------------------------------\n            |                              |\n H_t-1 ---&gt; |                              |---&gt; H_t\n            |                              |\n X_t   ---&gt; |                              |\n            |                              |\n            --------------------------------\n\nwhere:\nH_t is the neuron's output at current epoch t\nH_t-1 is the neuron's output from the previous epoch t-1\nX_t is the input data at current epoch t\n\nH_t = tanh(W[H_t-1, X_t] + b)\nAn LSTM (Long Short Term Memory) unit:\n\n            --------------------------------\n            |                              |\n H_t-1 ---&gt; |                              |---&gt; H_t\n            |                              |\n C_t-1 ---&gt; |                              |---&gt; C_t\n            |                              |\n X_t   ---&gt; |                              |\n            |                              |\n            --------------------------------\n            \nThe `forget gate` determines which part of the old short-term memory and current input is \"forgotten\" by the new long-term memory.\nThese are a set of weights (between 0 and 1) that get applied to the old long-term memory to downweight it.\nF_t = sigmoid(W_F[H_t-1, X_t] + b_F)\n\nThe `input gate` i_t similarly gates the input and old short-term memory.\nThis will later (in the update gate) get combined  with a candidate value for the new long-term memory `C_candidate_t`\nI_t = sigmoid(W_I[H_t-1, X_t] + b_I)\nC_candidate_t = tanh(W_C_cand[H_t-1, X_t] + b_C_cand) \n\nThe `update gate` for the new long-term memory `C_t` is then calculated as a sum of forgotten old memory and input-weighted candidate memory:\nC_t = F_t*C_t-1 + I_t*C_candidate_t \n\nThe `output gate` O_t is a combination of the old short-term memory and latest input data.\nThis is then combined with the latest long-term memory to produce the output of the recurrent neuron, which is also the updated short-term memory H_t:\nO_t = sigmoid(W_O[H_t-1, X_t] + b_O)\nH_t = O_t * tanh(C_t) \n\nwhere:\nH_t is short-term memory at epoch t\nC_t is long-term memory at epoch t\nX_t is the input data at current epoch t\n\nsigmoid is a sigmoid  activation function\nF_t is an intermediate forget gate weight\nI_t is an intermediate input gate weight\nO_t is an intermediate output gate weight\nThere are several variants of RNNs. RNNs with peepholes This leaks long-term memory into the forget, input and output gates. Note that the forget gate and input gate each get the OLD long-term memory, whereas the output gate gets the NEW long-term memory.\nF_t = sigmoid(W_F[C_t-1, H_t-1, X_t] + b_F)\nI_t = sigmoid(W_I[C_t-1, H_t-1, X_t] + b_I)\nO_t = sigmoid(W_O[C_t, H_t-1, X_t] + b_O)\nGated Recurrent Unit (GRU) This combines the forget and input gates into a single gate. It also has some other changes. This is simpler than a typical LSTM model as it has fewer parameters. This makes it more computationally efficient, and in practice they can have similar performance.\nz_t = sigmoid(W_z[H_t-1, X_t])\nr_t = sigmoid(W_r[H_t-1, X_t])\nH_candidate_t = tanh(W_H_candidate[r_t*h_t-1, x_t])\nH_t = (1 - z_t) * H_t-1 + z_t * H_candidate_t\nThe sequences modelled with RNNs can be: - One-to-many - Many-to-many - Many-to-one\nConsiderations for choosing the input sequence length: - It should be long enough to capture any patterns or seasonality in the data - The validation set and test set each need to have a size at least (input_length + output_length), so the longer the input length, the more data is required to be set aside.\nThe validation loss for RNNs/LSTMs can be volatile, so allow a higher patience when using early stopping.\nExample model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,LSTM\n\n# Generators for helper functions to chunk up the input series into an array of pairs of (input_batch, target_output)\ntrain_generator = TimeseriesGenerator(scaled_train, scaled_train, length=batch_length, batch_size=1)\nvalidation_generator = TimeseriesGenerator(scaled_test, scaled_test, length=batch_length, batch_size=1)\n\n# Create an LSTM model\nmodel = Sequential()\nmodel.add(LSTM(100, activation='relu', input_shape=(batch_length, n_features)))\nmodel.add(Dense(1))  # Output layer\nmodel.compile(optimizer='adam', loss='mse')  # Problem setup for regression\n\n# Fit the model\nearly_stop = EarlyStopping(monitor='val_loss', patience=10)  # RNNs can be volatile so if use a higher patience with an early stopping callback\nmodel.fit(train_generator, epochs=20, validation_data=validation_generator, callbacks=[early_stop])\n\n# Evaluate test data\ntest_predictions = []\nfinal_batch = scaled_train[-batch_length:, :]\ncurrent_batch = final_batch.reshape((1, batch_length, n_features))\n\nfor test_val in scaled_test:\n    pred_val = model.predict(current_batch)  # These will later need to use the scaler.inverse_transform of the scaler originally used on the training data\n    test_predictions.append(pred_val[0])\n    current_batch = np.append(current_batch[:,1:,:], pred_val.reshape(1,1,1), axis=1)\n\n\n\nCharacter-based generative NLP: given an input string, e.g. [‘h’, ‘e’, ‘l’, ‘l’], predict the sequence shifted by one character, e.g. [‘e’, ‘l’, ‘l’, ‘o’]\nThe embedding, GRU and dense layers work on the sequence in the following way: \nSteps: 1. Read in text data 2. Text processing and vectorisation - one-hot encoding 3. Create batches 4. Create the model 5. Train the model 6. Generate new text\nStep 1: Read in text data - A corpus of &gt;1 million characters is a good size - The more distinct the style of your corpus, the more obvious it will be if your model has worked.\nGiven some structured input_text string, we can get the one-hot encoded vocab list of unique characters.\nvocab_list = sorted(set(input_text))\nStep 2: Text processing Vectorise the text - create a mapping between character and integer. This is the encoding dictionary used to vectorise the corpus.\n# Mappings back and forth between characters and their encoded integers\nchar_to_ind = {char: ind for ind, char in enumerate(vocab_list)}\nind_to_char = {ind: char for ind, char in enumerate(vocab_list)}\n\nencoded_text = np.array([char_to_ind[char] for char in input_text])\nStep 3: Create batches The sequence length should be long enough to capture structure, e.g. for poetry with rhyming couplets it should be the number of characters in 3 lines to capture the rhyme and non-rhyme. Too long a sequence makes the model take longer to train and captures too much historical noise.\nCreate a shuffled dataset where: - input sequence is the first n characters - output sequence is n characters lagged by one\nWe want to shuffle these sequence pairs into a random order so the model doesn’t overfit to any section of the text, but can instead generate characters given any seed text.\nseq_len = 120\ntotal_num_seq = len(input_text) // (seq_len + 1)\n\n# Create Training Sequences\nchar_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\nsequences = char_dataset.batch(seq_len + 1, drop_remainder=True)  # drop_remainder drops the last incomplete sequence\n\n# Create tuples of input and output sequences\ndef create_seq_targets(seq):\n    input_seq = seq[:-1]\n    output_seq = seq[-1:]\n    return input_seq, output_seq\ndataset = sequences.map(create_seq_targets)\n\n# Generate training batches\nbatch_size = 128\nbuffer_size = 10000  # Shuffle this amount of batches rather than shuffling the entire dataset in memory\ndataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\nStep 4: Create the model Set up the loss function and layers.\nThis model uses 3 layers: 1. Embedding - Embed positive integers, e.g. “a”&lt;-&gt;1, as dense vectors of fixed size. This embedding size is a hyperparameter for the user to decide. - Number of layers should be smaller but a similar scale to the vocab size; typically use the power of 2 that is just smaller than the vocab size. 2. GRU - This is the main thing to vary in the model: number of hidden layers and number of neurons in each, types of neurons (RNN, LSTM, GRU), etc. Typically use lots of layers here. 3. Dense - One neuron per character (one-hot encoded), so this produces a probability per character. The user can vary the “temperature”, choosing less probable characters more/less often.\nLoss function: - Sparse categorical cross-entropy - Use sparse categorical cross-entropy because the classes are mutually exclusive. Use regular categorical cross-entropy when one smaple can have multiple classes, or the labels are soft probabilities. - See link in NLP appendix for discussion. - Use logits=True because vocab input is one hot encoded.\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, GRU\nfrom tensorflow.keras.losses import sparse_categorical_crossentropy\n\n# Hyperparameters\nvocab_size = len(vocab_list)\nembed_dim = 64\nrnn_neurons = 1026\n\n# Create model \nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embed_dim, batch_input_shape=[batch_size, None]))\nmodel.add(GRU(rnn_neurons, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'))\nmodel.add(Dense(vocab_size))\nsparse_cat_loss = lambda y_true,y_pred: sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)  # We want a custom loss function with logits=True\nmodel.compile(optimizer='adam', loss=sparse_cat_loss) \nStep 5: Train the model\nepochs = 30\nmodel.fit(dataset,epochs=epochs)\nStep 6: Generate text Allow the model to accept a batch size of 1 to allow us to give an input seed text from which to generate text from.\nWe format the seed text so it can be fed into the network, then loop through generating one character at a time and feeding that back into the model.\nmodel.build(tf.TensorShape([1, None]))\n\ndef generate_text(model, input_seed_text, gen_size=100, temperature=1.0):\n    \"\"\"\n    model: tensorflow.model\n    input_seed_text: str\n    gen_size: int\n    temperature: float\n    \"\"\"\n    generated_text_result = []\n    vectorised_seed = tf.expand_dims([char_to_ind[s] for s in input_seed_text], 0)\n    model.reset_states()\n    \n    for k in range(gen_size):\n        raw_predictions = model(vectorised_seed)\n        predictions = tf.squeeze(raw_predictions, 0) / temperature\n        predicted_index = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n        generated_text_result.append(ind_to_char[predicted_index])\n        \n        # Set a new vectorised seed to generate the next character in the loop\n        vectorised_seed = tf.expand_dims([predicted_index], 0)\n        \n    return (input_seed_text + ''.join(generated_text_result))\n\n\n\nThis is an unsupervised learning problem, therefore evaluation metrics are different. Sometimes called semi-supervised as labels may be used in training the model, but not available when it’s used to predict new values.\nApplications: - Dimensionality reduction - Noise removal\nDesigned to reproduce its input in the output layer. Number of input neurons is the same as the number of output layers. The encoder reduces dimensionality to the hidden layer. The hidden layer should contain the most relevant information required to reconstruct the input. The decoder reconstructs a high dimensional output from a low dimensional input.\n\nEncoder: Input layer -&gt; Hidden layer\nDecoder: Hidden layer -&gt; Output\n\nStacked autoencoders include multiple hidden layers.\nAutoencoder for dimensionality reduction Let’s try to reduce an input dataset from 3 dimensions to 2. We want to train an autoencoder to go from 3 -&gt; 2 -&gt; 3.\nWhen we scale data, we fit and transform on the entire dataset rather than a train/test split, because there is no ground truth for the “correct” dimensionality reduction.\nUsing SGD in Autoencoders allows the learning rate to be varied as a hyperparameter to affect how well the hidden layer learns. Larger values will train quicker but potentially perform worse.\nTo retrieve the lower dimensionality representation, use just the encoder half of the trained autoencoder to predict the input.\n# Scale data\nscaler = MinMaxScaler()\nscaled_data = scaler.fit_transform(input_features)\n\n# Create model\nencoder = Sequential()\nencoder.add(Dense(units=2, acivation='relu', input_shape=[3]))\n\ndecoder = Sequential()\ndecoder.add(Dense(units=3, acivation='relu', input_shape=[2]))\n\nautoencoder = Sequential([encoder, decoder])\nautoencoder.compile(loss='mse', optimizer=SGD(lr=1.5))\n\n# Train the full autoencoder model\nautoencoder.fit(scaled_data, scaled_data, epochs=5)\n\n# Reduce dimensionality by using just the encoder part of the trained model\nencoded_2dim = encoder.predict(scaled_data)\nAutoencoder for noise removal Starting with clean images, we want to train an autoencoder to learn to remove noise. We create a noisy dataset by adding noise to our clean images. We then try to reconstruct that input, and compare to the original clean images as the ground truth.\nStarting with 28x28 images (e.g. MNIST dataset), this gives 784 input features when flattened. Use multiple hidden layers as there are a large number of input features that would be difficult to learn in one layer. The numbers of layers and number of neurons in each are hyperparameters to tune; decreasing in powers of 2 is a good rule of thumb.\nAdd a Gaussian noise layer to train the model to remove noise. Without the Gaussian layer, the model would be used for dimensionality reduction rather than noise removal.\nThe final layer uses a sigmoid activation because it is a binary classification problem - does the output match the input?\nencoder = Sequential()\nencoder.add(Flatten(input_shape=[28, 28]))\nencoder.add(Gaussian(0.2))  # Optional layer if training for noise removal rather than dimensionality reduction\nencoder.add(Dense(400, activation='relu'))\nencoder.add(Dense(200, activation='relu'))\nencoder.add(Dense(100, activation='relu'))\nencoder.add(Dense(50, activation='relu'))\nencoder.add(Dense(25, activation='relu'))\n\ndecoder = Sequential()\ndecoder.add(Dense(50, input_shape=[25], activation='relu'))\ndecoder.add(Dense(100, activation='relu'))\ndecoder.add(Dense(200, activation='relu'))\ndecoder.add(Dense(400, activation='relu'))\ndecoder.add(Dense(28*28, activation='sigmoid'))\n\nautoencoder = Sequential([encoder, decoder])\nautoencoder.compile(loss='binary_crossentropy', optimizer=SGD(lr=1.5), metrics=['accuracy'])\nautoencoder.fit(X_train, X_train, epochs=5)\n\n\n\nUse 2 networks competing against each other to generate data - counterfeiter (generator) vs detective (discriminator). - Generator: recieves random noise - Discriminator: receives data set containing real data and fake generated data, and attempts to calssify real vs fake (binary classificaiton).\nTwo training phases, each only training one of the networks: 1. Train discriminator - real images (labeled 1) and generated images (labeled 0) are fed to the discriminator network. 2. Train generator - only feed generated images to discriminator, ALL labeled 1.\nThe generator never sees real images, it creates images based only off of the gradients flowing back from the discriminator.\nDifficulties with GANs: 1. Training resources - GPUs generally required 2. Mode collapse - generator learns an image that fools the discriminator, then only produces this image. Deep convolutional GANs and mini-batch discrimination are two solution to this problem. 3. Instability - True performance is hard to measure; just because the discriminator was fooled, it doesn’t mean that the generated image was actually realistic.\nCreating the model We create the generator and discriminator as separate models, then join them into a GAN object. This is analogous to how we joined an encoder and decoder into an autoencoder.\nThe discriminator is trained on a binary classification problem, real or fake, so uses a binary cross entropy loss function. Backpropagation only alters the weights of the discriminator in the first phase, not the generator.\ndiscriminator = Sequential()\n\n# Flatten the image\ndiscriminator.add(Flatten(input_shape=[28,28]))\n\n# Some hidden layers\ndiscriminator.add(Dense(150,activation='relu'))\ndiscriminator.add(Dense(100,activation='relu'))\n\n# Binary classification - real vs fake\ndiscriminator.add(Dense(1,activation=\"sigmoid\"))\ndiscriminator.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\nThe generator is trained in the second phase.\nWe set a codings_size, which is the size of the latent representation from which the generator will create a full image. For example, if we want to create a 28x28 image (784 pixels), we may want to generate this from a 100 element input. This is analogous to the decoder part of an autoencoder.\nThe codings_size should be smaller than the total number of features but large enough so that there is something to learn from.\nThe generator is NOT compiled at this step, it is compiled in the combined GAN later so that it is only trained at that step.\ncodings_size = 100\ngenerator = Sequential()\n\n# Input shape of first layer is the codings_size\ngenerator.add(Dense(150, activation=\"relu\", input_shape=[codings_size]))\n\n# Some hidden layers\ngenerator.add(Dense(200,activation='relu'))\n\n# Output is the size of the image we want to create\ngenerator.add(Dense(784, activation=\"sigmoid\"))\ngenerator.add(Reshape([28,28]))\nThe GAN combines the generator and discriminator.\nThe discriminator is only trained in the first phase, not the second phase.\nGAN = Sequential([generator, discriminator])\ndiscriminator.trainable = False\nGAN.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\nTraining the model We first create batches of images from our input data, similarly to previous models.\nbatch_size = 32  # Size of training input batches\nbuffer_size = 1000  # Don't load the entire dataset into memory\nepochs = 1\n\nraw_dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(buffer_size=buffer_size)\nbatched_dataset = raw_dataset.batch(batch_size, drop_remainder=True).prefetch(1)\nThe training loop for each epoch trains the discriminator to distinguish real vs fake images (binary classification). Then it trains the generator to generate fake images using ONLY the gradients learned in the discriminator training step; the generator NEVER sees any real images.\ngenerator, discriminator = GAN.layers\n\nfor epoch in range(epochs):\n    for index, X_batch in enumerate(batched_dataset):\n        # Phase 1: Train the discriminator\n        noise = tf.random.normal(shape=[batch_size, codings_size])\n        generated_images = generator(noise)\n        real_images = tf.dtypes.cast(X_batch,tf.float32)\n        X_train_discriminator = tf.concat([generated_images, real_images], axis=0)\n        y_train_discriminator = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)  # Targets set to zero for fake _images and 1 for real _images\n        discriminator.trainable = True\n        discriminator.train_on_batch(X_train_discriminator, y_train_discriminator)\n\n        # Phase 2: Train the generator\n        noise = tf.random.normal(shape=[batch_size, codings_size])\n        y_train_generator = tf.constant([[1.]] * batch_size)  # We want discriminator to believe that fake _images are real\n        discriminator.trainable = False\n        GAN.train_on_batch(noise, y_train_generator)    \nDeep convolutional GANs These are GANs which use convolutional layers inside the discriminator and generator models.\nThe models are almost identical to the models above, just with additional Conv2D, Conv2DTranspose and BatchNormalization layers.\nChanges compared to regular GANs: - Additional convolutional layers in model. - Reshape the training set to match the images. - Rescale the training data to be between -1 and 1 so that tanh activation function works. - Activation of discriminator output layer is tanh rather than sigmoid.\n\n\n\n\n\n\n\nNeural networks: - Intuition behind neural networks http://neuralnetworksanddeeplearning.com/ - The deep learning bible (with lectures) https://www.deeplearningbook.org/ - Udemy course: https://www.udemy.com/course/complete-tensorflow-2-and-keras-deep-learning-bootcamp - Heuristics for choosing hidden layers https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw - Alternatives to the deprecated model predict for different classification problems https://stackoverflow.com/questions/68776790/model-predict-classes-is-deprecated-what-to-use-instead - MIT course https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/course/\nCNNs: - Image kernels explained https://setosa.io/ev/image-kernels/ - Choosing CNN layers https://stats.stackexchange.com/questions/148139/rules-for-selecting-convolutional-neural-network-hyperparameters\nRNNs: - Overview of RNNs http://karpathy.github.io/2015/05/21/rnn-effectiveness/ - Wikipedia page contains the equations of LSTMs and peepholes https://en.wikipedia.org/wiki/Long_short-term_memory - LSTMs vs GRUs https://datascience.stackexchange.com/questions/14581/when-to-use-gru-over-lstm - Worked example of LSTM http://blog.echen.me/2017/05/30/exploring-lstms/ - RNN cheatsheet https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\nNLP: - The unreasonable effectiveness of RNNs, essay on RNNs applied to NLP http://karpathy.github.io/2015/05/21/rnn-effectiveness/ - Sparse vs dense categorical crossentropy loss function https://datascience.stackexchange.com/questions/41921/sparse-categorical-crossentropy-vs-categorical-crossentropy-keras-accuracy\nGANs: - buffer_size argument in tensorflow prefetch and shuffle https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle - Mode collapse https://www.quora.com/What-does-it-mean-if-all-produced-images-of-a-GAN-look-the-same"
  },
  {
    "objectID": "posts/ml/tensorflow/tensorflow.html#end-end-process",
    "href": "posts/ml/tensorflow/tensorflow.html#end-end-process",
    "title": "Tensorflow Notes",
    "section": "",
    "text": "Data sourcing - train_test_split\nExploratory data analysis - plot distributions, correlations\nData cleaning - drop redundant columns, handle missing data (drop or impute)\nFeature engineering - one hot encoding categories, scaling/normalising numerical values, combining columns, extracting info from columns (e.g. zip code from address)\nModel selection and training - model and hyperparameters\nModel tuning and evaluation metrics - classification: classification_report, confusion matrix, accuracy, recall, F1. Regression: error (RMSE, MAE, etc)\nPredictions"
  },
  {
    "objectID": "posts/ml/tensorflow/tensorflow.html#models",
    "href": "posts/ml/tensorflow/tensorflow.html#models",
    "title": "Tensorflow Notes",
    "section": "",
    "text": "Supervised learning\n\nSVM\n…\n\nUnsupervised learning\n\nDimensionality reduction\nClustering\n\nReinforcement Learning\n\nFor supervised learning: - Regression - Classification - Single class - Multi class\n\n\n\n\n\nGeneral idea of a neural network that can then be extended to specific cases, e.g. CNNs and RNNs.\n\nPerceptron: a weighted average of inputs passed through some activation gate function to arrive at an output decision\nNetwork of perceptrons\nActivation function: a non-linear transfer function f(wx+b)\nCost function and gradient descent: convex optimisation of a loss function using sub-gradient descent. The optimizer can be set in the compile method of the model.\nBackpropagation: use chain rule to determine partial gradients of each weight and bias. This means we only need a single forward pass followed by a single backward pass. Contrast this to if we perturbed each weight or bias to determine each partial gradient: in that case, for each epoch we would need to run a forward pass per weight/bias in the network, which is potentially millions!\nDropout: a technique to avoid overfitting by randomly dropping neurons in each epoch.\n\nGeneral structure: 1. Input layer 2. Hidden layer(s) 3. Output layer\nInput and output layers are determined by the problem: - Input size: number of features in the data - Output size number of targets to predict, i.e. one for single class classification or single target regression, or multiple for multiclass (one per class) - Output layer activation determined by problem. For single class classification activation='sigmoid', for multiclass classification activation='softmax' - Loss function determined by problem. For single class classification loss='binary_crossentropy', for multiclass classification loss='categorical_crossentropy'\nHidden layers are less well-defined. Some heuristics here: https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\nVanishing gradients can be an issue for lower layers in particular, where the partial gradients of individual layers can be very small, so when multiplied together in chain rule the gradient is vanishingly small. Exploding gradients are a similar issue but where gradients get increasingly large when multiplied together. Some techniques to rectify vanishing/exploding gradients: - Use different activation functions with larger gradients close to 0 and 1, e.g. leaky ReLU or ELU (exponential linear unit) - Batch normalisation: scale each gradient by the mean and standard deviation of the batch - Different weight initialisation methods, e.g. Xavier initialisation\nExample model outline\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\n\nmodel = Sequential()\n\n# Input layer\nmodel.add(Dense(78, activation='relu'))  # Number of input features\nmodel.add(Dropout(0.2))  # Optional dropout layer after each layer. Omitted for next layers for clarity\n\n# Hidden layers\nmodel.add(Dense(39, activation='relu'))\nmodel.add(Dense(19, activation='relu'))\n\n# Output layer\nmodel.add(Dense(1, activation='sigmoid'))  # Number of target classes (single class in this case)\n\n# Problem setup\nmodel.compile(loss='binary_crossentropy', optimizer='adam')  # Type of problem (single class classification in this case)\n\n# Training\nmodel.fit(\n    x=X_train,\n    y=y_train,\n    batch_size=256,\n    epochs=30,\n    validation_data=(X_test, y_test)\n)\n\n# Prediction\nmodel.predict(new_input)  # The new input needs to be shaped and scaled the sane as the training data\n\n\n\nThese are used for image classification problems where convolutional filters are useful for extracting features from input arrays.\n\nImage kernels/filters\n\nGrayscale 2D arrays\nRGB 3D tensors\n\nConvolutional layers\nPooling layers\n\nGeneral structure: 1. Input layer 2. Convolutional layer 3. Pooling layer 4. (Optionally more pairs of convolutional and pooling layers) 5. Flattening layer 6. Dense hidden layer(s) 7. Output layer\nExample model outline\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nmodel = Sequential()\n\n# Convolutional layer followed by pooling. Deeper networks may have multiple pairs of these.\nmodel.add(Conv2D(filters=32, kernel_size=(4,4),input_shape=(28, 28, 1), activation='relu',))  # Input shape determined by input data size\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\n# Flatten _images from 2D to 1D before final layer\nmodel.add(Flatten())\n\n# Dense hidden layer\nmodel.add(Dense(128, activation='relu'))\n\n# Output layer\nmodel.add(Dense(10, activation='softmax'))  # Size and activation determined by problem; multiclass classification in this case\n\n# Problem setup\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')  # Loss determined by problem, multiclass classification in this case\n\n# Training (with optional early stopping)\nearly_stop = EarlyStopping(monitor='val_loss',patience=2)\nmodel.fit(x_train,y_cat_train,epochs=10,validation_data=(x_test,y_cat_test),callbacks=[early_stop])\n\n# Prediction\nmodel.predict(new_input)  # The new input needs to be shaped and scaled the sane as the training data\n\n\n\nThese are used for modelling sequences with variable lengths of inputs and outputs.\nRecurrent neurons take as their input the current data AND the previous epoch’s output. We could try to pass EVERY previous epoch’s output as an input, but would run into issues of vanushing gradients.\nLSTMs take this idea a step further by incorporating the previous epochs input AND some longer lookback of epoch where some old outputs are “forgotten”.\nA basic RNN:\n\n            --------------------------------\n            |                              |\n H_t-1 ---&gt; |                              |---&gt; H_t\n            |                              |\n X_t   ---&gt; |                              |\n            |                              |\n            --------------------------------\n\nwhere:\nH_t is the neuron's output at current epoch t\nH_t-1 is the neuron's output from the previous epoch t-1\nX_t is the input data at current epoch t\n\nH_t = tanh(W[H_t-1, X_t] + b)\nAn LSTM (Long Short Term Memory) unit:\n\n            --------------------------------\n            |                              |\n H_t-1 ---&gt; |                              |---&gt; H_t\n            |                              |\n C_t-1 ---&gt; |                              |---&gt; C_t\n            |                              |\n X_t   ---&gt; |                              |\n            |                              |\n            --------------------------------\n            \nThe `forget gate` determines which part of the old short-term memory and current input is \"forgotten\" by the new long-term memory.\nThese are a set of weights (between 0 and 1) that get applied to the old long-term memory to downweight it.\nF_t = sigmoid(W_F[H_t-1, X_t] + b_F)\n\nThe `input gate` i_t similarly gates the input and old short-term memory.\nThis will later (in the update gate) get combined  with a candidate value for the new long-term memory `C_candidate_t`\nI_t = sigmoid(W_I[H_t-1, X_t] + b_I)\nC_candidate_t = tanh(W_C_cand[H_t-1, X_t] + b_C_cand) \n\nThe `update gate` for the new long-term memory `C_t` is then calculated as a sum of forgotten old memory and input-weighted candidate memory:\nC_t = F_t*C_t-1 + I_t*C_candidate_t \n\nThe `output gate` O_t is a combination of the old short-term memory and latest input data.\nThis is then combined with the latest long-term memory to produce the output of the recurrent neuron, which is also the updated short-term memory H_t:\nO_t = sigmoid(W_O[H_t-1, X_t] + b_O)\nH_t = O_t * tanh(C_t) \n\nwhere:\nH_t is short-term memory at epoch t\nC_t is long-term memory at epoch t\nX_t is the input data at current epoch t\n\nsigmoid is a sigmoid  activation function\nF_t is an intermediate forget gate weight\nI_t is an intermediate input gate weight\nO_t is an intermediate output gate weight\nThere are several variants of RNNs. RNNs with peepholes This leaks long-term memory into the forget, input and output gates. Note that the forget gate and input gate each get the OLD long-term memory, whereas the output gate gets the NEW long-term memory.\nF_t = sigmoid(W_F[C_t-1, H_t-1, X_t] + b_F)\nI_t = sigmoid(W_I[C_t-1, H_t-1, X_t] + b_I)\nO_t = sigmoid(W_O[C_t, H_t-1, X_t] + b_O)\nGated Recurrent Unit (GRU) This combines the forget and input gates into a single gate. It also has some other changes. This is simpler than a typical LSTM model as it has fewer parameters. This makes it more computationally efficient, and in practice they can have similar performance.\nz_t = sigmoid(W_z[H_t-1, X_t])\nr_t = sigmoid(W_r[H_t-1, X_t])\nH_candidate_t = tanh(W_H_candidate[r_t*h_t-1, x_t])\nH_t = (1 - z_t) * H_t-1 + z_t * H_candidate_t\nThe sequences modelled with RNNs can be: - One-to-many - Many-to-many - Many-to-one\nConsiderations for choosing the input sequence length: - It should be long enough to capture any patterns or seasonality in the data - The validation set and test set each need to have a size at least (input_length + output_length), so the longer the input length, the more data is required to be set aside.\nThe validation loss for RNNs/LSTMs can be volatile, so allow a higher patience when using early stopping.\nExample model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,LSTM\n\n# Generators for helper functions to chunk up the input series into an array of pairs of (input_batch, target_output)\ntrain_generator = TimeseriesGenerator(scaled_train, scaled_train, length=batch_length, batch_size=1)\nvalidation_generator = TimeseriesGenerator(scaled_test, scaled_test, length=batch_length, batch_size=1)\n\n# Create an LSTM model\nmodel = Sequential()\nmodel.add(LSTM(100, activation='relu', input_shape=(batch_length, n_features)))\nmodel.add(Dense(1))  # Output layer\nmodel.compile(optimizer='adam', loss='mse')  # Problem setup for regression\n\n# Fit the model\nearly_stop = EarlyStopping(monitor='val_loss', patience=10)  # RNNs can be volatile so if use a higher patience with an early stopping callback\nmodel.fit(train_generator, epochs=20, validation_data=validation_generator, callbacks=[early_stop])\n\n# Evaluate test data\ntest_predictions = []\nfinal_batch = scaled_train[-batch_length:, :]\ncurrent_batch = final_batch.reshape((1, batch_length, n_features))\n\nfor test_val in scaled_test:\n    pred_val = model.predict(current_batch)  # These will later need to use the scaler.inverse_transform of the scaler originally used on the training data\n    test_predictions.append(pred_val[0])\n    current_batch = np.append(current_batch[:,1:,:], pred_val.reshape(1,1,1), axis=1)\n\n\n\nCharacter-based generative NLP: given an input string, e.g. [‘h’, ‘e’, ‘l’, ‘l’], predict the sequence shifted by one character, e.g. [‘e’, ‘l’, ‘l’, ‘o’]\nThe embedding, GRU and dense layers work on the sequence in the following way: \nSteps: 1. Read in text data 2. Text processing and vectorisation - one-hot encoding 3. Create batches 4. Create the model 5. Train the model 6. Generate new text\nStep 1: Read in text data - A corpus of &gt;1 million characters is a good size - The more distinct the style of your corpus, the more obvious it will be if your model has worked.\nGiven some structured input_text string, we can get the one-hot encoded vocab list of unique characters.\nvocab_list = sorted(set(input_text))\nStep 2: Text processing Vectorise the text - create a mapping between character and integer. This is the encoding dictionary used to vectorise the corpus.\n# Mappings back and forth between characters and their encoded integers\nchar_to_ind = {char: ind for ind, char in enumerate(vocab_list)}\nind_to_char = {ind: char for ind, char in enumerate(vocab_list)}\n\nencoded_text = np.array([char_to_ind[char] for char in input_text])\nStep 3: Create batches The sequence length should be long enough to capture structure, e.g. for poetry with rhyming couplets it should be the number of characters in 3 lines to capture the rhyme and non-rhyme. Too long a sequence makes the model take longer to train and captures too much historical noise.\nCreate a shuffled dataset where: - input sequence is the first n characters - output sequence is n characters lagged by one\nWe want to shuffle these sequence pairs into a random order so the model doesn’t overfit to any section of the text, but can instead generate characters given any seed text.\nseq_len = 120\ntotal_num_seq = len(input_text) // (seq_len + 1)\n\n# Create Training Sequences\nchar_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\nsequences = char_dataset.batch(seq_len + 1, drop_remainder=True)  # drop_remainder drops the last incomplete sequence\n\n# Create tuples of input and output sequences\ndef create_seq_targets(seq):\n    input_seq = seq[:-1]\n    output_seq = seq[-1:]\n    return input_seq, output_seq\ndataset = sequences.map(create_seq_targets)\n\n# Generate training batches\nbatch_size = 128\nbuffer_size = 10000  # Shuffle this amount of batches rather than shuffling the entire dataset in memory\ndataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\nStep 4: Create the model Set up the loss function and layers.\nThis model uses 3 layers: 1. Embedding - Embed positive integers, e.g. “a”&lt;-&gt;1, as dense vectors of fixed size. This embedding size is a hyperparameter for the user to decide. - Number of layers should be smaller but a similar scale to the vocab size; typically use the power of 2 that is just smaller than the vocab size. 2. GRU - This is the main thing to vary in the model: number of hidden layers and number of neurons in each, types of neurons (RNN, LSTM, GRU), etc. Typically use lots of layers here. 3. Dense - One neuron per character (one-hot encoded), so this produces a probability per character. The user can vary the “temperature”, choosing less probable characters more/less often.\nLoss function: - Sparse categorical cross-entropy - Use sparse categorical cross-entropy because the classes are mutually exclusive. Use regular categorical cross-entropy when one smaple can have multiple classes, or the labels are soft probabilities. - See link in NLP appendix for discussion. - Use logits=True because vocab input is one hot encoded.\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, GRU\nfrom tensorflow.keras.losses import sparse_categorical_crossentropy\n\n# Hyperparameters\nvocab_size = len(vocab_list)\nembed_dim = 64\nrnn_neurons = 1026\n\n# Create model \nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embed_dim, batch_input_shape=[batch_size, None]))\nmodel.add(GRU(rnn_neurons, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'))\nmodel.add(Dense(vocab_size))\nsparse_cat_loss = lambda y_true,y_pred: sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)  # We want a custom loss function with logits=True\nmodel.compile(optimizer='adam', loss=sparse_cat_loss) \nStep 5: Train the model\nepochs = 30\nmodel.fit(dataset,epochs=epochs)\nStep 6: Generate text Allow the model to accept a batch size of 1 to allow us to give an input seed text from which to generate text from.\nWe format the seed text so it can be fed into the network, then loop through generating one character at a time and feeding that back into the model.\nmodel.build(tf.TensorShape([1, None]))\n\ndef generate_text(model, input_seed_text, gen_size=100, temperature=1.0):\n    \"\"\"\n    model: tensorflow.model\n    input_seed_text: str\n    gen_size: int\n    temperature: float\n    \"\"\"\n    generated_text_result = []\n    vectorised_seed = tf.expand_dims([char_to_ind[s] for s in input_seed_text], 0)\n    model.reset_states()\n    \n    for k in range(gen_size):\n        raw_predictions = model(vectorised_seed)\n        predictions = tf.squeeze(raw_predictions, 0) / temperature\n        predicted_index = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n        generated_text_result.append(ind_to_char[predicted_index])\n        \n        # Set a new vectorised seed to generate the next character in the loop\n        vectorised_seed = tf.expand_dims([predicted_index], 0)\n        \n    return (input_seed_text + ''.join(generated_text_result))\n\n\n\nThis is an unsupervised learning problem, therefore evaluation metrics are different. Sometimes called semi-supervised as labels may be used in training the model, but not available when it’s used to predict new values.\nApplications: - Dimensionality reduction - Noise removal\nDesigned to reproduce its input in the output layer. Number of input neurons is the same as the number of output layers. The encoder reduces dimensionality to the hidden layer. The hidden layer should contain the most relevant information required to reconstruct the input. The decoder reconstructs a high dimensional output from a low dimensional input.\n\nEncoder: Input layer -&gt; Hidden layer\nDecoder: Hidden layer -&gt; Output\n\nStacked autoencoders include multiple hidden layers.\nAutoencoder for dimensionality reduction Let’s try to reduce an input dataset from 3 dimensions to 2. We want to train an autoencoder to go from 3 -&gt; 2 -&gt; 3.\nWhen we scale data, we fit and transform on the entire dataset rather than a train/test split, because there is no ground truth for the “correct” dimensionality reduction.\nUsing SGD in Autoencoders allows the learning rate to be varied as a hyperparameter to affect how well the hidden layer learns. Larger values will train quicker but potentially perform worse.\nTo retrieve the lower dimensionality representation, use just the encoder half of the trained autoencoder to predict the input.\n# Scale data\nscaler = MinMaxScaler()\nscaled_data = scaler.fit_transform(input_features)\n\n# Create model\nencoder = Sequential()\nencoder.add(Dense(units=2, acivation='relu', input_shape=[3]))\n\ndecoder = Sequential()\ndecoder.add(Dense(units=3, acivation='relu', input_shape=[2]))\n\nautoencoder = Sequential([encoder, decoder])\nautoencoder.compile(loss='mse', optimizer=SGD(lr=1.5))\n\n# Train the full autoencoder model\nautoencoder.fit(scaled_data, scaled_data, epochs=5)\n\n# Reduce dimensionality by using just the encoder part of the trained model\nencoded_2dim = encoder.predict(scaled_data)\nAutoencoder for noise removal Starting with clean images, we want to train an autoencoder to learn to remove noise. We create a noisy dataset by adding noise to our clean images. We then try to reconstruct that input, and compare to the original clean images as the ground truth.\nStarting with 28x28 images (e.g. MNIST dataset), this gives 784 input features when flattened. Use multiple hidden layers as there are a large number of input features that would be difficult to learn in one layer. The numbers of layers and number of neurons in each are hyperparameters to tune; decreasing in powers of 2 is a good rule of thumb.\nAdd a Gaussian noise layer to train the model to remove noise. Without the Gaussian layer, the model would be used for dimensionality reduction rather than noise removal.\nThe final layer uses a sigmoid activation because it is a binary classification problem - does the output match the input?\nencoder = Sequential()\nencoder.add(Flatten(input_shape=[28, 28]))\nencoder.add(Gaussian(0.2))  # Optional layer if training for noise removal rather than dimensionality reduction\nencoder.add(Dense(400, activation='relu'))\nencoder.add(Dense(200, activation='relu'))\nencoder.add(Dense(100, activation='relu'))\nencoder.add(Dense(50, activation='relu'))\nencoder.add(Dense(25, activation='relu'))\n\ndecoder = Sequential()\ndecoder.add(Dense(50, input_shape=[25], activation='relu'))\ndecoder.add(Dense(100, activation='relu'))\ndecoder.add(Dense(200, activation='relu'))\ndecoder.add(Dense(400, activation='relu'))\ndecoder.add(Dense(28*28, activation='sigmoid'))\n\nautoencoder = Sequential([encoder, decoder])\nautoencoder.compile(loss='binary_crossentropy', optimizer=SGD(lr=1.5), metrics=['accuracy'])\nautoencoder.fit(X_train, X_train, epochs=5)\n\n\n\nUse 2 networks competing against each other to generate data - counterfeiter (generator) vs detective (discriminator). - Generator: recieves random noise - Discriminator: receives data set containing real data and fake generated data, and attempts to calssify real vs fake (binary classificaiton).\nTwo training phases, each only training one of the networks: 1. Train discriminator - real images (labeled 1) and generated images (labeled 0) are fed to the discriminator network. 2. Train generator - only feed generated images to discriminator, ALL labeled 1.\nThe generator never sees real images, it creates images based only off of the gradients flowing back from the discriminator.\nDifficulties with GANs: 1. Training resources - GPUs generally required 2. Mode collapse - generator learns an image that fools the discriminator, then only produces this image. Deep convolutional GANs and mini-batch discrimination are two solution to this problem. 3. Instability - True performance is hard to measure; just because the discriminator was fooled, it doesn’t mean that the generated image was actually realistic.\nCreating the model We create the generator and discriminator as separate models, then join them into a GAN object. This is analogous to how we joined an encoder and decoder into an autoencoder.\nThe discriminator is trained on a binary classification problem, real or fake, so uses a binary cross entropy loss function. Backpropagation only alters the weights of the discriminator in the first phase, not the generator.\ndiscriminator = Sequential()\n\n# Flatten the image\ndiscriminator.add(Flatten(input_shape=[28,28]))\n\n# Some hidden layers\ndiscriminator.add(Dense(150,activation='relu'))\ndiscriminator.add(Dense(100,activation='relu'))\n\n# Binary classification - real vs fake\ndiscriminator.add(Dense(1,activation=\"sigmoid\"))\ndiscriminator.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\nThe generator is trained in the second phase.\nWe set a codings_size, which is the size of the latent representation from which the generator will create a full image. For example, if we want to create a 28x28 image (784 pixels), we may want to generate this from a 100 element input. This is analogous to the decoder part of an autoencoder.\nThe codings_size should be smaller than the total number of features but large enough so that there is something to learn from.\nThe generator is NOT compiled at this step, it is compiled in the combined GAN later so that it is only trained at that step.\ncodings_size = 100\ngenerator = Sequential()\n\n# Input shape of first layer is the codings_size\ngenerator.add(Dense(150, activation=\"relu\", input_shape=[codings_size]))\n\n# Some hidden layers\ngenerator.add(Dense(200,activation='relu'))\n\n# Output is the size of the image we want to create\ngenerator.add(Dense(784, activation=\"sigmoid\"))\ngenerator.add(Reshape([28,28]))\nThe GAN combines the generator and discriminator.\nThe discriminator is only trained in the first phase, not the second phase.\nGAN = Sequential([generator, discriminator])\ndiscriminator.trainable = False\nGAN.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\nTraining the model We first create batches of images from our input data, similarly to previous models.\nbatch_size = 32  # Size of training input batches\nbuffer_size = 1000  # Don't load the entire dataset into memory\nepochs = 1\n\nraw_dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(buffer_size=buffer_size)\nbatched_dataset = raw_dataset.batch(batch_size, drop_remainder=True).prefetch(1)\nThe training loop for each epoch trains the discriminator to distinguish real vs fake images (binary classification). Then it trains the generator to generate fake images using ONLY the gradients learned in the discriminator training step; the generator NEVER sees any real images.\ngenerator, discriminator = GAN.layers\n\nfor epoch in range(epochs):\n    for index, X_batch in enumerate(batched_dataset):\n        # Phase 1: Train the discriminator\n        noise = tf.random.normal(shape=[batch_size, codings_size])\n        generated_images = generator(noise)\n        real_images = tf.dtypes.cast(X_batch,tf.float32)\n        X_train_discriminator = tf.concat([generated_images, real_images], axis=0)\n        y_train_discriminator = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)  # Targets set to zero for fake _images and 1 for real _images\n        discriminator.trainable = True\n        discriminator.train_on_batch(X_train_discriminator, y_train_discriminator)\n\n        # Phase 2: Train the generator\n        noise = tf.random.normal(shape=[batch_size, codings_size])\n        y_train_generator = tf.constant([[1.]] * batch_size)  # We want discriminator to believe that fake _images are real\n        discriminator.trainable = False\n        GAN.train_on_batch(noise, y_train_generator)    \nDeep convolutional GANs These are GANs which use convolutional layers inside the discriminator and generator models.\nThe models are almost identical to the models above, just with additional Conv2D, Conv2DTranspose and BatchNormalization layers.\nChanges compared to regular GANs: - Additional convolutional layers in model. - Reshape the training set to match the images. - Rescale the training data to be between -1 and 1 so that tanh activation function works. - Activation of discriminator output layer is tanh rather than sigmoid."
  },
  {
    "objectID": "posts/ml/tensorflow/tensorflow.html#a.-appendix",
    "href": "posts/ml/tensorflow/tensorflow.html#a.-appendix",
    "title": "Tensorflow Notes",
    "section": "",
    "text": "Neural networks: - Intuition behind neural networks http://neuralnetworksanddeeplearning.com/ - The deep learning bible (with lectures) https://www.deeplearningbook.org/ - Udemy course: https://www.udemy.com/course/complete-tensorflow-2-and-keras-deep-learning-bootcamp - Heuristics for choosing hidden layers https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw - Alternatives to the deprecated model predict for different classification problems https://stackoverflow.com/questions/68776790/model-predict-classes-is-deprecated-what-to-use-instead - MIT course https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/course/\nCNNs: - Image kernels explained https://setosa.io/ev/image-kernels/ - Choosing CNN layers https://stats.stackexchange.com/questions/148139/rules-for-selecting-convolutional-neural-network-hyperparameters\nRNNs: - Overview of RNNs http://karpathy.github.io/2015/05/21/rnn-effectiveness/ - Wikipedia page contains the equations of LSTMs and peepholes https://en.wikipedia.org/wiki/Long_short-term_memory - LSTMs vs GRUs https://datascience.stackexchange.com/questions/14581/when-to-use-gru-over-lstm - Worked example of LSTM http://blog.echen.me/2017/05/30/exploring-lstms/ - RNN cheatsheet https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\nNLP: - The unreasonable effectiveness of RNNs, essay on RNNs applied to NLP http://karpathy.github.io/2015/05/21/rnn-effectiveness/ - Sparse vs dense categorical crossentropy loss function https://datascience.stackexchange.com/questions/41921/sparse-categorical-crossentropy-vs-categorical-crossentropy-keras-accuracy\nGANs: - buffer_size argument in tensorflow prefetch and shuffle https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle - Mode collapse https://www.quora.com/What-does-it-mean-if-all-produced-images-of-a-GAN-look-the-same"
  },
  {
    "objectID": "posts/ml/fastai/lesson1/lesson.html",
    "href": "posts/ml/fastai/lesson1/lesson.html",
    "title": "FastAI Lesson 1: Image Classification Models",
    "section": "",
    "text": "These are notes from lesson 1 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\nTrain an image classifier: see car classification notebook\n\n\n\n\nThere is a companion course focusing on AI ethics here\nThoughts on education: play the whole game first to get an idea of the big picture. Don’t become afflicted with “elementitis” getting too bogged down in details too early.\n\nColoured cups - green, amber, red - for each student to indicate how well they are following\nMeta Learning by Radek Osmulski - a book based on the author’s experience of learning about deep learning (via the fastai course)\nMathematician’s Lament by Paul Lockhart - a book about the state of mathematics education\nMaking Learning Whole by David Perkins - a book about apporaches to holistic learning\n\nRISE is a jupyter notebook extension to turn notebooks into slides. Jeremy uses notebooks for: source code, book, blogging, CI/CD.\n\n\n\nBefore deep learning, the approach to machine learning was to enlist many domain experts to handcraft features and feed this into a constrained linear model (e.g. ridge regression). This is time-consuming, expensive and requires many domain experts.\nNeural networks learn these features. Looking inside a CNN, for example, shows that these learned features match interpretable features that an expert might handcraft. An illustration of the features learned is given in this paper by Zeiler and Fergus.\nFor image classifiers, you don’t need particularly large images as inputs. GPUs are so quick now that if you use large images, most of the time is spent on opening the file rather than computations. So often we resize images down to 400x400 pixels or smaller.\nFor most use cases, there are pre-trained models and sensible default values that we can use. In practice, most of the time is spent on the input layer and output layer. For most models the middle layers are identical.\n\n\n\nData blocks structure the input to learners. An overview of the DataBlock class:\n\nblocks determines the input and output type as a tuple. For multi-target classification this tuple can be arbitrary length.\nget_items is a function that returns a list of all the inputs.\nsplitter defines how to split the training/validation set.\nget_y is a function that returns the label of a given input image.\nitem_tfms defines what transforms to apply to the inputs before training, e.g. resize.\ndataloaders is a method that parallelises loading the data.\n\nA learner combines the model (e.g. resnet or something from timm library) and the data to run that model on (the dataloaders from the DataBlock).\nThe fine_tune method starts with a pretrained model weights rather than randomised weights, and only needs to learn the differences between your data and the original model.\nOther image problems that can utilise deep learning:\n\nImage classification\nImage segmentation\n\nOther problem types use the same process, just with different DataBlock blocks types and the rest is the same. For example, tabular data, collaborative filtering.\n\n\n\n\nTraditional computer programs are essentially:\n\n\n\n\n\n\nFigure 1: Traditional computer programs\n\n\n\nDeep learning models are:\n\n\n\n\n\n\nFigure 2: Deep learning model\n\n\n\n\n\n\n\nCourse lesson page\nVisualizing and Understanding Convolutional Networks\nTIMM library\nStatistical Modeling: The Two Cultures\n\n\n\n\nFigure 1: Traditional computer programs\nFigure 2: Deep learning model"
  },
  {
    "objectID": "posts/ml/fastai/lesson1/lesson.html#notes-on-learning",
    "href": "posts/ml/fastai/lesson1/lesson.html#notes-on-learning",
    "title": "FastAI Lesson 1: Image Classification Models",
    "section": "",
    "text": "There is a companion course focusing on AI ethics here\nThoughts on education: play the whole game first to get an idea of the big picture. Don’t become afflicted with “elementitis” getting too bogged down in details too early.\n\nColoured cups - green, amber, red - for each student to indicate how well they are following\nMeta Learning by Radek Osmulski - a book based on the author’s experience of learning about deep learning (via the fastai course)\nMathematician’s Lament by Paul Lockhart - a book about the state of mathematics education\nMaking Learning Whole by David Perkins - a book about apporaches to holistic learning\n\nRISE is a jupyter notebook extension to turn notebooks into slides. Jeremy uses notebooks for: source code, book, blogging, CI/CD."
  },
  {
    "objectID": "posts/ml/fastai/lesson1/lesson.html#background-on-deep-learning-and-image-classification",
    "href": "posts/ml/fastai/lesson1/lesson.html#background-on-deep-learning-and-image-classification",
    "title": "FastAI Lesson 1: Image Classification Models",
    "section": "",
    "text": "Before deep learning, the approach to machine learning was to enlist many domain experts to handcraft features and feed this into a constrained linear model (e.g. ridge regression). This is time-consuming, expensive and requires many domain experts.\nNeural networks learn these features. Looking inside a CNN, for example, shows that these learned features match interpretable features that an expert might handcraft. An illustration of the features learned is given in this paper by Zeiler and Fergus.\nFor image classifiers, you don’t need particularly large images as inputs. GPUs are so quick now that if you use large images, most of the time is spent on opening the file rather than computations. So often we resize images down to 400x400 pixels or smaller.\nFor most use cases, there are pre-trained models and sensible default values that we can use. In practice, most of the time is spent on the input layer and output layer. For most models the middle layers are identical."
  },
  {
    "objectID": "posts/ml/fastai/lesson1/lesson.html#overview-of-the-fastai-learner",
    "href": "posts/ml/fastai/lesson1/lesson.html#overview-of-the-fastai-learner",
    "title": "FastAI Lesson 1: Image Classification Models",
    "section": "",
    "text": "Data blocks structure the input to learners. An overview of the DataBlock class:\n\nblocks determines the input and output type as a tuple. For multi-target classification this tuple can be arbitrary length.\nget_items is a function that returns a list of all the inputs.\nsplitter defines how to split the training/validation set.\nget_y is a function that returns the label of a given input image.\nitem_tfms defines what transforms to apply to the inputs before training, e.g. resize.\ndataloaders is a method that parallelises loading the data.\n\nA learner combines the model (e.g. resnet or something from timm library) and the data to run that model on (the dataloaders from the DataBlock).\nThe fine_tune method starts with a pretrained model weights rather than randomised weights, and only needs to learn the differences between your data and the original model.\nOther image problems that can utilise deep learning:\n\nImage classification\nImage segmentation\n\nOther problem types use the same process, just with different DataBlock blocks types and the rest is the same. For example, tabular data, collaborative filtering."
  },
  {
    "objectID": "posts/ml/fastai/lesson1/lesson.html#deep-learning-vs-traditional-computer-programs",
    "href": "posts/ml/fastai/lesson1/lesson.html#deep-learning-vs-traditional-computer-programs",
    "title": "FastAI Lesson 1: Image Classification Models",
    "section": "",
    "text": "Traditional computer programs are essentially:\n\n\n\n\n\n\nFigure 1: Traditional computer programs\n\n\n\nDeep learning models are:\n\n\n\n\n\n\nFigure 2: Deep learning model"
  },
  {
    "objectID": "posts/ml/fastai/lesson1/lesson.html#references",
    "href": "posts/ml/fastai/lesson1/lesson.html#references",
    "title": "FastAI Lesson 1: Image Classification Models",
    "section": "",
    "text": "Course lesson page\nVisualizing and Understanding Convolutional Networks\nTIMM library\nStatistical Modeling: The Two Cultures\n\n\n\n\nFigure 1: Traditional computer programs\nFigure 2: Deep learning model"
  },
  {
    "objectID": "posts/ml/fastai/lesson5/lesson.html",
    "href": "posts/ml/fastai/lesson5/lesson.html",
    "title": "FastAI Lesson 5: Natural Language Processing",
    "section": "",
    "text": "These are notes from lesson 5 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\n\nRecreate the Jupyter notebook to train a linear model and a neural network from scratch - see from scratch notebook\nThen repeat the exercise using the fastai framework (it’s much easier!) - see framework notebook\nRead numpy broadcasting rules\n\n\n\n\n\nTrain a linear model from scratch in Python using on the Titanic dataset. Then train a neural network from scratch in a similar way. This is an extension of the spreadsheet approach to make a neural network from scratch in lesson 3.\n\n\nNever throw away data.\nAn easy way of imputing missing values is to fill them with the mode. This is good enough for a first pass at creating a model.\n\n\n\nNumeric values that can grow exponentially like prices or population sizes often have long-tailed distributions.\nAn easy way to scale is to take log(x+1). The +1 term is just to avoid taking log of 0.\n\n\n\nOne-hot encode any categorical variables.\nWe should include an “other” category for each in case the validation or test set contains a category we didn’t encounter in the training set.\nIf there are categories with ony a small number of observations we can group them into an “other” category too.\n\n\n\nBroadcasting arrays together avoids boilerplate code to make dimensions match.\nFor reference, see numpy’s broadcasting rules and try APL\n\n\n\nFor a binary classification model, the outputs should be between 0 and 1. If you train a linear model, it might result in negative values or values &gt;1.\nThis means we can improve the loss by just clipping to ensure they stay between 0 and 1.\nThis is the idea behind the sigmoid function for output layers: smaller values will tend to 0 and larger values will tend to 1. In general, for any binary classification model, we should always have a sigmoid as the final layer. If the model isn’t training well, it’s worth checking that the final activation function is a sigmoid.\nThe sigmoid function is defined as: \\[ y = \\frac{1}{1+e^{-x}} \\]\n\n\n\nIn general, the middle layers of a neural network are similar between different problems.\nThe input layer will depend on the data for our specific problem. The output will depend on the target for our specific problem.\nSo we spend most of our time thinking about the correct input and output layers, and the hidden layers are less important.\n\n\n\n\nWhen creating the models from scratch, there was a lot of boilerplate code to:\n\nImpute missing values using the “obvious” method (fill with mode)\nNormalise continuous variables to be between 0 and 1\nOne-hot encode categorical variables\nRepeat all of these steps in the same order for the test set\n\nThe benefits of using a framework like fastai:\n\nLess boilerplate, so the obvious things are done automatically unless you say otherwise\nRepeating all of the feature engineering steps on the output is trivial with DataLoaders\n\n\n\n\nCreating independent models and taking the mean of their predictions improves the accuracy. There are a few different approaches to ensembling a categorical variable:\n\nTake the mean of the predictions (binary 0 or 1)\nTake the mean of the probabilities (continuous between 0 and 1) then threshold the result\nTake the mode of the predictions (binary 0 or 1)\n\nIn general the mean approaches work better but there’s no rule as to why, so try all of them.\n\n\n\n\nCourse lesson page\nNumpy broadcasting rules\nAPL"
  },
  {
    "objectID": "posts/ml/fastai/lesson5/lesson.html#training-a-model-from-scratch",
    "href": "posts/ml/fastai/lesson5/lesson.html#training-a-model-from-scratch",
    "title": "FastAI Lesson 5: Natural Language Processing",
    "section": "",
    "text": "Train a linear model from scratch in Python using on the Titanic dataset. Then train a neural network from scratch in a similar way. This is an extension of the spreadsheet approach to make a neural network from scratch in lesson 3.\n\n\nNever throw away data.\nAn easy way of imputing missing values is to fill them with the mode. This is good enough for a first pass at creating a model.\n\n\n\nNumeric values that can grow exponentially like prices or population sizes often have long-tailed distributions.\nAn easy way to scale is to take log(x+1). The +1 term is just to avoid taking log of 0.\n\n\n\nOne-hot encode any categorical variables.\nWe should include an “other” category for each in case the validation or test set contains a category we didn’t encounter in the training set.\nIf there are categories with ony a small number of observations we can group them into an “other” category too.\n\n\n\nBroadcasting arrays together avoids boilerplate code to make dimensions match.\nFor reference, see numpy’s broadcasting rules and try APL\n\n\n\nFor a binary classification model, the outputs should be between 0 and 1. If you train a linear model, it might result in negative values or values &gt;1.\nThis means we can improve the loss by just clipping to ensure they stay between 0 and 1.\nThis is the idea behind the sigmoid function for output layers: smaller values will tend to 0 and larger values will tend to 1. In general, for any binary classification model, we should always have a sigmoid as the final layer. If the model isn’t training well, it’s worth checking that the final activation function is a sigmoid.\nThe sigmoid function is defined as: \\[ y = \\frac{1}{1+e^{-x}} \\]\n\n\n\nIn general, the middle layers of a neural network are similar between different problems.\nThe input layer will depend on the data for our specific problem. The output will depend on the target for our specific problem.\nSo we spend most of our time thinking about the correct input and output layers, and the hidden layers are less important."
  },
  {
    "objectID": "posts/ml/fastai/lesson5/lesson.html#using-a-framework",
    "href": "posts/ml/fastai/lesson5/lesson.html#using-a-framework",
    "title": "FastAI Lesson 5: Natural Language Processing",
    "section": "",
    "text": "When creating the models from scratch, there was a lot of boilerplate code to:\n\nImpute missing values using the “obvious” method (fill with mode)\nNormalise continuous variables to be between 0 and 1\nOne-hot encode categorical variables\nRepeat all of these steps in the same order for the test set\n\nThe benefits of using a framework like fastai:\n\nLess boilerplate, so the obvious things are done automatically unless you say otherwise\nRepeating all of the feature engineering steps on the output is trivial with DataLoaders"
  },
  {
    "objectID": "posts/ml/fastai/lesson5/lesson.html#ensembles",
    "href": "posts/ml/fastai/lesson5/lesson.html#ensembles",
    "title": "FastAI Lesson 5: Natural Language Processing",
    "section": "",
    "text": "Creating independent models and taking the mean of their predictions improves the accuracy. There are a few different approaches to ensembling a categorical variable:\n\nTake the mean of the predictions (binary 0 or 1)\nTake the mean of the probabilities (continuous between 0 and 1) then threshold the result\nTake the mode of the predictions (binary 0 or 1)\n\nIn general the mean approaches work better but there’s no rule as to why, so try all of them."
  },
  {
    "objectID": "posts/ml/fastai/lesson5/lesson.html#references",
    "href": "posts/ml/fastai/lesson5/lesson.html#references",
    "title": "FastAI Lesson 5: Natural Language Processing",
    "section": "",
    "text": "Course lesson page\nNumpy broadcasting rules\nAPL"
  },
  {
    "objectID": "posts/ml/fastai/lesson3/lesson.html",
    "href": "posts/ml/fastai/lesson3/lesson.html",
    "title": "FastAI Lesson 3: How Does a Neural Network Learn?",
    "section": "",
    "text": "These are notes from lesson 3 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\nRecreate the spreadsheet to train a linear model and a neural network from scratch: see spreadsheet\n\n\n\n\nSome options for cloud environments: Kaggle, Colab, Paperspace.\nA comparison of performance vs training time for different image models useful for model selection is provided here. Resnet and convnext are generally good to start with.\nThe best practice is to start with a “good enough” model with a quick training time, so you can iterate quickly. Then as you get further on with your research and need a better model, you can move to a slower, more accurate model.\nThe criteria we generally care about are:\n\nHow fast are they\nHow much memory do they use\nHow accurate are they\n\nA learner object contains the pre-processing steps and the model itself.\n\n\n\nHow do we fit a function to data? An ML model is just a function fitted to data. Deep learning is just fitting an infinitely flexible model to data.\nIn this notebook there is an interactive cell to fit a quadratic function to some noisy data: y = ax^2 +bx + c\nWe can vary a, b and c to get a better fit by eye. We can make this more rigorous by defining a loss function to quantify how good the fit is.\nIn this case, use the mean absolute error: mae = mean(abs(actual - preds)) We can then use stochastic gradient descent to autome the tweaking of a, b and c to minimise the loss.\nWe can store the parameters as a tensor, then pytorch will calculate gradients of the loss function based on that tensor.\nabc = torch.tensor([1.1,1.1,1.1]) \nabc.requires_grad_()  # Modifies abc in place so that it will include gradient calculations on anything which uses abc downstream\nloss = quad_mae(abc)  # `loss` uses abc so pytorch will give the gradient of loss too\nloss.backward()  # Back-propagate the gradients\nabc.grad  # Returns a tensor of the loss gradients\nWe can then take a “small step” in the direction that will decrease the gradient. The size of the step should be proportional to the size of the gradient. We define a learning rate hyperparameter to determine how much to scale the gradients by.\nlearning_rate = 0.01\nabc -= abc.grad * learning_rate\nloss = quad_mae(abc)  # The loss should now be smaller\nWe can repeat this process to take multiple steps to minimise the gradient.\nfor step in range(10):\n    loss = quad_mae(abc)\n    loss.backward()\n    with torch.no_grad():\n        abc -= abc.grad * learning_rate\n    print(f'step={step}; loss={loss:.2f}')\nThe learning rate should decrease as we get closer to the minimum to ensure we don’t overshoot the minimum and increase the loss. A learning rate schedule can be specified to do this.\n\n\n\nFor deep learning, the premise is the same but instead of quadratic functions, we fit ReLUs and other non-linear functions.\nUniversal approximation theorem states that this is infinitely expressive if enough ReLUs (or other non-linear units) are combined. This means we can learn any computable function.\nA ReLU is essentially a linear function with the negative values clipped to 0.\ndef relu(m,c,x):\n    y = m * x + c\n    return np.clip(y, 0.)\nThis is all that deep learning is! All we need is:\n\nA model - a bunch of ReLUs combined will be flexible\nA loss function - mean absolute error between the actual data values and the values predicted by the model\nAn optimiser - stochastic gradient descent can start from random weights to incrementally improve the loss until we get a “good enough” fit\n\nWe just need enough time and data. There are a few hacks to decrease the time and data required:\n\nData augmentation\nRunning on GPUs to parallelise matrix multiplications\nConvolutions to skip over values to reduce the number of matrix multiplications required\nTransfer learning - initialise with parameters from another pre-trained model instead of random weights.\n\nThis spreadsheet from the homework task is a worked example of manually training a multivariate linear model, then extending that to a neural network summing two ReLUs.\n\n\n\n\nCourse lesson page\nComparison of computer vision models\n“How does a neural network really work?” notebook"
  },
  {
    "objectID": "posts/ml/fastai/lesson3/lesson.html#model-selection",
    "href": "posts/ml/fastai/lesson3/lesson.html#model-selection",
    "title": "FastAI Lesson 3: How Does a Neural Network Learn?",
    "section": "",
    "text": "Some options for cloud environments: Kaggle, Colab, Paperspace.\nA comparison of performance vs training time for different image models useful for model selection is provided here. Resnet and convnext are generally good to start with.\nThe best practice is to start with a “good enough” model with a quick training time, so you can iterate quickly. Then as you get further on with your research and need a better model, you can move to a slower, more accurate model.\nThe criteria we generally care about are:\n\nHow fast are they\nHow much memory do they use\nHow accurate are they\n\nA learner object contains the pre-processing steps and the model itself."
  },
  {
    "objectID": "posts/ml/fastai/lesson3/lesson.html#fitting-a-quadratic-function",
    "href": "posts/ml/fastai/lesson3/lesson.html#fitting-a-quadratic-function",
    "title": "FastAI Lesson 3: How Does a Neural Network Learn?",
    "section": "",
    "text": "How do we fit a function to data? An ML model is just a function fitted to data. Deep learning is just fitting an infinitely flexible model to data.\nIn this notebook there is an interactive cell to fit a quadratic function to some noisy data: y = ax^2 +bx + c\nWe can vary a, b and c to get a better fit by eye. We can make this more rigorous by defining a loss function to quantify how good the fit is.\nIn this case, use the mean absolute error: mae = mean(abs(actual - preds)) We can then use stochastic gradient descent to autome the tweaking of a, b and c to minimise the loss.\nWe can store the parameters as a tensor, then pytorch will calculate gradients of the loss function based on that tensor.\nabc = torch.tensor([1.1,1.1,1.1]) \nabc.requires_grad_()  # Modifies abc in place so that it will include gradient calculations on anything which uses abc downstream\nloss = quad_mae(abc)  # `loss` uses abc so pytorch will give the gradient of loss too\nloss.backward()  # Back-propagate the gradients\nabc.grad  # Returns a tensor of the loss gradients\nWe can then take a “small step” in the direction that will decrease the gradient. The size of the step should be proportional to the size of the gradient. We define a learning rate hyperparameter to determine how much to scale the gradients by.\nlearning_rate = 0.01\nabc -= abc.grad * learning_rate\nloss = quad_mae(abc)  # The loss should now be smaller\nWe can repeat this process to take multiple steps to minimise the gradient.\nfor step in range(10):\n    loss = quad_mae(abc)\n    loss.backward()\n    with torch.no_grad():\n        abc -= abc.grad * learning_rate\n    print(f'step={step}; loss={loss:.2f}')\nThe learning rate should decrease as we get closer to the minimum to ensure we don’t overshoot the minimum and increase the loss. A learning rate schedule can be specified to do this."
  },
  {
    "objectID": "posts/ml/fastai/lesson3/lesson.html#fit-a-deep-learning-model",
    "href": "posts/ml/fastai/lesson3/lesson.html#fit-a-deep-learning-model",
    "title": "FastAI Lesson 3: How Does a Neural Network Learn?",
    "section": "",
    "text": "For deep learning, the premise is the same but instead of quadratic functions, we fit ReLUs and other non-linear functions.\nUniversal approximation theorem states that this is infinitely expressive if enough ReLUs (or other non-linear units) are combined. This means we can learn any computable function.\nA ReLU is essentially a linear function with the negative values clipped to 0.\ndef relu(m,c,x):\n    y = m * x + c\n    return np.clip(y, 0.)\nThis is all that deep learning is! All we need is:\n\nA model - a bunch of ReLUs combined will be flexible\nA loss function - mean absolute error between the actual data values and the values predicted by the model\nAn optimiser - stochastic gradient descent can start from random weights to incrementally improve the loss until we get a “good enough” fit\n\nWe just need enough time and data. There are a few hacks to decrease the time and data required:\n\nData augmentation\nRunning on GPUs to parallelise matrix multiplications\nConvolutions to skip over values to reduce the number of matrix multiplications required\nTransfer learning - initialise with parameters from another pre-trained model instead of random weights.\n\nThis spreadsheet from the homework task is a worked example of manually training a multivariate linear model, then extending that to a neural network summing two ReLUs."
  },
  {
    "objectID": "posts/ml/fastai/lesson3/lesson.html#references",
    "href": "posts/ml/fastai/lesson3/lesson.html#references",
    "title": "FastAI Lesson 3: How Does a Neural Network Learn?",
    "section": "",
    "text": "Course lesson page\nComparison of computer vision models\n“How does a neural network really work?” notebook"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html",
    "href": "posts/software/software_architecture/software_architect_notes.html",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "Notes from udemy course https://www.udemy.com/course/the-complete-guide-to-becoming-a-software-architect/\n\n\nA developer knows what can be done, an architect knows what should be done. How do we use technology to meet business requirements.\nGeneral system requirements: - Fast - Secure - Reliable - Easy to maintain\nArchitect’s need to know how to code for: - Architecture’s trustworthiness - Support developers - Respect of developers\n\n\n\n\nUnderstand the business - Strengths, weaknesses, compettions, growth strategy.\nDefine the system’s goals - Goals are not requirements. Goals describe the effect on the organisation, requirements describe what the system should do.\nWork for your client’s clients - Prioritise the end user.\nTalk to the right people with the right language - What is the thing that really matters to the person I’m talking to? Project managers care about how things affect deadlines, developers care about the technologies used, CEOs care about business continuity and bottom line.\n\n\n\n\n\nUnderstand the system requirements - What the system should do.\nUnderstand the non-functional requirements - Technical and service level attributes, e.g. number of users, loads, volumes, performance.\nMap the components - Understand the system functionality and communicate this to your client. Completely non-technical at this point, no mention of specific technologies. A diagram of high level components helps.\nSelect the technology stack - Backend, frontend, data store.\nDesign the architecture\nWrite the architecture document\nSupport the team\n\nInclude developers in non-functional requirements and architecture design for two reasons: 1. Learn about unknown scenarios early 2. Create ambassadors\n\n\n\nThe 2 types of requirements: functional and non-functional.\n\n\n“What the system should do”\n\nBusiness flows\nBusiness services\nUser interfaces\n\n\n\n\n“What the system should deal with”\n\nPerformance - latency and throughput\n\nLatency - How long does it take to perform a single task?\nThroughput - How many tasks can be performed in a given time unit?\n\nLoad - Quantity of work without crashing. Determines the availability of the system. Always look at peak (worst case) numbers.\nData volume - How much data will the system accumulate over time?\n\nData required on day one\nData growth (say, annually)\n\nConcurrent users - How many users will be using the system? This includes “dead times” which differentiates it from the load requirement. Rule of thumb is concurrent_users = load * 10\nSLA - Service Level Agreement for uptime.\n\nThe non-functional requirements are generally the more important in determining the architecture. The client will generally need guiding towards sensible values, otherwise they just want as much load as possible, as much uptime as possible etc.\n\n\n\n\nThe application type should be established early based on the use case and expected user interaction.\n\nWeb apps\n\nServe html pages.\nUI; user-initiated actions; large scale; short, focused actions.\nRequest-response model.\n\nWeb API\n\nServe data (often JSON).\nData retrieval and storage; client-initiated actions; large scale; short, focused actions.\nRequest-response model.\n\nMobile\n\nRequire user interaction; frontend for web API; location-based.\n\nConsole\n\nNo UI; limited interation; long-running processes; short actions for power users.\nRequire technical knowledge.\n\nService\n\nNo UI; managed by service manager; long-running processes.\n\nDesktop\n\nAll resources on the PC; UI; user-centric actions.\n\nFunction-as-a-service\n\nAWS lambda\n\n\n\n\n\nConsiderations: - Appropriate for the task - Community - e.g. stack overflow activity - Popularity - google trends over 2 years\n\n\nCovers web app, web API, console, service.\nOptions: .NET, Java, node.js, PHP, Python\n\n\n\n\n\n\nFigure 1: Backend Tech\n\n\n\n\n\n\nCovers: - Web app - Angular, React - Mobile - Native (Swift, Java/Kotlin), Xamarin, React Native - Desktop - depends on target OS\n\n\n\nSQL - small, structured data - Relational tables - Transactions - Atomicity - Consistency - Isolation - Durability - Querying language is universal\nNoSQL - huge, unstructured or semi-structured data - Emphasis on scale and performance. - Schema-less, with entities stored as JSON. - Eventual consistency - data can be temporarily inconsistent - No universal querying language\n\n\n\n\nQuality attributes that describe technical capabilities to fulfill the non-functional requirements. Non-functional requirements map to quality attributes, which are designed in the architecture.\n\nScalability - Adding computing resources without any interruption. Scale out (add more compute instances) is preferred over scale up (increase CPU, RAM on the existing instance). Scaling out introduces redundancy and is not limited by hardware.\nManageability - Know what’s going on and take action accordingly. The question “who reports the problems” determines if a system is manageable. The system should flag problems, not the suer.\nModularity - A system that is built from blcoks that can be changed or replaced without affecting the whole system.\nExtensibility - Functionality of the system can be extended without modifying existing code.\nTestability - How easy is it to test the application. Independent modules and methods and single responsibility principle aid testability.\n\n\n\n\nA software component is a piece of code that runs in a single process. Distributed systems are composed of independent software components deployed on separate processes/servers/containers.\nComponent architecture relates to the lower level details, whereas system architecture is the higher level view of how the components fit together to achieve the non-functional requirements.\n\n\nLayers represent horizontal functionality: 1. UI/SI - user interface or service interface (i.e. an API), authentication 2. Business logic - validation, enrichment, computation 3. Data access layer - connection handling, transaction handling, querying/saving data\nCode can flow downwards by one layer only. It can never skip a layer and can never go up a layer. This enforces modularity.\nA service might sit across layers, for example logging sits across the 3 layers described above. In this case, the logging service is called a “sross-cutting concern”.\nEach layer should be independent of the implementation of other layers. Layers should handle exceptions from those below them, log the error and then throw a more generic exception, so that there is no reference to other layers’ implementation.\nLayers are part of the same process. This is different from tiers, which are processes that are distributed across a network.\n\n\n\nAn interface declares the signature of an implementation. This means that each implementation must implement the methods described. The abstract base class in Python is an example of this.\nClose coupling should be avoided; “new is glue”.\n\n\n\n\nSingle responsibility principle: Each class, module or method should have exactly one responsibility.\nOpen/closed principle: Software should be open for extension but closed for modification. Can be implemented using class inheritance.\nLiskov substitution principle: If S is a subtype of T, then objects of type T can be replaced with objects of type S without altering the program. This looks similar to polymorphism but is stricter; not only should the classes implement the same methods, but the behaviour of those methods should be the same too, there should be no hidden functionality performed by S that is not performed by T or vice versa.\nInterface segregation principle: Many client-specific interfaces are preferable to one general purpose interface.\nDependency inversion principle: High-level modules should depend on abstractions rather than concrete implementations. Relies on dependency injection, where one object supplies the dependencies of another object. A factory method determines which class to load and returns an instance of that class. This also makes testing easier, as you can inject a mock class rather than having to mock out specific functionality.\n\n\n\n\n\nStructure - case, underscores\nContent - class names should be nouns, methods should be imperative verbs\n\n\n\n\nSome best practices: 1. Only catch exceptions if you are going to do something with it 2. Catch specific exceptions 3. Use try-catch on the smallest code fragments possible 4. Layers should handle exceptions raised by layers below them, without exposing the specific implementation of the other layer. Catch, log, re-raise a more generic error.\nPurposes of logging: 1. Track errors 2. Gather data\n\n\n\n\nA collection of general, reusable solutions to common software design problems. Popularised in the book “Design patterns: elements of reusable object-oriented software”.\n\nThe factory pattern: Creating objects without specifying the exact class of the object. Avoids strong coupling between classes. Create an interface, then a factory method returns an instance of a class which implements that interface. If the implementation needs to change, we only need to change the factory method.\nThe repository pattern: Modules which don’t work with the data store should be oblivious to the type of data store. This is similar to the concept of the Data Access Layer. Layers are for architects, design patterns are for developers. They both seek to solve the same issue.\nThe facade pattern: Creating a layer of abstraction to mask complex actions. The facade does not create any new functionality, it is just a wrapper integrating existing modular functions together.\nThe command pattern: All the action’s information is encapsulated within an object.\n\n\n\n\nThe architecture design is the big picture that should answer the following: - How will the system work under heavy load? - What will happen if the system crashes at a critical moment? - How complicated is it to update?\nThe architecture should: 1. Define components 2. Define how components communicate 3. Define the system’s quality attributes\nMake the correct choice as early as possible\n\n\nEnsuring the services are not strongly tied to other services. Avoid the “spiderweb” - when the graph of connections between services is densely connected.\nPrevents coupling of platforms and URLs.\nTo avoid URL coupling between services, two options are: - “Yellow pages” directory - Gateway\nThe “yellow pages” contains a directory of all service URLs. If service A wants to query service D, A queries the yellow pages for D’s URL, then uses that URL to query D. This means that service A does not hardcode any information about service D. If D’s URLs change, then only the yellow pages need to be updated. Services only need to know the yellow pages directory’s URL.\n\n\n\n\n\n\nFigure 2: Yellow Pages\n\n\n\nThe gateway acts as a middleman. It holds a mapping table of all URLs Service A queries the gateway, which in turn queries service E. Service A doesn’t need to know about E or any other services. Services only need to know the gateway’s URL.\n\n\n\n\n\n\nFigure 3: Gateway\n\n\n\n\n\n\nThe application’s state is stored in only 2 places: 1. The data store 2. The user interface\nStateless architecture is preferred in virtually all circumstances.\nIn a stateful architecture, when a user logs in, the login service retrieves the user details from the database and stored them for future use. Data is stored in code.\nIf the login request was routed to server A, the user’s details are stored there. If the user later tried to add itemsto the cart and their cart service request is routed to server B, then their user details will not exist as those are stored on server A.\nDisadvantages of stateful: - Lack of scalability - Lack of redundancy\n\n\n\n\n\n\nFigure 4: Stateful\n\n\n\nIn a stateless architecture, no data is stored in the service itself. This means the behaviour will be the same regardless of which server a request was routed to.\n\n\n\n\n\n\nFigure 5: Stateless\n\n\n\n\n\n\nCaches store data locally to avoid retrieving the same data from the database multiple times. It trades reliability of data (it is stored in volatile memory) for improved performance.\nA cache should store data that is frequently accessed and rarely modified.\nTwo types of cache: - In-memory cache - Cache stored in memory on a single service - Pros: - Best performance - Can store any objects - Easy to use - Cons: - Size is limited by the process’s memory - Can grow stale/inconsistent if the service is scaled out to multiple servers - Distributed cache - Cache is independent of the services and can be accessed by all servers - Pros: - Supports scaled out servers - Failover capabilities - Storage is unlimited - Cons: - Setup is more complex - Often only support primitive data types - Worse performance than in-memory cache\n\n\n\nMessaging methods can be evaluated on these criteria: - Performance - Message size - Execution model - Feedback (handshaking) and reliability - Complexity\nMessaging methods include: - REST API - HTTP Push - Queue - File-based and database-based methods\n\n\nUniversal standard for HTTP-based systems.\nUseful for traditional web apps.\n\n\n\n\n\n\nFigure 6: REST API\n\n\n\n\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nVery fast\n\n\nMessage size\nSame as HTTP protocol limitations - GET 8KB, POST ~10MB\n\n\nExecution model\nRequest/response - ideal for quick, short actions\n\n\nFeedback\nImmediate feedback via response codes\n\n\nComplexity\nExtremely easy\n\n\n\n\n\n\nA client subscribes to the service waiting for an event. When that event occurs, the server notifies the client.\nFor real-time messaging, these often use web sockets to maintain the subscription connection, rather than a traditional request/response model.\nUseful for chat or monitoring.\n\n\n\n\n\n\nFigure 7: HTTP Push\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nVery fast\n\n\nMessage size\nLimited, few KB\n\n\nExecution model\nWeb socket connection / long polling\n\n\nFeedback\nNone (fire and forget)\n\n\nComplexity\nExtremely easy\n\n\n\n\n\n\nThe queue sits between two (or more) services. If service A wants to send a message to service B, A places the message in the queue and B periodically pulls from the queue.\nThis ensures messages will be handled exactly once and in the order received.\nUseful for complex systems with lots of data, when order and reliability are important.\n\n\n\n\n\n\nFigure 8: Queue\n\n\n\n\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nSlow - push/poll time and database persistence\n\n\nMessage size\nUnlimited but best practice to use small messages\n\n\nExecution model\nPolling\n\n\nFeedback\nVery reliable but feedback depends on the monitoring in place to ensure messages make it to the queue and get executed\n\n\nComplexity\nRequires training and setup to maintain a queue engine\n\n\n\n\n\n\nSimilar to queue, but rather han placing messages in a queue it places them in a file directory or database. There is no guarantee that messages are processed once and only once.\nIf multiple services are polling the same file folder, then when a new file is added we can get two issues: 1. File locked 2. Duplicate processing\nSimilar use case to queues, but queues are generally preferred.\n\n\n\n\n\n\nFigure 9: File-based Messaging\n\n\n\n\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nSlow - push/poll time and database persistence\n\n\nMessage size\nUnlimited\n\n\nExecution model\nPolling\n\n\nFeedback\nVery reliable but feedback depends on the monitoring\n\n\nComplexity\nRequires training and setup to maintain the filestore or database\n\n\n\n\n\n\n\n\n\nCreate a central logging service that all other services call to write to a central database. This solves the problem where each microservice has its own log format, data and location. For example, some might be in files, SQL database, NoSQL database, etc.\nImplementation can be via an API or polling folders that each of the services write to.\n\n\n\n\n\n\nFigure 10: Central Logging Service\n\n\n\n\n\n\nCorrelation ID is an identifier attached to the beginning of a user flow and is attached to any action taken by that user, so that if there is an error at any point the user flow can be traced back through the different services’ logs.\n\n\n\n\n\nExternal considerations can affect architecture and design decisions. - Deadlines - Dev team skills - New technologies can introduce uncertainty, delays and low quality - IT support - Assign who will support the product from the outset. This should not be developers. - Cost - Build vs buy. Estimate cost vs value. Spend money to save time, don’t spend time to save money.\n\n\n\nThis should describe the basic elements of the system: - Technology stack - Components - Services - Communication between components and services\nNo development should begin before the document is complete.\nGoals of the document: - Describe what should be developed and how - List the functional and non-functional requirements\nAudience: - Everyone involved with the system: project manager, CTO, QA leader, developers - Sections for management appear first as they are unlikely to read the whole document - QA lead can begin preparing test infrastructure ahead of time\nFormat: UML (universal modeling language) is often used. It visualises the system’s design with concepts and diagrams. However, this assumes the audience is familiar with UML which is often not the case.\n\nKeep the contents as simple as possible\nUse plain English\nVisualise using whatever software is comfortable and appropriate\n\nStructure: - Background - Requirements - Executive summary - Architecture overview - Components drill-down\n\n\nThis section validates your point of view and instils confidence that you understand the project.\nOne page for all team and management.\nDescribe the system from a business POV: - The system’s role - Reasons for replacing the old system - Expected business impact\n\n\nFunctional and non-functional requirements - what should the system do and deal with? This section validates your understadning of the requirements. The requirements dictate the architecture so this section sets the scene for the chosen architecture.\nOne page for all team and management.\nThis should be a brief, bullet point list of requirements with no more than 3 lines on each.\n\n\n\n\nProvide a high-level view of the architecture.\nManagers will not read the whole document; &lt;3 pages for management.\n\nUse charts and diagrams\nWrite this after the rest of the document\nUse technical terms sparsely and only well known ones\nBe concise and don’t repeat yourself\n\n\n\n\nProvide a high-level view of the architecture in technical terms. Do not deep dive into specific components\n&lt;10 pages for developers and QA lead.\n\nGeneral description: type and major non-functional requirements\nHigh-level diagram of logical components, no specifics about hardware or technologies\nDiagram walkthrough: describe the various parts and their roles\nTechnology stack: iff there is a single stack include it here, otherwise include it in the components drill down section\n\n\n\n\nDetailed description of each component.\nUnlimited length, for developers and QA lead.\nFor each component: - Component’s role - Technology stack: Data store, backend, frontend - Component’s architecture: Describe the API (URL, logic, response code, comments). Describe the layers. Mention design patterns here. - Development instructions: Specific development guidelines\nBe specific and include rationale behind decisions.\n\n\n\n\nThese architecture patterns solve specific problems but add complexity to the system, so the costs and benefits should be compared.\n\n\nAn architecture in which functionalities are implemented as separate, loosely coupled services that interact with each other using a standard lightweight protocol.\nProblems with monolithic services: - A single exception can crash the whole process - Updates impact all components - Limited to one development platform/language - Unoptimised compute resources\nWith microservices, each service is independent of others so can be updated separately, use a different platform, and be optimised separately.\nAn example of splitting a monolithic architecture into microservices:\n\n\n\n\n\n\nFigure 11: Monolith Example\n\n\n\nAs a microservice architecture, this becomes:\n\n\n\n\n\n\nFigure 12: Microservice Example\n\n\n\nProblems with microservices: - Complex monitoring of all services and their interactions - Complex architecture - Complex testing\n\n\n\nStoring the deltas to each entity rather than updating it. The events can then be “rebuilt” from the start to give a view of the state at any given point in time.\nUse when history matters.\n\n\n\n\n\n\nFigure 13: Event Sourcing\n\n\n\nPros: - Tracing history - Simple data model - Performance - Reporting\nCons: - No unified view - need to rebuild all events from the start to see the current state - Storage usage\n\n\n\nCommand query responsibility segregation. Data storage and data retrieval are two separate databases, with a sync service between the two.\nThis integrates nicely with event sourcing, where events (deltas) are stored in one database and the current state is periodically built and stored in the retrieval database.\n\n\n\n\n\n\nFigure 14: CQRS\n\n\n\nPros: - Useful with high-frequency updates that require near real-time querying\nCons: - Complexity - need 2 databases, a sync service, ETL between the storage and retrieval database\n\n\n\n\nThe architect is often not the direct line manager of the developers implementing the software. You ultimately work with people, not software. Influence without authority is key.\n\nListening - assume you are not the smartest person in the room, collective wisdom is always better.\nDealing with criticism\n\nGenuine questioning: Provide facts and logic, be willing to go back and check again\nMocking: Don’t attack back, provide facts and logic\n\nBe smart not right\nOrganisational politics - be aware of it but don’t engage in it\nPublic speaking - define a goal, know your audience, be confident, don’t read, maintain eye contact\nLearning - blogs (DZone, InfoQ, O’Reilly), articles, conferences\n\n\n\n\n\nUdemy course https://www.udemy.com/course/the-complete-guide-to-becoming-a-software-architect/\nDesign patterns https://refactoring.guru/\n\n\n\n\nFigure 1: Backend Tech\nFigure 2: Yellow Pages\nFigure 3: Gateway\nFigure 4: Stateful\nFigure 5: Stateless\nFigure 6: REST API\nFigure 7: HTTP Push\nFigure 8: Queue\nFigure 9: File-based Messaging\nFigure 10: Central Logging Service\nFigure 11: Monolith Example\nFigure 12: Microservice Example\nFigure 13: Event Sourcing\nFigure 14: CQRS"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#what-is-a-software-architect",
    "href": "posts/software/software_architecture/software_architect_notes.html#what-is-a-software-architect",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "A developer knows what can be done, an architect knows what should be done. How do we use technology to meet business requirements.\nGeneral system requirements: - Fast - Secure - Reliable - Easy to maintain\nArchitect’s need to know how to code for: - Architecture’s trustworthiness - Support developers - Respect of developers"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#the-architects-mindset",
    "href": "posts/software/software_architecture/software_architect_notes.html#the-architects-mindset",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "Understand the business - Strengths, weaknesses, compettions, growth strategy.\nDefine the system’s goals - Goals are not requirements. Goals describe the effect on the organisation, requirements describe what the system should do.\nWork for your client’s clients - Prioritise the end user.\nTalk to the right people with the right language - What is the thing that really matters to the person I’m talking to? Project managers care about how things affect deadlines, developers care about the technologies used, CEOs care about business continuity and bottom line."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#the-architecture-process",
    "href": "posts/software/software_architecture/software_architect_notes.html#the-architecture-process",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "Understand the system requirements - What the system should do.\nUnderstand the non-functional requirements - Technical and service level attributes, e.g. number of users, loads, volumes, performance.\nMap the components - Understand the system functionality and communicate this to your client. Completely non-technical at this point, no mention of specific technologies. A diagram of high level components helps.\nSelect the technology stack - Backend, frontend, data store.\nDesign the architecture\nWrite the architecture document\nSupport the team\n\nInclude developers in non-functional requirements and architecture design for two reasons: 1. Learn about unknown scenarios early 2. Create ambassadors"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#system-requirements",
    "href": "posts/software/software_architecture/software_architect_notes.html#system-requirements",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "The 2 types of requirements: functional and non-functional.\n\n\n“What the system should do”\n\nBusiness flows\nBusiness services\nUser interfaces\n\n\n\n\n“What the system should deal with”\n\nPerformance - latency and throughput\n\nLatency - How long does it take to perform a single task?\nThroughput - How many tasks can be performed in a given time unit?\n\nLoad - Quantity of work without crashing. Determines the availability of the system. Always look at peak (worst case) numbers.\nData volume - How much data will the system accumulate over time?\n\nData required on day one\nData growth (say, annually)\n\nConcurrent users - How many users will be using the system? This includes “dead times” which differentiates it from the load requirement. Rule of thumb is concurrent_users = load * 10\nSLA - Service Level Agreement for uptime.\n\nThe non-functional requirements are generally the more important in determining the architecture. The client will generally need guiding towards sensible values, otherwise they just want as much load as possible, as much uptime as possible etc."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#application-types",
    "href": "posts/software/software_architecture/software_architect_notes.html#application-types",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "The application type should be established early based on the use case and expected user interaction.\n\nWeb apps\n\nServe html pages.\nUI; user-initiated actions; large scale; short, focused actions.\nRequest-response model.\n\nWeb API\n\nServe data (often JSON).\nData retrieval and storage; client-initiated actions; large scale; short, focused actions.\nRequest-response model.\n\nMobile\n\nRequire user interaction; frontend for web API; location-based.\n\nConsole\n\nNo UI; limited interation; long-running processes; short actions for power users.\nRequire technical knowledge.\n\nService\n\nNo UI; managed by service manager; long-running processes.\n\nDesktop\n\nAll resources on the PC; UI; user-centric actions.\n\nFunction-as-a-service\n\nAWS lambda"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#select-technology-stack",
    "href": "posts/software/software_architecture/software_architect_notes.html#select-technology-stack",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "Considerations: - Appropriate for the task - Community - e.g. stack overflow activity - Popularity - google trends over 2 years\n\n\nCovers web app, web API, console, service.\nOptions: .NET, Java, node.js, PHP, Python\n\n\n\n\n\n\nFigure 1: Backend Tech\n\n\n\n\n\n\nCovers: - Web app - Angular, React - Mobile - Native (Swift, Java/Kotlin), Xamarin, React Native - Desktop - depends on target OS\n\n\n\nSQL - small, structured data - Relational tables - Transactions - Atomicity - Consistency - Isolation - Durability - Querying language is universal\nNoSQL - huge, unstructured or semi-structured data - Emphasis on scale and performance. - Schema-less, with entities stored as JSON. - Eventual consistency - data can be temporarily inconsistent - No universal querying language"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#the--ilities",
    "href": "posts/software/software_architecture/software_architect_notes.html#the--ilities",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "Quality attributes that describe technical capabilities to fulfill the non-functional requirements. Non-functional requirements map to quality attributes, which are designed in the architecture.\n\nScalability - Adding computing resources without any interruption. Scale out (add more compute instances) is preferred over scale up (increase CPU, RAM on the existing instance). Scaling out introduces redundancy and is not limited by hardware.\nManageability - Know what’s going on and take action accordingly. The question “who reports the problems” determines if a system is manageable. The system should flag problems, not the suer.\nModularity - A system that is built from blcoks that can be changed or replaced without affecting the whole system.\nExtensibility - Functionality of the system can be extended without modifying existing code.\nTestability - How easy is it to test the application. Independent modules and methods and single responsibility principle aid testability."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#components-architecture",
    "href": "posts/software/software_architecture/software_architect_notes.html#components-architecture",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "A software component is a piece of code that runs in a single process. Distributed systems are composed of independent software components deployed on separate processes/servers/containers.\nComponent architecture relates to the lower level details, whereas system architecture is the higher level view of how the components fit together to achieve the non-functional requirements.\n\n\nLayers represent horizontal functionality: 1. UI/SI - user interface or service interface (i.e. an API), authentication 2. Business logic - validation, enrichment, computation 3. Data access layer - connection handling, transaction handling, querying/saving data\nCode can flow downwards by one layer only. It can never skip a layer and can never go up a layer. This enforces modularity.\nA service might sit across layers, for example logging sits across the 3 layers described above. In this case, the logging service is called a “sross-cutting concern”.\nEach layer should be independent of the implementation of other layers. Layers should handle exceptions from those below them, log the error and then throw a more generic exception, so that there is no reference to other layers’ implementation.\nLayers are part of the same process. This is different from tiers, which are processes that are distributed across a network.\n\n\n\nAn interface declares the signature of an implementation. This means that each implementation must implement the methods described. The abstract base class in Python is an example of this.\nClose coupling should be avoided; “new is glue”.\n\n\n\n\nSingle responsibility principle: Each class, module or method should have exactly one responsibility.\nOpen/closed principle: Software should be open for extension but closed for modification. Can be implemented using class inheritance.\nLiskov substitution principle: If S is a subtype of T, then objects of type T can be replaced with objects of type S without altering the program. This looks similar to polymorphism but is stricter; not only should the classes implement the same methods, but the behaviour of those methods should be the same too, there should be no hidden functionality performed by S that is not performed by T or vice versa.\nInterface segregation principle: Many client-specific interfaces are preferable to one general purpose interface.\nDependency inversion principle: High-level modules should depend on abstractions rather than concrete implementations. Relies on dependency injection, where one object supplies the dependencies of another object. A factory method determines which class to load and returns an instance of that class. This also makes testing easier, as you can inject a mock class rather than having to mock out specific functionality.\n\n\n\n\n\nStructure - case, underscores\nContent - class names should be nouns, methods should be imperative verbs\n\n\n\n\nSome best practices: 1. Only catch exceptions if you are going to do something with it 2. Catch specific exceptions 3. Use try-catch on the smallest code fragments possible 4. Layers should handle exceptions raised by layers below them, without exposing the specific implementation of the other layer. Catch, log, re-raise a more generic error.\nPurposes of logging: 1. Track errors 2. Gather data"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#design-patterns",
    "href": "posts/software/software_architecture/software_architect_notes.html#design-patterns",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "A collection of general, reusable solutions to common software design problems. Popularised in the book “Design patterns: elements of reusable object-oriented software”.\n\nThe factory pattern: Creating objects without specifying the exact class of the object. Avoids strong coupling between classes. Create an interface, then a factory method returns an instance of a class which implements that interface. If the implementation needs to change, we only need to change the factory method.\nThe repository pattern: Modules which don’t work with the data store should be oblivious to the type of data store. This is similar to the concept of the Data Access Layer. Layers are for architects, design patterns are for developers. They both seek to solve the same issue.\nThe facade pattern: Creating a layer of abstraction to mask complex actions. The facade does not create any new functionality, it is just a wrapper integrating existing modular functions together.\nThe command pattern: All the action’s information is encapsulated within an object."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#system-architecture",
    "href": "posts/software/software_architecture/software_architect_notes.html#system-architecture",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "The architecture design is the big picture that should answer the following: - How will the system work under heavy load? - What will happen if the system crashes at a critical moment? - How complicated is it to update?\nThe architecture should: 1. Define components 2. Define how components communicate 3. Define the system’s quality attributes\nMake the correct choice as early as possible\n\n\nEnsuring the services are not strongly tied to other services. Avoid the “spiderweb” - when the graph of connections between services is densely connected.\nPrevents coupling of platforms and URLs.\nTo avoid URL coupling between services, two options are: - “Yellow pages” directory - Gateway\nThe “yellow pages” contains a directory of all service URLs. If service A wants to query service D, A queries the yellow pages for D’s URL, then uses that URL to query D. This means that service A does not hardcode any information about service D. If D’s URLs change, then only the yellow pages need to be updated. Services only need to know the yellow pages directory’s URL.\n\n\n\n\n\n\nFigure 2: Yellow Pages\n\n\n\nThe gateway acts as a middleman. It holds a mapping table of all URLs Service A queries the gateway, which in turn queries service E. Service A doesn’t need to know about E or any other services. Services only need to know the gateway’s URL.\n\n\n\n\n\n\nFigure 3: Gateway\n\n\n\n\n\n\nThe application’s state is stored in only 2 places: 1. The data store 2. The user interface\nStateless architecture is preferred in virtually all circumstances.\nIn a stateful architecture, when a user logs in, the login service retrieves the user details from the database and stored them for future use. Data is stored in code.\nIf the login request was routed to server A, the user’s details are stored there. If the user later tried to add itemsto the cart and their cart service request is routed to server B, then their user details will not exist as those are stored on server A.\nDisadvantages of stateful: - Lack of scalability - Lack of redundancy\n\n\n\n\n\n\nFigure 4: Stateful\n\n\n\nIn a stateless architecture, no data is stored in the service itself. This means the behaviour will be the same regardless of which server a request was routed to.\n\n\n\n\n\n\nFigure 5: Stateless\n\n\n\n\n\n\nCaches store data locally to avoid retrieving the same data from the database multiple times. It trades reliability of data (it is stored in volatile memory) for improved performance.\nA cache should store data that is frequently accessed and rarely modified.\nTwo types of cache: - In-memory cache - Cache stored in memory on a single service - Pros: - Best performance - Can store any objects - Easy to use - Cons: - Size is limited by the process’s memory - Can grow stale/inconsistent if the service is scaled out to multiple servers - Distributed cache - Cache is independent of the services and can be accessed by all servers - Pros: - Supports scaled out servers - Failover capabilities - Storage is unlimited - Cons: - Setup is more complex - Often only support primitive data types - Worse performance than in-memory cache\n\n\n\nMessaging methods can be evaluated on these criteria: - Performance - Message size - Execution model - Feedback (handshaking) and reliability - Complexity\nMessaging methods include: - REST API - HTTP Push - Queue - File-based and database-based methods\n\n\nUniversal standard for HTTP-based systems.\nUseful for traditional web apps.\n\n\n\n\n\n\nFigure 6: REST API\n\n\n\n\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nVery fast\n\n\nMessage size\nSame as HTTP protocol limitations - GET 8KB, POST ~10MB\n\n\nExecution model\nRequest/response - ideal for quick, short actions\n\n\nFeedback\nImmediate feedback via response codes\n\n\nComplexity\nExtremely easy\n\n\n\n\n\n\nA client subscribes to the service waiting for an event. When that event occurs, the server notifies the client.\nFor real-time messaging, these often use web sockets to maintain the subscription connection, rather than a traditional request/response model.\nUseful for chat or monitoring.\n\n\n\n\n\n\nFigure 7: HTTP Push\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nVery fast\n\n\nMessage size\nLimited, few KB\n\n\nExecution model\nWeb socket connection / long polling\n\n\nFeedback\nNone (fire and forget)\n\n\nComplexity\nExtremely easy\n\n\n\n\n\n\nThe queue sits between two (or more) services. If service A wants to send a message to service B, A places the message in the queue and B periodically pulls from the queue.\nThis ensures messages will be handled exactly once and in the order received.\nUseful for complex systems with lots of data, when order and reliability are important.\n\n\n\n\n\n\nFigure 8: Queue\n\n\n\n\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nSlow - push/poll time and database persistence\n\n\nMessage size\nUnlimited but best practice to use small messages\n\n\nExecution model\nPolling\n\n\nFeedback\nVery reliable but feedback depends on the monitoring in place to ensure messages make it to the queue and get executed\n\n\nComplexity\nRequires training and setup to maintain a queue engine\n\n\n\n\n\n\nSimilar to queue, but rather han placing messages in a queue it places them in a file directory or database. There is no guarantee that messages are processed once and only once.\nIf multiple services are polling the same file folder, then when a new file is added we can get two issues: 1. File locked 2. Duplicate processing\nSimilar use case to queues, but queues are generally preferred.\n\n\n\n\n\n\nFigure 9: File-based Messaging\n\n\n\n\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nSlow - push/poll time and database persistence\n\n\nMessage size\nUnlimited\n\n\nExecution model\nPolling\n\n\nFeedback\nVery reliable but feedback depends on the monitoring\n\n\nComplexity\nRequires training and setup to maintain the filestore or database\n\n\n\n\n\n\n\n\n\nCreate a central logging service that all other services call to write to a central database. This solves the problem where each microservice has its own log format, data and location. For example, some might be in files, SQL database, NoSQL database, etc.\nImplementation can be via an API or polling folders that each of the services write to.\n\n\n\n\n\n\nFigure 10: Central Logging Service\n\n\n\n\n\n\nCorrelation ID is an identifier attached to the beginning of a user flow and is attached to any action taken by that user, so that if there is an error at any point the user flow can be traced back through the different services’ logs."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#external-considerations",
    "href": "posts/software/software_architecture/software_architect_notes.html#external-considerations",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "External considerations can affect architecture and design decisions. - Deadlines - Dev team skills - New technologies can introduce uncertainty, delays and low quality - IT support - Assign who will support the product from the outset. This should not be developers. - Cost - Build vs buy. Estimate cost vs value. Spend money to save time, don’t spend time to save money."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#architecture-document",
    "href": "posts/software/software_architecture/software_architect_notes.html#architecture-document",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "This should describe the basic elements of the system: - Technology stack - Components - Services - Communication between components and services\nNo development should begin before the document is complete.\nGoals of the document: - Describe what should be developed and how - List the functional and non-functional requirements\nAudience: - Everyone involved with the system: project manager, CTO, QA leader, developers - Sections for management appear first as they are unlikely to read the whole document - QA lead can begin preparing test infrastructure ahead of time\nFormat: UML (universal modeling language) is often used. It visualises the system’s design with concepts and diagrams. However, this assumes the audience is familiar with UML which is often not the case.\n\nKeep the contents as simple as possible\nUse plain English\nVisualise using whatever software is comfortable and appropriate\n\nStructure: - Background - Requirements - Executive summary - Architecture overview - Components drill-down\n\n\nThis section validates your point of view and instils confidence that you understand the project.\nOne page for all team and management.\nDescribe the system from a business POV: - The system’s role - Reasons for replacing the old system - Expected business impact\n\n\nFunctional and non-functional requirements - what should the system do and deal with? This section validates your understadning of the requirements. The requirements dictate the architecture so this section sets the scene for the chosen architecture.\nOne page for all team and management.\nThis should be a brief, bullet point list of requirements with no more than 3 lines on each.\n\n\n\n\nProvide a high-level view of the architecture.\nManagers will not read the whole document; &lt;3 pages for management.\n\nUse charts and diagrams\nWrite this after the rest of the document\nUse technical terms sparsely and only well known ones\nBe concise and don’t repeat yourself\n\n\n\n\nProvide a high-level view of the architecture in technical terms. Do not deep dive into specific components\n&lt;10 pages for developers and QA lead.\n\nGeneral description: type and major non-functional requirements\nHigh-level diagram of logical components, no specifics about hardware or technologies\nDiagram walkthrough: describe the various parts and their roles\nTechnology stack: iff there is a single stack include it here, otherwise include it in the components drill down section\n\n\n\n\nDetailed description of each component.\nUnlimited length, for developers and QA lead.\nFor each component: - Component’s role - Technology stack: Data store, backend, frontend - Component’s architecture: Describe the API (URL, logic, response code, comments). Describe the layers. Mention design patterns here. - Development instructions: Specific development guidelines\nBe specific and include rationale behind decisions."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#advanced-architecture-topics",
    "href": "posts/software/software_architecture/software_architect_notes.html#advanced-architecture-topics",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "These architecture patterns solve specific problems but add complexity to the system, so the costs and benefits should be compared.\n\n\nAn architecture in which functionalities are implemented as separate, loosely coupled services that interact with each other using a standard lightweight protocol.\nProblems with monolithic services: - A single exception can crash the whole process - Updates impact all components - Limited to one development platform/language - Unoptimised compute resources\nWith microservices, each service is independent of others so can be updated separately, use a different platform, and be optimised separately.\nAn example of splitting a monolithic architecture into microservices:\n\n\n\n\n\n\nFigure 11: Monolith Example\n\n\n\nAs a microservice architecture, this becomes:\n\n\n\n\n\n\nFigure 12: Microservice Example\n\n\n\nProblems with microservices: - Complex monitoring of all services and their interactions - Complex architecture - Complex testing\n\n\n\nStoring the deltas to each entity rather than updating it. The events can then be “rebuilt” from the start to give a view of the state at any given point in time.\nUse when history matters.\n\n\n\n\n\n\nFigure 13: Event Sourcing\n\n\n\nPros: - Tracing history - Simple data model - Performance - Reporting\nCons: - No unified view - need to rebuild all events from the start to see the current state - Storage usage\n\n\n\nCommand query responsibility segregation. Data storage and data retrieval are two separate databases, with a sync service between the two.\nThis integrates nicely with event sourcing, where events (deltas) are stored in one database and the current state is periodically built and stored in the retrieval database.\n\n\n\n\n\n\nFigure 14: CQRS\n\n\n\nPros: - Useful with high-frequency updates that require near real-time querying\nCons: - Complexity - need 2 databases, a sync service, ETL between the storage and retrieval database"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#soft-skills",
    "href": "posts/software/software_architecture/software_architect_notes.html#soft-skills",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "The architect is often not the direct line manager of the developers implementing the software. You ultimately work with people, not software. Influence without authority is key.\n\nListening - assume you are not the smartest person in the room, collective wisdom is always better.\nDealing with criticism\n\nGenuine questioning: Provide facts and logic, be willing to go back and check again\nMocking: Don’t attack back, provide facts and logic\n\nBe smart not right\nOrganisational politics - be aware of it but don’t engage in it\nPublic speaking - define a goal, know your audience, be confident, don’t read, maintain eye contact\nLearning - blogs (DZone, InfoQ, O’Reilly), articles, conferences"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#references",
    "href": "posts/software/software_architecture/software_architect_notes.html#references",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "Udemy course https://www.udemy.com/course/the-complete-guide-to-becoming-a-software-architect/\nDesign patterns https://refactoring.guru/\n\n\n\n\nFigure 1: Backend Tech\nFigure 2: Yellow Pages\nFigure 3: Gateway\nFigure 4: Stateful\nFigure 5: Stateless\nFigure 6: REST API\nFigure 7: HTTP Push\nFigure 8: Queue\nFigure 9: File-based Messaging\nFigure 10: Central Logging Service\nFigure 11: Monolith Example\nFigure 12: Microservice Example\nFigure 13: Event Sourcing\nFigure 14: CQRS"
  },
  {
    "objectID": "posts/business/marketing/marketing.html",
    "href": "posts/business/marketing/marketing.html",
    "title": "Marketing Notes",
    "section": "",
    "text": "Notes from “The 1-Page Marketing Plan” by Allan Dib.\n\n\n\n\nThe overall phases of the direct response marketing strategy are:\n\n\n\nPhase\nStatus\nGoal\n\n\n\n\nBefore\nProspect\nGet them to know you\n\n\nDuring\nLead\nGet them to like you\n\n\nAfter\nCustomer\nGet them to trust you\n\n\n\nDirect response marketing should be: - Specific - Trackable - Include an offer - Follow up\nA marketing plan encompasses:\n\n\n\n\n\n\n\nStage\nExample\n\n\n\n\nAdvertising\nSign for the circus\n\n\nPromotion\nPut the sign on an elephant and walk it into town\n\n\nPublicity\nElephant tramples the mayor’s flowers and it makes the news\n\n\nPR\nYou get the mayor to laugh it off\n\n\nSales\nPeople come to the circus and spend money\n\n\n\nStrategy changes with scale - Don’t blindly copy large companies. “Branding” to “get your name out there” is expensive and ineffective. - The one and only objective of marketing for a small company is “make a profit” For large companies it may be: appease shareholders, board, managers, existing clients, win advertising awards. Making a profit is a low priority for them. - Strategy without tactics = paralysis by analysis - Tactics without strategy = bright shiny object syndrome\nProduct quality is a customer retention tool, not a customer acquisition tool. - People don’t know how good the product is until they’ve already bought it - “If you build it, they will come” doesn’t work\nThe 80/20 rule (Pareto principle) states that 80% of the outputs come from 20% of the inputs. Applying the 80/20 rule to the 80/20 rule gives the 64/4 rule: 64% of the outputs come from 4% of the inputs. Identify this 4% and focus on it. Often this is marketing.\nSpend money to make time, not vice versa. Money is a renewable resource, time is finite.\n\n\n\nThe niche should be as specific as possible - an inch wide and a mile deep. - Too broad a target market is useless as the message gets diluted - Mass marketing/branding can work for larger companies but not small. Large companies have enough firepower to fire arrows in all directions. Small companies need to fire in a specific direction.\nBecome a specialist in your field to make price irrelevant - Become an educator and build trust - You don’t want to be a commodity shopped on price. A customer won on price will be lost on price.\nUse the PVP index to identify the ideal niche. Assign values to each possible segment for: - Personal fulfillment - Value to the marketplace - Profitability\nQuestions for the target prospect: - What keeps them up at night? - What are they afraid of? - What are they angry about? Who are they angry at? - What trends affect them? - What do they secretly desire most? What one thing do they crave above all else? - How do they make decisions? Analytical, emotional, social proof - What language/jargon do they use? - What magazines do they read? What websites do they visit? - What is a typical day like? - What is the single most dominant emotion this market feels?\nCreate an avatar for each prospect. Put a name and photo against each market segment and describe a specific person in detail. Describe their life, job, activities, relationships. Answer each of the questions above from their POV.\n\n\n\nThere are two key elements of the message: 1. Purpose of the ad 2. What it focuses on\nKeep the message focused: 1 ad = 1 objective. “Marketing on purpose” rather than “marketing by accident”. There should be a clear call to action. You should know the exact action you want the prospect to take next.\nThe following two questions help refine the USP: 1. Why should they buy? 2. Why should they buy from me?\nKey takeaways for USP: - Why should they buy from you rather than competitor - If you remove the company name and logo, would people still know it’s you? - Quality and service are not USPs. The customer only finds these out after they buy from you. USPs should attract customers before they buy. - Lowest price is not a USP. There’s always someone willing to go out of business quicker than you.\nPeople rarely want what you’re selling, they want the result of what you’re selling. Sell the problem. People don’t know what they want until they’re presented with it. Purchasing is done with emotions and justified with logic after the fact. “If I had asked people what they wanted, they’d have said faster horses” - Henry Ford.\nWhen you confuse them, you lose them. - Explain the product and its benefit in one sentence. - The biggest threat isn’t that someone buys from a competitor, it’s that they do nothing. - Elevator pitch of the form: “You know ? What we do is ? In fact .”\nBe remarkable - Transform something ordinary into something that makes the customer smile. - Unique is dangerous, you just need to be “different enough”.\nCraft the offer - If you don’t give a compelling reason to buy, customers will default to price, and you become a commodity. - Don’t just offer a discount, think about which product will definitely solve their biggest problem. - Add value rather than discounting, through bundles and customisation. - Create “apples to oranges” comparisons to your competitors to get out of the commmodity game. - Target pain points. Sell the problem, not a feature list. Customers in pain are price insensitive.\nCreate the offer: - Value to customer - Language and jargon they use - Reason why this offer is here. People are skeptical if it is too good to be true. - Value stacking. Add in high-margin bonuses. - Upsells - Outrageous guarantee - Create scarcity. People react more to fear of loss than prospect of gain.\nWriting copy: - Be yourself and be authentic. “Professional” = boring. - “The enemy in common”. People buy with emotions: fear, love, greed, guilt, pride. - Copy should address the bad parts of the product, not just the good. Who is this not for?\nNaming: - Title=content - Choose clarity over cleverness\n\n\n\n“Half the money I spend on advertising is wasted; the trouble is I don’t know which half” - John Wanamaker\nMeasure ROI of ad campaign - If it made more money that it lost, it was a success. The only metric that matters. - Customer acquisition cost vs lifetime value of customer - Other metrics like response rate are only useful to the extent that they help you calculate the above. - Specific target market is key. The goal of the ad is to make they prospect say “Hey, that’s for me”. - What gets measured gets managed. Conversely, what you don’t know will hurt you.\nLifetime value of customer = Frontend (initial purchase) + Backend (subsequent purchases) - Ideally the frontend alone exceeds the customer acquisition cost to be sustainable. Then the cost of the ad is immediately recouped and the ad campaign can be reupped for another cycle.\nSuccessful marketing has three components: 1. Market 2. Message 3. Media\nSocial media is a media not a strategy and it’s not free. It’s only free if your time is worthless.\nEmail is preferable to social media because you own the mailing list. The customer database which is a valuable asset in its own right. - Email regularly - weekly at least - Give value - 3 value-building emails for each offer email - Snail mail complements email - less cluttered and less competitive because fewer people still use it.\nMarketing budget - The only time for a budget is during the testing phase when your finding what works (what delivers a positive ROI) - Once you have a campaign that works, go all in. It makes money, it’s not an expense so don’t limit it. - If you could buy £10 notes for £5 you’d buy as many as possible, you wouldn’t limit it with a budget. The same applies for a marketing campaign with a £5 cost of acquisition and £10 lifetime value of customer.\nAvoid single points of failure - One customer, supplier, ad medium, lead generation method. - Paid media is reliable and allows ROI to be tracked and measured.\n\n\n\n\n\n\nTreat marketing like farming not hunting. Avoid “random acts of marketing” that hunters do. Build a CRM system and infrastructure for new leads, follow-ups, conversions, etc.\nThe first goal is to generate leads - find people who are interested. - Avoid trying to sell from the ad immediately - Capture all who could be interested, not just those ready to buy immediately - Build a customer database, then sell to them later\nThe market for your product might break down something like: - 3% - ready to buy now - 7% - open to buying - 30% - interested but not right now - 60% - not interested\nBy selling directly from the ad you limit yourself just to the 3%. Capture leads first and nurture them to appeal to the 40% who are interested.\nEducate customers to offer value, then you become a trusted advisor rather than a pest salesman.\n\n\n\nThe process of taking people from being vaguely interested to desiring it. The nurturing process means they should be predisposed to buying from you before you ever try to sell to them.\nStay in contact. - Most stop after 1 or 2 follow-ups. The most effective salespeople do 10+ follow-ups. - The money is in the follow-up\nBuild trust and add value. Don’t pester to close. - When the customer is ready to buy, you will be top of mind and they are predisposed to buy. It should be when they are ready to buy, not when you are ready to sell. - Do not pressure prospects to close. Don’t be a pest, be a welcome guest. - Contact regularly - “market until they buy or die”. - Most contact should be value add, with occasional pitch/offer emails. 3:1 ratio of value add:pitch.\nShock and awe package. - Physical parcel mailed to high probability prospects. - “Lumpy” mail grabs attention - snail mail with something physical inside. - Need numbers on conversion probabilities and lifetime value of customer for this approach to be viable. Good marketing can’t beat bad math. - High value contents should: 1. Provide value 2. Position you as a trusted expert 3. Move prospect further down the buying cycle.\nMake lots of offers. A/B test with small segments. Take risks, write compelling copy and make outrageous guarantees.\nThree major types of people:\n\n\n\nType\nRole\nResponsibility\n\n\n\n\nEntrepreneur\n“Make it up”\nVision\n\n\nSpecialist\n“Make it real”\nImplement\n\n\nManager\n“Make it recur”\nDaily BAU\n\n\n\nMarketing calendar. - What needs to be done and when - Daily, weekly, monthly, quarterly, annually - Event-triggers tasks, e.g. on signup, on complaint\nDelegate. - If someone else can do the job 80% as good as you, delegate it. - Separate majors and minors. Focus on the majors, dump or delegate the minors. - Don’t mistake movement for achievement. - Days are expensive. You can get more money but not more time.\n\n\n\nOnce you reach a certain level of expertise, the real profit is in how you market yourself. A concert violinist makes more money performing concerts than busking.\nTrust is key in sales. - Difficult for unknown businesses. - People are wary because of dodgy sellers. Assume every dog bites. - Educate -&gt; Position as expert -&gt; Trust - Delay the sale to show: 1. You are willing to give before you take 2. You are an educator, so can be trusted - Stop selling and start educating, consulting, advising. - Entrepreneur is someone who solves people’s problems for profit.\nOutrageous guarantees. - Reverse the risk from the customer to you. - Guarantee needs to be specific and address the prospect’s reservations directly. Not just a generic money-back guarantee. - If you’re an ethical operator, you’re already implicitly providing a guarantee, just not marketing it. Use it!\nPricing. - The marketing potential of price is often underutilised. - Don’t just naively undercut the market leader or use cost-plus pricing. - Small number of options/tiers - More choice means you are more likely to pique interest but less likely to keep interest. - Fear of a suboptimal choice; the paradox of choice. - Standard and premium tiers and a good rule of thumb. - “Unlimited” option can often be profitable because customers overestimate how much they will use, so overpay for unlimited use. - Ultra high ticket item - Makes other tiers seem more reasonably priced - 10% of customers would pay 10x more. 1% of customers would pay 100x more.\nOther price considerations: - Don’t discount, unless specifically using a loss-leader strategy. - Try before you buy (the puppy dog close) - The sale feels less permanent so more palatable for the customer. - The onus is on the customer to return - shifts the power balance. - Don’t segregate departments - everyone can warm up a prospect. - Payment plans - people are less attached to future money than present money.\n\n\n\n\n\n\nThe goal is to turn customers into a tribe of raving fans. Most companies stop marketing at the sell, but this is where the process begins. Most of the money is in the backend not the frontend.\nSell them what they want, give them what they need. - These don’t always overlap - If they buy a treadmill and don’t use it, they’ll blame you when they don’t lose weight. You need to make sure they use your product correctly after they buy it. - Split the process up for them to make it less daunting.\nCreate a sense of theatre - People don’t just want ot be serviced, they want to be entertained. - Innovate on: pricing, financing, packaging, support, delivery etc, not just product. For example, “will it blend” YouTube videos to sell ordinary blenders.\nUse technology to reduce friction. Treat tech like an employee. Why am I hiring it? What are its KPIs?\nBecome a voice of value to your tribe. - Education-based marketing leads to trust and becoming an expert in the field. - Tell your audience the effort that goes into the product. For example, shampoo bottle that says how many mint leaves went into it.\nCreate replicable systems - Systems ensure you have a business, rather than you are the business - Often overlooked because it is “back office” or “not urgent” - Benefits: build valuable asset; leverage and scalability; consistency; lower labour costs - Areas: 1. Marketing (leads) 2. Sales (follow-up) 3. Fulfillment 4. Admin - Create an operations manual: 1. Imagine the business is 10x the size 2. Identify all roles 3. Identify all tasks - what does each role do 4. Checklist for each task\nExit strategy. Start with the end in mind. If the goal is to sell for $50 million, any decision can be framed as “will this help me get to $50 million?”. Who will buy the company? Why? Customer base, revenue or IP?\n\n\n\nPeople are 21x more likely to buy from a company they’ve previously bought from. Increase revenue by selling more to existing customers.\n\nRaise prices\n\n\nPeople are less price sensitive than you’d expect.\nSome may leave but these were the lowest-value customers - this may be polluted revenue you want to fire anyway. A customer won on price can be lost on price.\nGive a reason why prices increased. Explain the benefits they’ve received and future innovations.\nGrandfather existing customers on old price as a loyalty bonus.\n\n\nUpselling\n\n\n“Contrast principle” - people are less price sensitive to add-ons. If they’ve already bought the suit then the shirt seems cheap.\n“Customers who bought X also bought Y” - people want to fit social norms.\nSell more when the client is “hot and heavy” in the buying mood. Some people naively think you should overwhelm them with more selling.\n\n\nAscension\n\n\nMove customers up to a higher-priced tier.\nProduct mix - standard, premium, ultra-high.\n\n\nFrequency of purchases\n\n\nReminders, subscriptions\nReasons to come back; voucher for next purchase.\n\n\nReactivation\n\n\nLure back lapsed customers with strong offer and call to action.\nAsk why they haven’t returned.\nApologise and make corrections if necessary. Make the feel special if they do return.\n\nKey numbers - what gets measured gets managed 1. Leads 2. Conversion rate 3. Average transaction value 4. Break even point\nFor subscription models measure: 1. Monthly recurring revenue 2. Churn rate 3. Customer lifetime value\nPolluted revenue - dollars from toxic customers != dollars from raving fans. Types of customers: 1. Tribe 2. Churners - can’t afford you so drop you on price or intro offer ending 3. Vampires - suck up all your time, you can’t afford them 4. Snow leopard - big customer but not replicable\nNet promoter score - “How likely are you to recommend us to a friend?” - Detractors 0-6; Passive 7-8; Promoters 9-10 - NPS=+100 if everyone is a promoter, -100 if everyone is a detractor\nFire problem customers - “The customer is always right” is naive. It should be “the right customer is always right”. - Genuine customer complaints are valuable and may help retain other customers who had the same issue but didn’t speak up. - Low-value, price-sensitive customers complain the most. They take up your time and energy at the expense of your tribe. - Firing detractors creates scarcity and frees your time to focus on the tribe. With limited supply, customers have to play and pay by your rules.\n\n\n\nOrchestrating referrals is an active task, not needy or desperate, - Don’t just deliver a good product and hope for the best - People refer because it makes them feel good, helpful or knowledgeable, not to do you a favour. - Appeal to their ego, offer them something valuable, give them a reason to refer.\nAsk (for a referral) and ye shall receive. - “Law of 250” - most people know 250 people well enough to invite to a wedding or funeral. This is 250 potential referrals per customer. - Set expectation of referrals beforehand - “We’re going to do a great job and we need your help… Most people refer 3 others.”\nBe specific about who you ask and what kind of referral you want. Conquer the bystander effect.\nMake referral profiles for each customer group in your database. - Who do they know? - How can you make them remember you? - What will make them look good? - How will they provide value to their referral target\nCustomer buying behaviour - Who has your customers before you? - Joint venture for referrals - Sell/exchange leads, resell complementary products, affiliate referral partner\nBrand = personality of the business. Think of the business as a person: - Name - Design (what they wear) - Positioning (how they communicate) - Brand promise (core values) - Target market (who they associate with) - Brand awareness (how well known / popular)\nBest form of brand building is selling - Branding is what you do after someone has bought from you. - Brand equity - customers crossing the road to buy from you and walk past competitors.\n\n\n\nImplementation is crucial - Knowing and not doing is the same as not knowing. - Common pitfalls: 1. Paralysis by analysis - 80% out the door is better than 100% in the drawer; money loves speed. 2. Fail to delegate 3. “My business is different” - Spend your effort figuring out how to make it work rather than why it won’t work.\nTime is not money - Entrepreneurs get paid for the value they create, not the time or effort they put in. - Making money is a side effect of creating value.\nThe value of a business is the eyeballs it has access to.\nQuestions to ponder: - What business should I be in? - What technologies will disrupt it? - How can I take advantage of tech rather than fight it?"
  },
  {
    "objectID": "posts/business/marketing/marketing.html#the-before-phase",
    "href": "posts/business/marketing/marketing.html#the-before-phase",
    "title": "Marketing Notes",
    "section": "",
    "text": "The overall phases of the direct response marketing strategy are:\n\n\n\nPhase\nStatus\nGoal\n\n\n\n\nBefore\nProspect\nGet them to know you\n\n\nDuring\nLead\nGet them to like you\n\n\nAfter\nCustomer\nGet them to trust you\n\n\n\nDirect response marketing should be: - Specific - Trackable - Include an offer - Follow up\nA marketing plan encompasses:\n\n\n\n\n\n\n\nStage\nExample\n\n\n\n\nAdvertising\nSign for the circus\n\n\nPromotion\nPut the sign on an elephant and walk it into town\n\n\nPublicity\nElephant tramples the mayor’s flowers and it makes the news\n\n\nPR\nYou get the mayor to laugh it off\n\n\nSales\nPeople come to the circus and spend money\n\n\n\nStrategy changes with scale - Don’t blindly copy large companies. “Branding” to “get your name out there” is expensive and ineffective. - The one and only objective of marketing for a small company is “make a profit” For large companies it may be: appease shareholders, board, managers, existing clients, win advertising awards. Making a profit is a low priority for them. - Strategy without tactics = paralysis by analysis - Tactics without strategy = bright shiny object syndrome\nProduct quality is a customer retention tool, not a customer acquisition tool. - People don’t know how good the product is until they’ve already bought it - “If you build it, they will come” doesn’t work\nThe 80/20 rule (Pareto principle) states that 80% of the outputs come from 20% of the inputs. Applying the 80/20 rule to the 80/20 rule gives the 64/4 rule: 64% of the outputs come from 4% of the inputs. Identify this 4% and focus on it. Often this is marketing.\nSpend money to make time, not vice versa. Money is a renewable resource, time is finite.\n\n\n\nThe niche should be as specific as possible - an inch wide and a mile deep. - Too broad a target market is useless as the message gets diluted - Mass marketing/branding can work for larger companies but not small. Large companies have enough firepower to fire arrows in all directions. Small companies need to fire in a specific direction.\nBecome a specialist in your field to make price irrelevant - Become an educator and build trust - You don’t want to be a commodity shopped on price. A customer won on price will be lost on price.\nUse the PVP index to identify the ideal niche. Assign values to each possible segment for: - Personal fulfillment - Value to the marketplace - Profitability\nQuestions for the target prospect: - What keeps them up at night? - What are they afraid of? - What are they angry about? Who are they angry at? - What trends affect them? - What do they secretly desire most? What one thing do they crave above all else? - How do they make decisions? Analytical, emotional, social proof - What language/jargon do they use? - What magazines do they read? What websites do they visit? - What is a typical day like? - What is the single most dominant emotion this market feels?\nCreate an avatar for each prospect. Put a name and photo against each market segment and describe a specific person in detail. Describe their life, job, activities, relationships. Answer each of the questions above from their POV.\n\n\n\nThere are two key elements of the message: 1. Purpose of the ad 2. What it focuses on\nKeep the message focused: 1 ad = 1 objective. “Marketing on purpose” rather than “marketing by accident”. There should be a clear call to action. You should know the exact action you want the prospect to take next.\nThe following two questions help refine the USP: 1. Why should they buy? 2. Why should they buy from me?\nKey takeaways for USP: - Why should they buy from you rather than competitor - If you remove the company name and logo, would people still know it’s you? - Quality and service are not USPs. The customer only finds these out after they buy from you. USPs should attract customers before they buy. - Lowest price is not a USP. There’s always someone willing to go out of business quicker than you.\nPeople rarely want what you’re selling, they want the result of what you’re selling. Sell the problem. People don’t know what they want until they’re presented with it. Purchasing is done with emotions and justified with logic after the fact. “If I had asked people what they wanted, they’d have said faster horses” - Henry Ford.\nWhen you confuse them, you lose them. - Explain the product and its benefit in one sentence. - The biggest threat isn’t that someone buys from a competitor, it’s that they do nothing. - Elevator pitch of the form: “You know ? What we do is ? In fact .”\nBe remarkable - Transform something ordinary into something that makes the customer smile. - Unique is dangerous, you just need to be “different enough”.\nCraft the offer - If you don’t give a compelling reason to buy, customers will default to price, and you become a commodity. - Don’t just offer a discount, think about which product will definitely solve their biggest problem. - Add value rather than discounting, through bundles and customisation. - Create “apples to oranges” comparisons to your competitors to get out of the commmodity game. - Target pain points. Sell the problem, not a feature list. Customers in pain are price insensitive.\nCreate the offer: - Value to customer - Language and jargon they use - Reason why this offer is here. People are skeptical if it is too good to be true. - Value stacking. Add in high-margin bonuses. - Upsells - Outrageous guarantee - Create scarcity. People react more to fear of loss than prospect of gain.\nWriting copy: - Be yourself and be authentic. “Professional” = boring. - “The enemy in common”. People buy with emotions: fear, love, greed, guilt, pride. - Copy should address the bad parts of the product, not just the good. Who is this not for?\nNaming: - Title=content - Choose clarity over cleverness\n\n\n\n“Half the money I spend on advertising is wasted; the trouble is I don’t know which half” - John Wanamaker\nMeasure ROI of ad campaign - If it made more money that it lost, it was a success. The only metric that matters. - Customer acquisition cost vs lifetime value of customer - Other metrics like response rate are only useful to the extent that they help you calculate the above. - Specific target market is key. The goal of the ad is to make they prospect say “Hey, that’s for me”. - What gets measured gets managed. Conversely, what you don’t know will hurt you.\nLifetime value of customer = Frontend (initial purchase) + Backend (subsequent purchases) - Ideally the frontend alone exceeds the customer acquisition cost to be sustainable. Then the cost of the ad is immediately recouped and the ad campaign can be reupped for another cycle.\nSuccessful marketing has three components: 1. Market 2. Message 3. Media\nSocial media is a media not a strategy and it’s not free. It’s only free if your time is worthless.\nEmail is preferable to social media because you own the mailing list. The customer database which is a valuable asset in its own right. - Email regularly - weekly at least - Give value - 3 value-building emails for each offer email - Snail mail complements email - less cluttered and less competitive because fewer people still use it.\nMarketing budget - The only time for a budget is during the testing phase when your finding what works (what delivers a positive ROI) - Once you have a campaign that works, go all in. It makes money, it’s not an expense so don’t limit it. - If you could buy £10 notes for £5 you’d buy as many as possible, you wouldn’t limit it with a budget. The same applies for a marketing campaign with a £5 cost of acquisition and £10 lifetime value of customer.\nAvoid single points of failure - One customer, supplier, ad medium, lead generation method. - Paid media is reliable and allows ROI to be tracked and measured."
  },
  {
    "objectID": "posts/business/marketing/marketing.html#the-during-phase",
    "href": "posts/business/marketing/marketing.html#the-during-phase",
    "title": "Marketing Notes",
    "section": "",
    "text": "Treat marketing like farming not hunting. Avoid “random acts of marketing” that hunters do. Build a CRM system and infrastructure for new leads, follow-ups, conversions, etc.\nThe first goal is to generate leads - find people who are interested. - Avoid trying to sell from the ad immediately - Capture all who could be interested, not just those ready to buy immediately - Build a customer database, then sell to them later\nThe market for your product might break down something like: - 3% - ready to buy now - 7% - open to buying - 30% - interested but not right now - 60% - not interested\nBy selling directly from the ad you limit yourself just to the 3%. Capture leads first and nurture them to appeal to the 40% who are interested.\nEducate customers to offer value, then you become a trusted advisor rather than a pest salesman.\n\n\n\nThe process of taking people from being vaguely interested to desiring it. The nurturing process means they should be predisposed to buying from you before you ever try to sell to them.\nStay in contact. - Most stop after 1 or 2 follow-ups. The most effective salespeople do 10+ follow-ups. - The money is in the follow-up\nBuild trust and add value. Don’t pester to close. - When the customer is ready to buy, you will be top of mind and they are predisposed to buy. It should be when they are ready to buy, not when you are ready to sell. - Do not pressure prospects to close. Don’t be a pest, be a welcome guest. - Contact regularly - “market until they buy or die”. - Most contact should be value add, with occasional pitch/offer emails. 3:1 ratio of value add:pitch.\nShock and awe package. - Physical parcel mailed to high probability prospects. - “Lumpy” mail grabs attention - snail mail with something physical inside. - Need numbers on conversion probabilities and lifetime value of customer for this approach to be viable. Good marketing can’t beat bad math. - High value contents should: 1. Provide value 2. Position you as a trusted expert 3. Move prospect further down the buying cycle.\nMake lots of offers. A/B test with small segments. Take risks, write compelling copy and make outrageous guarantees.\nThree major types of people:\n\n\n\nType\nRole\nResponsibility\n\n\n\n\nEntrepreneur\n“Make it up”\nVision\n\n\nSpecialist\n“Make it real”\nImplement\n\n\nManager\n“Make it recur”\nDaily BAU\n\n\n\nMarketing calendar. - What needs to be done and when - Daily, weekly, monthly, quarterly, annually - Event-triggers tasks, e.g. on signup, on complaint\nDelegate. - If someone else can do the job 80% as good as you, delegate it. - Separate majors and minors. Focus on the majors, dump or delegate the minors. - Don’t mistake movement for achievement. - Days are expensive. You can get more money but not more time.\n\n\n\nOnce you reach a certain level of expertise, the real profit is in how you market yourself. A concert violinist makes more money performing concerts than busking.\nTrust is key in sales. - Difficult for unknown businesses. - People are wary because of dodgy sellers. Assume every dog bites. - Educate -&gt; Position as expert -&gt; Trust - Delay the sale to show: 1. You are willing to give before you take 2. You are an educator, so can be trusted - Stop selling and start educating, consulting, advising. - Entrepreneur is someone who solves people’s problems for profit.\nOutrageous guarantees. - Reverse the risk from the customer to you. - Guarantee needs to be specific and address the prospect’s reservations directly. Not just a generic money-back guarantee. - If you’re an ethical operator, you’re already implicitly providing a guarantee, just not marketing it. Use it!\nPricing. - The marketing potential of price is often underutilised. - Don’t just naively undercut the market leader or use cost-plus pricing. - Small number of options/tiers - More choice means you are more likely to pique interest but less likely to keep interest. - Fear of a suboptimal choice; the paradox of choice. - Standard and premium tiers and a good rule of thumb. - “Unlimited” option can often be profitable because customers overestimate how much they will use, so overpay for unlimited use. - Ultra high ticket item - Makes other tiers seem more reasonably priced - 10% of customers would pay 10x more. 1% of customers would pay 100x more.\nOther price considerations: - Don’t discount, unless specifically using a loss-leader strategy. - Try before you buy (the puppy dog close) - The sale feels less permanent so more palatable for the customer. - The onus is on the customer to return - shifts the power balance. - Don’t segregate departments - everyone can warm up a prospect. - Payment plans - people are less attached to future money than present money."
  },
  {
    "objectID": "posts/business/marketing/marketing.html#the-after-phase",
    "href": "posts/business/marketing/marketing.html#the-after-phase",
    "title": "Marketing Notes",
    "section": "",
    "text": "The goal is to turn customers into a tribe of raving fans. Most companies stop marketing at the sell, but this is where the process begins. Most of the money is in the backend not the frontend.\nSell them what they want, give them what they need. - These don’t always overlap - If they buy a treadmill and don’t use it, they’ll blame you when they don’t lose weight. You need to make sure they use your product correctly after they buy it. - Split the process up for them to make it less daunting.\nCreate a sense of theatre - People don’t just want ot be serviced, they want to be entertained. - Innovate on: pricing, financing, packaging, support, delivery etc, not just product. For example, “will it blend” YouTube videos to sell ordinary blenders.\nUse technology to reduce friction. Treat tech like an employee. Why am I hiring it? What are its KPIs?\nBecome a voice of value to your tribe. - Education-based marketing leads to trust and becoming an expert in the field. - Tell your audience the effort that goes into the product. For example, shampoo bottle that says how many mint leaves went into it.\nCreate replicable systems - Systems ensure you have a business, rather than you are the business - Often overlooked because it is “back office” or “not urgent” - Benefits: build valuable asset; leverage and scalability; consistency; lower labour costs - Areas: 1. Marketing (leads) 2. Sales (follow-up) 3. Fulfillment 4. Admin - Create an operations manual: 1. Imagine the business is 10x the size 2. Identify all roles 3. Identify all tasks - what does each role do 4. Checklist for each task\nExit strategy. Start with the end in mind. If the goal is to sell for $50 million, any decision can be framed as “will this help me get to $50 million?”. Who will buy the company? Why? Customer base, revenue or IP?\n\n\n\nPeople are 21x more likely to buy from a company they’ve previously bought from. Increase revenue by selling more to existing customers.\n\nRaise prices\n\n\nPeople are less price sensitive than you’d expect.\nSome may leave but these were the lowest-value customers - this may be polluted revenue you want to fire anyway. A customer won on price can be lost on price.\nGive a reason why prices increased. Explain the benefits they’ve received and future innovations.\nGrandfather existing customers on old price as a loyalty bonus.\n\n\nUpselling\n\n\n“Contrast principle” - people are less price sensitive to add-ons. If they’ve already bought the suit then the shirt seems cheap.\n“Customers who bought X also bought Y” - people want to fit social norms.\nSell more when the client is “hot and heavy” in the buying mood. Some people naively think you should overwhelm them with more selling.\n\n\nAscension\n\n\nMove customers up to a higher-priced tier.\nProduct mix - standard, premium, ultra-high.\n\n\nFrequency of purchases\n\n\nReminders, subscriptions\nReasons to come back; voucher for next purchase.\n\n\nReactivation\n\n\nLure back lapsed customers with strong offer and call to action.\nAsk why they haven’t returned.\nApologise and make corrections if necessary. Make the feel special if they do return.\n\nKey numbers - what gets measured gets managed 1. Leads 2. Conversion rate 3. Average transaction value 4. Break even point\nFor subscription models measure: 1. Monthly recurring revenue 2. Churn rate 3. Customer lifetime value\nPolluted revenue - dollars from toxic customers != dollars from raving fans. Types of customers: 1. Tribe 2. Churners - can’t afford you so drop you on price or intro offer ending 3. Vampires - suck up all your time, you can’t afford them 4. Snow leopard - big customer but not replicable\nNet promoter score - “How likely are you to recommend us to a friend?” - Detractors 0-6; Passive 7-8; Promoters 9-10 - NPS=+100 if everyone is a promoter, -100 if everyone is a detractor\nFire problem customers - “The customer is always right” is naive. It should be “the right customer is always right”. - Genuine customer complaints are valuable and may help retain other customers who had the same issue but didn’t speak up. - Low-value, price-sensitive customers complain the most. They take up your time and energy at the expense of your tribe. - Firing detractors creates scarcity and frees your time to focus on the tribe. With limited supply, customers have to play and pay by your rules.\n\n\n\nOrchestrating referrals is an active task, not needy or desperate, - Don’t just deliver a good product and hope for the best - People refer because it makes them feel good, helpful or knowledgeable, not to do you a favour. - Appeal to their ego, offer them something valuable, give them a reason to refer.\nAsk (for a referral) and ye shall receive. - “Law of 250” - most people know 250 people well enough to invite to a wedding or funeral. This is 250 potential referrals per customer. - Set expectation of referrals beforehand - “We’re going to do a great job and we need your help… Most people refer 3 others.”\nBe specific about who you ask and what kind of referral you want. Conquer the bystander effect.\nMake referral profiles for each customer group in your database. - Who do they know? - How can you make them remember you? - What will make them look good? - How will they provide value to their referral target\nCustomer buying behaviour - Who has your customers before you? - Joint venture for referrals - Sell/exchange leads, resell complementary products, affiliate referral partner\nBrand = personality of the business. Think of the business as a person: - Name - Design (what they wear) - Positioning (how they communicate) - Brand promise (core values) - Target market (who they associate with) - Brand awareness (how well known / popular)\nBest form of brand building is selling - Branding is what you do after someone has bought from you. - Brand equity - customers crossing the road to buy from you and walk past competitors.\n\n\n\nImplementation is crucial - Knowing and not doing is the same as not knowing. - Common pitfalls: 1. Paralysis by analysis - 80% out the door is better than 100% in the drawer; money loves speed. 2. Fail to delegate 3. “My business is different” - Spend your effort figuring out how to make it work rather than why it won’t work.\nTime is not money - Entrepreneurs get paid for the value they create, not the time or effort they put in. - Making money is a side effect of creating value.\nThe value of a business is the eyeballs it has access to.\nQuestions to ponder: - What business should I be in? - What technologies will disrupt it? - How can I take advantage of tech rather than fight it?"
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html",
    "href": "posts/business/public_speaking/public_speaking.html",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "Notes from reading “Ted Talks: The offical TED guide to public speaking” by Chris Anderson.\n\n\n\n\nPresentation literacy is its own skill. Your goal is to be you, not a version of someone you like.\n\n\n\nIdeas are all that matter in the talk. This can be a novel idea, or a novel story or presentation of an existing idea.\nYour goal is to recreate the core idea inside your audience’s mind. The language you use must be shared by the speaker and listener for it to have this effect. Focus on what you can give to the audience.\nVisualise the talk as a journey that you guide the user through. Start where the audience starts. Don’t make any impossible leaps or unexpected changes of direction.\n\nWhat issues matter the most\nHow are they related?\nHow can they be easily explained?\nWhat are the key open questions? Controversies?\n\n\n\n\n\nFour talk styles to AVOID.\n\nSales Pitch\n\nYou should give not take.\nGenerosity evokes a response.\n\nRamble\n\nInsults the audience; suggests that their time doesn’t matter to you.\n\nOrganisation bore\n\nYour company’s structure is boring.\nFocus on the work and ideas, not the company, products or individuals.\n\nInspiration tryhard\n\nAll style and no substance.\nComes across as manipulative.\nInspiration has to be earned. “Like love you don’t get it by pursuing it”.\nInspiration is the response to authenticity, courage, wisdom, selflessness.\n\n\n\n\n\nThe throughline is a connecting theme that ties the narrative elements - A focused, precise, unexpected idea - Not the same as a theme - You should be able to encapsulate it in &lt;15 words\nTrace the path the journey will take. If the audience knows where you’re going, it will be easier to follow.\nCut down the talk to keep the throughline focused. - DON’T just keep all the content but say it in less detail. Overstuffed = underexplained - Cut back the range of topics. Say less with more impact. - “The power of the deleted word”. - An 18 minute time limit is short enough to keep an audience’s attention, long enough to cover enough ground and precise enough to be taken seriously.\nThe overall structure can be thought of in 3 parts: What? So what? What now? In more detail, this is: 1. Intro - what will be covered 2. Context - why this issue matters 3. Main concepts 4. Practical implications\nTalk about ideas not issues. Write for an audience of one.\nChecklist: - Is this a topic I’m passionate about? Do I know enough to be worth the audience’s time? Do I have credibility? - Does it inspire curiosity? - Will knowing this make a difference to the audience? - Is the talk a gift or an ask? - Is the information fresh? - Can the topic be covered in enough detail with examples in the time slot? - What are the 15 words that encapsulate the talk? - Would those 15 words persuade someone they’re interested in hearing the talk?\n\n\n\n\n\nA human connection is required before you can inform, persuade or inspire. Knowledge has to be pulled in, not pushed in. Be AUTHENTIC.\nDos - Eye contact. - Vulnerability, but authentic. - Humour signals to the group that you’ve all connected.\nDon’ts - Ego breeds contempt. - Avoid tribal topics/language, e.g. politics.\n\n\n\nStorytelling helps people to imagine and dream -&gt; empathise -&gt; care\nKey elements: 1. Character 2. Tension 3. Right level of detail 4. Satisfying resolution\nIf you’re telling a story, know why you’re telling it. The goal is to give, not boost your ego.\nConnect the dots enough, but don’t force-feed the conclusion.\n\n\n\nSteps: 1. Start where the audience is. No assumed knowledge or jargon. 2. Spark curiosity. Create a knowledge gap that the listener wants to fill. 3. Introduce new concepts one-by-one. Name each concept. 4. Metaphors and analogies. Reveal the “shape” of new concepts. 5. Examples confirm understanding.\nKnowledge is built hierarchically. Explain the structure and where each concept fits. Convert a web of ideas into a string of words.\nCurse of knowledge. Revisit assumed knowledge.\nMake clear what the concept isn’t. An explanation is a small mental model in a large space of possibilities. Reduce the size of that space.\n\n\n\nDemolish the listener’s existing knowledge and rebuild it; replace it with something better. Genius is something that comes to you, not something you are.\nAn “intuition pump” is an example or metaphor that makes the conclusion more plausible. Not a rigorous argument, but nudges the listener in the right direction. Prime with emotion or story, then persuade with reason.\nTurn the audience into detectives. Tell a story. Start with a big mystery, traverse the possible solutions until only one plausible conclusion remains.\n\n\n\nThree approaches to revelatory talks:\n\nThe wonder walk\n\nReveal a succession of inspiration images\nObvious throughline with accessible language and silence\nThe message should be “how intriguing is this” not “look what I achieved”\n\nDemo\n\nInitial tease, background context, demo, implications\n\nDreamscape\n\nCreate a world that doesn’t exist but someday might\nPaint a bold picture of an alternative future\nMake others desire that future\nEmphasise human values not just clever tech, otherwise the audience might freak out at the possible negative implications of the technology\n\n\n\n\n\n\n\n\nSlides can distract. If you do use them, make sure they add something.\nVisuals should: - Reveal - set contecxt, prime, BAM! - Explain - one idea per slide with a clickbait headline not a summary. Highlight the point you’re making. - Delight - show impressive things and let them speak for themselves. Don’t need to explain every image/slide.\nDon’t let the slides be spoilers for what you’re about to say. The message loses its impact, it’s not news anymore.\n\n\n\nThe goal is to have a set structure that you speak naturally and authentically about.\nBoth approaches have the same result: 1. Script and memorise until natural, or 2. Freestyle around bullet points and rehearse until structure is set\nBeing read to and being talked to are very different experiences - Dictating the speech rather than writing it can help make sure it comes across how you would speak rather than how you would write. - Don’t end up in the uncanny valley between reading and speaking - In some cases, reading is powerful if it’s clear that this is a poetic, written piece. Abandon the script for an impactful finale.\nWhat you are saying matters more than the exact word choice.\nRehearse your impromptu remarks.\n\n\n\nPractice speaking by speaking!\nrehearse in phases: 1. Editing and cutting 2. Pacing and timing 3. Delivery\nAim to use &lt;90% of the time limit. results in tighter writing, and time to riff and enjoy the reaction.\n\n\n\nYou have the audience’s attention at the start, then it’s yours to lose. Opener has to capture attention and keep it. 2 stages: 1. 10 seconds to capture attention 2. 1 minute to hold it\nOpening techniques: 1. Drama 2. Curiosity - more specific questions are more intriguing 3. Compelling slide/image - “the next image changed my life” 4. Tense but don’t give away - show where you’re going but save the reveal\nThe closer dictates how the whole talk will be remembered.\nClosing techniques: 1. Camera pullback - big picture and implications 2. Call to action 3. Personal commitment 4. Inspiring values/vision 5. Encapsulation - neatly reframe the main idea 6. Narrative symmetry - callback to opener 7. Lyrical inspiration - poetic conclusion\n\n\n\n\n\n\n\nChoose an outfit early\nDress like the audience but smarter\nDress for the people in the back row\n\n\n\n\n\nEmbrace the nerves and draw attention to it. Vulnerability humanises you.\nPick friendly faces in the audience and speak to them.\nBackup plan - have a story ready to fill in during unexpected technical issues\nFocus on the message - “THIS MATTERS”\n\n\n\n\nVisual barriers between the speaker and audience can create a sense of authority but at the expense of a human connection. If they can see you, you’re vulnerable. If you’re vulnerable, they can connect.\nReferring to notes is fine if done sparingly and unambiguously. It humanises you if done honestly. It appears sneaky and dishonest if you try to do it secretly.\n\n\n\nSpeaking style adds another stream of input parallel to the words themselves. It tells the listener how they should interpret the words.\nSix voice tools. Vary each of them depending on the meaning and emotion. 1. Volume 2. Pitch 3. Pace 4. Timbre 5. Tone 6. Prosody (singsong rise and fall)\nSpeed should be a conversational pace, approx 130-170 words per minute. - People often worry about speaking too fast - Speaking too slow is a more common problem (overcorrection?) - Understanding outpaces articulation, the listener is “dying of word starvation”\nSpeak, don’t orate.\nBody language - Stand and move intentionally. - Stop to make a point then walk to the next point.\n\n\n\nAdd too many ingredients and you risk losing attention. What you say can just be signposts for what you show - set it up, show it, shut up.\n\n\n\n\n\n\nKnowledge of specific facts is becoming more specialised and commoditised. Understanding how everything fits together is becoming broader, more unified.\nConcepts of specialised knowledge are outdated, stemming from industrial age thinknig. Modern thinking values: 1. Contextual knowledge 2. Creating knowledge 3. Understanding of humanity\nAs people become more interconnected, innovation becomes crowd-accelerated.\nHappiness is finding something bigger than you and dedicating your life to it. Nudge the world - give it questions to spark conversations. “The future is not yet written, we are all collectively in the process of writing it”"
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html#foundation",
    "href": "posts/business/public_speaking/public_speaking.html#foundation",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "Presentation literacy is its own skill. Your goal is to be you, not a version of someone you like.\n\n\n\nIdeas are all that matter in the talk. This can be a novel idea, or a novel story or presentation of an existing idea.\nYour goal is to recreate the core idea inside your audience’s mind. The language you use must be shared by the speaker and listener for it to have this effect. Focus on what you can give to the audience.\nVisualise the talk as a journey that you guide the user through. Start where the audience starts. Don’t make any impossible leaps or unexpected changes of direction.\n\nWhat issues matter the most\nHow are they related?\nHow can they be easily explained?\nWhat are the key open questions? Controversies?"
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html#common-traps",
    "href": "posts/business/public_speaking/public_speaking.html#common-traps",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "Four talk styles to AVOID.\n\nSales Pitch\n\nYou should give not take.\nGenerosity evokes a response.\n\nRamble\n\nInsults the audience; suggests that their time doesn’t matter to you.\n\nOrganisation bore\n\nYour company’s structure is boring.\nFocus on the work and ideas, not the company, products or individuals.\n\nInspiration tryhard\n\nAll style and no substance.\nComes across as manipulative.\nInspiration has to be earned. “Like love you don’t get it by pursuing it”.\nInspiration is the response to authenticity, courage, wisdom, selflessness."
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html#the-throughline",
    "href": "posts/business/public_speaking/public_speaking.html#the-throughline",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "The throughline is a connecting theme that ties the narrative elements - A focused, precise, unexpected idea - Not the same as a theme - You should be able to encapsulate it in &lt;15 words\nTrace the path the journey will take. If the audience knows where you’re going, it will be easier to follow.\nCut down the talk to keep the throughline focused. - DON’T just keep all the content but say it in less detail. Overstuffed = underexplained - Cut back the range of topics. Say less with more impact. - “The power of the deleted word”. - An 18 minute time limit is short enough to keep an audience’s attention, long enough to cover enough ground and precise enough to be taken seriously.\nThe overall structure can be thought of in 3 parts: What? So what? What now? In more detail, this is: 1. Intro - what will be covered 2. Context - why this issue matters 3. Main concepts 4. Practical implications\nTalk about ideas not issues. Write for an audience of one.\nChecklist: - Is this a topic I’m passionate about? Do I know enough to be worth the audience’s time? Do I have credibility? - Does it inspire curiosity? - Will knowing this make a difference to the audience? - Is the talk a gift or an ask? - Is the information fresh? - Can the topic be covered in enough detail with examples in the time slot? - What are the 15 words that encapsulate the talk? - Would those 15 words persuade someone they’re interested in hearing the talk?"
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html#talk-tools",
    "href": "posts/business/public_speaking/public_speaking.html#talk-tools",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "A human connection is required before you can inform, persuade or inspire. Knowledge has to be pulled in, not pushed in. Be AUTHENTIC.\nDos - Eye contact. - Vulnerability, but authentic. - Humour signals to the group that you’ve all connected.\nDon’ts - Ego breeds contempt. - Avoid tribal topics/language, e.g. politics.\n\n\n\nStorytelling helps people to imagine and dream -&gt; empathise -&gt; care\nKey elements: 1. Character 2. Tension 3. Right level of detail 4. Satisfying resolution\nIf you’re telling a story, know why you’re telling it. The goal is to give, not boost your ego.\nConnect the dots enough, but don’t force-feed the conclusion.\n\n\n\nSteps: 1. Start where the audience is. No assumed knowledge or jargon. 2. Spark curiosity. Create a knowledge gap that the listener wants to fill. 3. Introduce new concepts one-by-one. Name each concept. 4. Metaphors and analogies. Reveal the “shape” of new concepts. 5. Examples confirm understanding.\nKnowledge is built hierarchically. Explain the structure and where each concept fits. Convert a web of ideas into a string of words.\nCurse of knowledge. Revisit assumed knowledge.\nMake clear what the concept isn’t. An explanation is a small mental model in a large space of possibilities. Reduce the size of that space.\n\n\n\nDemolish the listener’s existing knowledge and rebuild it; replace it with something better. Genius is something that comes to you, not something you are.\nAn “intuition pump” is an example or metaphor that makes the conclusion more plausible. Not a rigorous argument, but nudges the listener in the right direction. Prime with emotion or story, then persuade with reason.\nTurn the audience into detectives. Tell a story. Start with a big mystery, traverse the possible solutions until only one plausible conclusion remains.\n\n\n\nThree approaches to revelatory talks:\n\nThe wonder walk\n\nReveal a succession of inspiration images\nObvious throughline with accessible language and silence\nThe message should be “how intriguing is this” not “look what I achieved”\n\nDemo\n\nInitial tease, background context, demo, implications\n\nDreamscape\n\nCreate a world that doesn’t exist but someday might\nPaint a bold picture of an alternative future\nMake others desire that future\nEmphasise human values not just clever tech, otherwise the audience might freak out at the possible negative implications of the technology"
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html#preparation-process",
    "href": "posts/business/public_speaking/public_speaking.html#preparation-process",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "Slides can distract. If you do use them, make sure they add something.\nVisuals should: - Reveal - set contecxt, prime, BAM! - Explain - one idea per slide with a clickbait headline not a summary. Highlight the point you’re making. - Delight - show impressive things and let them speak for themselves. Don’t need to explain every image/slide.\nDon’t let the slides be spoilers for what you’re about to say. The message loses its impact, it’s not news anymore.\n\n\n\nThe goal is to have a set structure that you speak naturally and authentically about.\nBoth approaches have the same result: 1. Script and memorise until natural, or 2. Freestyle around bullet points and rehearse until structure is set\nBeing read to and being talked to are very different experiences - Dictating the speech rather than writing it can help make sure it comes across how you would speak rather than how you would write. - Don’t end up in the uncanny valley between reading and speaking - In some cases, reading is powerful if it’s clear that this is a poetic, written piece. Abandon the script for an impactful finale.\nWhat you are saying matters more than the exact word choice.\nRehearse your impromptu remarks.\n\n\n\nPractice speaking by speaking!\nrehearse in phases: 1. Editing and cutting 2. Pacing and timing 3. Delivery\nAim to use &lt;90% of the time limit. results in tighter writing, and time to riff and enjoy the reaction.\n\n\n\nYou have the audience’s attention at the start, then it’s yours to lose. Opener has to capture attention and keep it. 2 stages: 1. 10 seconds to capture attention 2. 1 minute to hold it\nOpening techniques: 1. Drama 2. Curiosity - more specific questions are more intriguing 3. Compelling slide/image - “the next image changed my life” 4. Tense but don’t give away - show where you’re going but save the reveal\nThe closer dictates how the whole talk will be remembered.\nClosing techniques: 1. Camera pullback - big picture and implications 2. Call to action 3. Personal commitment 4. Inspiring values/vision 5. Encapsulation - neatly reframe the main idea 6. Narrative symmetry - callback to opener 7. Lyrical inspiration - poetic conclusion"
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html#on-stage",
    "href": "posts/business/public_speaking/public_speaking.html#on-stage",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "Choose an outfit early\nDress like the audience but smarter\nDress for the people in the back row\n\n\n\n\n\nEmbrace the nerves and draw attention to it. Vulnerability humanises you.\nPick friendly faces in the audience and speak to them.\nBackup plan - have a story ready to fill in during unexpected technical issues\nFocus on the message - “THIS MATTERS”\n\n\n\n\nVisual barriers between the speaker and audience can create a sense of authority but at the expense of a human connection. If they can see you, you’re vulnerable. If you’re vulnerable, they can connect.\nReferring to notes is fine if done sparingly and unambiguously. It humanises you if done honestly. It appears sneaky and dishonest if you try to do it secretly.\n\n\n\nSpeaking style adds another stream of input parallel to the words themselves. It tells the listener how they should interpret the words.\nSix voice tools. Vary each of them depending on the meaning and emotion. 1. Volume 2. Pitch 3. Pace 4. Timbre 5. Tone 6. Prosody (singsong rise and fall)\nSpeed should be a conversational pace, approx 130-170 words per minute. - People often worry about speaking too fast - Speaking too slow is a more common problem (overcorrection?) - Understanding outpaces articulation, the listener is “dying of word starvation”\nSpeak, don’t orate.\nBody language - Stand and move intentionally. - Stop to make a point then walk to the next point.\n\n\n\nAdd too many ingredients and you risk losing attention. What you say can just be signposts for what you show - set it up, show it, shut up."
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html#reflection",
    "href": "posts/business/public_speaking/public_speaking.html#reflection",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "Knowledge of specific facts is becoming more specialised and commoditised. Understanding how everything fits together is becoming broader, more unified.\nConcepts of specialised knowledge are outdated, stemming from industrial age thinknig. Modern thinking values: 1. Contextual knowledge 2. Creating knowledge 3. Understanding of humanity\nAs people become more interconnected, innovation becomes crowd-accelerated.\nHappiness is finding something bigger than you and dedicating your life to it. Nudge the world - give it questions to spark conversations. “The future is not yet written, we are all collectively in the process of writing it”"
  },
  {
    "objectID": "posts/business/pitching/pitching.html",
    "href": "posts/business/pitching/pitching.html",
    "title": "Pitching Notes",
    "section": "",
    "text": "Notes from “Pitch Anything” by Oren Klaff.\nSome of the book comes across as a bit incel sigma vibes, talking about alphas and betas. I don’t agree with it, but summarising it here for the parts that are interesting.\n\n\nFrames, AKA perspective AKA point of view, describe a person’s mental model of controlling the world. The person who owns the frame owns the conversation.\nInformation must penetrate through the following layers of the brain in order. 1. Survival - “crocodile brain”. Ignore anything that isn’t new, exciting, dangerous, unexpected. Needs big picture, high contrast points. 2. Social 3. Reason\nLogic won’t reach the “reason” part of the brain until the croc brain and social brain are satiated. Data is perceived as a threat by layer 1. the croc brain needs unexpected information that is concrete not abstract.\nSTRONG process: - Set the frame - Tell the story - Reveal the intrigue - Offer the prize - Nail the hook point - Get the deal\n\n\n\nFrames are the mental structure that shape how you see the world. When frames collide, the stronger frame absorbs the weaker frame. If you have to explain your authority, power, position, leverage then you have the weaker frame.\nOpposing frames and how to counter them. 1. Power frame - fuelled by status and ego - Don’t react to our strengthen their status - Acts of denial/defiance - establish your own rituals of power rather than abiding by theirs. - Mildly shocking but not unfriendly. Humour is key. 2. Prize frame - Make yourself the prize. Make them qualify themselves to you. The money has to earn you, not the other way around. - Make statements not “trial close” questions 3. Time frame - occurs later in the interaction - Stop as soon as you are done OR have lost their attention. Continuing further signals desperation. 4. Analyst frame - stats, technical details - Intrigue frame counters this - The brain can’t perform cold cognitions (analysis) at the same time as hot cognitions (excitement) - Tell a brief, relevant, intriguing story with tension involving you at the centre of it to break the cold cognition.\n\n\n\nGlobal status is a function of wealth, power and status.\nBeta traps enforce your lower status. E.g. making you wait, big dogs not showing up to meetings, small talk before meetings.\nSituational status is the temporary shift of status based on the situation. Create local star power to increase your situational status. - Ignore power rituals, avoid beta traps, ignore their global status - Frame control - small, friendly acts of defiance - Local star power - shift conversation to a topic where you are the domain expert - Prize frame - position yourself as the reward, e.g. leave after a hard cut off time - Confirm your status - make them say you’re the alpha\nShock -&gt; frame control -&gt; local star power -&gt; hook point -&gt; leave\n\n\n\nEvery pitch should tell a story.\nFour sections of the pitch: 1. Introduce yourself and the big idea (5 mins) 2. Explain the budget and secret sauce (10 mins) 3. Offer the deal (2 mins) 4. Stack hot cognition frames (3 mins)\n\n\nKeep the pitch short - 20 mins. - Announce the time constraint at the start to appease the croc brain. Otherwise people don’t know how long they are trapped for and panic. - DNA presentation by Crick and Watson was 5 mins\nIntro should be highlights of successes NOT a life story. - The impression you leave is based the average not the sum, so don’t dilute it.\nThe “why now?” frame. 3 market forces pattern: 1. Economic 2. Social 3. Technology\nTell the story of how your idea came to be. - Movement is key to overcoming change blindness. Don’t just show 2 states, describe the transition between them. - Trends, impacts of those trends, how have these opened a (temporary) market window (time frame). - This story places you as the hero at the centre of solving this problem (prize frame).\nIdea introduction pattern: &gt; For [target customers] &gt; &gt; Who are dissatisfied with [current offerings in the market] &gt; &gt; My idea/product is a [new idea or product category] &gt; &gt; That provides [key problem/solution features] &gt; &gt; Unlike [the competing product] &gt; &gt; My idea/product is [describe key features]\n\n\n\nTune the message to the croc brain of the target. - Simplicity can come across as naive, but focus on describing concrete things like relationships between people. This is high-level enough that it will not engage cold cognitions or threaten the croc brain.\nAttention = Desire (anticipation of reward) + Tension (fear of loss, introduce consequences) - Attention is high when novelty is high. - Push/pull patterns to increase tension and desire - The pitch narrative is a series of tension loops. - When the tension is released and attention is lost the pitch is over. - Desire eventually becomes fear, so keep the pitch short.\nMust haves: - Numbers and projections - This should demonstrate your skill at budgeting (a difficult, precise skill), not your skill at projecting (a simple, inaccurate skill). - Competition - How easy is it for new competitors to enter? - How easy is it for customers to switch TO you and AWAY from you? - Secret sauce - Your competitive advantage. - Phrasing it this way frames you as the prize.\n\n\n\nDescribe what they get if they do business with you - What you will deliver, when, and how. - What are their roles, responsibilities and next steps.\n\n\n\n\nPropose something concrete and actionable in such a compelling way that the target wants to chase it. Don’t wait for the target to evaluate you on the spot at the end of the pitch as that triggers cold cognitions. Pile up the hot cognitions instead.\nA hot cognition is emotional rather than analytical. - You decide that you like something before you understand it. - Data is used to justify decisions after the fact. - Hot cognitions are generally unavoidable; you may be able to control the expression of emotion, but the initial experience is still there. - Hot cognitions tend to be instant and enduring. - Hot vs cold cognitions can be characterised by spinach vs chocolate - you know one is good for you but you want the other anyway. - Hot cognitions are “knowing” something through feeling it, cold cognitions are “knowing something through evaluating it.”\nStacking frames. - We want to stack frames to create “wanting”, an emotional response - This is not the same as getting the target to “like” us, as that is a slow, intellectual cold cognition. - Appealing to the analytical brain can only trigger a “like”; we want to trigger a “want” in the croc brain.\nThe 4 frame stack of hot cognitions: 1. The intrigue frame - A story where most important things are who it happened to and how the characters reacted to their situation. - The events don’t need to be extreme but the characters’ reactions should be. - A narrative that feels correct in time with convey a strong sense of truth and accuracy. - “Man in the jungle” formula - Put a man in the jungle; have beasts attack him; will he get to safety?; get the man to the edge of the jungle but don’t get him out of it (cliffhanger) 2. The prize frame - Position yourself as the most important party in the deal. Conviction is key. - “I am the prize. You are trying to impress me. You are trying to win my approval.” 3. The time frame - This creates a scarcity bias that triggers fear that triggers action. - Extreme time pressure feels forced and cutrate. There is a balance between pressure and fairness; there should be a real-world time constraint. 4. The moral authority frame\nOnce the frame stacking is complete, you have the target’s attention for about 30 seconds.\n\n\n\nValidation-seeking behaviour (i.e. neediness) is the number one deal killer. - Neediness is a threat signal, it triggers fear and uncertainty in the target which triggers self-protection.\nAvoid “soft closes” aiming to seek validation, e.g. “So what do you think so far?”, “Do you still think this is a good deal?”\nCauses of neediness: - Anxiety and insecurity turn to fear, so you resort to acceptance-seeking behaviour to confirm that you’re still “part of the pack” - Response to disappointment is to seek validation - We want something only the target can give us - We need cooperation from the target, and the lack of cooperation causes anxiety - We believe the target can make us feel good by accepting our offer\nCounteracting validation-seeking behaviours: 1. Want nothing 2. Focus only on things you do well 3. Announce your intention to leave - time frame. When you finish the pitch, withdraw and deny your audience; this creates a prize frame.\nMantra: “I don’t need these people, they need me. I am the prize”.\nExample: 1. Time frame - “The deal will be fully subscribed in the next 10 days”. 2. Power frame - “We don’t need VC money but want a big name on cap sheet for later IPO” 3. Prize frame - “Are you really the right investor for this? We need to know more about the relationships and value you can bring.”\n\n\n\nYou are not pushing, you are interacting with people using basic rules of social dynamics.\nCroc brain: - Boring: Ignore it - Dangerous? Fight it or run - Complicated? Radically summarise\nThe presenter’s problem boils down to deciding which information to include. It is not like an engineering problem where more information is better.\nThree key insights: 1. Appeal to the croc brain 2. Control frames 3. Use humour - this is not to relieve tension but to signal that you are so confident that you’re able to play around a little\nIf the pitch stops being fun, you’re doing it wrong.\nSteps to learning the method: 1. Recognise beta traps; anything designed to control your behaviour 2. Step around beta traps 3. Identify social frames 4. Initiate frame collisions with safe targets - always with good humour and “a twinkle in your eye” 5. Practice the push/pull of small acts of defiance and denial\nThe only rule is that you make the rules that others follow."
  },
  {
    "objectID": "posts/business/pitching/pitching.html#the-method",
    "href": "posts/business/pitching/pitching.html#the-method",
    "title": "Pitching Notes",
    "section": "",
    "text": "Frames, AKA perspective AKA point of view, describe a person’s mental model of controlling the world. The person who owns the frame owns the conversation.\nInformation must penetrate through the following layers of the brain in order. 1. Survival - “crocodile brain”. Ignore anything that isn’t new, exciting, dangerous, unexpected. Needs big picture, high contrast points. 2. Social 3. Reason\nLogic won’t reach the “reason” part of the brain until the croc brain and social brain are satiated. Data is perceived as a threat by layer 1. the croc brain needs unexpected information that is concrete not abstract.\nSTRONG process: - Set the frame - Tell the story - Reveal the intrigue - Offer the prize - Nail the hook point - Get the deal"
  },
  {
    "objectID": "posts/business/pitching/pitching.html#frame-control",
    "href": "posts/business/pitching/pitching.html#frame-control",
    "title": "Pitching Notes",
    "section": "",
    "text": "Frames are the mental structure that shape how you see the world. When frames collide, the stronger frame absorbs the weaker frame. If you have to explain your authority, power, position, leverage then you have the weaker frame.\nOpposing frames and how to counter them. 1. Power frame - fuelled by status and ego - Don’t react to our strengthen their status - Acts of denial/defiance - establish your own rituals of power rather than abiding by theirs. - Mildly shocking but not unfriendly. Humour is key. 2. Prize frame - Make yourself the prize. Make them qualify themselves to you. The money has to earn you, not the other way around. - Make statements not “trial close” questions 3. Time frame - occurs later in the interaction - Stop as soon as you are done OR have lost their attention. Continuing further signals desperation. 4. Analyst frame - stats, technical details - Intrigue frame counters this - The brain can’t perform cold cognitions (analysis) at the same time as hot cognitions (excitement) - Tell a brief, relevant, intriguing story with tension involving you at the centre of it to break the cold cognition."
  },
  {
    "objectID": "posts/business/pitching/pitching.html#status",
    "href": "posts/business/pitching/pitching.html#status",
    "title": "Pitching Notes",
    "section": "",
    "text": "Global status is a function of wealth, power and status.\nBeta traps enforce your lower status. E.g. making you wait, big dogs not showing up to meetings, small talk before meetings.\nSituational status is the temporary shift of status based on the situation. Create local star power to increase your situational status. - Ignore power rituals, avoid beta traps, ignore their global status - Frame control - small, friendly acts of defiance - Local star power - shift conversation to a topic where you are the domain expert - Prize frame - position yourself as the reward, e.g. leave after a hard cut off time - Confirm your status - make them say you’re the alpha\nShock -&gt; frame control -&gt; local star power -&gt; hook point -&gt; leave"
  },
  {
    "objectID": "posts/business/pitching/pitching.html#pitching-the-big-idea",
    "href": "posts/business/pitching/pitching.html#pitching-the-big-idea",
    "title": "Pitching Notes",
    "section": "",
    "text": "Every pitch should tell a story.\nFour sections of the pitch: 1. Introduce yourself and the big idea (5 mins) 2. Explain the budget and secret sauce (10 mins) 3. Offer the deal (2 mins) 4. Stack hot cognition frames (3 mins)\n\n\nKeep the pitch short - 20 mins. - Announce the time constraint at the start to appease the croc brain. Otherwise people don’t know how long they are trapped for and panic. - DNA presentation by Crick and Watson was 5 mins\nIntro should be highlights of successes NOT a life story. - The impression you leave is based the average not the sum, so don’t dilute it.\nThe “why now?” frame. 3 market forces pattern: 1. Economic 2. Social 3. Technology\nTell the story of how your idea came to be. - Movement is key to overcoming change blindness. Don’t just show 2 states, describe the transition between them. - Trends, impacts of those trends, how have these opened a (temporary) market window (time frame). - This story places you as the hero at the centre of solving this problem (prize frame).\nIdea introduction pattern: &gt; For [target customers] &gt; &gt; Who are dissatisfied with [current offerings in the market] &gt; &gt; My idea/product is a [new idea or product category] &gt; &gt; That provides [key problem/solution features] &gt; &gt; Unlike [the competing product] &gt; &gt; My idea/product is [describe key features]\n\n\n\nTune the message to the croc brain of the target. - Simplicity can come across as naive, but focus on describing concrete things like relationships between people. This is high-level enough that it will not engage cold cognitions or threaten the croc brain.\nAttention = Desire (anticipation of reward) + Tension (fear of loss, introduce consequences) - Attention is high when novelty is high. - Push/pull patterns to increase tension and desire - The pitch narrative is a series of tension loops. - When the tension is released and attention is lost the pitch is over. - Desire eventually becomes fear, so keep the pitch short.\nMust haves: - Numbers and projections - This should demonstrate your skill at budgeting (a difficult, precise skill), not your skill at projecting (a simple, inaccurate skill). - Competition - How easy is it for new competitors to enter? - How easy is it for customers to switch TO you and AWAY from you? - Secret sauce - Your competitive advantage. - Phrasing it this way frames you as the prize.\n\n\n\nDescribe what they get if they do business with you - What you will deliver, when, and how. - What are their roles, responsibilities and next steps."
  },
  {
    "objectID": "posts/business/pitching/pitching.html#stack-hot-cognition-frames-3-mins",
    "href": "posts/business/pitching/pitching.html#stack-hot-cognition-frames-3-mins",
    "title": "Pitching Notes",
    "section": "",
    "text": "Propose something concrete and actionable in such a compelling way that the target wants to chase it. Don’t wait for the target to evaluate you on the spot at the end of the pitch as that triggers cold cognitions. Pile up the hot cognitions instead.\nA hot cognition is emotional rather than analytical. - You decide that you like something before you understand it. - Data is used to justify decisions after the fact. - Hot cognitions are generally unavoidable; you may be able to control the expression of emotion, but the initial experience is still there. - Hot cognitions tend to be instant and enduring. - Hot vs cold cognitions can be characterised by spinach vs chocolate - you know one is good for you but you want the other anyway. - Hot cognitions are “knowing” something through feeling it, cold cognitions are “knowing something through evaluating it.”\nStacking frames. - We want to stack frames to create “wanting”, an emotional response - This is not the same as getting the target to “like” us, as that is a slow, intellectual cold cognition. - Appealing to the analytical brain can only trigger a “like”; we want to trigger a “want” in the croc brain.\nThe 4 frame stack of hot cognitions: 1. The intrigue frame - A story where most important things are who it happened to and how the characters reacted to their situation. - The events don’t need to be extreme but the characters’ reactions should be. - A narrative that feels correct in time with convey a strong sense of truth and accuracy. - “Man in the jungle” formula - Put a man in the jungle; have beasts attack him; will he get to safety?; get the man to the edge of the jungle but don’t get him out of it (cliffhanger) 2. The prize frame - Position yourself as the most important party in the deal. Conviction is key. - “I am the prize. You are trying to impress me. You are trying to win my approval.” 3. The time frame - This creates a scarcity bias that triggers fear that triggers action. - Extreme time pressure feels forced and cutrate. There is a balance between pressure and fairness; there should be a real-world time constraint. 4. The moral authority frame\nOnce the frame stacking is complete, you have the target’s attention for about 30 seconds."
  },
  {
    "objectID": "posts/business/pitching/pitching.html#eradicating-neediness",
    "href": "posts/business/pitching/pitching.html#eradicating-neediness",
    "title": "Pitching Notes",
    "section": "",
    "text": "Validation-seeking behaviour (i.e. neediness) is the number one deal killer. - Neediness is a threat signal, it triggers fear and uncertainty in the target which triggers self-protection.\nAvoid “soft closes” aiming to seek validation, e.g. “So what do you think so far?”, “Do you still think this is a good deal?”\nCauses of neediness: - Anxiety and insecurity turn to fear, so you resort to acceptance-seeking behaviour to confirm that you’re still “part of the pack” - Response to disappointment is to seek validation - We want something only the target can give us - We need cooperation from the target, and the lack of cooperation causes anxiety - We believe the target can make us feel good by accepting our offer\nCounteracting validation-seeking behaviours: 1. Want nothing 2. Focus only on things you do well 3. Announce your intention to leave - time frame. When you finish the pitch, withdraw and deny your audience; this creates a prize frame.\nMantra: “I don’t need these people, they need me. I am the prize”.\nExample: 1. Time frame - “The deal will be fully subscribed in the next 10 days”. 2. Power frame - “We don’t need VC money but want a big name on cap sheet for later IPO” 3. Prize frame - “Are you really the right investor for this? We need to know more about the relationships and value you can bring.”"
  },
  {
    "objectID": "posts/business/pitching/pitching.html#closing-thoughts",
    "href": "posts/business/pitching/pitching.html#closing-thoughts",
    "title": "Pitching Notes",
    "section": "",
    "text": "You are not pushing, you are interacting with people using basic rules of social dynamics.\nCroc brain: - Boring: Ignore it - Dangerous? Fight it or run - Complicated? Radically summarise\nThe presenter’s problem boils down to deciding which information to include. It is not like an engineering problem where more information is better.\nThree key insights: 1. Appeal to the croc brain 2. Control frames 3. Use humour - this is not to relieve tension but to signal that you are so confident that you’re able to play around a little\nIf the pitch stops being fun, you’re doing it wrong.\nSteps to learning the method: 1. Recognise beta traps; anything designed to control your behaviour 2. Step around beta traps 3. Identify social frames 4. Initiate frame collisions with safe targets - always with good humour and “a twinkle in your eye” 5. Practice the push/pull of small acts of defiance and denial\nThe only rule is that you make the rules that others follow."
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html",
    "href": "posts/software/system_design/system_design_notes.html",
    "title": "System Design Notes",
    "section": "",
    "text": "Steps: 1. Requirements engineering 2. Capacity estimation 3. Data modeling 4. API design 5. System design 6. Design discussion\n\n\nFunctional and non-functional requirements. Scale of the system.\n\n\nWhat the system should do. - Which problem does the system solve - Which features are essential to solve these problems\nCore features: bare minimum required to solve the user’s problem. Support features: features that help the user solve their problem more conveniently.\n\n\n\nWhat the system should handle.\n\nInterview tips - Identify the non-functional requirements the interviewer cares most about - Show awareness of system attributes, trade-offs and user experience\n\nSystem analysis: - Read or write heavy? - Impacts requirements for database scaling, redundancy of services, effectiveness of caching. - System priorities - is it worse if the system fails to read or to write? - Monolithic or distributed architecture? - Scale is the key consideration. Large scale applications must be distributed, smaller scale can be single server. - Data availability vs consistency - CAP trilemma - A system can have only two of: Consistency, Availability, Partition tolerance - Choice determines the impact of a network failure\nNon-functional requirements: - Availability - How long is the system up and running per year? - Availability of a system is the product of its components’ availability - Reliability, redundancy, fault tolerance - Consistency - Data appears the same on all nodes regardless of the user and location. - Linearizability - Scalability - Ability of a system to handle fast-growing user base and temporary load spikes - Vertical scaling - single server that we add more CPU/RAM to - Pros: fast inter-process communication, data consistency - Cons: single point of failure, hardware limits on maximum scale - Horizontal scaling - cluster of servers in parallel - Pros: redundancy improves availability, linear cost of scaling - Cons: complex architecture, data inconsistency - Latency - Amount of time taken for a single message to be delivered - Experienced latency = network latency + system latency - Database and data model choices affect latency - Caching can be effective - Compatibility - Ability of a system to operate seamlessly with other software, hardware and systems - Security - Basic security measures: TLS encrypted network traffic, API keys for rate limiting\nNon-functional requirements depend heavily on expected scale. The following help quantify expected scale, and relate to capacity estimation in the next step. - Daily active users (DAU) - Peak active users - Interactions per user - including read/write ratio - Request size - Replication factor - determines the storage requirement (typically 3x)\n\n\n\n\n\nInterview tips - Simplify wherever you can - simple assumptions and round heavily - Convert all numbers to scientific notation - Know powers of 2\n\n\n\nRequests per second (RPS) is the key measure.\nRequests per day = Daily active users * Activities per user\nRequests per second = RPD / 10^5\nPeak load = RPS * Peak load factor\nRead/write ratio is important to get the total requests. Usually, information on either reads or writes is missing and must be inferred using the other.\nTotal RPS = Read RPS + Write RPS\n\n\n\nThe amount of data that can be transmitted between two nodes in a fixed period of time. Bits per second.\nThroughput, bandwidth and latency are all related and can be thought of with the motorway analogy. - Throughput is the total number of vehicles that must pass through that road per day. - Bandwidth is the number of lanes. - Latency is the time taken to get from one end of the road to another.\nThe bandwidth must be large enough to support the required throughput.\nBandwidth = Total RPS * Request size\nEven high bandwidth can result in low latency if there is an unexpectedly high peak throughput. Rush hour traffic jam.\nRequest size can be varied according to bandwidth, e.g. YouTube lowers the resolution if the connection is slow.\n\n\n\nMeasured in bits per second using the Write RPS. Long term storage (assuming the same user base) is also helpful.\nStorage capacity per second = Write RPS * Request size * Replication factor\nStorage capacity in 5 years = Storage capacity per second * 2*10^8 \n\n\n\n\nKey concepts are: entities, attributes, relations.\nOptimisation discussions are anchored by the data model. Relational databases: denormalization, SQL tuning, sharding, federation Non-relational databases: indexing, caching\nThe data model is not a full-blown data schema (which only applies to relational databases) but a more informal, high-level version.\nMust haves: 1. Derive entities and attributes. 2. Draw connections between entities. Then: 3. Select a database based on the requirements 4. Extend data model to a proper data schema 5. Identify bottlenecks and apply quick fixes Later: 6. Defer any detailed discussions to the design discussion section.\n\n\nEntities are “things” with a distinct and independent existence, e.g. users, files, tweets, posts, channels. Create a unique table for each entity.\nAttributes describe properties of an entity, e.g. a user’s name, date of birth. These are the fields in the table of that entity.\n\nInterview tips - Do not focus on irrelevant details - Do not miss any critical functionality - Double check all tables after you extract all info from requirements\n\n\n\n\nExtract connections between entities from the requirements. Relationships can be one-to-one, one-to-many or many-to-many.\n\n\n\n\nSpecify the API endpoints with: - Function signature - Parameters - Response and status codes\nThis specifies a binding contract between the client and the application server. There can be APIs between every system component, but just focus on the client/server interface.\nProcess: 1. Revisit the functional requirements 2. Derive the goal of each endpoint 3. Derive the signature from the goal and the inputs from the data model 4. Define the response outputs\nNaming conventions. Use HTTP request types in the call signature where appropriate: GET, POST, PUT, PATCH, DELETE\n\nInterview tips - Avoid vague inputs - Don’t add extra parameters that are out of scope - Avoid redundant parameters - Don’t miss any parameters - Confirm the output repsonse satisfies the requirements - Identify inefficiencies of the data structure\n\n\n\n\nSystem components: - Representation layer - Web app - Mobile app - Console - Service (and load balancer) - Data store - Relational database - Pros: linked tables reduce duplicate data; flexible queries - Key-value store - Useful for lots of small continuous reads and writes - Pros: Fast access, lightweight, highly scalable - Cons: Cannot query by value, no standard query language so data access in written in the application layer no data normalisation\nScaling: - Scale services by having multiple instances of the service with a load balancer in front - Scale database with federation (functional partitioning) - split up data by function\nSystem diagrams: arrows point in direction of user flow (not data flow)\n\n\n\nTypes of questions: - NFR questions - How to achieve scalability - Identify each bottleneck and remedy it - How to improve resiliency - Justification question - Choice of components, e.g. what would change if you changed block storage for object storage or relational database - Additional components, e.g. is there a use case for a message queue and where - Extension questions - Additional functional requirements not in the original scope\n\n\n\n\n\nThere are two parameters we can vary: 1. Length of the key - should be limited so the URL is short enough 2. Range of allowed characters - should be URL-safe\nUsing Base-64 encoding as the character range, then a 6-digit key length gives enough unique URLs; 64^6 = 68 billion\nEncoding options: 1. MD5 hash: not collision resistant and too long 2. Encoded counter, each URL is just an index int, and its value is the base-64 encoding of that int: length is not fixed and increases over time 3. Key range, generate all unique keys beforehand: no collisions, but storage intensive 4. Key range modified, generate 5-digit keys beforehand, then add the 6th digit on the fly.\n\n\n\n\n\n4 main operations (CRUD): Create, Read, Update, Delete.\nTransactions - group several operations together - Either the entire transaction succeeds or it fails and rolls back to the initial state. Commit or abort on error. - Without transactions, if an individual operation fails, it can be complex to unwind the related transactions to return to the initial state. - Error handling is simpler as manual rollback of operations is not required. - Transactions provide guarantees so we can reason about the database state before and after the transaction.\nTransactions enforce “ACID” guarantees: - Atomicity - Transactions cannot be broken down into smaller parts. - Consistency - All data points within the database must align to be properly read and accepted. Raise a consistency error if this is not the case. Database consistency is different from system consistency which states the data should be the same across all nodes. - Isolation - Concurrently executing transactions are isolated from each other. Avoids race conditions if multiple clients are accessing the same database record. - Durability - Once a transaction is committed to the database, it is permanently preserved. Backups and transaction logs can restore committed transactions in the case of a failure.\nRelational databases are a good fit when: - Data is well-structured - Use case requires complex querying - Data consistency is important\nLimitations - Horizontal scaling is complex - Distributed databases are more complex to keep transaction guarantees - Two phase commit protocol (2PC): 1. Prepare - Ask each node if it;s able to promise to carry out the transaction 2. Commit - Block the nodes and do the commit - Blocking causes problems on distributed systems where it may cause unexpected consequences if the database is unavailable for this short time.\n\n\n\nExamples of non-relational databases: - Key-value store - Document store - Wide-column store - Graph store\nThese vary a lot, and in general NoSQL simply means “no ACID guarantees”.\nTransactions were seen as the enemy of scalability, so needed to be abandoned completely for performance and scalability. ACID enforces consistency for relational databases; for non-relational databases there is eventual consistency.\nBASE: - Basically Available - Always possible to read and write data even though it may not be consistent. E.g. reads may not reflect latest changes, writes may not be persisted. - Soft state - Lack of consistency guarantees mean data state may change without any interactions with the application as the database reaches eventual consistency. - Eventual consistency - Data will eventually become consistent once inputs stops.\nBenefits: - Without atomicity constraint, overheads like 2PC are not required - Without consistency constraint, horizontal scaling is trivial - Without isolation constraint, no blocking is required which improves availability.\nNon-relational databases are a good fit when: - Large data volume that isn’t tabular - High availability requirement - Lack of consistency across nodes is acceptable\nLimitations: - Consistency is necessary for some use cases - Lack of standardisation\n\n\n\n\nProblem: if we have large files with small changes, we don’t want to have to re-upload and re-download the whole file every time something changes.\nSolution: identify the changes and only push/pull these. Similar to git.\nThe rsync algorithm makes it easier to compare changes by: 1. Split the file into blocks 2. Calculate a checksum for each block (using MD5 hashing algorithm) 3. To compare files between the client and the server, we only need to send the hashes back and forth 4. For the mismatching hashes, transfer the corresponding blocks\n\n\n\nOne-way communication that broadcasts file updates one-to-many.\n\n\nThese are synchronous.\nPolling is an example of a pull API. We periodically query the server to see if there are any updates. If not, the server responds immediately saying there is no new data. This may result in delayed updates and can overwhelm the server with too many requests.\n\n\n\n\n\n\nFigure 1: Polling\n\n\n\n\n\n\nThese are asynchronous.\nLong-polling. The client connects to the server and makes a request. Instead of replying immediately, the server waits until there is an update and then sends the response. If there is no update for a long time, the connection times out and the client must reconnect. Server resources are tied up until the connection is closed, even if there is no update.\n\n\n\n\n\n\nFigure 2: LongPolling\n\n\n\nWebsockets. The client establishes a connection using an HTTP request and response. This establishes a TCP/IP connection for 2-way communication. This allows for real-time applications without long-polling.\n\n\n\n\n\n\nFigure 3: Web Sockets\n\n\n\nServer sent events. Event-based approach. EventSource API supported by all browsers. The connection is 1-directional; the client can only pass data to the server at the point of connection. After that, only the server can send data to the client. The server doesn’t know if the client loses connection.\n\n\n\n\n\n\nFigure 4: SSE\n\n\n\n\n\n\n\nA system component that implements asynchronous communication between services, decoupling them. An independent server which producers and consumers connect to. AKA message queues.\nMessage brokers persist the messages until there are consumers ready to receive them.\nMessage brokers are similar in principle to databases since they persist data. The differences are they: automatically delete messages after delivery, do not support queries, typically have a small working set, and notify clients about changes.\nRabbitMQ, Kafka and Redis are open source message brokers. Amazon, GCP and Azure have managed service implementations.\n\n\nA one-to-one relationship between sender and receiver. Each message is consumed by exactly one receiver.\nThe message broker serves as an abstraction layer. The producer only sends to the broker. The receiver only receives from the broker. The services do not need to know about one another. The broker also guarantees delivery, so the message does not get lost if the consumer is unavailable, it will retry.\n\n\n\n\n\n\nFigure 5: Point-to-Point\n\n\n\n\n\n\nA one-to-many relationship between sender and receiver. The publisher publishes to a certain topic. Any consumer who is interested can then subscribe to receive that message.\n\n\n\n\n\n\nFigure 6: Pub-Sub\n\n\n\n\n\n\n\n\n\nFiles are stored in folders, with metadata about creation time and modification time.\nBenefits: - Simplicity - simple and well-known pattern - Compatibility - works with most applications and OSes\n\nLimitations:\nPerformance degradation - as size increases, the resource demands increase.\nExpensive - although cheap in principle, they can get expensive when trying to work around the performance issues.\n\nUse cases: data protection, local archiving, data retrieval done by users.\n\n\n\nA system component that manages data as objects in a flat structure. An object contains the file and its metadata.\nBenefits: - Horizontal scalability\nLimitations: - Objects must be edited as a unit - degrades performance if only a small part of the file needs to be updated.\nUse cases: large amounts of unstructured data.\n\n\n\nA system component that breaks up and then stores data as fixed-size blocks, each with a unique identifier.\nBenefits: - Highly structured - easy to index, search and retrieve blocks - Low data transfer overheads - Supports frequent data writes without performance degradation - Low latency\nLimitations: - No metadata, so metadata must be handled manually - Expensive\n\n\n\nA subclass of key-value stores, which hold computer-readable documents, like XML or JSON objects.\n\n\n\n\nIssues: - Massive files: 4TB for 4 hours of 4K video - How can we reliably upload very large files? - Lots of different end-user clients/devices/connection speeds - How can we process and store a range of versions/resolutions?\nProcessing pipeline:\n\n\n\n\n\n\nFigure 7: Video Processing Pipeline\n\n\n\n\nFile chunker\n\nSame principle as the file-sharing problem.\nSplit the file into chunks, then use checksums to determine that files are present and correct.\nIf there is a network outage, we only need to send the remaining chunks.\n\nContent filter\n\nCheck if the video complies with the platform’s content policy wrt copyright, piracy, NSFW\n\nTranscoder\n\nData is decoded to an uncompressed format\nThis will fan out in the next step to create multiple optimised versions of the file\n\nQuality conversion\n\nConvert the uncompressed file into multiple different resolution options.\n\n\nArchitecture considerations: - Wait for the full file to upload before processing? Or process each chunk as it is uploaded? - An upload service persists chunks to a database. Object store is a good choice as in the file-sharing example. - Once a chunk is ready to be processed, it can be read and placed in the message queue for the processing pipeline. - The processing pipeline handles fixed-size chunks, so the hardware requirements are known ahead of time. A chunk can be processed as soon as a hardware unit becomes available.\n\n\n\n\n\n\nFigure 8: Video Upload Architecture\n\n\n\n\n\n\nThe challenges are similar to that of file-sharing, as this is essentially large files being transferred. Latency and processing power of the end-user device.\nTwo main transfer protocols: - UDP - TCP\n\n\nA connection-less protocol. Nodes do NOT establish a stable end-to-end connection before sending data. Instead, each packet has its destination address attached so the network can route it to the correct place.\nAdvantages: - Fast - Packets are routed independently so if some are lost the others can still reach their destination\nDisadvantages: - No guarantee that data will arrive intact.\nUse cases: - Low-latency applications like video conferencing or gaming. - Not suitable for movie or audio streaming, as missing bits cannot be tolerated.\n\n\n\nA connection is established between 2 nodes. The nodes agree on certain parameters before any data is transferred. Parameters: IP addresses of source and destination, port numbers of source and destination.\nThree-way handshake establishes the connection: 1. Sender transfers their sequence number to the Receiver. 2. Receiver acknowledges and sends its own sequence number. 3. Sender acknowledges.\nAdvantages: - Reliability; guarantees delivery and receives acknowledgements before further packets are sent. - Receiver acknowledgements ensure the receiver is able to process/buffer the data in time before more packets are sent.\nDisadvantages: - Slower due to error checking and resending lost packets. - Requires three-way handshake to establish connection which is slower.\nUse cases: - Media streaming (HTTP live-streaming or MPEG-DASH)\nAdaptive bitrate streaming - TCP adjusts transmission speed in response to network conditions - When packet loss is detected, smaller chunks are sent at a lower transmission speed. The lower resolution file can be sent in this case.\n\n\n\n\nPerceived latency = Network latency + System latency\n\n\nCaching is the process of storing copies of frequently accessed data in a temporary storage location.\nCaches utilise the difference in read performance between memory and storage. Times to fetch 1MB from: - HDD: 3 ms - SSD: 0.2 ms - RAM: 0.01 ms\nWhen a user requests data it first requests from the cache - Cache HIT: If the data is in the cache, return it - Cache MISS: If the data is not in the cache, pass the request to the database, update the cache and return the data\nA cache can grow stale over time. There are 2 common approaches to mitigate this: 1. Set up a time-to-live (TTL) policy within the cache - Trade off between short TTL (fresh data but worse performance) vs long TTL (data can be stale) 2. Implement active cache invalidation mechanism - Complicated to implement. Requires an “invalidation service” component to monitor and read from the database, then update the cache when necessary.\nApplication-level caching: Insert caching logic into the application’s source code to temporarily store data in memory.\nTwo approaches to application-level caching: 1. Query-based implementation - Hash the query as a key and store the value against the key in a key-value store. - Limitations: hard to delete cached result with a complex query; if a cell changes then all cached query which might include it need updating. 2. Object-based implementation - The result of a query is stored as an object - Benefits: only one serialisation/deserialisation overhead when reading/writing; complexity of object doesn’t matter, serialized objects in cache can be used by multiple applications.\nPopular implementations: Redis, Memcache, Hazlecast\n\n\n\nA network of caching layer in different locations (Points of Presence, PoPs). Full data is stored on SSD and most frequently accessed data is stored in RAM.\nDon’t cache dynamic or time-sensitive data.\nInvalidating the cache. You can manually update the CDN, but there may be additional caches at the ISP- and browser-level which would still serve stale data.\n\n\n\n\n\n\nFigure 9: CDN Invalidation\n\n\n\nApproaches to invalidating cache: - Caching headers - set a time when an object should be cleared, e.g. max-age=86400 caches for a day. - Cache busting - change the link to all files and assets, so the CDN treats them like new files.\n\n\n\n\nThese are specialised NoSQL databases with the following benefits: - Can match even with typos or non-exact matches - Full-text search means you can suggest autocomplete results and related queries as the user types - Indexing allows faster search performance on big data.\nThe database is structures as a hashmap where the inverted index points at documents. - Document - Represents a specific entity, like a row in a relational database - Inverted index - Maps from content to the documents that include it. The inverted index splits each document into individual search terms and makes each point to the document itself. - Ranking algorithm - Produces a list of possibly relevant documents. Additional factors may impact this, like a user’s previous search history.\nThe database is a dictionary of {search_term_id: [document_ids, …]}\n\n\n\n\n\n\nFigure 10: Inverted Index\n\n\n\nPopular implementations: Lucene, Elastic search, Apache solr, Atas search (MongoDB), Redis search.\nBenefits of search engine databases: - Scalable - NoSQL so scale horizontally - Schemaless - Works out of the box - just need ot decide upfront what attributes should be searchable.\nLimitations: - No ACID guarantees. - Not efficient at reading/writing data, so requires another database to manage state. - Maintaining consistency between the main database and the search engine database can be tricky. - Some SQL databases have full-text search so may not require a dedicated search engine database.\n\n\n\n\n\n\nAn app that lets a user add and delete items from a todo list. - Representation layer is a web app - Microservices handle the todo CRUD operations and the user details separately - Each service is scaled horizontally, so a load balancer is placed in front of it - Relational database stores data - There are two databases, one per service, which is an example of functional partitioning. This means todo service I/O does not interfere with user service I/O and vice versa.\n\n\n\n\n\n\nFigure 11: To-do App System Design\n\n\n\n\n\n\nTake an input URL and return a unique, shortened URL that redirects to the original.\nPlanned system architecture: - Pre-generate all 5 digit keys (1 billion entries rather than 64 billion for 6 digits) - System retrieves 5-difit keys from database - System appends 1 out of 64 characters - The system is extendable because if we run out of keys we can append 2 more characters rather than 1\n\n\nSystem analysis: - System is read heavy - Once a short URL is created it will be read multiple times - Distributed system as this has to scale - Availability &gt; Consistency - not so much of a concern if a link isn’t available to all users at the same time, but they must be unique and lead to their destination.\nRequirements engineering:- Core feature: - A user can input a URL of arbitrary length and receive a unique short URL of fixed size. - A user can navigate to a short link and be redirected to the original URL.\nSupport features: - A user can see their link history of all created short URLs. - Lifecycle policy - links should expire after a default time span.\nNon-functional requirements: - Availability - the system should be available 99% of the time. - Scalability - the system should support billions of short URLs, and thousands of concurrent users. - Latency - the system should return redirect from a short URL to the original in under 1 second.\nQuestions to capture scale: - Daily active users, and how often do users interact per day - 100 million, 1 interaction per day - Peak active users - are there events that lead to traffic spikes? - No spikes - Read/write ratio - 10 to 1 - Request size - how long are the originals URLs typically - 200 characters =&gt; 200 Bytes - Replication factor - 1x - ignore replication\nCapacity estimation:- - Requests per second - Reads per second = DAU * interactions per day / seconds in day = 10^8 * 1 / 10^5 = 1000 reads/s - Writes per second = Reads per second / read write ratio = 1000 / 10 = 100 writes/s - Requests per second = Reads per second + Writes per second = 1000 + 100 = 1100 requests/s - No peak loads to consider - Bandwidth - Bandwidth = Requests per second * Message size - Read bandwidth = 1000 * 200 Bytes = 200kB/s - Write bandwidth = 100 * 200 Bytes = 20 kB/s - Storage - Storage per year = Write bandwidth * seconds per year * Replication factor = 20000 Bytes * (360024365) * 1 = 630 GB\nData model:- Identify entities, attributes and relationships.\nEntities and attributes: - Links: Key (used to create short URL, Original URL, Expiry date - Users: UserID, Links - Key ranges: Key range, In use (bool)\nRelationships: - Users own Links - Links belong to Key ranges\nData stores: - Users - User data is typically relational and we rarely want it all returned at once. - Consistency is important as we want the user to have the same experience regardless of which server handles their log in, and don’t want userIds to clash. - Relational database. - Links - Non-functional requirements of low latency and high availability. - Data and relationships are not complex. - Key-value store. - Key ranges - Favour data consistency as we don’t want to accidentally reuse the same key range. - Filtering keys by status would help to find available key ranges. - Relational database.\nAPI design:- Endpoints: - createUrl(originalUrl: str) -&gt; shortUrl - response code 200, {shortURL: str} - getLinkHistory(userId: str, sorting: {‘asc’, ‘desc’}) - response code 200, {links: [str], order: ‘asc’} - redirectURL(shortUrl: str) -&gt; originalUrl - response code 200, {originalUrl: str}\nSystem design:- User flows for each of the required functional requirements.\n\n\n\n\n\n\nFigure 12: URL Shortener System Design\n\n\n\n\n\n\n\nRequirements engineering Requirements gathering: - Read-heavy - Distributed - Data consistency is more important than availability - files should be consistent for all users\nCore functional requirements: 1. A user can upload a file to the server 2. A user can download their files from the server\nSecondary functional requirements: 1. A user can see a history of files uploaded and downloaded 2. A user can see who has downloaded a specific file and when\nNon-functional requirements: 1. Consistency - data should be consistent across users and devices 2. Resilience - customer data should never be lost 3. Minimal latency 4. Compatibility across different devices 5. Security - multi-tenancy; customer data should be separate of one another\nQuestions to determine scale: 1. Daily active users 2. Peak active users - what events lead to a spike? 3. Interactions per user - how many file syncs, how many files does a user store 4. Request size - how large are files? 5. Read/write ratio 6. Replication factor\nCapacity estimation Throughput - Peak write RPS = 2 peak load ratio * 10^8 users * 2 files/day / 10^5 seconds = 4000 RPS - Peak read RPS = 10 read/write ration * 4000 write RPS = 40000 RPS - Peak total RPS = 44000 RPS\nBandwidth - Write bandwidth = 4000 Write RPS * 100 kB Request size = 400 MB/s - Read bandwidth = 40000 Read RPS * 100 kB request size = 4 GB/s - Total bandwidth = Write bandwidth + Read bandwidth = 4.4 GB/s\nStorage - Storage capacity = 510^8 Total users  100 files per user * 1MB File Size * 3 Replication factor = 15*10^10 MB = 15000 TB\nData model Users: Store Ids for the files that belong to them\nFiles: Metadata on the owner, file edit history, filename, size, chunks that comprise the file, version history\nAPI Design Endpoints: - compareHashes(fileId: str, chunkHashes: list) - Takes a file ID and a list of chunk hashes and returns a list of the hashes that need resyncing . - response code 200, {syncChunks: [Hash,…]} - uploadChange(fileId: str, chunks: list) - Upload the chunks to resync the file - response code 200, {message: “Chunks uploaded successfully”} - requestUpdate(fileId: str, chunkHashes: list) - Update the local version of the file to match that on the server. - response code 200, {chunks: [Chunk,…]}\nSystem Design\n\n\n\n\n\n\nFigure 13: Dropbox System Design\n\n\n\nDesign Discussion - What can we do to achieve a scalable design? - Identify bottlenecks and remedy each. - What can we do to achieve resiliency?\n\n\n\n\n\nUdemy course https://www.udemy.com/course/the-bigtech-system-design-interview-bootcamp\nExcalidraw session https://excalidraw.com/#json=QM7dLZcHbESVnuTPiu06v,pdjPoskF0KknQ6YORHxeHw\nCapacity estimation cheat sheet in _resources folder\nDatabase book - “Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems” by Martin Kleppmann\nRsync algorithm https://openresearch-repository.anu.edu.au/bitstream/1885/40765/3/TR-CS-96-05.pdf\nSearch Engines Information Retrieval in Practice book https://ciir.cs.umass.edu/irbook/\n\n\n\n\nFigure 1: Polling\nFigure 2: LongPolling\nFigure 3: Web Sockets\nFigure 4: SSE\nFigure 5: Point-to-Point\nFigure 6: Pub-Sub\nFigure 7: Video Processing Pipeline\nFigure 8: Video Upload Architecture\nFigure 9: CDN Invalidation\nFigure 10: Inverted Index\nFigure 11: To-do App System Design\nFigure 12: URL Shortener System Design\nFigure 13: Dropbox System Design"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#requirements-engineering",
    "href": "posts/software/system_design/system_design_notes.html#requirements-engineering",
    "title": "System Design Notes",
    "section": "",
    "text": "Functional and non-functional requirements. Scale of the system.\n\n\nWhat the system should do. - Which problem does the system solve - Which features are essential to solve these problems\nCore features: bare minimum required to solve the user’s problem. Support features: features that help the user solve their problem more conveniently.\n\n\n\nWhat the system should handle.\n\nInterview tips - Identify the non-functional requirements the interviewer cares most about - Show awareness of system attributes, trade-offs and user experience\n\nSystem analysis: - Read or write heavy? - Impacts requirements for database scaling, redundancy of services, effectiveness of caching. - System priorities - is it worse if the system fails to read or to write? - Monolithic or distributed architecture? - Scale is the key consideration. Large scale applications must be distributed, smaller scale can be single server. - Data availability vs consistency - CAP trilemma - A system can have only two of: Consistency, Availability, Partition tolerance - Choice determines the impact of a network failure\nNon-functional requirements: - Availability - How long is the system up and running per year? - Availability of a system is the product of its components’ availability - Reliability, redundancy, fault tolerance - Consistency - Data appears the same on all nodes regardless of the user and location. - Linearizability - Scalability - Ability of a system to handle fast-growing user base and temporary load spikes - Vertical scaling - single server that we add more CPU/RAM to - Pros: fast inter-process communication, data consistency - Cons: single point of failure, hardware limits on maximum scale - Horizontal scaling - cluster of servers in parallel - Pros: redundancy improves availability, linear cost of scaling - Cons: complex architecture, data inconsistency - Latency - Amount of time taken for a single message to be delivered - Experienced latency = network latency + system latency - Database and data model choices affect latency - Caching can be effective - Compatibility - Ability of a system to operate seamlessly with other software, hardware and systems - Security - Basic security measures: TLS encrypted network traffic, API keys for rate limiting\nNon-functional requirements depend heavily on expected scale. The following help quantify expected scale, and relate to capacity estimation in the next step. - Daily active users (DAU) - Peak active users - Interactions per user - including read/write ratio - Request size - Replication factor - determines the storage requirement (typically 3x)"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#capacity-estimation",
    "href": "posts/software/system_design/system_design_notes.html#capacity-estimation",
    "title": "System Design Notes",
    "section": "",
    "text": "Interview tips - Simplify wherever you can - simple assumptions and round heavily - Convert all numbers to scientific notation - Know powers of 2\n\n\n\nRequests per second (RPS) is the key measure.\nRequests per day = Daily active users * Activities per user\nRequests per second = RPD / 10^5\nPeak load = RPS * Peak load factor\nRead/write ratio is important to get the total requests. Usually, information on either reads or writes is missing and must be inferred using the other.\nTotal RPS = Read RPS + Write RPS\n\n\n\nThe amount of data that can be transmitted between two nodes in a fixed period of time. Bits per second.\nThroughput, bandwidth and latency are all related and can be thought of with the motorway analogy. - Throughput is the total number of vehicles that must pass through that road per day. - Bandwidth is the number of lanes. - Latency is the time taken to get from one end of the road to another.\nThe bandwidth must be large enough to support the required throughput.\nBandwidth = Total RPS * Request size\nEven high bandwidth can result in low latency if there is an unexpectedly high peak throughput. Rush hour traffic jam.\nRequest size can be varied according to bandwidth, e.g. YouTube lowers the resolution if the connection is slow.\n\n\n\nMeasured in bits per second using the Write RPS. Long term storage (assuming the same user base) is also helpful.\nStorage capacity per second = Write RPS * Request size * Replication factor\nStorage capacity in 5 years = Storage capacity per second * 2*10^8"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#data-modeling",
    "href": "posts/software/system_design/system_design_notes.html#data-modeling",
    "title": "System Design Notes",
    "section": "",
    "text": "Key concepts are: entities, attributes, relations.\nOptimisation discussions are anchored by the data model. Relational databases: denormalization, SQL tuning, sharding, federation Non-relational databases: indexing, caching\nThe data model is not a full-blown data schema (which only applies to relational databases) but a more informal, high-level version.\nMust haves: 1. Derive entities and attributes. 2. Draw connections between entities. Then: 3. Select a database based on the requirements 4. Extend data model to a proper data schema 5. Identify bottlenecks and apply quick fixes Later: 6. Defer any detailed discussions to the design discussion section.\n\n\nEntities are “things” with a distinct and independent existence, e.g. users, files, tweets, posts, channels. Create a unique table for each entity.\nAttributes describe properties of an entity, e.g. a user’s name, date of birth. These are the fields in the table of that entity.\n\nInterview tips - Do not focus on irrelevant details - Do not miss any critical functionality - Double check all tables after you extract all info from requirements\n\n\n\n\nExtract connections between entities from the requirements. Relationships can be one-to-one, one-to-many or many-to-many."
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#api-design",
    "href": "posts/software/system_design/system_design_notes.html#api-design",
    "title": "System Design Notes",
    "section": "",
    "text": "Specify the API endpoints with: - Function signature - Parameters - Response and status codes\nThis specifies a binding contract between the client and the application server. There can be APIs between every system component, but just focus on the client/server interface.\nProcess: 1. Revisit the functional requirements 2. Derive the goal of each endpoint 3. Derive the signature from the goal and the inputs from the data model 4. Define the response outputs\nNaming conventions. Use HTTP request types in the call signature where appropriate: GET, POST, PUT, PATCH, DELETE\n\nInterview tips - Avoid vague inputs - Don’t add extra parameters that are out of scope - Avoid redundant parameters - Don’t miss any parameters - Confirm the output repsonse satisfies the requirements - Identify inefficiencies of the data structure"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#system-design-1",
    "href": "posts/software/system_design/system_design_notes.html#system-design-1",
    "title": "System Design Notes",
    "section": "",
    "text": "System components: - Representation layer - Web app - Mobile app - Console - Service (and load balancer) - Data store - Relational database - Pros: linked tables reduce duplicate data; flexible queries - Key-value store - Useful for lots of small continuous reads and writes - Pros: Fast access, lightweight, highly scalable - Cons: Cannot query by value, no standard query language so data access in written in the application layer no data normalisation\nScaling: - Scale services by having multiple instances of the service with a load balancer in front - Scale database with federation (functional partitioning) - split up data by function\nSystem diagrams: arrows point in direction of user flow (not data flow)"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#design-discussion",
    "href": "posts/software/system_design/system_design_notes.html#design-discussion",
    "title": "System Design Notes",
    "section": "",
    "text": "Types of questions: - NFR questions - How to achieve scalability - Identify each bottleneck and remedy it - How to improve resiliency - Justification question - Choice of components, e.g. what would change if you changed block storage for object storage or relational database - Additional components, e.g. is there a use case for a message queue and where - Extension questions - Additional functional requirements not in the original scope"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#system-components-deep-dive",
    "href": "posts/software/system_design/system_design_notes.html#system-components-deep-dive",
    "title": "System Design Notes",
    "section": "",
    "text": "There are two parameters we can vary: 1. Length of the key - should be limited so the URL is short enough 2. Range of allowed characters - should be URL-safe\nUsing Base-64 encoding as the character range, then a 6-digit key length gives enough unique URLs; 64^6 = 68 billion\nEncoding options: 1. MD5 hash: not collision resistant and too long 2. Encoded counter, each URL is just an index int, and its value is the base-64 encoding of that int: length is not fixed and increases over time 3. Key range, generate all unique keys beforehand: no collisions, but storage intensive 4. Key range modified, generate 5-digit keys beforehand, then add the 6th digit on the fly.\n\n\n\n\n\n4 main operations (CRUD): Create, Read, Update, Delete.\nTransactions - group several operations together - Either the entire transaction succeeds or it fails and rolls back to the initial state. Commit or abort on error. - Without transactions, if an individual operation fails, it can be complex to unwind the related transactions to return to the initial state. - Error handling is simpler as manual rollback of operations is not required. - Transactions provide guarantees so we can reason about the database state before and after the transaction.\nTransactions enforce “ACID” guarantees: - Atomicity - Transactions cannot be broken down into smaller parts. - Consistency - All data points within the database must align to be properly read and accepted. Raise a consistency error if this is not the case. Database consistency is different from system consistency which states the data should be the same across all nodes. - Isolation - Concurrently executing transactions are isolated from each other. Avoids race conditions if multiple clients are accessing the same database record. - Durability - Once a transaction is committed to the database, it is permanently preserved. Backups and transaction logs can restore committed transactions in the case of a failure.\nRelational databases are a good fit when: - Data is well-structured - Use case requires complex querying - Data consistency is important\nLimitations - Horizontal scaling is complex - Distributed databases are more complex to keep transaction guarantees - Two phase commit protocol (2PC): 1. Prepare - Ask each node if it;s able to promise to carry out the transaction 2. Commit - Block the nodes and do the commit - Blocking causes problems on distributed systems where it may cause unexpected consequences if the database is unavailable for this short time.\n\n\n\nExamples of non-relational databases: - Key-value store - Document store - Wide-column store - Graph store\nThese vary a lot, and in general NoSQL simply means “no ACID guarantees”.\nTransactions were seen as the enemy of scalability, so needed to be abandoned completely for performance and scalability. ACID enforces consistency for relational databases; for non-relational databases there is eventual consistency.\nBASE: - Basically Available - Always possible to read and write data even though it may not be consistent. E.g. reads may not reflect latest changes, writes may not be persisted. - Soft state - Lack of consistency guarantees mean data state may change without any interactions with the application as the database reaches eventual consistency. - Eventual consistency - Data will eventually become consistent once inputs stops.\nBenefits: - Without atomicity constraint, overheads like 2PC are not required - Without consistency constraint, horizontal scaling is trivial - Without isolation constraint, no blocking is required which improves availability.\nNon-relational databases are a good fit when: - Large data volume that isn’t tabular - High availability requirement - Lack of consistency across nodes is acceptable\nLimitations: - Consistency is necessary for some use cases - Lack of standardisation\n\n\n\n\nProblem: if we have large files with small changes, we don’t want to have to re-upload and re-download the whole file every time something changes.\nSolution: identify the changes and only push/pull these. Similar to git.\nThe rsync algorithm makes it easier to compare changes by: 1. Split the file into blocks 2. Calculate a checksum for each block (using MD5 hashing algorithm) 3. To compare files between the client and the server, we only need to send the hashes back and forth 4. For the mismatching hashes, transfer the corresponding blocks\n\n\n\nOne-way communication that broadcasts file updates one-to-many.\n\n\nThese are synchronous.\nPolling is an example of a pull API. We periodically query the server to see if there are any updates. If not, the server responds immediately saying there is no new data. This may result in delayed updates and can overwhelm the server with too many requests.\n\n\n\n\n\n\nFigure 1: Polling\n\n\n\n\n\n\nThese are asynchronous.\nLong-polling. The client connects to the server and makes a request. Instead of replying immediately, the server waits until there is an update and then sends the response. If there is no update for a long time, the connection times out and the client must reconnect. Server resources are tied up until the connection is closed, even if there is no update.\n\n\n\n\n\n\nFigure 2: LongPolling\n\n\n\nWebsockets. The client establishes a connection using an HTTP request and response. This establishes a TCP/IP connection for 2-way communication. This allows for real-time applications without long-polling.\n\n\n\n\n\n\nFigure 3: Web Sockets\n\n\n\nServer sent events. Event-based approach. EventSource API supported by all browsers. The connection is 1-directional; the client can only pass data to the server at the point of connection. After that, only the server can send data to the client. The server doesn’t know if the client loses connection.\n\n\n\n\n\n\nFigure 4: SSE\n\n\n\n\n\n\n\nA system component that implements asynchronous communication between services, decoupling them. An independent server which producers and consumers connect to. AKA message queues.\nMessage brokers persist the messages until there are consumers ready to receive them.\nMessage brokers are similar in principle to databases since they persist data. The differences are they: automatically delete messages after delivery, do not support queries, typically have a small working set, and notify clients about changes.\nRabbitMQ, Kafka and Redis are open source message brokers. Amazon, GCP and Azure have managed service implementations.\n\n\nA one-to-one relationship between sender and receiver. Each message is consumed by exactly one receiver.\nThe message broker serves as an abstraction layer. The producer only sends to the broker. The receiver only receives from the broker. The services do not need to know about one another. The broker also guarantees delivery, so the message does not get lost if the consumer is unavailable, it will retry.\n\n\n\n\n\n\nFigure 5: Point-to-Point\n\n\n\n\n\n\nA one-to-many relationship between sender and receiver. The publisher publishes to a certain topic. Any consumer who is interested can then subscribe to receive that message.\n\n\n\n\n\n\nFigure 6: Pub-Sub\n\n\n\n\n\n\n\n\n\nFiles are stored in folders, with metadata about creation time and modification time.\nBenefits: - Simplicity - simple and well-known pattern - Compatibility - works with most applications and OSes\n\nLimitations:\nPerformance degradation - as size increases, the resource demands increase.\nExpensive - although cheap in principle, they can get expensive when trying to work around the performance issues.\n\nUse cases: data protection, local archiving, data retrieval done by users.\n\n\n\nA system component that manages data as objects in a flat structure. An object contains the file and its metadata.\nBenefits: - Horizontal scalability\nLimitations: - Objects must be edited as a unit - degrades performance if only a small part of the file needs to be updated.\nUse cases: large amounts of unstructured data.\n\n\n\nA system component that breaks up and then stores data as fixed-size blocks, each with a unique identifier.\nBenefits: - Highly structured - easy to index, search and retrieve blocks - Low data transfer overheads - Supports frequent data writes without performance degradation - Low latency\nLimitations: - No metadata, so metadata must be handled manually - Expensive\n\n\n\nA subclass of key-value stores, which hold computer-readable documents, like XML or JSON objects.\n\n\n\n\nIssues: - Massive files: 4TB for 4 hours of 4K video - How can we reliably upload very large files? - Lots of different end-user clients/devices/connection speeds - How can we process and store a range of versions/resolutions?\nProcessing pipeline:\n\n\n\n\n\n\nFigure 7: Video Processing Pipeline\n\n\n\n\nFile chunker\n\nSame principle as the file-sharing problem.\nSplit the file into chunks, then use checksums to determine that files are present and correct.\nIf there is a network outage, we only need to send the remaining chunks.\n\nContent filter\n\nCheck if the video complies with the platform’s content policy wrt copyright, piracy, NSFW\n\nTranscoder\n\nData is decoded to an uncompressed format\nThis will fan out in the next step to create multiple optimised versions of the file\n\nQuality conversion\n\nConvert the uncompressed file into multiple different resolution options.\n\n\nArchitecture considerations: - Wait for the full file to upload before processing? Or process each chunk as it is uploaded? - An upload service persists chunks to a database. Object store is a good choice as in the file-sharing example. - Once a chunk is ready to be processed, it can be read and placed in the message queue for the processing pipeline. - The processing pipeline handles fixed-size chunks, so the hardware requirements are known ahead of time. A chunk can be processed as soon as a hardware unit becomes available.\n\n\n\n\n\n\nFigure 8: Video Upload Architecture\n\n\n\n\n\n\nThe challenges are similar to that of file-sharing, as this is essentially large files being transferred. Latency and processing power of the end-user device.\nTwo main transfer protocols: - UDP - TCP\n\n\nA connection-less protocol. Nodes do NOT establish a stable end-to-end connection before sending data. Instead, each packet has its destination address attached so the network can route it to the correct place.\nAdvantages: - Fast - Packets are routed independently so if some are lost the others can still reach their destination\nDisadvantages: - No guarantee that data will arrive intact.\nUse cases: - Low-latency applications like video conferencing or gaming. - Not suitable for movie or audio streaming, as missing bits cannot be tolerated.\n\n\n\nA connection is established between 2 nodes. The nodes agree on certain parameters before any data is transferred. Parameters: IP addresses of source and destination, port numbers of source and destination.\nThree-way handshake establishes the connection: 1. Sender transfers their sequence number to the Receiver. 2. Receiver acknowledges and sends its own sequence number. 3. Sender acknowledges.\nAdvantages: - Reliability; guarantees delivery and receives acknowledgements before further packets are sent. - Receiver acknowledgements ensure the receiver is able to process/buffer the data in time before more packets are sent.\nDisadvantages: - Slower due to error checking and resending lost packets. - Requires three-way handshake to establish connection which is slower.\nUse cases: - Media streaming (HTTP live-streaming or MPEG-DASH)\nAdaptive bitrate streaming - TCP adjusts transmission speed in response to network conditions - When packet loss is detected, smaller chunks are sent at a lower transmission speed. The lower resolution file can be sent in this case.\n\n\n\n\nPerceived latency = Network latency + System latency\n\n\nCaching is the process of storing copies of frequently accessed data in a temporary storage location.\nCaches utilise the difference in read performance between memory and storage. Times to fetch 1MB from: - HDD: 3 ms - SSD: 0.2 ms - RAM: 0.01 ms\nWhen a user requests data it first requests from the cache - Cache HIT: If the data is in the cache, return it - Cache MISS: If the data is not in the cache, pass the request to the database, update the cache and return the data\nA cache can grow stale over time. There are 2 common approaches to mitigate this: 1. Set up a time-to-live (TTL) policy within the cache - Trade off between short TTL (fresh data but worse performance) vs long TTL (data can be stale) 2. Implement active cache invalidation mechanism - Complicated to implement. Requires an “invalidation service” component to monitor and read from the database, then update the cache when necessary.\nApplication-level caching: Insert caching logic into the application’s source code to temporarily store data in memory.\nTwo approaches to application-level caching: 1. Query-based implementation - Hash the query as a key and store the value against the key in a key-value store. - Limitations: hard to delete cached result with a complex query; if a cell changes then all cached query which might include it need updating. 2. Object-based implementation - The result of a query is stored as an object - Benefits: only one serialisation/deserialisation overhead when reading/writing; complexity of object doesn’t matter, serialized objects in cache can be used by multiple applications.\nPopular implementations: Redis, Memcache, Hazlecast\n\n\n\nA network of caching layer in different locations (Points of Presence, PoPs). Full data is stored on SSD and most frequently accessed data is stored in RAM.\nDon’t cache dynamic or time-sensitive data.\nInvalidating the cache. You can manually update the CDN, but there may be additional caches at the ISP- and browser-level which would still serve stale data.\n\n\n\n\n\n\nFigure 9: CDN Invalidation\n\n\n\nApproaches to invalidating cache: - Caching headers - set a time when an object should be cleared, e.g. max-age=86400 caches for a day. - Cache busting - change the link to all files and assets, so the CDN treats them like new files.\n\n\n\n\nThese are specialised NoSQL databases with the following benefits: - Can match even with typos or non-exact matches - Full-text search means you can suggest autocomplete results and related queries as the user types - Indexing allows faster search performance on big data.\nThe database is structures as a hashmap where the inverted index points at documents. - Document - Represents a specific entity, like a row in a relational database - Inverted index - Maps from content to the documents that include it. The inverted index splits each document into individual search terms and makes each point to the document itself. - Ranking algorithm - Produces a list of possibly relevant documents. Additional factors may impact this, like a user’s previous search history.\nThe database is a dictionary of {search_term_id: [document_ids, …]}\n\n\n\n\n\n\nFigure 10: Inverted Index\n\n\n\nPopular implementations: Lucene, Elastic search, Apache solr, Atas search (MongoDB), Redis search.\nBenefits of search engine databases: - Scalable - NoSQL so scale horizontally - Schemaless - Works out of the box - just need ot decide upfront what attributes should be searchable.\nLimitations: - No ACID guarantees. - Not efficient at reading/writing data, so requires another database to manage state. - Maintaining consistency between the main database and the search engine database can be tricky. - Some SQL databases have full-text search so may not require a dedicated search engine database."
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#system-design-examples-and-discussion",
    "href": "posts/software/system_design/system_design_notes.html#system-design-examples-and-discussion",
    "title": "System Design Notes",
    "section": "",
    "text": "An app that lets a user add and delete items from a todo list. - Representation layer is a web app - Microservices handle the todo CRUD operations and the user details separately - Each service is scaled horizontally, so a load balancer is placed in front of it - Relational database stores data - There are two databases, one per service, which is an example of functional partitioning. This means todo service I/O does not interfere with user service I/O and vice versa.\n\n\n\n\n\n\nFigure 11: To-do App System Design\n\n\n\n\n\n\nTake an input URL and return a unique, shortened URL that redirects to the original.\nPlanned system architecture: - Pre-generate all 5 digit keys (1 billion entries rather than 64 billion for 6 digits) - System retrieves 5-difit keys from database - System appends 1 out of 64 characters - The system is extendable because if we run out of keys we can append 2 more characters rather than 1\n\n\nSystem analysis: - System is read heavy - Once a short URL is created it will be read multiple times - Distributed system as this has to scale - Availability &gt; Consistency - not so much of a concern if a link isn’t available to all users at the same time, but they must be unique and lead to their destination.\nRequirements engineering:- Core feature: - A user can input a URL of arbitrary length and receive a unique short URL of fixed size. - A user can navigate to a short link and be redirected to the original URL.\nSupport features: - A user can see their link history of all created short URLs. - Lifecycle policy - links should expire after a default time span.\nNon-functional requirements: - Availability - the system should be available 99% of the time. - Scalability - the system should support billions of short URLs, and thousands of concurrent users. - Latency - the system should return redirect from a short URL to the original in under 1 second.\nQuestions to capture scale: - Daily active users, and how often do users interact per day - 100 million, 1 interaction per day - Peak active users - are there events that lead to traffic spikes? - No spikes - Read/write ratio - 10 to 1 - Request size - how long are the originals URLs typically - 200 characters =&gt; 200 Bytes - Replication factor - 1x - ignore replication\nCapacity estimation:- - Requests per second - Reads per second = DAU * interactions per day / seconds in day = 10^8 * 1 / 10^5 = 1000 reads/s - Writes per second = Reads per second / read write ratio = 1000 / 10 = 100 writes/s - Requests per second = Reads per second + Writes per second = 1000 + 100 = 1100 requests/s - No peak loads to consider - Bandwidth - Bandwidth = Requests per second * Message size - Read bandwidth = 1000 * 200 Bytes = 200kB/s - Write bandwidth = 100 * 200 Bytes = 20 kB/s - Storage - Storage per year = Write bandwidth * seconds per year * Replication factor = 20000 Bytes * (360024365) * 1 = 630 GB\nData model:- Identify entities, attributes and relationships.\nEntities and attributes: - Links: Key (used to create short URL, Original URL, Expiry date - Users: UserID, Links - Key ranges: Key range, In use (bool)\nRelationships: - Users own Links - Links belong to Key ranges\nData stores: - Users - User data is typically relational and we rarely want it all returned at once. - Consistency is important as we want the user to have the same experience regardless of which server handles their log in, and don’t want userIds to clash. - Relational database. - Links - Non-functional requirements of low latency and high availability. - Data and relationships are not complex. - Key-value store. - Key ranges - Favour data consistency as we don’t want to accidentally reuse the same key range. - Filtering keys by status would help to find available key ranges. - Relational database.\nAPI design:- Endpoints: - createUrl(originalUrl: str) -&gt; shortUrl - response code 200, {shortURL: str} - getLinkHistory(userId: str, sorting: {‘asc’, ‘desc’}) - response code 200, {links: [str], order: ‘asc’} - redirectURL(shortUrl: str) -&gt; originalUrl - response code 200, {originalUrl: str}\nSystem design:- User flows for each of the required functional requirements.\n\n\n\n\n\n\nFigure 12: URL Shortener System Design\n\n\n\n\n\n\n\nRequirements engineering Requirements gathering: - Read-heavy - Distributed - Data consistency is more important than availability - files should be consistent for all users\nCore functional requirements: 1. A user can upload a file to the server 2. A user can download their files from the server\nSecondary functional requirements: 1. A user can see a history of files uploaded and downloaded 2. A user can see who has downloaded a specific file and when\nNon-functional requirements: 1. Consistency - data should be consistent across users and devices 2. Resilience - customer data should never be lost 3. Minimal latency 4. Compatibility across different devices 5. Security - multi-tenancy; customer data should be separate of one another\nQuestions to determine scale: 1. Daily active users 2. Peak active users - what events lead to a spike? 3. Interactions per user - how many file syncs, how many files does a user store 4. Request size - how large are files? 5. Read/write ratio 6. Replication factor\nCapacity estimation Throughput - Peak write RPS = 2 peak load ratio * 10^8 users * 2 files/day / 10^5 seconds = 4000 RPS - Peak read RPS = 10 read/write ration * 4000 write RPS = 40000 RPS - Peak total RPS = 44000 RPS\nBandwidth - Write bandwidth = 4000 Write RPS * 100 kB Request size = 400 MB/s - Read bandwidth = 40000 Read RPS * 100 kB request size = 4 GB/s - Total bandwidth = Write bandwidth + Read bandwidth = 4.4 GB/s\nStorage - Storage capacity = 510^8 Total users  100 files per user * 1MB File Size * 3 Replication factor = 15*10^10 MB = 15000 TB\nData model Users: Store Ids for the files that belong to them\nFiles: Metadata on the owner, file edit history, filename, size, chunks that comprise the file, version history\nAPI Design Endpoints: - compareHashes(fileId: str, chunkHashes: list) - Takes a file ID and a list of chunk hashes and returns a list of the hashes that need resyncing . - response code 200, {syncChunks: [Hash,…]} - uploadChange(fileId: str, chunks: list) - Upload the chunks to resync the file - response code 200, {message: “Chunks uploaded successfully”} - requestUpdate(fileId: str, chunkHashes: list) - Update the local version of the file to match that on the server. - response code 200, {chunks: [Chunk,…]}\nSystem Design\n\n\n\n\n\n\nFigure 13: Dropbox System Design\n\n\n\nDesign Discussion - What can we do to achieve a scalable design? - Identify bottlenecks and remedy each. - What can we do to achieve resiliency?"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#references",
    "href": "posts/software/system_design/system_design_notes.html#references",
    "title": "System Design Notes",
    "section": "",
    "text": "Udemy course https://www.udemy.com/course/the-bigtech-system-design-interview-bootcamp\nExcalidraw session https://excalidraw.com/#json=QM7dLZcHbESVnuTPiu06v,pdjPoskF0KknQ6YORHxeHw\nCapacity estimation cheat sheet in _resources folder\nDatabase book - “Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems” by Martin Kleppmann\nRsync algorithm https://openresearch-repository.anu.edu.au/bitstream/1885/40765/3/TR-CS-96-05.pdf\nSearch Engines Information Retrieval in Practice book https://ciir.cs.umass.edu/irbook/\n\n\n\n\nFigure 1: Polling\nFigure 2: LongPolling\nFigure 3: Web Sockets\nFigure 4: SSE\nFigure 5: Point-to-Point\nFigure 6: Pub-Sub\nFigure 7: Video Processing Pipeline\nFigure 8: Video Upload Architecture\nFigure 9: CDN Invalidation\nFigure 10: Inverted Index\nFigure 11: To-do App System Design\nFigure 12: URL Shortener System Design\nFigure 13: Dropbox System Design"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson1/lesson.html",
    "href": "posts/ml/generative_deep_learning/lesson1/lesson.html",
    "title": "Generative AI: Chapter 1",
    "section": "",
    "text": "Introduction to Generative Deep Learning\nThese are notes from chapter 1 of Generative Deep Learning by David Foster.\n\n\n\n\n\n\nTip\n\n\n\nIn progress…\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/ml/fastai/lesson4/lesson.html",
    "href": "posts/ml/fastai/lesson4/lesson.html",
    "title": "FastAI Lesson 4: Natural Language Processing",
    "section": "",
    "text": "These are notes from lesson 4 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\nKaggle NLP pattern similarity notebook: see notebook\n\n\n\n\nNLP applications: categorising documents, translation, text generation.\nWe are using the Huggingface transformers library for this lesson. It is now incorporated into the fastai library.\nULMFit is an algorithm which uses fine-tuning, in this example to train a positve/negative sentiment classifier in 3 steps: 1. Train an RNN on wikipedia to predict the next word. No labels required. 2. Fine-tune this for IMDb reviews to predict the next word of a movie review. Still no labels required. 3. Fine-tune this to classify the sentiment.\nTransformers have overtaken ULMFit as the state-of-the-art.\nLooking “inside” a CNN, the first layer contains elementary detectors like edge detectors, blob detectors, gradient detectors etc. These get combined in non-linear ways to make increasingly complex detectors. Layer 2 might combine vertical and horizontal edge detectors into a corner detector. By the later layers, it is detecting rich features like lizard eyes, dog faces etc. See Zeiler and Fegus.\nFor the fine-tuning process, the earlier layers are unlikely to need changing because they are more general. So we only need to fine-tune (i.e. re-train) the later layers.\n\n\n\nAs an example of NLP in practice, we walk through this notebook for U.S. Patent Phrase to Phrase Matching.\nReshape the input to fit a standard NLP task\n\nWe want to learn the similarity between two fields and are provided with similarity scores.\nWe concat the fields of interest (with identifiers in between). The identifiers themselves are not important, they just need to be consistent.\nThe NLP model is then a supervised regression task to predict the score given the concatendated string.\n\ndf['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor\n\n\nSplit the text into tokens (words). Tokens are, broadly speaking, words.\nThere are some caveats to that, as some languages like Chinese don’t fit nicely into that model. We don’t want the vocabulary to be too big. Alternative approaches use character tokenization rather than word tokenization.\nIn practice, we tokenize into subwords.\n\n\n\nMap each unique token to a number. One-hot encoding.\nThe choice of tokenization and numericalization depends on the model you use. Whoever trained the model chose a convention for tokenizing. We need to be consistent with that if we want the model to work correctly.\n\n\n\nThe Huggingface model hub contains thousands of pretrained models.\nFor NLP tasks, it is useful to choose a model that was trained on a similar corpus, so you can search the model hub. In this case, we search for “patent”.\nSome models are general purpose, e.g. deberta-v3 used in the lesson.\nULMFit handles large documents better as it can split up the document. Transformer approaches require loading the whole document into GPU memory, so struggle for larger documents.\n\n\n\nIf a model is too simple (i.e. not flexible enough) then it cannot fit the data and be biased, leading to underfitting.\nIf the model fits the data points too closely, it is overfitting.\nA good validation set, and monitoring validation error rather than training error as a metric, is key to avoiding overfitting. See this article for an in-depth discussion on the importance of choosing a good validation set.\nOften people will default to using a random train/test split. This is what scikit-learn uses. This is a BAD idea very often.\nFor time-series data, it’s easier to infer gaps than it is to predict a block in the future. The latter is the more common task but a random split simulates the former, giving unrealistically good performance.\nFor image data, there may be people, boats, etc that are in the training set but not the test set. By failing to have new people in the validation set, the model can learn things about specific people/boats that it can’t rely on in practice.\n\n\n\nMetrics are things that are human-understandable.\nLoss functions should be smooth and differentiable to aid in training.\nThese can sometimes be the same thing, but not in general. For example, accuracy is a good metric in image classification. We could tweak the weights in such a way that it improves the model slightly, but not so much that it now correctly classifies a previously incorrect image. This means the metric function is bumpy, therefore a bad loss function.\nThis article goes into more detail on the choice of metrics.\nAI can be particularly dangerous at confirming systematic biases, because it is so good at optimising metrics, so it will conform to any biases present in the training data. Making decisions based on the model then reinforces those biases.\n\nGoodhart’s law applies: If a metric becomes a target it’s no longer a good metric\n\n\n\n\nThe best way to understand a metric is not to look at the mathematical formula, but to plot some data for which the metric is high, medium and low, then see what that tells you.\nAfter that, look at the equation to see if your intuition matches the logic.\n\n\n\nFast AI has a function to find a good starting point. Otherwise, pick a small value and keep doubling it until it falls apart.\n\n\n\n\n\nCourse lesson page\nHuggingFace Transformers\nVisualizing and Understanding Convolutional Networks\nHuggingFace models\nValidation sets\nModel metrics"
  },
  {
    "objectID": "posts/ml/fastai/lesson4/lesson.html#nlp-background-and-transformers",
    "href": "posts/ml/fastai/lesson4/lesson.html#nlp-background-and-transformers",
    "title": "FastAI Lesson 4: Natural Language Processing",
    "section": "",
    "text": "NLP applications: categorising documents, translation, text generation.\nWe are using the Huggingface transformers library for this lesson. It is now incorporated into the fastai library.\nULMFit is an algorithm which uses fine-tuning, in this example to train a positve/negative sentiment classifier in 3 steps: 1. Train an RNN on wikipedia to predict the next word. No labels required. 2. Fine-tune this for IMDb reviews to predict the next word of a movie review. Still no labels required. 3. Fine-tune this to classify the sentiment.\nTransformers have overtaken ULMFit as the state-of-the-art.\nLooking “inside” a CNN, the first layer contains elementary detectors like edge detectors, blob detectors, gradient detectors etc. These get combined in non-linear ways to make increasingly complex detectors. Layer 2 might combine vertical and horizontal edge detectors into a corner detector. By the later layers, it is detecting rich features like lizard eyes, dog faces etc. See Zeiler and Fegus.\nFor the fine-tuning process, the earlier layers are unlikely to need changing because they are more general. So we only need to fine-tune (i.e. re-train) the later layers."
  },
  {
    "objectID": "posts/ml/fastai/lesson4/lesson.html#kaggle-competition-walkthrough",
    "href": "posts/ml/fastai/lesson4/lesson.html#kaggle-competition-walkthrough",
    "title": "FastAI Lesson 4: Natural Language Processing",
    "section": "",
    "text": "As an example of NLP in practice, we walk through this notebook for U.S. Patent Phrase to Phrase Matching.\nReshape the input to fit a standard NLP task\n\nWe want to learn the similarity between two fields and are provided with similarity scores.\nWe concat the fields of interest (with identifiers in between). The identifiers themselves are not important, they just need to be consistent.\nThe NLP model is then a supervised regression task to predict the score given the concatendated string.\n\ndf['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor\n\n\nSplit the text into tokens (words). Tokens are, broadly speaking, words.\nThere are some caveats to that, as some languages like Chinese don’t fit nicely into that model. We don’t want the vocabulary to be too big. Alternative approaches use character tokenization rather than word tokenization.\nIn practice, we tokenize into subwords.\n\n\n\nMap each unique token to a number. One-hot encoding.\nThe choice of tokenization and numericalization depends on the model you use. Whoever trained the model chose a convention for tokenizing. We need to be consistent with that if we want the model to work correctly.\n\n\n\nThe Huggingface model hub contains thousands of pretrained models.\nFor NLP tasks, it is useful to choose a model that was trained on a similar corpus, so you can search the model hub. In this case, we search for “patent”.\nSome models are general purpose, e.g. deberta-v3 used in the lesson.\nULMFit handles large documents better as it can split up the document. Transformer approaches require loading the whole document into GPU memory, so struggle for larger documents.\n\n\n\nIf a model is too simple (i.e. not flexible enough) then it cannot fit the data and be biased, leading to underfitting.\nIf the model fits the data points too closely, it is overfitting.\nA good validation set, and monitoring validation error rather than training error as a metric, is key to avoiding overfitting. See this article for an in-depth discussion on the importance of choosing a good validation set.\nOften people will default to using a random train/test split. This is what scikit-learn uses. This is a BAD idea very often.\nFor time-series data, it’s easier to infer gaps than it is to predict a block in the future. The latter is the more common task but a random split simulates the former, giving unrealistically good performance.\nFor image data, there may be people, boats, etc that are in the training set but not the test set. By failing to have new people in the validation set, the model can learn things about specific people/boats that it can’t rely on in practice.\n\n\n\nMetrics are things that are human-understandable.\nLoss functions should be smooth and differentiable to aid in training.\nThese can sometimes be the same thing, but not in general. For example, accuracy is a good metric in image classification. We could tweak the weights in such a way that it improves the model slightly, but not so much that it now correctly classifies a previously incorrect image. This means the metric function is bumpy, therefore a bad loss function.\nThis article goes into more detail on the choice of metrics.\nAI can be particularly dangerous at confirming systematic biases, because it is so good at optimising metrics, so it will conform to any biases present in the training data. Making decisions based on the model then reinforces those biases.\n\nGoodhart’s law applies: If a metric becomes a target it’s no longer a good metric\n\n\n\n\nThe best way to understand a metric is not to look at the mathematical formula, but to plot some data for which the metric is high, medium and low, then see what that tells you.\nAfter that, look at the equation to see if your intuition matches the logic.\n\n\n\nFast AI has a function to find a good starting point. Otherwise, pick a small value and keep doubling it until it falls apart."
  },
  {
    "objectID": "posts/ml/fastai/lesson4/lesson.html#references",
    "href": "posts/ml/fastai/lesson4/lesson.html#references",
    "title": "FastAI Lesson 4: Natural Language Processing",
    "section": "",
    "text": "Course lesson page\nHuggingFace Transformers\nVisualizing and Understanding Convolutional Networks\nHuggingFace models\nValidation sets\nModel metrics"
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html",
    "href": "posts/ml/fastai/lesson2/lesson.html",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "These are notes from lesson 2 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\n\nDeploy a model to Huggingface Spaces: see car classifier model\nDeploy a model to a Github Pages website: see car classifier website\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nA website for quizzes based on the book: AI quizzes\n\n\n\n\nIt can be useful to train a model on the data BEFORE you clean it\n\nCounterintuitive!\nThe confusion matrix output of the learner gives you a good intuition about which classifications are hard\nplot_top_losses shows which examples were hardest for the model to classify. This can find (1) when the model is correct but not confident, and (2) when the model was confident but incorrect\nImageClassifierCleaner shows the examples in the training and validation set ordered by loss, so we can choose to keep, reclassify or remove them\n\nFor image resizing, random resize crop can often be more effective.\n\nSquishing can result in weird, unrealistic images.\nPadding or mirroring can add false information that the model will erroneously learn.\nRandom crops give different sections of the image which acts as a form of data augmentation.\naug_transforms can be use for more sophisticated data augmentation like warping and recoloring images.\n\n\n\n\nOnce you are happy with the model you’ve trained, you can pickle the learner object and save it.\nlearn.export('model.pkl')\nThen you can add the saved model to the hugging face space. To use the model to make predictions, any external functions you used to create the model will need to be instantiated too.\nlearn = load_learner('model.pkl')\nlearn.predict(image)\n\n\n\nHugging face spaces hosts models with a choice of pre-canned interfaces to quickly deploy a model to the public. Gradio is used in the example in the lecture. Streamlit is an alternative to Gradio that is more flexible.\nGradio requires a dict of classes as keys and probabilities (as floats not tensors) as the values. To go from the Gradio prototype to a production app, you can view the Gradio API from the huggingface space which will show you the API. The API exposes an endpoint which you can then hit from your own frontend app.\nGithub pages is a free and simple way to host a public website. See this fastai repo as an example of a minimal example html website which issues GET requests to the Gradio API\n\n\n\nTo convert a notebook to a python script, you can add #|export to the top of any cells to include in the script, then use:\nfrom nbdev.export import notebook2script\nnotebook2script('name_of_output_file.py')\nUse #|default_exp app in the first cell of the notebook to set the default name of the exported python file.\n\n\n\nHow do we choose the number of epochs to train for? Whenever it is “good enough” for your use case.\nIf you need to train for longer, you may need to use data augmentation to prevent overfitting. Keep an eye on the validation error to check overfitting.\n\n\n\n\nCourse lesson page\nHuggingFace Spaces"
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html#data-pre-processing",
    "href": "posts/ml/fastai/lesson2/lesson.html#data-pre-processing",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "It can be useful to train a model on the data BEFORE you clean it\n\nCounterintuitive!\nThe confusion matrix output of the learner gives you a good intuition about which classifications are hard\nplot_top_losses shows which examples were hardest for the model to classify. This can find (1) when the model is correct but not confident, and (2) when the model was confident but incorrect\nImageClassifierCleaner shows the examples in the training and validation set ordered by loss, so we can choose to keep, reclassify or remove them\n\nFor image resizing, random resize crop can often be more effective.\n\nSquishing can result in weird, unrealistic images.\nPadding or mirroring can add false information that the model will erroneously learn.\nRandom crops give different sections of the image which acts as a form of data augmentation.\naug_transforms can be use for more sophisticated data augmentation like warping and recoloring images."
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html#saving-a-model",
    "href": "posts/ml/fastai/lesson2/lesson.html#saving-a-model",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "Once you are happy with the model you’ve trained, you can pickle the learner object and save it.\nlearn.export('model.pkl')\nThen you can add the saved model to the hugging face space. To use the model to make predictions, any external functions you used to create the model will need to be instantiated too.\nlearn = load_learner('model.pkl')\nlearn.predict(image)"
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html#hosting-a-model",
    "href": "posts/ml/fastai/lesson2/lesson.html#hosting-a-model",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "Hugging face spaces hosts models with a choice of pre-canned interfaces to quickly deploy a model to the public. Gradio is used in the example in the lecture. Streamlit is an alternative to Gradio that is more flexible.\nGradio requires a dict of classes as keys and probabilities (as floats not tensors) as the values. To go from the Gradio prototype to a production app, you can view the Gradio API from the huggingface space which will show you the API. The API exposes an endpoint which you can then hit from your own frontend app.\nGithub pages is a free and simple way to host a public website. See this fastai repo as an example of a minimal example html website which issues GET requests to the Gradio API"
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html#using-nbdev",
    "href": "posts/ml/fastai/lesson2/lesson.html#using-nbdev",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "To convert a notebook to a python script, you can add #|export to the top of any cells to include in the script, then use:\nfrom nbdev.export import notebook2script\nnotebook2script('name_of_output_file.py')\nUse #|default_exp app in the first cell of the notebook to set the default name of the exported python file."
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html#training-the-model-and-when-to-stop",
    "href": "posts/ml/fastai/lesson2/lesson.html#training-the-model-and-when-to-stop",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "How do we choose the number of epochs to train for? Whenever it is “good enough” for your use case.\nIf you need to train for longer, you may need to use data augmentation to prevent overfitting. Keep an eye on the validation error to check overfitting."
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html#references",
    "href": "posts/ml/fastai/lesson2/lesson.html#references",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "Course lesson page\nHuggingFace Spaces"
  },
  {
    "objectID": "posts/ml/fastai/lesson6/lesson.html",
    "href": "posts/ml/fastai/lesson6/lesson.html",
    "title": "FastAI Lesson 6: Random Forests",
    "section": "",
    "text": "These are notes from lesson 6 of Fast AI Practical Deep Learning for Coders.\n\n\nIt’s worth starting with a random forest as a baseline model because “it’s very hard to screw up”. Decision trees only consider the ordering of data, so are not sensitive to outliers, skewed distributions, dummy variables etc.\nA random forest is an ensemble of trees. A tree is an ensemble of binary splits.\nbinary split -&gt; tree -&gt; forest\n\n\nPick a column of the data set and split the rows into two groups. Predict the outcome based just on that.\nFor example, in the titanic dataset, pick the Sex column. This splits into male vs female. If we predict that all female passengers survived and all male passengers died, that’s a reasonably accurate prediction.\nA model which iterates through the variables and finds the best binary split is called a “OneR” or one rule model.\nThis was actually the state-of-the-art in the 90s and performs reasonably well across a lot of problems.\n\n\n\nThis extends the “OneR” idea to two or more splits.\nFor example, if we first split by sex, then find the next best variable to split on to create a two layer tree.\nLeaf nodes are the number of terminating nodes in the tree. OneR creates 2 leaf nodes. If we split one side again, we’ll have 3 leaf nodes. If we split the other side, to create a balanced two layer tree, we’ll have 4 leaf nodes.\n\n\n\nThe idea behind bagging is that if we have many unbiased, uncorrelated models, we can average their predictions to get an aggregate model that is better than any of them.\nEach individual model will overfit, so either be too high or too low for a given point, a positive or negative error term respectively. By combining multiple uncorrelated models, the error will average to 0.\nAn easy way to get many uncorrelated models is to only use a random subset of the data each time. Then build a decision tree for each subset.\nThis is the idea behind random forests.\nThe error decreases with the number of trees, with diminishing returns.\n\nJeremy’s rule of thumb: improvements level off after about 30 trees and he doesn’t often use &gt;100.\n\n\n\n\n\n\n\nA nice side effect of using decision trees is that we get feature importance plots for free.\nGini is a measure of inequality, similar to the score used in the notebook to quantify how good a split was.\nIntuitively, this measures the likelihood that if you picked two samples from a group, how likely is it they’d be the same every time. If the group is all the same, the probability is 1. If every item is different, the probability is 0.\nThe idea is for each split of a decision tree, we can track the column used to split and the amount that the gini decreased by. If we loop through the tree and accumulate the “gini reduction” for each column, we have a metric of how important that column was in splitting the dataset.\nThis makes random forests a useful first model when tackling a new big dataset, as it can give an insight into how useful each column is.\n\n\n\nA comment on explainability, regarding feature importance compared to SHAP, LIME etc. These explain the model, so to be useful the model needs to be accurate.\nIf you use feature importance from a bad model then the columns it claims are important might not actually be. So the usefulness of explainability techniques boils down to what models can they explain and how accurate are those models.\n\n\n\nFor each tree, we trained on a subset of the rows. We can then see how each one performed on the held out data; this is the out-of-bag (OOB) error per tree.\nWe can average this over all of the trees to get an overall OOB error for the forest.\n\n\n\nThese are not specific to random forests and can be applied to any ML model including deep neural networks.\nIf we want to know how an independent variable affects the dependent variable, a naive approach would be to just plot them.\nBut this could include a confounding variable. For example, the price of bulldozers increases over time, but driven by the presence of air conditioning which also increased over time.\nA partial dependence plot takes each row in turn and sets the column(s) of interest to the first value, say, year=1950. Then we predict the target variable using our model. Then repeat this for year=1951, 1952, etc.\nWe can then average the target variable per year to get a view of how it depends on the independent variable, all else being equal.\n\n\n\n\nWe make multiple trees, but instead of fitting all to different data subsets, we fit to residuals.\nSo we fit a very small tree (OneR even) to the data to get a first prediction. Then we calculate the error term. Then we fit another tree to predict the error term. Then calculate the second order error term. Then fit a tree to this, etc.\nThen our prediction is the sum of these trees, rather than the average like with a random forest.\nThis is “boosting”; calculating an error term then fitting another model to it.\nContrast this with “bagging” which was when we calculate multiple models to different subsets of data and average them.\n\n\n\nThe focus should be:\n\nCreate an effective validation set\nIterate quickly to find changes which improve the validation set\n\nTrain a simple model straight away, get a result and submit it. You can iterate and improve later.\nData augmentation, in particular test-time augmentation (TTA), can improve results.\nRules of thumb on model selection by problem type:\n\nFor computer vision use CNNs, fine-tune a pre-trained model. See comparison of pre-trained models here\n\nIf unsure which model to choose, start with (small) convnext.\nDifferent models can have big differences in accuracy so try a few small models from different families.\nOnce you are happy with the model, try a size up in that family.\nResizing images to a square is a good, easy compromise to accomodate many different image sizes, orientations and aspect ratios. A more involved approach that may improve results is to batch images together and resize them to the median rectangle size.\n\nFor tabular data, random forests will give a reasonably good results.\n\nGBMs will give a better result eventually, but with more effort required to get a good model.\nWorth running a hyperparameter grid search for GBM because it’s fiddly.\n\n\nRules of thumb on hardware:\n\nYou generally want ~8 physical CPUs per GPU\nIf model training is CPU bound, it can help to resize images to be smaller. The file I/O on the CPU may be taking too long.\nIf model training is CPU bound and GPU usage is low, you might as well go up to a “better” (i.e. bigger) model with no increase of overall execution time.\n\n\n\n\n\nCourse lesson page\nComparison of computer vision models for fine-tuning\nBagging predictors"
  },
  {
    "objectID": "posts/ml/fastai/lesson6/lesson.html#background-on-random-forests",
    "href": "posts/ml/fastai/lesson6/lesson.html#background-on-random-forests",
    "title": "FastAI Lesson 6: Random Forests",
    "section": "",
    "text": "It’s worth starting with a random forest as a baseline model because “it’s very hard to screw up”. Decision trees only consider the ordering of data, so are not sensitive to outliers, skewed distributions, dummy variables etc.\nA random forest is an ensemble of trees. A tree is an ensemble of binary splits.\nbinary split -&gt; tree -&gt; forest\n\n\nPick a column of the data set and split the rows into two groups. Predict the outcome based just on that.\nFor example, in the titanic dataset, pick the Sex column. This splits into male vs female. If we predict that all female passengers survived and all male passengers died, that’s a reasonably accurate prediction.\nA model which iterates through the variables and finds the best binary split is called a “OneR” or one rule model.\nThis was actually the state-of-the-art in the 90s and performs reasonably well across a lot of problems.\n\n\n\nThis extends the “OneR” idea to two or more splits.\nFor example, if we first split by sex, then find the next best variable to split on to create a two layer tree.\nLeaf nodes are the number of terminating nodes in the tree. OneR creates 2 leaf nodes. If we split one side again, we’ll have 3 leaf nodes. If we split the other side, to create a balanced two layer tree, we’ll have 4 leaf nodes.\n\n\n\nThe idea behind bagging is that if we have many unbiased, uncorrelated models, we can average their predictions to get an aggregate model that is better than any of them.\nEach individual model will overfit, so either be too high or too low for a given point, a positive or negative error term respectively. By combining multiple uncorrelated models, the error will average to 0.\nAn easy way to get many uncorrelated models is to only use a random subset of the data each time. Then build a decision tree for each subset.\nThis is the idea behind random forests.\nThe error decreases with the number of trees, with diminishing returns.\n\nJeremy’s rule of thumb: improvements level off after about 30 trees and he doesn’t often use &gt;100."
  },
  {
    "objectID": "posts/ml/fastai/lesson6/lesson.html#attributes-of-random-forests",
    "href": "posts/ml/fastai/lesson6/lesson.html#attributes-of-random-forests",
    "title": "FastAI Lesson 6: Random Forests",
    "section": "",
    "text": "A nice side effect of using decision trees is that we get feature importance plots for free.\nGini is a measure of inequality, similar to the score used in the notebook to quantify how good a split was.\nIntuitively, this measures the likelihood that if you picked two samples from a group, how likely is it they’d be the same every time. If the group is all the same, the probability is 1. If every item is different, the probability is 0.\nThe idea is for each split of a decision tree, we can track the column used to split and the amount that the gini decreased by. If we loop through the tree and accumulate the “gini reduction” for each column, we have a metric of how important that column was in splitting the dataset.\nThis makes random forests a useful first model when tackling a new big dataset, as it can give an insight into how useful each column is.\n\n\n\nA comment on explainability, regarding feature importance compared to SHAP, LIME etc. These explain the model, so to be useful the model needs to be accurate.\nIf you use feature importance from a bad model then the columns it claims are important might not actually be. So the usefulness of explainability techniques boils down to what models can they explain and how accurate are those models.\n\n\n\nFor each tree, we trained on a subset of the rows. We can then see how each one performed on the held out data; this is the out-of-bag (OOB) error per tree.\nWe can average this over all of the trees to get an overall OOB error for the forest.\n\n\n\nThese are not specific to random forests and can be applied to any ML model including deep neural networks.\nIf we want to know how an independent variable affects the dependent variable, a naive approach would be to just plot them.\nBut this could include a confounding variable. For example, the price of bulldozers increases over time, but driven by the presence of air conditioning which also increased over time.\nA partial dependence plot takes each row in turn and sets the column(s) of interest to the first value, say, year=1950. Then we predict the target variable using our model. Then repeat this for year=1951, 1952, etc.\nWe can then average the target variable per year to get a view of how it depends on the independent variable, all else being equal."
  },
  {
    "objectID": "posts/ml/fastai/lesson6/lesson.html#gradient-boosting-forests",
    "href": "posts/ml/fastai/lesson6/lesson.html#gradient-boosting-forests",
    "title": "FastAI Lesson 6: Random Forests",
    "section": "",
    "text": "We make multiple trees, but instead of fitting all to different data subsets, we fit to residuals.\nSo we fit a very small tree (OneR even) to the data to get a first prediction. Then we calculate the error term. Then we fit another tree to predict the error term. Then calculate the second order error term. Then fit a tree to this, etc.\nThen our prediction is the sum of these trees, rather than the average like with a random forest.\nThis is “boosting”; calculating an error term then fitting another model to it.\nContrast this with “bagging” which was when we calculate multiple models to different subsets of data and average them."
  },
  {
    "objectID": "posts/ml/fastai/lesson6/lesson.html#kaggle-iterations",
    "href": "posts/ml/fastai/lesson6/lesson.html#kaggle-iterations",
    "title": "FastAI Lesson 6: Random Forests",
    "section": "",
    "text": "The focus should be:\n\nCreate an effective validation set\nIterate quickly to find changes which improve the validation set\n\nTrain a simple model straight away, get a result and submit it. You can iterate and improve later.\nData augmentation, in particular test-time augmentation (TTA), can improve results.\nRules of thumb on model selection by problem type:\n\nFor computer vision use CNNs, fine-tune a pre-trained model. See comparison of pre-trained models here\n\nIf unsure which model to choose, start with (small) convnext.\nDifferent models can have big differences in accuracy so try a few small models from different families.\nOnce you are happy with the model, try a size up in that family.\nResizing images to a square is a good, easy compromise to accomodate many different image sizes, orientations and aspect ratios. A more involved approach that may improve results is to batch images together and resize them to the median rectangle size.\n\nFor tabular data, random forests will give a reasonably good results.\n\nGBMs will give a better result eventually, but with more effort required to get a good model.\nWorth running a hyperparameter grid search for GBM because it’s fiddly.\n\n\nRules of thumb on hardware:\n\nYou generally want ~8 physical CPUs per GPU\nIf model training is CPU bound, it can help to resize images to be smaller. The file I/O on the CPU may be taking too long.\nIf model training is CPU bound and GPU usage is low, you might as well go up to a “better” (i.e. bigger) model with no increase of overall execution time."
  },
  {
    "objectID": "posts/ml/fastai/lesson6/lesson.html#references",
    "href": "posts/ml/fastai/lesson6/lesson.html#references",
    "title": "FastAI Lesson 6: Random Forests",
    "section": "",
    "text": "Course lesson page\nComparison of computer vision models for fine-tuning\nBagging predictors"
  },
  {
    "objectID": "posts/ml/kalman_filter/kalman_filter.html",
    "href": "posts/ml/kalman_filter/kalman_filter.html",
    "title": "Kalman Filter Notes",
    "section": "",
    "text": "Notes from https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python\n\n\n\n\nWith measurements from 2 noisy sensors, we can combine them to get a much more accurate measurement. Never throw data away.\nIf we have a prediction model for how the world evolves (e.g. predicted daily weight gain) we can combine this with noisy measurements in the same way. Treat the prediction as a noisy sensor and combine as before, so that our estimate is some way between the prediction and the estimate. How close it is to one or the other depends on the relative accuracy of each, and this is a hyperparameter we can set.\nprediction =  x_est + dx*dt \n\nwhere:\n`x_est` is an initial constant that gets updated\n`dx` is a predicted daily weight gain rate\n`dt` is the time step\n\n\n\nWe can create a single parameter filter, where the parameter g corresponds to how strongly we trust the prediction over the measurement\nresidual = prediction - measurement\nestimate = measurement + g * residual\n\ng=0 =&gt; estimate=measurement\ng=1 =&gt; estimate=prediction\nfor other values of g, the estimate will be somewhere between the measurement and prediction\n\n\n\n\n\n\n\nFigure 1: g-h filter diagram\n\n\n\n\n\n\nWe can go one step further and use our noisy estimates of weight to refine our the daily weight gain w used in our prediction\ndx_t+1 = dx_t + h * residual / dt\nThis gives us the g-h filter - a generic filter which allows us to set a parameter g for the weight measurement confidence and h for the weight change measurement confidence.\nWe can then update the estimates for weight and weight change on every time step (or every measurement if time steps are irregular).\nThis insight forms the basis for Kalman filters, where we will set g and h dynamically on each time step.\n\n\n\nKey takeaways: - Multiple data points are more accurate than one data point, so throw nothing away no matter how inaccurate it is. - Always choose a number part way between two data points to create a more accurate estimate. - Predict the next measurement and rate of change based on the current estimate and how much we think it will change. - The new estimate is then chosen as part way between the prediction and next measurement scaled by how accurate each is. - The filter is only as good as the mathematical model used to express the system. - Filters are designed, not selected ad hoc. No choice of g and h applies to all scenarios.\nTerminology: - System - the object we want to estimate - State, x - the current configuration of the system. Hidden. - Measurement, z - measured value of the state from a noisy sensor. Observable. - State estimate - our filter’s estimate of the state. - Process model - the model we use to predict the next state based on the current state. - System propagation - the predict step - Measurement update - the update step - Epoch - one iteration of system propagation and measurement update.\n\n\n\n\n\n\nBayesian statistics treats probability as a belief about a single event. Frequentist statistics describes past events based on their frequency. It has no bearing on future events.\nIf I flip a coin 100 times and get 50 heads and 50 tails, frequentist statistics states the probability of heads was 50% for those cases. On the next coin flip, frequentist statistics has nothing to say about the probability. The state is simply unknown. Bayesian statistics incorporates these past events as a prior belief, so that we can say the next coin flip has a 50% chance of landing heads. “Belief” is a measure of the strength of our knowledge.\nWhen talking about the probability of something, we are implicitly saying “the probability that this event is true given past events”. This is a Bayesian approach. In practice, we may incorporate frequentist techniques too, as in the example above when the 100 previous coin tosses were used to inform our prior.\n\nPrior is the probability distribution before including the measurement’s information. This corresponds to the prediction in the Kalman filter.\nPosterior is a probability distribution after incorporating the measurement’s information. This corresponds to the estimated state in the Kalman filter.\nLikelihood is the joint probability of the observed data - how likely is each position given the measurement. This is not a probability distribution as it does not sum to 1.\n\nThe filter will use Bayes theorem:\nposterior = likelihood * prior / normalization\nIn even simpler filter terms:\nudpated knowledge = || likelihood of new knowledge * prior knowledge ||\nIf we have a prior distribution of positions and system model of the subsequent movement, we can convolve the two to calculate the posterior.\n\n\n\nThe discrete Bayes filter is a form of g-h filter. It is useful for multimodal, discrete problems.\nThe equations are:\nPredict step:\nx_bar = x * f_x(.)\nwhere:\n- x is the current state\n- f_x(.) is the state propagation function for x, i.e. the system model.\n\n\nUpdate step:\nx = ||L . x_bar||\nwhere:\n- L is the likely function\nIn pseudocode this is:\nInitialisation:\n1. Initialise our belief in the state.\n\nPredict:\n1. Predict state for the next time step using the system model.\n2. Adjust belief to account for uncertainty in prediction.\n\nUpdate:\n1. Get a measurement and belief about its accuracy (noise estimate).\n2. Compute likelihood of measurement matching each state.\n3. Update posterior state belief based on likelihood.\nAlgorithms of this form a called “predictor correctors”; we make a prediction then correct it. The predict step will always degrade our knowledge due to the uncertainty in the second step of the predict stage. But adding another measurement, even if noisy, improves our knowledge again. So we can converge on the most likely result.\n\n\n\nThe algorithm is trivial to implement, debug and understand.\nLimitations: - Scaling - Tracking i state variables results in O(n^i) runtime complexity. - Discrete - Most real-world examples are continuous. We can increase the granularity to get a discrete approximation of continuous measurements, but this increases the scale again. - Multimodal - Sometimes you require a single output value. - Needs a state change measurement.\nThe Kalman filter is based on the same idea that we can use Bayesian reasoning to combine measurements and system models. The fundamental insight in this chapter is that we multiply (convolve) probabilities when we measure and shift probabilities when we update, which leads to a converging solution.\n\n\n\n\nGaussian distributions address some of the limitations of the discrete Bayes filter, as they are continuous and unimodal.\n\n\n\nRandom variables - the possible values and associated probabilities of a particular event. Denoted with capital letter, e.g. X\nSample space - the range of values a random variable can take. This is not unique, e.g. for a dice roll could be {1,2,3,4,5,6}, or {even,odd} or {d}\nProbability distribution - the probability for the random variable to take any value in the sample space. Probability distribution denoted with lower case p, and probability of a single event denoted with upper case P, e.g. P(X=1) = p(1)\nTotal probability - Probabilities are non-negative and sum/integrate to 1.\nSummary statistics - mean, median, mode\nExpected value is the probability-weighted mean of values. This equals the mean if the probability distribution is uniform. E[X] = \\sum{p_i x_i} mean = \\sum{x_i}/n\nVariance - var(X) = E[(x-mu)^2] = \\sum{(x_i - mu)^2}/n The square in the variance (rather han absolute value) is somewhat arbitrary, and reflects that the sign of the deviation shouldn’t matter and larger deviations are “worse” than smaller deviations. Precision is sometimes used which is the inverse of the variance.\n\n\n\n\nContinuous, unimodal probability distributions.\nf(x, mu, sigma) = (1 / sigma sqrt(2*pi)) * e^(-(x-mu)^2/2*sigma^2)\n\nwhich, removing constants, is proportional to\ne^-x^2\nThe probability of a single point is infinitesimally small. We can think of this in Bayesian terms or frequentist terms. As a Bayesian, if the thermometer reads exactly 22°C, then our belief is described by the Gaussian curve; our belief that the actual (system) temperature is near 22°C is very high, and our belief that the actual temperature is, say, near 18 is very low. As a frequentist we would say that if we took 1 billion temperature measurements of a system at exactly 22°C, then a histogram of the measurements would look like a Gaussian curve.\nGaussians are nice to work with because the sum of independent Gaussian random variables is another Gaussian random variable. The product of Gaussian distributions will be a Gaussian function, i.e. it is Gaussian but may not sum to 1 so will need to be scaled. Sum of two Gaussians: - mu = mu_1 + mu_2 - var = var_1 + var_2 Product of two Gaussians: - mu = (var_1mu_2 + var_2mu_1) / (var_1 + var_2) - var = (var_1*var_2) / (var_1 + var_2)\nThey also mean we can summarise/approximate large datasets with only two numbers: mean and variance.\nBayes theorem applies to probability distribution functions (p) just like it does to individual events (P). From the general form of Bayes theorem:\np(A|B) = p(B|A)p(A)/p(B)\nRecasting this for the sensor problem, where A=x_i (the position) and B=z ())sensor reading):\np(x_i|z) = p(z|x_i)p(x_i)/p(z)\n\np(z|x_i) is the likelihood - the probability that we get the sensor measurement z at each position x_i\np(x_i) is the prior - our belief before incorporating the measurement\np(x_i) is the evidence - in practice this is often an intractable integral which we can sidestep by treating this as a normalisation factor, as we know the posterior pdf must sum to 1.\np(x_i|z) is the posterior - this is typically a hard question to answer, but using Bayes we can substitute it with the easier question answered by the likelihood and prior. Rather than answering “what is the probability of cancer given this test result” (hard problem) we instead answer “what is the probability of this test result given someone has cancer” (easier problem)\n\n\n\n\nA limitation of using Gaussians to model physical systems is that it has infinitely long tails, but in practice many physical quantities have hard bounds, e.g. weight cannot be negative. Under a Gaussian assumption, model people’s weight as a Gaussian would suggest there is a miniscule chance that someone weights 1000000kg, which is obviously wrong.\nThis can impact the performance of Kalman filters as they rely on assumptions of Gaussian noise, which aren’t strictly true.\n\n\n\n\nA filter to track one state variable, position. A Kalman filter is essentially a Bayesian filter that uses Gaussians, so combines the previous two chapters.\n\n\n\n\n“Artificial Intelligence for Robotics”. https://www.udacity.com/course/cs373\nThink Stats ebook https://greenteapress.com/thinkstats/\n\n\n\n\nFigure 1: g-h filter diagram"
  },
  {
    "objectID": "posts/ml/kalman_filter/kalman_filter.html#g-h-filters",
    "href": "posts/ml/kalman_filter/kalman_filter.html#g-h-filters",
    "title": "Kalman Filter Notes",
    "section": "",
    "text": "With measurements from 2 noisy sensors, we can combine them to get a much more accurate measurement. Never throw data away.\nIf we have a prediction model for how the world evolves (e.g. predicted daily weight gain) we can combine this with noisy measurements in the same way. Treat the prediction as a noisy sensor and combine as before, so that our estimate is some way between the prediction and the estimate. How close it is to one or the other depends on the relative accuracy of each, and this is a hyperparameter we can set.\nprediction =  x_est + dx*dt \n\nwhere:\n`x_est` is an initial constant that gets updated\n`dx` is a predicted daily weight gain rate\n`dt` is the time step\n\n\n\nWe can create a single parameter filter, where the parameter g corresponds to how strongly we trust the prediction over the measurement\nresidual = prediction - measurement\nestimate = measurement + g * residual\n\ng=0 =&gt; estimate=measurement\ng=1 =&gt; estimate=prediction\nfor other values of g, the estimate will be somewhere between the measurement and prediction\n\n\n\n\n\n\n\nFigure 1: g-h filter diagram\n\n\n\n\n\n\nWe can go one step further and use our noisy estimates of weight to refine our the daily weight gain w used in our prediction\ndx_t+1 = dx_t + h * residual / dt\nThis gives us the g-h filter - a generic filter which allows us to set a parameter g for the weight measurement confidence and h for the weight change measurement confidence.\nWe can then update the estimates for weight and weight change on every time step (or every measurement if time steps are irregular).\nThis insight forms the basis for Kalman filters, where we will set g and h dynamically on each time step.\n\n\n\nKey takeaways: - Multiple data points are more accurate than one data point, so throw nothing away no matter how inaccurate it is. - Always choose a number part way between two data points to create a more accurate estimate. - Predict the next measurement and rate of change based on the current estimate and how much we think it will change. - The new estimate is then chosen as part way between the prediction and next measurement scaled by how accurate each is. - The filter is only as good as the mathematical model used to express the system. - Filters are designed, not selected ad hoc. No choice of g and h applies to all scenarios.\nTerminology: - System - the object we want to estimate - State, x - the current configuration of the system. Hidden. - Measurement, z - measured value of the state from a noisy sensor. Observable. - State estimate - our filter’s estimate of the state. - Process model - the model we use to predict the next state based on the current state. - System propagation - the predict step - Measurement update - the update step - Epoch - one iteration of system propagation and measurement update."
  },
  {
    "objectID": "posts/ml/kalman_filter/kalman_filter.html#discrete-bayes-filter",
    "href": "posts/ml/kalman_filter/kalman_filter.html#discrete-bayes-filter",
    "title": "Kalman Filter Notes",
    "section": "",
    "text": "Bayesian statistics treats probability as a belief about a single event. Frequentist statistics describes past events based on their frequency. It has no bearing on future events.\nIf I flip a coin 100 times and get 50 heads and 50 tails, frequentist statistics states the probability of heads was 50% for those cases. On the next coin flip, frequentist statistics has nothing to say about the probability. The state is simply unknown. Bayesian statistics incorporates these past events as a prior belief, so that we can say the next coin flip has a 50% chance of landing heads. “Belief” is a measure of the strength of our knowledge.\nWhen talking about the probability of something, we are implicitly saying “the probability that this event is true given past events”. This is a Bayesian approach. In practice, we may incorporate frequentist techniques too, as in the example above when the 100 previous coin tosses were used to inform our prior.\n\nPrior is the probability distribution before including the measurement’s information. This corresponds to the prediction in the Kalman filter.\nPosterior is a probability distribution after incorporating the measurement’s information. This corresponds to the estimated state in the Kalman filter.\nLikelihood is the joint probability of the observed data - how likely is each position given the measurement. This is not a probability distribution as it does not sum to 1.\n\nThe filter will use Bayes theorem:\nposterior = likelihood * prior / normalization\nIn even simpler filter terms:\nudpated knowledge = || likelihood of new knowledge * prior knowledge ||\nIf we have a prior distribution of positions and system model of the subsequent movement, we can convolve the two to calculate the posterior.\n\n\n\nThe discrete Bayes filter is a form of g-h filter. It is useful for multimodal, discrete problems.\nThe equations are:\nPredict step:\nx_bar = x * f_x(.)\nwhere:\n- x is the current state\n- f_x(.) is the state propagation function for x, i.e. the system model.\n\n\nUpdate step:\nx = ||L . x_bar||\nwhere:\n- L is the likely function\nIn pseudocode this is:\nInitialisation:\n1. Initialise our belief in the state.\n\nPredict:\n1. Predict state for the next time step using the system model.\n2. Adjust belief to account for uncertainty in prediction.\n\nUpdate:\n1. Get a measurement and belief about its accuracy (noise estimate).\n2. Compute likelihood of measurement matching each state.\n3. Update posterior state belief based on likelihood.\nAlgorithms of this form a called “predictor correctors”; we make a prediction then correct it. The predict step will always degrade our knowledge due to the uncertainty in the second step of the predict stage. But adding another measurement, even if noisy, improves our knowledge again. So we can converge on the most likely result.\n\n\n\nThe algorithm is trivial to implement, debug and understand.\nLimitations: - Scaling - Tracking i state variables results in O(n^i) runtime complexity. - Discrete - Most real-world examples are continuous. We can increase the granularity to get a discrete approximation of continuous measurements, but this increases the scale again. - Multimodal - Sometimes you require a single output value. - Needs a state change measurement.\nThe Kalman filter is based on the same idea that we can use Bayesian reasoning to combine measurements and system models. The fundamental insight in this chapter is that we multiply (convolve) probabilities when we measure and shift probabilities when we update, which leads to a converging solution."
  },
  {
    "objectID": "posts/ml/kalman_filter/kalman_filter.html#probability-and-gaussians",
    "href": "posts/ml/kalman_filter/kalman_filter.html#probability-and-gaussians",
    "title": "Kalman Filter Notes",
    "section": "",
    "text": "Gaussian distributions address some of the limitations of the discrete Bayes filter, as they are continuous and unimodal.\n\n\n\nRandom variables - the possible values and associated probabilities of a particular event. Denoted with capital letter, e.g. X\nSample space - the range of values a random variable can take. This is not unique, e.g. for a dice roll could be {1,2,3,4,5,6}, or {even,odd} or {d}\nProbability distribution - the probability for the random variable to take any value in the sample space. Probability distribution denoted with lower case p, and probability of a single event denoted with upper case P, e.g. P(X=1) = p(1)\nTotal probability - Probabilities are non-negative and sum/integrate to 1.\nSummary statistics - mean, median, mode\nExpected value is the probability-weighted mean of values. This equals the mean if the probability distribution is uniform. E[X] = \\sum{p_i x_i} mean = \\sum{x_i}/n\nVariance - var(X) = E[(x-mu)^2] = \\sum{(x_i - mu)^2}/n The square in the variance (rather han absolute value) is somewhat arbitrary, and reflects that the sign of the deviation shouldn’t matter and larger deviations are “worse” than smaller deviations. Precision is sometimes used which is the inverse of the variance.\n\n\n\n\nContinuous, unimodal probability distributions.\nf(x, mu, sigma) = (1 / sigma sqrt(2*pi)) * e^(-(x-mu)^2/2*sigma^2)\n\nwhich, removing constants, is proportional to\ne^-x^2\nThe probability of a single point is infinitesimally small. We can think of this in Bayesian terms or frequentist terms. As a Bayesian, if the thermometer reads exactly 22°C, then our belief is described by the Gaussian curve; our belief that the actual (system) temperature is near 22°C is very high, and our belief that the actual temperature is, say, near 18 is very low. As a frequentist we would say that if we took 1 billion temperature measurements of a system at exactly 22°C, then a histogram of the measurements would look like a Gaussian curve.\nGaussians are nice to work with because the sum of independent Gaussian random variables is another Gaussian random variable. The product of Gaussian distributions will be a Gaussian function, i.e. it is Gaussian but may not sum to 1 so will need to be scaled. Sum of two Gaussians: - mu = mu_1 + mu_2 - var = var_1 + var_2 Product of two Gaussians: - mu = (var_1mu_2 + var_2mu_1) / (var_1 + var_2) - var = (var_1*var_2) / (var_1 + var_2)\nThey also mean we can summarise/approximate large datasets with only two numbers: mean and variance.\nBayes theorem applies to probability distribution functions (p) just like it does to individual events (P). From the general form of Bayes theorem:\np(A|B) = p(B|A)p(A)/p(B)\nRecasting this for the sensor problem, where A=x_i (the position) and B=z ())sensor reading):\np(x_i|z) = p(z|x_i)p(x_i)/p(z)\n\np(z|x_i) is the likelihood - the probability that we get the sensor measurement z at each position x_i\np(x_i) is the prior - our belief before incorporating the measurement\np(x_i) is the evidence - in practice this is often an intractable integral which we can sidestep by treating this as a normalisation factor, as we know the posterior pdf must sum to 1.\np(x_i|z) is the posterior - this is typically a hard question to answer, but using Bayes we can substitute it with the easier question answered by the likelihood and prior. Rather than answering “what is the probability of cancer given this test result” (hard problem) we instead answer “what is the probability of this test result given someone has cancer” (easier problem)\n\n\n\n\nA limitation of using Gaussians to model physical systems is that it has infinitely long tails, but in practice many physical quantities have hard bounds, e.g. weight cannot be negative. Under a Gaussian assumption, model people’s weight as a Gaussian would suggest there is a miniscule chance that someone weights 1000000kg, which is obviously wrong.\nThis can impact the performance of Kalman filters as they rely on assumptions of Gaussian noise, which aren’t strictly true."
  },
  {
    "objectID": "posts/ml/kalman_filter/kalman_filter.html#one-dimensional-kalman-filter",
    "href": "posts/ml/kalman_filter/kalman_filter.html#one-dimensional-kalman-filter",
    "title": "Kalman Filter Notes",
    "section": "",
    "text": "A filter to track one state variable, position. A Kalman filter is essentially a Bayesian filter that uses Gaussians, so combines the previous two chapters."
  },
  {
    "objectID": "posts/ml/kalman_filter/kalman_filter.html#references",
    "href": "posts/ml/kalman_filter/kalman_filter.html#references",
    "title": "Kalman Filter Notes",
    "section": "",
    "text": "“Artificial Intelligence for Robotics”. https://www.udacity.com/course/cs373\nThink Stats ebook https://greenteapress.com/thinkstats/\n\n\n\n\nFigure 1: g-h filter diagram"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Projects\n\n\n Back to top"
  },
  {
    "objectID": "plan.html",
    "href": "plan.html",
    "title": "Gurpreet Johl",
    "section": "",
    "text": "Fast AI\nSales\nAWS\nKalman filter"
  },
  {
    "objectID": "plan.html#current",
    "href": "plan.html#current",
    "title": "Gurpreet Johl",
    "section": "",
    "text": "Fast AI\nSales\nAWS\nKalman filter"
  },
  {
    "objectID": "plan.html#done",
    "href": "plan.html#done",
    "title": "Gurpreet Johl",
    "section": "Done",
    "text": "Done\n\nSQL\nDeep learning (Tensorflow)\nSoftware architecture\nMarketing\nPublic speaking\nPitching\nSaaS\nConsulting\nSystem design"
  },
  {
    "objectID": "plan.html#skills",
    "href": "plan.html#skills",
    "title": "Gurpreet Johl",
    "section": "Skills",
    "text": "Skills\n\nSoft skills\nDone: - Marketing - 1 page marketing plan; 22 immutable laws of marketing? - Public speaking - TED talk book. Steal the show by Michael Port? - Pitching - Pitch Anything by Oren Klaff - SaaS: Start small stay small; SaaS playbook - Consulting business book\nIn Progress: - Sales - Spin selling by Neil Rackham\nTo Do: - Strategy - startup bible book - Negotiating - never split the difference book and masterclass; influence the psychology of persuasion by Robert Cialdini - Sales - Founder Sales\nBacklog: - Operations - the goal by Eliyahu Goldratt - Leadership - teams of teams by Stanley mchrystal; Start with Why: How Great Leaders Inspire Everyone to Take Action by Simon Sinek - Networking - how to be a power connector by Judy Robinett - Fundraising - crack the funding code by Judy Robinett; venture deals by Brad Feld; the art of startup fundraising by Alejandro Cremades; the startup checklist by David Rose\n\n\nSoftware\nDone: - SQL - Udemy course - Software architecture - Udemy course - System design - Udemy course\nIn progress: - AWS solutions architect Udemy course\nTo Do: - Docker - Udemy course\nBacklog: - Game development - Unity udemy course - Cracking the coding interview - Data structures and algorithms https://allendowney.github.io/DSIRP/ - Devops - Continuous deployment book\n\n\nEngineering/maths\nDone: - Deep learning (Tensorflow) - Udemy course\nIn progress: - Fast AI course - Kalman filter book\nBacklog: - Computational linear algebra https://www.fast.ai/posts/2017-07-17-num-lin-alg.html - Timeseries forecasting: https://www.kaggle.com/learn/time-series?rvi=1 - RNNs? - https://greenteapress.com/wp/think-complexity-2e/\n\n\nStats\nBacklog: - Intro to probability book - Classical stats: t-stat, ANOVA https://www.udacity.com/course/intro-to-inferential-statistics–ud201 - Filters https://www.udacity.com/course/artificial-intelligence-for-robotics–cs373\n\n\nBig data\nBacklog: - https://www.tutorialspoint.com/pyspark/index.htm - https://www.udemy.com/course/best-hands-on-big-data-practices-and-use-cases-using-pyspark/ - https://www.udacity.com/course/learn-spark-at-udacity–ud2002\n\n\nInterview prep\nBacklog: - https://www.udacity.com/course/refresh-your-resume–ud243 - https://www.udacity.com/course/data-science-interview-prep–ud944 - https://www.udacity.com/course/machine-learning-interview-prep–ud1001"
  },
  {
    "objectID": "gen-deep-learning-series.html",
    "href": "gen-deep-learning-series.html",
    "title": "Series: Generative Deep Learning",
    "section": "",
    "text": "Generative AI: Chapter 1\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\n\nNotes from Generative Deep Learning book\n\n\n\n\n\nFeb 8, 2024\n\n\n1 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  }
]