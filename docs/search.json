[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Notes",
    "section": "",
    "text": "Series\n\nFastAI Series\nThis series contains notes from the fastai “Practical Deep Learning for Coders” course and related book.\n\n\nGen AI Series\nA series of posts on Generative AI techniques and projects.\n\n\nReact Series\nMaking pretty front-ends.\n\n\n\nAll Posts\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nGenerative AI: GANs\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\nGAN\n\n\n\nPart 4: Generative Adversarial Networks\n\n\n\n\n\nApr 10, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: TypeScript Essentials\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 7: I don’t got no type, bad code is the only thing that I like\n\n\n\n\n\nMar 21, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: Testing\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 6: Testing my patience\n\n\n\n\n\nMar 20, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: Deployment\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 5: Deploying React Apps\n\n\n\n\n\nMar 18, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: Debugging\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 4: A Bug’s Life\n\n\n\n\n\nMar 17, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: Styling\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 3: Styling it Out\n\n\n\n\n\nMar 16, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: JavaScript Essentials\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 2: WTF is JSX\n\n\n\n\n\nMar 14, 2024\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: A Gentle Introduction\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 1: Getting Started with React\n\n\n\n\n\nMar 12, 2024\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\nMeta Learning\n\n\n\n\n\n\nLearning\n\n\nAI\n\n\n\nMeta Meta Learning: What I Learned From Meta Learning by Radek Osmulski\n\n\n\n\n\nMar 10, 2024\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI: VAEs\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\nVAE\n\n\n\nPart 3: Variational Autoencoders\n\n\n\n\n\nMar 6, 2024\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 8: Convolutions\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 8\n\n\n\n\n\nMar 5, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 7: Collaborative Filtering\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 7\n\n\n\n\n\nMar 1, 2024\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI: Deep Learning Foundations\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\n\nPart 2: The Building Blocks for Generative AI\n\n\n\n\n\nFeb 26, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI: Intro\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\n\nPart 1: Introduction to Generative AI\n\n\n\n\n\nFeb 19, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 6.5: Road to the Top\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 6.5\n\n\n\n\n\nFeb 13, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 6: Random Forests\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 6\n\n\n\n\n\nOct 9, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 5: Natural Language Processing\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 5\n\n\n\n\n\nSep 23, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 4: Natural Language Processing\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 4\n\n\n\n\n\nSep 7, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 3: How Does a Neural Network Learn?\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 3\n\n\n\n\n\nSep 1, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 2: Deployment\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 2\n\n\n\n\n\nAug 30, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 1: Image Classification Models\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 1\n\n\n\n\n\nAug 23, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nSystem Design Notes\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\n\nNotes on System Design\n\n\n\n\n\nAug 14, 2023\n\n\n24 min\n\n\n\n\n\n\n\n\n\n\n\n\nPitching Notes\n\n\n\n\n\n\nBusiness\n\n\n\nNotes on Pitching\n\n\n\n\n\nAug 4, 2023\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nKalman Filter Notes\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\n\nNotes on Kalman filters\n\n\n\n\n\nJul 23, 2023\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nPublic Speaking Notes\n\n\n\n\n\n\nBusiness\n\n\n\nNotes on Public Speaking\n\n\n\n\n\nJul 20, 2023\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nMarketing Notes\n\n\n\n\n\n\nBusiness\n\n\n\nNotes on Marketing\n\n\n\n\n\nJul 18, 2023\n\n\n18 min\n\n\n\n\n\n\n\n\n\n\n\n\nSoftware Architecture Notes\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\n\nNotes on Software Architecture\n\n\n\n\n\nJun 23, 2023\n\n\n22 min\n\n\n\n\n\n\n\n\n\n\n\n\nTensorflow Notes\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\n\nNotes on Tensorflow\n\n\n\n\n\nFeb 23, 2023\n\n\n21 min\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n Back to top"
  },
  {
    "objectID": "fastai-series.html",
    "href": "fastai-series.html",
    "title": "Series: FastAI course",
    "section": "",
    "text": "FastAI Lesson 8: Convolutions\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 8\n\n\n\n\n\nMar 5, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 7: Collaborative Filtering\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 7\n\n\n\n\n\nMar 1, 2024\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 6.5: Road to the Top\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 6.5\n\n\n\n\n\nFeb 13, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 6: Random Forests\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 6\n\n\n\n\n\nOct 9, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 5: Natural Language Processing\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 5\n\n\n\n\n\nSep 23, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 4: Natural Language Processing\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 4\n\n\n\n\n\nSep 7, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 3: How Does a Neural Network Learn?\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 3\n\n\n\n\n\nSep 1, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 2: Deployment\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 2\n\n\n\n\n\nAug 30, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson 1: Image Classification Models\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nFastAI\n\n\n\nPractical Deep Learning for Coders: Lesson 1\n\n\n\n\n\nAug 23, 2023\n\n\n3 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "This site contains my personal software and AI projects.\nFor projects that I’ve worked on, see my projects section. For notes on various topics that I’ve made, see my notes section.\nIf you’re interested in any of this stuff, my socials are at the bottom of this page - get in touch!"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "About",
    "section": "Bio",
    "text": "Bio\nI’m a data scientist with experience at big hedge funds and plucky start-ups. I spent a few years at Man Group, both as a discretionary long-short equity analyst in GLG and on the systematic trading side as a quant in AHL, where I focused on equities and futures.\nI currently work at a fintech startup called BMLL Technologies, where I lead projects doing interesting things with massive amounts of financial order book data."
  },
  {
    "objectID": "react-series.html",
    "href": "react-series.html",
    "title": "Series: React",
    "section": "",
    "text": "React: TypeScript Essentials\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 7: I don’t got no type, bad code is the only thing that I like\n\n\n\n\n\nMar 21, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: Testing\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 6: Testing my patience\n\n\n\n\n\nMar 20, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: Deployment\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 5: Deploying React Apps\n\n\n\n\n\nMar 18, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: Debugging\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 4: A Bug’s Life\n\n\n\n\n\nMar 17, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: Styling\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 3: Styling it Out\n\n\n\n\n\nMar 16, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: JavaScript Essentials\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 2: WTF is JSX\n\n\n\n\n\nMar 14, 2024\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nReact: A Gentle Introduction\n\n\n\n\n\n\nEngineering\n\n\nSoftware\n\n\nReact\n\n\nWebDev\n\n\n\nPart 1: Getting Started with React\n\n\n\n\n\nMar 12, 2024\n\n\n12 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/ml/kalman_filter/kalman_filter.html",
    "href": "posts/ml/kalman_filter/kalman_filter.html",
    "title": "Kalman Filter Notes",
    "section": "",
    "text": "Notes from https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python\n\n\n\n\nWith measurements from 2 noisy sensors, we can combine them to get a much more accurate measurement. Never throw data away.\nIf we have a prediction model for how the world evolves (e.g. predicted daily weight gain) we can combine this with noisy measurements in the same way. Treat the prediction as a noisy sensor and combine as before, so that our estimate is some way between the prediction and the estimate. How close it is to one or the other depends on the relative accuracy of each, and this is a hyperparameter we can set.\nprediction =  x_est + dx*dt \n\nwhere:\n`x_est` is an initial constant that gets updated\n`dx` is a predicted daily weight gain rate\n`dt` is the time step\n\n\n\nWe can create a single parameter filter, where the parameter g corresponds to how strongly we trust the prediction over the measurement\nresidual = prediction - measurement\nestimate = measurement + g * residual\n\ng=0 =&gt; estimate=measurement\ng=1 =&gt; estimate=prediction\nfor other values of g, the estimate will be somewhere between the measurement and prediction\n\n\n\n\n\n\n\nFigure 1: g-h filter diagram\n\n\n\n\n\n\nWe can go one step further and use our noisy estimates of weight to refine our the daily weight gain w used in our prediction\ndx_t+1 = dx_t + h * residual / dt\nThis gives us the g-h filter - a generic filter which allows us to set a parameter g for the weight measurement confidence and h for the weight change measurement confidence.\nWe can then update the estimates for weight and weight change on every time step (or every measurement if time steps are irregular).\nThis insight forms the basis for Kalman filters, where we will set g and h dynamically on each time step.\n\n\n\nKey takeaways:\n\nMultiple data points are more accurate than one data point, so throw nothing away no matter how inaccurate it is.\nAlways choose a number part way between two data points to create a more accurate estimate.\nPredict the next measurement and rate of change based on the current estimate and how much we think it will change.\nThe new estimate is then chosen as part way between the prediction and next measurement scaled by how accurate each is.\nThe filter is only as good as the mathematical model used to express the system.\nFilters are designed, not selected ad hoc. No choice of g and h applies to all scenarios.\n\nTerminology:\n\nSystem - the object we want to estimate\nState, x - the current configuration of the system. Hidden.\nMeasurement, z - measured value of the state from a noisy sensor. Observable.\nState estimate - our filter’s estimate of the state.\nProcess model - the model we use to predict the next state based on the current state.\nSystem propagation - the predict step\nMeasurement update - the update step\nEpoch - one iteration of system propagation and measurement update.\n\n\n\n\n\n\n\nBayesian statistics treats probability as a belief about a single event. Frequentist statistics describes past events based on their frequency. It has no bearing on future events.\nIf I flip a coin 100 times and get 50 heads and 50 tails, frequentist statistics states the probability of heads was 50% for those cases. On the next coin flip, frequentist statistics has nothing to say about the probability. The state is simply unknown. Bayesian statistics incorporates these past events as a prior belief, so that we can say the next coin flip has a 50% chance of landing heads. “Belief” is a measure of the strength of our knowledge.\nWhen talking about the probability of something, we are implicitly saying “the probability that this event is true given past events”. This is a Bayesian approach. In practice, we may incorporate frequentist techniques too, as in the example above when the 100 previous coin tosses were used to inform our prior.\n\nPrior is the probability distribution before including the measurement’s information. This corresponds to the prediction in the Kalman filter.\nPosterior is a probability distribution after incorporating the measurement’s information. This corresponds to the estimated state in the Kalman filter.\nLikelihood is the joint probability of the observed data - how likely is each position given the measurement. This is not a probability distribution as it does not sum to 1.\n\nThe filter will use Bayes theorem:\nposterior = likelihood * prior / normalization\nIn even simpler filter terms:\nupdated knowledge = || likelihood of new knowledge * prior knowledge ||\nIf we have a prior distribution of positions and system model of the subsequent movement, we can convolve the two to calculate the posterior.\n\n\n\nThe discrete Bayes filter is a form of g-h filter. It is useful for multimodal, discrete problems.\nThe equations are:\nPredict step:\nx_bar = x * f_x(.)\nwhere:\n- x is the current state\n- f_x(.) is the state propagation function for x, i.e. the system model.\n\n\nUpdate step:\nx = ||L . x_bar||\nwhere:\n- L is the likely function\nIn pseudocode this is:\nInitialisation:\n1. Initialise our belief in the state.\n\nPredict:\n1. Predict state for the next time step using the system model.\n2. Adjust belief to account for uncertainty in prediction.\n\nUpdate:\n1. Get a measurement and belief about its accuracy (noise estimate).\n2. Compute likelihood of measurement matching each state.\n3. Update posterior state belief based on likelihood.\nAlgorithms of this form a called “predictor correctors”; we make a prediction then correct it. The predict step will always degrade our knowledge due to the uncertainty in the second step of the predict stage. But adding another measurement, even if noisy, improves our knowledge again. So we can converge on the most likely result.\n\n\n\nThe algorithm is trivial to implement, debug and understand.\nLimitations:\n\nScaling - Tracking i state variables results in O(n^i) runtime complexity.\nDiscrete - Most real-world examples are continuous. We can increase the granularity to get a discrete approximation of continuous measurements, but this increases the scale again.\nMultimodal - Sometimes you require a single output value.\nNeeds a state change measurement.\n\nThe Kalman filter is based on the same idea that we can use Bayesian reasoning to combine measurements and system models. The fundamental insight in this chapter is that we multiply (convolve) probabilities when we measure and shift probabilities when we update, which leads to a converging solution.\n\n\n\n\nGaussian distributions address some of the limitations of the discrete Bayes filter, as they are continuous and unimodal.\n\n\n\nRandom variables - the possible values and associated probabilities of a particular event. Denoted with capital letter, e.g. X\nSample space - the range of values a random variable can take. This is not unique, e.g. for a dice roll could be {1,2,3,4,5,6}, or {even,odd} or {d}\nProbability distribution - the probability for the random variable to take any value in the sample space. Probability distribution denoted with lower case p, and probability of a single event denoted with upper case P, e.g. P(X=1) = p(1)\nTotal probability - Probabilities are non-negative and sum/integrate to 1.\nSummary statistics - mean, median, mode\nExpected value is the probability-weighted mean of values. This equals the mean if the probability distribution is uniform. E[X] = \\sum{p_i x_i} mean = \\sum{x_i}/n\nVariance - var(X) = E[(x-mu)^2] = \\sum{(x_i - mu)^2}/n The square in the variance (rather han absolute value) is somewhat arbitrary, and reflects that the sign of the deviation shouldn’t matter and larger deviations are “worse” than smaller deviations. Precision is sometimes used which is the inverse of the variance.\n\n\n\n\nContinuous, unimodal probability distributions.\nf(x, mu, sigma) = (1 / sigma sqrt(2*pi)) * e^(-(x-mu)^2/2*sigma^2)\n\nwhich, removing constants, is proportional to e^-x^2\nThe probability of a single point is infinitesimally small. We can think of this in Bayesian terms or frequentist terms. As a Bayesian, if the thermometer reads exactly 22°C, then our belief is described by the Gaussian curve; our belief that the actual (system) temperature is near 22°C is very high, and our belief that the actual temperature is, say, near 18 is very low. As a frequentist we would say that if we took 1 billion temperature measurements of a system at exactly 22°C, then a histogram of the measurements would look like a Gaussian curve.\nGaussians are nice to work with because the sum of independent Gaussian random variables is another Gaussian random variable. The product of Gaussian distributions will be a Gaussian function, i.e. it is Gaussian but may not sum to 1 so will need to be scaled. Sum of two Gaussians:\n\nmu = mu_1 + mu_2\nvar = var_1 + var_2\n\nProduct of two Gaussians:\n\nmu = (var_1mu_2 + var_2mu_1) / (var_1 + var_2)\nvar = (var_1*var_2) / (var_1 + var_2)\n\nThey also mean we can summarise/approximate large datasets with only two numbers: mean and variance.\nBayes theorem applies to probability distribution functions (p) just like it does to individual events (P). From the general form of Bayes theorem:\np(A|B) = p(B|A)p(A)/p(B)\nRecasting this for the sensor problem, where A=x_i (the position) and B=z (sensor reading):\np(x_i|z) = p(z|x_i)p(x_i)/p(z)\n\np(z|x_i) is the likelihood - the probability that we get the sensor measurement z at each position x_i\np(x_i) is the prior - our belief before incorporating the measurement\np(x_i) is the evidence - in practice this is often an intractable integral which we can sidestep by treating this as a normalisation factor, as we know the posterior pdf must sum to 1.\np(x_i|z) is the posterior - this is typically a hard question to answer, but using Bayes we can substitute it with the easier question answered by the likelihood and prior. Rather than answering “what is the probability of cancer given this test result” (hard problem) we instead answer “what is the probability of this test result given someone has cancer” (easier problem)\n\n\n\n\nA limitation of using Gaussians to model physical systems is that it has infinitely long tails, but in practice many physical quantities have hard bounds, e.g. weight cannot be negative. Under a Gaussian assumption, model people’s weight as a Gaussian would suggest there is a miniscule chance that someone weights 1000000kg, which is obviously wrong.\nThis can impact the performance of Kalman filters as they rely on assumptions of Gaussian noise, which aren’t strictly true.\n\n\n\n\nA filter to track one state variable, position. A Kalman filter is essentially a Bayesian filter that uses Gaussians, so combines the previous two chapters.\n\n\n\n\nKalman filter repo\nArtificial Intelligence for Robotics course\nThink Stats ebook\n\n\n\n\nFigure 1: g-h filter diagram"
  },
  {
    "objectID": "posts/ml/kalman_filter/kalman_filter.html#g-h-filters",
    "href": "posts/ml/kalman_filter/kalman_filter.html#g-h-filters",
    "title": "Kalman Filter Notes",
    "section": "",
    "text": "With measurements from 2 noisy sensors, we can combine them to get a much more accurate measurement. Never throw data away.\nIf we have a prediction model for how the world evolves (e.g. predicted daily weight gain) we can combine this with noisy measurements in the same way. Treat the prediction as a noisy sensor and combine as before, so that our estimate is some way between the prediction and the estimate. How close it is to one or the other depends on the relative accuracy of each, and this is a hyperparameter we can set.\nprediction =  x_est + dx*dt \n\nwhere:\n`x_est` is an initial constant that gets updated\n`dx` is a predicted daily weight gain rate\n`dt` is the time step\n\n\n\nWe can create a single parameter filter, where the parameter g corresponds to how strongly we trust the prediction over the measurement\nresidual = prediction - measurement\nestimate = measurement + g * residual\n\ng=0 =&gt; estimate=measurement\ng=1 =&gt; estimate=prediction\nfor other values of g, the estimate will be somewhere between the measurement and prediction\n\n\n\n\n\n\n\nFigure 1: g-h filter diagram\n\n\n\n\n\n\nWe can go one step further and use our noisy estimates of weight to refine our the daily weight gain w used in our prediction\ndx_t+1 = dx_t + h * residual / dt\nThis gives us the g-h filter - a generic filter which allows us to set a parameter g for the weight measurement confidence and h for the weight change measurement confidence.\nWe can then update the estimates for weight and weight change on every time step (or every measurement if time steps are irregular).\nThis insight forms the basis for Kalman filters, where we will set g and h dynamically on each time step.\n\n\n\nKey takeaways:\n\nMultiple data points are more accurate than one data point, so throw nothing away no matter how inaccurate it is.\nAlways choose a number part way between two data points to create a more accurate estimate.\nPredict the next measurement and rate of change based on the current estimate and how much we think it will change.\nThe new estimate is then chosen as part way between the prediction and next measurement scaled by how accurate each is.\nThe filter is only as good as the mathematical model used to express the system.\nFilters are designed, not selected ad hoc. No choice of g and h applies to all scenarios.\n\nTerminology:\n\nSystem - the object we want to estimate\nState, x - the current configuration of the system. Hidden.\nMeasurement, z - measured value of the state from a noisy sensor. Observable.\nState estimate - our filter’s estimate of the state.\nProcess model - the model we use to predict the next state based on the current state.\nSystem propagation - the predict step\nMeasurement update - the update step\nEpoch - one iteration of system propagation and measurement update."
  },
  {
    "objectID": "posts/ml/kalman_filter/kalman_filter.html#discrete-bayes-filter",
    "href": "posts/ml/kalman_filter/kalman_filter.html#discrete-bayes-filter",
    "title": "Kalman Filter Notes",
    "section": "",
    "text": "Bayesian statistics treats probability as a belief about a single event. Frequentist statistics describes past events based on their frequency. It has no bearing on future events.\nIf I flip a coin 100 times and get 50 heads and 50 tails, frequentist statistics states the probability of heads was 50% for those cases. On the next coin flip, frequentist statistics has nothing to say about the probability. The state is simply unknown. Bayesian statistics incorporates these past events as a prior belief, so that we can say the next coin flip has a 50% chance of landing heads. “Belief” is a measure of the strength of our knowledge.\nWhen talking about the probability of something, we are implicitly saying “the probability that this event is true given past events”. This is a Bayesian approach. In practice, we may incorporate frequentist techniques too, as in the example above when the 100 previous coin tosses were used to inform our prior.\n\nPrior is the probability distribution before including the measurement’s information. This corresponds to the prediction in the Kalman filter.\nPosterior is a probability distribution after incorporating the measurement’s information. This corresponds to the estimated state in the Kalman filter.\nLikelihood is the joint probability of the observed data - how likely is each position given the measurement. This is not a probability distribution as it does not sum to 1.\n\nThe filter will use Bayes theorem:\nposterior = likelihood * prior / normalization\nIn even simpler filter terms:\nupdated knowledge = || likelihood of new knowledge * prior knowledge ||\nIf we have a prior distribution of positions and system model of the subsequent movement, we can convolve the two to calculate the posterior.\n\n\n\nThe discrete Bayes filter is a form of g-h filter. It is useful for multimodal, discrete problems.\nThe equations are:\nPredict step:\nx_bar = x * f_x(.)\nwhere:\n- x is the current state\n- f_x(.) is the state propagation function for x, i.e. the system model.\n\n\nUpdate step:\nx = ||L . x_bar||\nwhere:\n- L is the likely function\nIn pseudocode this is:\nInitialisation:\n1. Initialise our belief in the state.\n\nPredict:\n1. Predict state for the next time step using the system model.\n2. Adjust belief to account for uncertainty in prediction.\n\nUpdate:\n1. Get a measurement and belief about its accuracy (noise estimate).\n2. Compute likelihood of measurement matching each state.\n3. Update posterior state belief based on likelihood.\nAlgorithms of this form a called “predictor correctors”; we make a prediction then correct it. The predict step will always degrade our knowledge due to the uncertainty in the second step of the predict stage. But adding another measurement, even if noisy, improves our knowledge again. So we can converge on the most likely result.\n\n\n\nThe algorithm is trivial to implement, debug and understand.\nLimitations:\n\nScaling - Tracking i state variables results in O(n^i) runtime complexity.\nDiscrete - Most real-world examples are continuous. We can increase the granularity to get a discrete approximation of continuous measurements, but this increases the scale again.\nMultimodal - Sometimes you require a single output value.\nNeeds a state change measurement.\n\nThe Kalman filter is based on the same idea that we can use Bayesian reasoning to combine measurements and system models. The fundamental insight in this chapter is that we multiply (convolve) probabilities when we measure and shift probabilities when we update, which leads to a converging solution."
  },
  {
    "objectID": "posts/ml/kalman_filter/kalman_filter.html#probability-and-gaussians",
    "href": "posts/ml/kalman_filter/kalman_filter.html#probability-and-gaussians",
    "title": "Kalman Filter Notes",
    "section": "",
    "text": "Gaussian distributions address some of the limitations of the discrete Bayes filter, as they are continuous and unimodal.\n\n\n\nRandom variables - the possible values and associated probabilities of a particular event. Denoted with capital letter, e.g. X\nSample space - the range of values a random variable can take. This is not unique, e.g. for a dice roll could be {1,2,3,4,5,6}, or {even,odd} or {d}\nProbability distribution - the probability for the random variable to take any value in the sample space. Probability distribution denoted with lower case p, and probability of a single event denoted with upper case P, e.g. P(X=1) = p(1)\nTotal probability - Probabilities are non-negative and sum/integrate to 1.\nSummary statistics - mean, median, mode\nExpected value is the probability-weighted mean of values. This equals the mean if the probability distribution is uniform. E[X] = \\sum{p_i x_i} mean = \\sum{x_i}/n\nVariance - var(X) = E[(x-mu)^2] = \\sum{(x_i - mu)^2}/n The square in the variance (rather han absolute value) is somewhat arbitrary, and reflects that the sign of the deviation shouldn’t matter and larger deviations are “worse” than smaller deviations. Precision is sometimes used which is the inverse of the variance.\n\n\n\n\nContinuous, unimodal probability distributions.\nf(x, mu, sigma) = (1 / sigma sqrt(2*pi)) * e^(-(x-mu)^2/2*sigma^2)\n\nwhich, removing constants, is proportional to e^-x^2\nThe probability of a single point is infinitesimally small. We can think of this in Bayesian terms or frequentist terms. As a Bayesian, if the thermometer reads exactly 22°C, then our belief is described by the Gaussian curve; our belief that the actual (system) temperature is near 22°C is very high, and our belief that the actual temperature is, say, near 18 is very low. As a frequentist we would say that if we took 1 billion temperature measurements of a system at exactly 22°C, then a histogram of the measurements would look like a Gaussian curve.\nGaussians are nice to work with because the sum of independent Gaussian random variables is another Gaussian random variable. The product of Gaussian distributions will be a Gaussian function, i.e. it is Gaussian but may not sum to 1 so will need to be scaled. Sum of two Gaussians:\n\nmu = mu_1 + mu_2\nvar = var_1 + var_2\n\nProduct of two Gaussians:\n\nmu = (var_1mu_2 + var_2mu_1) / (var_1 + var_2)\nvar = (var_1*var_2) / (var_1 + var_2)\n\nThey also mean we can summarise/approximate large datasets with only two numbers: mean and variance.\nBayes theorem applies to probability distribution functions (p) just like it does to individual events (P). From the general form of Bayes theorem:\np(A|B) = p(B|A)p(A)/p(B)\nRecasting this for the sensor problem, where A=x_i (the position) and B=z (sensor reading):\np(x_i|z) = p(z|x_i)p(x_i)/p(z)\n\np(z|x_i) is the likelihood - the probability that we get the sensor measurement z at each position x_i\np(x_i) is the prior - our belief before incorporating the measurement\np(x_i) is the evidence - in practice this is often an intractable integral which we can sidestep by treating this as a normalisation factor, as we know the posterior pdf must sum to 1.\np(x_i|z) is the posterior - this is typically a hard question to answer, but using Bayes we can substitute it with the easier question answered by the likelihood and prior. Rather than answering “what is the probability of cancer given this test result” (hard problem) we instead answer “what is the probability of this test result given someone has cancer” (easier problem)\n\n\n\n\nA limitation of using Gaussians to model physical systems is that it has infinitely long tails, but in practice many physical quantities have hard bounds, e.g. weight cannot be negative. Under a Gaussian assumption, model people’s weight as a Gaussian would suggest there is a miniscule chance that someone weights 1000000kg, which is obviously wrong.\nThis can impact the performance of Kalman filters as they rely on assumptions of Gaussian noise, which aren’t strictly true."
  },
  {
    "objectID": "posts/ml/kalman_filter/kalman_filter.html#one-dimensional-kalman-filter",
    "href": "posts/ml/kalman_filter/kalman_filter.html#one-dimensional-kalman-filter",
    "title": "Kalman Filter Notes",
    "section": "",
    "text": "A filter to track one state variable, position. A Kalman filter is essentially a Bayesian filter that uses Gaussians, so combines the previous two chapters."
  },
  {
    "objectID": "posts/ml/kalman_filter/kalman_filter.html#references",
    "href": "posts/ml/kalman_filter/kalman_filter.html#references",
    "title": "Kalman Filter Notes",
    "section": "",
    "text": "Kalman filter repo\nArtificial Intelligence for Robotics course\nThink Stats ebook\n\n\n\n\nFigure 1: g-h filter diagram"
  },
  {
    "objectID": "posts/ml/tensorflow/tensorflow.html",
    "href": "posts/ml/tensorflow/tensorflow.html",
    "title": "Tensorflow Notes",
    "section": "",
    "text": "These are notes taken primarily from the Complete Tensorflow and Keras Udemy course\n\n\n\nData sourcing - train_test_split\nExploratory data analysis - plot distributions, correlations\nData cleaning - drop redundant columns, handle missing data (drop or impute)\nFeature engineering - one hot encoding categories, scaling/normalising numerical values, combining columns, extracting info from columns (e.g. zip code from address)\nModel selection and training - model and hyperparameters\nModel tuning and evaluation metrics - classification: classification_report, confusion matrix, accuracy, recall, F1. Regression: error (RMSE, MAE, etc)\nPredictions\n\n\n\n\n\nSupervised learning\n\nLinear regression\nSVM\nNaive Bayes\nKNN\nRandom forests\n\nUnsupervised learning\n\nDimensionality reduction\nClustering\n\nReinforcement Learning\n\nSupervised learning tasks can be broadly broken into:\n\nRegression\nClassification\n\nSingle class\nMulti class\n\n\n\n\n\n\n\nGeneral idea of a neural network that can then be extended to specific cases, e.g. CNNs and RNNs.\n\nPerceptron: a weighted average of inputs passed through some activation gate function to arrive at an output decision\nNetwork of perceptrons\nActivation function: a non-linear transfer function f(wx+b)\nCost function and gradient descent: convex optimisation of a loss function using sub-gradient descent. The optimizer can be set in the compile method of the model.\nBackpropagation: use chain rule to determine partial gradients of each weight and bias. This means we only need a single forward pass followed by a single backward pass. Contrast this to if we perturbed each weight or bias to determine each partial gradient: in that case, for each epoch we would need to run a forward pass per weight/bias in the network, which is potentially millions!\nDropout: a technique to avoid overfitting by randomly dropping neurons in each epoch.\n\nGeneral structure:\n\nInput layer\nHidden layer(s)\nOutput layer\n\nInput and output layers are determined by the problem:\n\nInput size: number of features in the data\nOutput size number of targets to predict, i.e. one for single class classification or single target regression, or multiple for multiclass (one per class)\nOutput layer activation determined by problem. For single class classification activation='sigmoid', for multiclass classification activation='softmax'\nLoss function determined by problem. For single class classification loss='binary_crossentropy', for multiclass classification loss='categorical_crossentropy'\n\nHidden layers are less well-defined. Some heuristics here.\nVanishing gradients can be an issue for lower layers in particular, where the partial gradients of individual layers can be very small, so when multiplied together in chain rule the gradient is vanishingly small. Exploding gradients are a similar issue but where gradients get increasingly large when multiplied together. Some techniques to rectify vanishing/exploding gradients:\n\nUse different activation functions with larger gradients close to 0 and 1, e.g. leaky ReLU or ELU (exponential linear unit)\nBatch normalisation: scale each gradient by the mean and standard deviation of the batch\nDifferent weight initialisation methods, e.g. Xavier initialisation\n\nExample model outline\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\n\nmodel = Sequential()\n\n# Input layer\nmodel.add(Dense(78, activation='relu'))  # Number of input features\nmodel.add(Dropout(0.2))  # Optional dropout layer after each layer. Omitted for next layers for clarity\n\n# Hidden layers\nmodel.add(Dense(39, activation='relu'))\nmodel.add(Dense(19, activation='relu'))\n\n# Output layer\nmodel.add(Dense(1, activation='sigmoid'))  # Number of target classes (single class in this case)\n\n# Problem setup\nmodel.compile(loss='binary_crossentropy', optimizer='adam')  # Type of problem (single class classification in this case)\n\n# Training\nmodel.fit(\n    x=X_train,\n    y=y_train,\n    batch_size=256,\n    epochs=30,\n    validation_data=(X_test, y_test)\n)\n\n# Prediction\nmodel.predict(new_input)  # The new input needs to be shaped and scaled the sane as the training data\n\n\n\nThese are used for image classification problems where convolutional filters are useful for extracting features from input arrays.\n\nImage kernels/filters\n\nGrayscale 2D arrays\nRGB 3D tensors\n\nConvolutional layers\nPooling layers\n\nGeneral structure:\n\nInput layer\nConvolutional layer\nPooling layer\n(Optionally more pairs of convolutional and pooling layers)\nFlattening layer\nDense hidden layer(s)\nOutput layer\n\nExample model outline\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nmodel = Sequential()\n\n# Convolutional layer followed by pooling. Deeper networks may have multiple pairs of these.\nmodel.add(Conv2D(filters=32, kernel_size=(4,4),input_shape=(28, 28, 1), activation='relu',))  # Input shape determined by input data size\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\n# Flatten _images from 2D to 1D before final layer\nmodel.add(Flatten())\n\n# Dense hidden layer\nmodel.add(Dense(128, activation='relu'))\n\n# Output layer\nmodel.add(Dense(10, activation='softmax'))  # Size and activation determined by problem; multiclass classification in this case\n\n# Problem setup\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')  # Loss determined by problem, multiclass classification in this case\n\n# Training (with optional early stopping)\nearly_stop = EarlyStopping(monitor='val_loss',patience=2)\nmodel.fit(x_train,y_cat_train,epochs=10,validation_data=(x_test,y_cat_test),callbacks=[early_stop])\n\n# Prediction\nmodel.predict(new_input)  # The new input needs to be shaped and scaled the sane as the training data\n\n\n\nThese are used for modelling sequences with variable lengths of inputs and outputs.\nRecurrent neurons take as their input the current data AND the previous epoch’s output. We could try to pass EVERY previous epoch’s output as an input, but would run into issues of vanushing gradients.\nLSTMs take this idea a step further by incorporating the previous epochs input AND some longer lookback of epoch where some old outputs are “forgotten”.\nA basic RNN:\n\n            --------------------------------\n            |                              |\n H_t-1 ---&gt; |                              |---&gt; H_t\n            |                              |\n X_t   ---&gt; |                              |\n            |                              |\n            --------------------------------\n\nwhere:\nH_t is the neuron's output at current epoch t\nH_t-1 is the neuron's output from the previous epoch t-1\nX_t is the input data at current epoch t\n\nH_t = tanh(W[H_t-1, X_t] + b)\nAn LSTM (Long Short Term Memory) unit:\n\n            --------------------------------\n            |                              |\n H_t-1 ---&gt; |                              |---&gt; H_t\n            |                              |\n C_t-1 ---&gt; |                              |---&gt; C_t\n            |                              |\n X_t   ---&gt; |                              |\n            |                              |\n            --------------------------------\n            \nThe `forget gate` determines which part of the old short-term memory and current input is \"forgotten\" by the new long-term memory.\nThese are a set of weights (between 0 and 1) that get applied to the old long-term memory to downweight it.\nF_t = sigmoid(W_F[H_t-1, X_t] + b_F)\n\nThe `input gate` i_t similarly gates the input and old short-term memory.\nThis will later (in the update gate) get combined  with a candidate value for the new long-term memory `C_candidate_t`\nI_t = sigmoid(W_I[H_t-1, X_t] + b_I)\nC_candidate_t = tanh(W_C_cand[H_t-1, X_t] + b_C_cand) \n\nThe `update gate` for the new long-term memory `C_t` is then calculated as a sum of forgotten old memory and input-weighted candidate memory:\nC_t = F_t*C_t-1 + I_t*C_candidate_t \n\nThe `output gate` O_t is a combination of the old short-term memory and latest input data.\nThis is then combined with the latest long-term memory to produce the output of the recurrent neuron, which is also the updated short-term memory H_t:\nO_t = sigmoid(W_O[H_t-1, X_t] + b_O)\nH_t = O_t * tanh(C_t) \n\nwhere:\nH_t is short-term memory at epoch t\nC_t is long-term memory at epoch t\nX_t is the input data at current epoch t\n\nsigmoid is a sigmoid  activation function\nF_t is an intermediate forget gate weight\nI_t is an intermediate input gate weight\nO_t is an intermediate output gate weight\nThere are several variants of RNNs. RNNs with peepholes\nThis leaks long-term memory into the forget, input and output gates. Note that the forget gate and input gate each get the OLD long-term memory, whereas the output gate gets the NEW long-term memory.\nF_t = sigmoid(W_F[C_t-1, H_t-1, X_t] + b_F)\nI_t = sigmoid(W_I[C_t-1, H_t-1, X_t] + b_I)\nO_t = sigmoid(W_O[C_t, H_t-1, X_t] + b_O)\nGated Recurrent Unit (GRU)\nThis combines the forget and input gates into a single gate. It also has some other changes. This is simpler than a typical LSTM model as it has fewer parameters. This makes it more computationally efficient, and in practice they can have similar performance.\nz_t = sigmoid(W_z[H_t-1, X_t])\nr_t = sigmoid(W_r[H_t-1, X_t])\nH_candidate_t = tanh(W_H_candidate[r_t*h_t-1, x_t])\nH_t = (1 - z_t) * H_t-1 + z_t * H_candidate_t\nThe sequences modelled with RNNs can be:\n\nOne-to-many\nMany-to-many\nMany-to-one\n\nConsiderations for choosing the input sequence length:\n\nIt should be long enough to capture any patterns or seasonality in the data\nThe validation set and test set each need to have a size at least (input_length + output_length), so the longer the input length, the more data is required to be set aside.\n\nThe validation loss for RNNs/LSTMs can be volatile, so allow a higher patience when using early stopping.\nExample model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,LSTM\n\n# Generators for helper functions to chunk up the input series into an array of pairs of (input_batch, target_output)\ntrain_generator = TimeseriesGenerator(scaled_train, scaled_train, length=batch_length, batch_size=1)\nvalidation_generator = TimeseriesGenerator(scaled_test, scaled_test, length=batch_length, batch_size=1)\n\n# Create an LSTM model\nmodel = Sequential()\nmodel.add(LSTM(100, activation='relu', input_shape=(batch_length, n_features)))\nmodel.add(Dense(1))  # Output layer\nmodel.compile(optimizer='adam', loss='mse')  # Problem setup for regression\n\n# Fit the model\nearly_stop = EarlyStopping(monitor='val_loss', patience=10)  # RNNs can be volatile so if use a higher patience with an early stopping callback\nmodel.fit(train_generator, epochs=20, validation_data=validation_generator, callbacks=[early_stop])\n\n# Evaluate test data\ntest_predictions = []\nfinal_batch = scaled_train[-batch_length:, :]\ncurrent_batch = final_batch.reshape((1, batch_length, n_features))\n\nfor test_val in scaled_test:\n    pred_val = model.predict(current_batch)  # These will later need to use the scaler.inverse_transform of the scaler originally used on the training data\n    test_predictions.append(pred_val[0])\n    current_batch = np.append(current_batch[:,1:,:], pred_val.reshape(1,1,1), axis=1)\n\n\n\nCharacter-based generative NLP: given an input string, e.g. [‘h’, ‘e’, ‘l’, ‘l’], predict the sequence shifted by one character, e.g. [‘e’, ‘l’, ‘l’, ‘o’]\nThe embedding, GRU and dense layers work on the sequence in the following way: \nSteps:\n\nRead in text data\nText processing and vectorisation - one-hot encoding\nCreate batches\nCreate the model\nTrain the model\nGenerate new text\n\nStep 1: Read in text data\n\nA corpus of &gt;1 million characters is a good size\nThe more distinct the style of your corpus, the more obvious it will be if your model has worked.\n\nGiven some structured input_text string, we can get the one-hot encoded vocab list of unique characters.\nvocab_list = sorted(set(input_text))\nStep 2: Text processing\nVectorise the text - create a mapping between character and integer. This is the encoding dictionary used to vectorise the corpus.\n# Mappings back and forth between characters and their encoded integers\nchar_to_ind = {char: ind for ind, char in enumerate(vocab_list)}\nind_to_char = {ind: char for ind, char in enumerate(vocab_list)}\n\nencoded_text = np.array([char_to_ind[char] for char in input_text])\nStep 3: Create batches\nThe sequence length should be long enough to capture structure, e.g. for poetry with rhyming couplets it should be the number of characters in 3 lines to capture the rhyme and non-rhyme. Too long a sequence makes the model take longer to train and captures too much historical noise.\nCreate a shuffled dataset where:\n\ninput sequence is the first n characters\noutput sequence is n characters lagged by one\n\nWe want to shuffle these sequence pairs into a random order so the model doesn’t overfit to any section of the text, but can instead generate characters given any seed text.\nseq_len = 120\ntotal_num_seq = len(input_text) // (seq_len + 1)\n\n# Create Training Sequences\nchar_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\nsequences = char_dataset.batch(seq_len + 1, drop_remainder=True)  # drop_remainder drops the last incomplete sequence\n\n# Create tuples of input and output sequences\ndef create_seq_targets(seq):\n    input_seq = seq[:-1]\n    output_seq = seq[-1:]\n    return input_seq, output_seq\ndataset = sequences.map(create_seq_targets)\n\n# Generate training batches\nbatch_size = 128\nbuffer_size = 10000  # Shuffle this amount of batches rather than shuffling the entire dataset in memory\ndataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\nStep 4: Create the model\nSet up the loss function and layers.\nThis model uses 3 layers:\n\nEmbedding\n\nEmbed positive integers, e.g. “a”&lt;-&gt;1, as dense vectors of fixed size. This embedding size is a hyperparameter for the user to decide.\nNumber of layers should be smaller but a similar scale to the vocab size; typically use the power of 2 that is just smaller than the vocab size.\n\nGRU\n\nThis is the main thing to vary in the model: number of hidden layers and number of neurons in each, types of neurons (RNN, LSTM, GRU), etc. Typically use lots of layers here.\n\nDense\n\nOne neuron per character (one-hot encoded), so this produces a probability per character. The user can vary the “temperature”, choosing less probable characters more/less often.\n\n\nLoss function:\n\nSparse categorical cross-entropy\nUse sparse categorical cross-entropy because the classes are mutually exclusive. Use regular categorical cross-entropy when one smaple can have multiple classes, or the labels are soft probabilities.\nSee link in NLP appendix for discussion.\nUse logits=True because vocab input is one hot encoded.\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, GRU\nfrom tensorflow.keras.losses import sparse_categorical_crossentropy\n\n# Hyperparameters\nvocab_size = len(vocab_list)\nembed_dim = 64\nrnn_neurons = 1026\n\n# Create model \nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embed_dim, batch_input_shape=[batch_size, None]))\nmodel.add(GRU(rnn_neurons, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'))\nmodel.add(Dense(vocab_size))\nsparse_cat_loss = lambda y_true,y_pred: sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)  # We want a custom loss function with logits=True\nmodel.compile(optimizer='adam', loss=sparse_cat_loss) \nStep 5: Train the model\nepochs = 30\nmodel.fit(dataset,epochs=epochs)\nStep 6: Generate text\nAllow the model to accept a batch size of 1 to allow us to give an input seed text from which to generate text from.\nWe format the seed text so it can be fed into the network, then loop through generating one character at a time and feeding that back into the model.\nmodel.build(tf.TensorShape([1, None]))\n\ndef generate_text(model, input_seed_text, gen_size=100, temperature=1.0):\n    \"\"\"\n    model: tensorflow.model\n    input_seed_text: str\n    gen_size: int\n    temperature: float\n    \"\"\"\n    generated_text_result = []\n    vectorised_seed = tf.expand_dims([char_to_ind[s] for s in input_seed_text], 0)\n    model.reset_states()\n    \n    for k in range(gen_size):\n        raw_predictions = model(vectorised_seed)\n        predictions = tf.squeeze(raw_predictions, 0) / temperature\n        predicted_index = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n        generated_text_result.append(ind_to_char[predicted_index])\n        \n        # Set a new vectorised seed to generate the next character in the loop\n        vectorised_seed = tf.expand_dims([predicted_index], 0)\n        \n    return (input_seed_text + ''.join(generated_text_result))\n\n\n\nThis is an unsupervised learning problem, therefore evaluation metrics are different. Sometimes called semi-supervised as labels may be used in training the model, but not available when it’s used to predict new values.\nApplications:\n\nDimensionality reduction\nNoise removal\n\nDesigned to reproduce its input in the output layer. Number of input neurons is the same as the number of output layers. The encoder reduces dimensionality to the hidden layer. The hidden layer should contain the most relevant information required to reconstruct the input. The decoder reconstructs a high dimensional output from a low dimensional input.\n\nEncoder: Input layer -&gt; Hidden layer\nDecoder: Hidden layer -&gt; Output\n\nStacked autoencoders include multiple hidden layers.\nAutoencoder for dimensionality reduction\nLet’s try to reduce an input dataset from 3 dimensions to 2. We want to train an autoencoder to go from 3 -&gt; 2 -&gt; 3.\nWhen we scale data, we fit and transform on the entire dataset rather than a train/test split, because there is no ground truth for the “correct” dimensionality reduction.\nUsing SGD in Autoencoders allows the learning rate to be varied as a hyperparameter to affect how well the hidden layer learns. Larger values will train quicker but potentially perform worse.\nTo retrieve the lower dimensionality representation, use just the encoder half of the trained autoencoder to predict the input.\n# Scale data\nscaler = MinMaxScaler()\nscaled_data = scaler.fit_transform(input_features)\n\n# Create model\nencoder = Sequential()\nencoder.add(Dense(units=2, acivation='relu', input_shape=[3]))\n\ndecoder = Sequential()\ndecoder.add(Dense(units=3, acivation='relu', input_shape=[2]))\n\nautoencoder = Sequential([encoder, decoder])\nautoencoder.compile(loss='mse', optimizer=SGD(lr=1.5))\n\n# Train the full autoencoder model\nautoencoder.fit(scaled_data, scaled_data, epochs=5)\n\n# Reduce dimensionality by using just the encoder part of the trained model\nencoded_2dim = encoder.predict(scaled_data)\nAutoencoder for noise removal\nStarting with clean images, we want to train an autoencoder to learn to remove noise. We create a noisy dataset by adding noise to our clean images. We then try to reconstruct that input, and compare to the original clean images as the ground truth.\nStarting with 28x28 images (e.g. MNIST dataset), this gives 784 input features when flattened. Use multiple hidden layers as there are a large number of input features that would be difficult to learn in one layer. The numbers of layers and number of neurons in each are hyperparameters to tune; decreasing in powers of 2 is a good rule of thumb.\nAdd a Gaussian noise layer to train the model to remove noise. Without the Gaussian layer, the model would be used for dimensionality reduction rather than noise removal.\nThe final layer uses a sigmoid activation because it is a binary classification problem - does the output match the input?\nencoder = Sequential()\nencoder.add(Flatten(input_shape=[28, 28]))\nencoder.add(Gaussian(0.2))  # Optional layer if training for noise removal rather than dimensionality reduction\nencoder.add(Dense(400, activation='relu'))\nencoder.add(Dense(200, activation='relu'))\nencoder.add(Dense(100, activation='relu'))\nencoder.add(Dense(50, activation='relu'))\nencoder.add(Dense(25, activation='relu'))\n\ndecoder = Sequential()\ndecoder.add(Dense(50, input_shape=[25], activation='relu'))\ndecoder.add(Dense(100, activation='relu'))\ndecoder.add(Dense(200, activation='relu'))\ndecoder.add(Dense(400, activation='relu'))\ndecoder.add(Dense(28*28, activation='sigmoid'))\n\nautoencoder = Sequential([encoder, decoder])\nautoencoder.compile(loss='binary_crossentropy', optimizer=SGD(lr=1.5), metrics=['accuracy'])\nautoencoder.fit(X_train, X_train, epochs=5)\n\n\n\nUse 2 networks competing against each other to generate data - counterfeiter (generator) vs detective (discriminator).\n\nGenerator: recieves random noise\nDiscriminator: receives data set containing real data and fake generated data, and attempts to calssify real vs fake (binary classificaiton).\n\nTwo training phases, each only training one of the networks:\n\nTrain discriminator - real images (labeled 1) and generated images (labeled 0) are fed to the discriminator network.\nTrain generator - only feed generated images to discriminator, ALL labeled 1.\n\nThe generator never sees real images, it creates images based only off of the gradients flowing back from the discriminator.\nDifficulties with GANs:\n\nTraining resources - GPUs generally required\nMode collapse - generator learns an image that fools the discriminator, then only produces this image. Deep convolutional GANs and mini-batch discrimination are two solution to this problem.\nInstability - True performance is hard to measure; just because the discriminator was fooled, it doesn’t mean that the generated image was actually realistic.\n\nCreating the model\nWe create the generator and discriminator as separate models, then join them into a GAN object. This is analogous to how we joined an encoder and decoder into an autoencoder.\nThe discriminator is trained on a binary classification problem, real or fake, so uses a binary cross entropy loss function. Backpropagation only alters the weights of the discriminator in the first phase, not the generator.\ndiscriminator = Sequential()\n\n# Flatten the image\ndiscriminator.add(Flatten(input_shape=[28,28]))\n\n# Some hidden layers\ndiscriminator.add(Dense(150,activation='relu'))\ndiscriminator.add(Dense(100,activation='relu'))\n\n# Binary classification - real vs fake\ndiscriminator.add(Dense(1,activation=\"sigmoid\"))\ndiscriminator.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\nThe generator is trained in the second phase.\nWe set a codings_size, which is the size of the latent representation from which the generator will create a full image. For example, if we want to create a 28x28 image (784 pixels), we may want to generate this from a 100 element input. This is analogous to the decoder part of an autoencoder.\nThe codings_size should be smaller than the total number of features but large enough so that there is something to learn from.\nThe generator is NOT compiled at this step, it is compiled in the combined GAN later so that it is only trained at that step.\ncodings_size = 100\ngenerator = Sequential()\n\n# Input shape of first layer is the codings_size\ngenerator.add(Dense(150, activation=\"relu\", input_shape=[codings_size]))\n\n# Some hidden layers\ngenerator.add(Dense(200,activation='relu'))\n\n# Output is the size of the image we want to create\ngenerator.add(Dense(784, activation=\"sigmoid\"))\ngenerator.add(Reshape([28,28]))\nThe GAN combines the generator and discriminator.\nThe discriminator is only trained in the first phase, not the second phase.\nGAN = Sequential([generator, discriminator])\ndiscriminator.trainable = False\nGAN.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\nTraining the model\nWe first create batches of images from our input data, similarly to previous models.\nbatch_size = 32  # Size of training input batches\nbuffer_size = 1000  # Don't load the entire dataset into memory\nepochs = 1\n\nraw_dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(buffer_size=buffer_size)\nbatched_dataset = raw_dataset.batch(batch_size, drop_remainder=True).prefetch(1)\nThe training loop for each epoch trains the discriminator to distinguish real vs fake images (binary classification). Then it trains the generator to generate fake images using ONLY the gradients learned in the discriminator training step; the generator NEVER sees any real images.\ngenerator, discriminator = GAN.layers\n\nfor epoch in range(epochs):\n    for index, X_batch in enumerate(batched_dataset):\n        # Phase 1: Train the discriminator\n        noise = tf.random.normal(shape=[batch_size, codings_size])\n        generated_images = generator(noise)\n        real_images = tf.dtypes.cast(X_batch,tf.float32)\n        X_train_discriminator = tf.concat([generated_images, real_images], axis=0)\n        y_train_discriminator = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)  # Targets set to zero for fake _images and 1 for real _images\n        discriminator.trainable = True\n        discriminator.train_on_batch(X_train_discriminator, y_train_discriminator)\n\n        # Phase 2: Train the generator\n        noise = tf.random.normal(shape=[batch_size, codings_size])\n        y_train_generator = tf.constant([[1.]] * batch_size)  # We want discriminator to believe that fake _images are real\n        discriminator.trainable = False\n        GAN.train_on_batch(noise, y_train_generator)    \nDeep convolutional GANs\nThese are GANs which use convolutional layers inside the discriminator and generator models.\nThe models are almost identical to the models above, just with additional Conv2D, Conv2DTranspose and BatchNormalization layers.\nChanges compared to regular GANs:\n\nAdditional convolutional layers in model.\nReshape the training set to match the images.\nRescale the training data to be between -1 and 1 so that tanh activation function works.\nActivation of discriminator output layer is tanh rather than sigmoid.\n\n\n\n\n\n\n\n\nComplete Tensorflow and Keras Udemy course\nIntuition behind neural networks\nThe deep learning bible (with lectures)\nHeuristics for choosing hidden layers\nAlternatives to the deprecated model predict for different classification problems\nMIT course\n\n\n\n\n\nImage kernels explained\nChoosing CNN layers\n\n\n\n\n\nOverview of RNNs\nWikipedia page contains the equations of LSTMs and peepholes\nLSTMs vs GRUs\nWorked example of LSTM\nRNN cheatsheet\n\n\n\n\n\nThe unreasonable effectiveness of RNNs - essay on RNNs applied to NLP\nSparse vs dense categorical crossentropy loss function\n\n\n\n\n\nbuffer_size argument in tensorflow prefetch and shuffle\nMode collapse"
  },
  {
    "objectID": "posts/ml/tensorflow/tensorflow.html#end-end-process",
    "href": "posts/ml/tensorflow/tensorflow.html#end-end-process",
    "title": "Tensorflow Notes",
    "section": "",
    "text": "Data sourcing - train_test_split\nExploratory data analysis - plot distributions, correlations\nData cleaning - drop redundant columns, handle missing data (drop or impute)\nFeature engineering - one hot encoding categories, scaling/normalising numerical values, combining columns, extracting info from columns (e.g. zip code from address)\nModel selection and training - model and hyperparameters\nModel tuning and evaluation metrics - classification: classification_report, confusion matrix, accuracy, recall, F1. Regression: error (RMSE, MAE, etc)\nPredictions"
  },
  {
    "objectID": "posts/ml/tensorflow/tensorflow.html#machine-learning-categories",
    "href": "posts/ml/tensorflow/tensorflow.html#machine-learning-categories",
    "title": "Tensorflow Notes",
    "section": "",
    "text": "Supervised learning\n\nLinear regression\nSVM\nNaive Bayes\nKNN\nRandom forests\n\nUnsupervised learning\n\nDimensionality reduction\nClustering\n\nReinforcement Learning\n\nSupervised learning tasks can be broadly broken into:\n\nRegression\nClassification\n\nSingle class\nMulti class"
  },
  {
    "objectID": "posts/ml/tensorflow/tensorflow.html#deep-learning-models",
    "href": "posts/ml/tensorflow/tensorflow.html#deep-learning-models",
    "title": "Tensorflow Notes",
    "section": "",
    "text": "General idea of a neural network that can then be extended to specific cases, e.g. CNNs and RNNs.\n\nPerceptron: a weighted average of inputs passed through some activation gate function to arrive at an output decision\nNetwork of perceptrons\nActivation function: a non-linear transfer function f(wx+b)\nCost function and gradient descent: convex optimisation of a loss function using sub-gradient descent. The optimizer can be set in the compile method of the model.\nBackpropagation: use chain rule to determine partial gradients of each weight and bias. This means we only need a single forward pass followed by a single backward pass. Contrast this to if we perturbed each weight or bias to determine each partial gradient: in that case, for each epoch we would need to run a forward pass per weight/bias in the network, which is potentially millions!\nDropout: a technique to avoid overfitting by randomly dropping neurons in each epoch.\n\nGeneral structure:\n\nInput layer\nHidden layer(s)\nOutput layer\n\nInput and output layers are determined by the problem:\n\nInput size: number of features in the data\nOutput size number of targets to predict, i.e. one for single class classification or single target regression, or multiple for multiclass (one per class)\nOutput layer activation determined by problem. For single class classification activation='sigmoid', for multiclass classification activation='softmax'\nLoss function determined by problem. For single class classification loss='binary_crossentropy', for multiclass classification loss='categorical_crossentropy'\n\nHidden layers are less well-defined. Some heuristics here.\nVanishing gradients can be an issue for lower layers in particular, where the partial gradients of individual layers can be very small, so when multiplied together in chain rule the gradient is vanishingly small. Exploding gradients are a similar issue but where gradients get increasingly large when multiplied together. Some techniques to rectify vanishing/exploding gradients:\n\nUse different activation functions with larger gradients close to 0 and 1, e.g. leaky ReLU or ELU (exponential linear unit)\nBatch normalisation: scale each gradient by the mean and standard deviation of the batch\nDifferent weight initialisation methods, e.g. Xavier initialisation\n\nExample model outline\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\n\nmodel = Sequential()\n\n# Input layer\nmodel.add(Dense(78, activation='relu'))  # Number of input features\nmodel.add(Dropout(0.2))  # Optional dropout layer after each layer. Omitted for next layers for clarity\n\n# Hidden layers\nmodel.add(Dense(39, activation='relu'))\nmodel.add(Dense(19, activation='relu'))\n\n# Output layer\nmodel.add(Dense(1, activation='sigmoid'))  # Number of target classes (single class in this case)\n\n# Problem setup\nmodel.compile(loss='binary_crossentropy', optimizer='adam')  # Type of problem (single class classification in this case)\n\n# Training\nmodel.fit(\n    x=X_train,\n    y=y_train,\n    batch_size=256,\n    epochs=30,\n    validation_data=(X_test, y_test)\n)\n\n# Prediction\nmodel.predict(new_input)  # The new input needs to be shaped and scaled the sane as the training data\n\n\n\nThese are used for image classification problems where convolutional filters are useful for extracting features from input arrays.\n\nImage kernels/filters\n\nGrayscale 2D arrays\nRGB 3D tensors\n\nConvolutional layers\nPooling layers\n\nGeneral structure:\n\nInput layer\nConvolutional layer\nPooling layer\n(Optionally more pairs of convolutional and pooling layers)\nFlattening layer\nDense hidden layer(s)\nOutput layer\n\nExample model outline\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nmodel = Sequential()\n\n# Convolutional layer followed by pooling. Deeper networks may have multiple pairs of these.\nmodel.add(Conv2D(filters=32, kernel_size=(4,4),input_shape=(28, 28, 1), activation='relu',))  # Input shape determined by input data size\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\n# Flatten _images from 2D to 1D before final layer\nmodel.add(Flatten())\n\n# Dense hidden layer\nmodel.add(Dense(128, activation='relu'))\n\n# Output layer\nmodel.add(Dense(10, activation='softmax'))  # Size and activation determined by problem; multiclass classification in this case\n\n# Problem setup\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')  # Loss determined by problem, multiclass classification in this case\n\n# Training (with optional early stopping)\nearly_stop = EarlyStopping(monitor='val_loss',patience=2)\nmodel.fit(x_train,y_cat_train,epochs=10,validation_data=(x_test,y_cat_test),callbacks=[early_stop])\n\n# Prediction\nmodel.predict(new_input)  # The new input needs to be shaped and scaled the sane as the training data\n\n\n\nThese are used for modelling sequences with variable lengths of inputs and outputs.\nRecurrent neurons take as their input the current data AND the previous epoch’s output. We could try to pass EVERY previous epoch’s output as an input, but would run into issues of vanushing gradients.\nLSTMs take this idea a step further by incorporating the previous epochs input AND some longer lookback of epoch where some old outputs are “forgotten”.\nA basic RNN:\n\n            --------------------------------\n            |                              |\n H_t-1 ---&gt; |                              |---&gt; H_t\n            |                              |\n X_t   ---&gt; |                              |\n            |                              |\n            --------------------------------\n\nwhere:\nH_t is the neuron's output at current epoch t\nH_t-1 is the neuron's output from the previous epoch t-1\nX_t is the input data at current epoch t\n\nH_t = tanh(W[H_t-1, X_t] + b)\nAn LSTM (Long Short Term Memory) unit:\n\n            --------------------------------\n            |                              |\n H_t-1 ---&gt; |                              |---&gt; H_t\n            |                              |\n C_t-1 ---&gt; |                              |---&gt; C_t\n            |                              |\n X_t   ---&gt; |                              |\n            |                              |\n            --------------------------------\n            \nThe `forget gate` determines which part of the old short-term memory and current input is \"forgotten\" by the new long-term memory.\nThese are a set of weights (between 0 and 1) that get applied to the old long-term memory to downweight it.\nF_t = sigmoid(W_F[H_t-1, X_t] + b_F)\n\nThe `input gate` i_t similarly gates the input and old short-term memory.\nThis will later (in the update gate) get combined  with a candidate value for the new long-term memory `C_candidate_t`\nI_t = sigmoid(W_I[H_t-1, X_t] + b_I)\nC_candidate_t = tanh(W_C_cand[H_t-1, X_t] + b_C_cand) \n\nThe `update gate` for the new long-term memory `C_t` is then calculated as a sum of forgotten old memory and input-weighted candidate memory:\nC_t = F_t*C_t-1 + I_t*C_candidate_t \n\nThe `output gate` O_t is a combination of the old short-term memory and latest input data.\nThis is then combined with the latest long-term memory to produce the output of the recurrent neuron, which is also the updated short-term memory H_t:\nO_t = sigmoid(W_O[H_t-1, X_t] + b_O)\nH_t = O_t * tanh(C_t) \n\nwhere:\nH_t is short-term memory at epoch t\nC_t is long-term memory at epoch t\nX_t is the input data at current epoch t\n\nsigmoid is a sigmoid  activation function\nF_t is an intermediate forget gate weight\nI_t is an intermediate input gate weight\nO_t is an intermediate output gate weight\nThere are several variants of RNNs. RNNs with peepholes\nThis leaks long-term memory into the forget, input and output gates. Note that the forget gate and input gate each get the OLD long-term memory, whereas the output gate gets the NEW long-term memory.\nF_t = sigmoid(W_F[C_t-1, H_t-1, X_t] + b_F)\nI_t = sigmoid(W_I[C_t-1, H_t-1, X_t] + b_I)\nO_t = sigmoid(W_O[C_t, H_t-1, X_t] + b_O)\nGated Recurrent Unit (GRU)\nThis combines the forget and input gates into a single gate. It also has some other changes. This is simpler than a typical LSTM model as it has fewer parameters. This makes it more computationally efficient, and in practice they can have similar performance.\nz_t = sigmoid(W_z[H_t-1, X_t])\nr_t = sigmoid(W_r[H_t-1, X_t])\nH_candidate_t = tanh(W_H_candidate[r_t*h_t-1, x_t])\nH_t = (1 - z_t) * H_t-1 + z_t * H_candidate_t\nThe sequences modelled with RNNs can be:\n\nOne-to-many\nMany-to-many\nMany-to-one\n\nConsiderations for choosing the input sequence length:\n\nIt should be long enough to capture any patterns or seasonality in the data\nThe validation set and test set each need to have a size at least (input_length + output_length), so the longer the input length, the more data is required to be set aside.\n\nThe validation loss for RNNs/LSTMs can be volatile, so allow a higher patience when using early stopping.\nExample model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,LSTM\n\n# Generators for helper functions to chunk up the input series into an array of pairs of (input_batch, target_output)\ntrain_generator = TimeseriesGenerator(scaled_train, scaled_train, length=batch_length, batch_size=1)\nvalidation_generator = TimeseriesGenerator(scaled_test, scaled_test, length=batch_length, batch_size=1)\n\n# Create an LSTM model\nmodel = Sequential()\nmodel.add(LSTM(100, activation='relu', input_shape=(batch_length, n_features)))\nmodel.add(Dense(1))  # Output layer\nmodel.compile(optimizer='adam', loss='mse')  # Problem setup for regression\n\n# Fit the model\nearly_stop = EarlyStopping(monitor='val_loss', patience=10)  # RNNs can be volatile so if use a higher patience with an early stopping callback\nmodel.fit(train_generator, epochs=20, validation_data=validation_generator, callbacks=[early_stop])\n\n# Evaluate test data\ntest_predictions = []\nfinal_batch = scaled_train[-batch_length:, :]\ncurrent_batch = final_batch.reshape((1, batch_length, n_features))\n\nfor test_val in scaled_test:\n    pred_val = model.predict(current_batch)  # These will later need to use the scaler.inverse_transform of the scaler originally used on the training data\n    test_predictions.append(pred_val[0])\n    current_batch = np.append(current_batch[:,1:,:], pred_val.reshape(1,1,1), axis=1)\n\n\n\nCharacter-based generative NLP: given an input string, e.g. [‘h’, ‘e’, ‘l’, ‘l’], predict the sequence shifted by one character, e.g. [‘e’, ‘l’, ‘l’, ‘o’]\nThe embedding, GRU and dense layers work on the sequence in the following way: \nSteps:\n\nRead in text data\nText processing and vectorisation - one-hot encoding\nCreate batches\nCreate the model\nTrain the model\nGenerate new text\n\nStep 1: Read in text data\n\nA corpus of &gt;1 million characters is a good size\nThe more distinct the style of your corpus, the more obvious it will be if your model has worked.\n\nGiven some structured input_text string, we can get the one-hot encoded vocab list of unique characters.\nvocab_list = sorted(set(input_text))\nStep 2: Text processing\nVectorise the text - create a mapping between character and integer. This is the encoding dictionary used to vectorise the corpus.\n# Mappings back and forth between characters and their encoded integers\nchar_to_ind = {char: ind for ind, char in enumerate(vocab_list)}\nind_to_char = {ind: char for ind, char in enumerate(vocab_list)}\n\nencoded_text = np.array([char_to_ind[char] for char in input_text])\nStep 3: Create batches\nThe sequence length should be long enough to capture structure, e.g. for poetry with rhyming couplets it should be the number of characters in 3 lines to capture the rhyme and non-rhyme. Too long a sequence makes the model take longer to train and captures too much historical noise.\nCreate a shuffled dataset where:\n\ninput sequence is the first n characters\noutput sequence is n characters lagged by one\n\nWe want to shuffle these sequence pairs into a random order so the model doesn’t overfit to any section of the text, but can instead generate characters given any seed text.\nseq_len = 120\ntotal_num_seq = len(input_text) // (seq_len + 1)\n\n# Create Training Sequences\nchar_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\nsequences = char_dataset.batch(seq_len + 1, drop_remainder=True)  # drop_remainder drops the last incomplete sequence\n\n# Create tuples of input and output sequences\ndef create_seq_targets(seq):\n    input_seq = seq[:-1]\n    output_seq = seq[-1:]\n    return input_seq, output_seq\ndataset = sequences.map(create_seq_targets)\n\n# Generate training batches\nbatch_size = 128\nbuffer_size = 10000  # Shuffle this amount of batches rather than shuffling the entire dataset in memory\ndataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\nStep 4: Create the model\nSet up the loss function and layers.\nThis model uses 3 layers:\n\nEmbedding\n\nEmbed positive integers, e.g. “a”&lt;-&gt;1, as dense vectors of fixed size. This embedding size is a hyperparameter for the user to decide.\nNumber of layers should be smaller but a similar scale to the vocab size; typically use the power of 2 that is just smaller than the vocab size.\n\nGRU\n\nThis is the main thing to vary in the model: number of hidden layers and number of neurons in each, types of neurons (RNN, LSTM, GRU), etc. Typically use lots of layers here.\n\nDense\n\nOne neuron per character (one-hot encoded), so this produces a probability per character. The user can vary the “temperature”, choosing less probable characters more/less often.\n\n\nLoss function:\n\nSparse categorical cross-entropy\nUse sparse categorical cross-entropy because the classes are mutually exclusive. Use regular categorical cross-entropy when one smaple can have multiple classes, or the labels are soft probabilities.\nSee link in NLP appendix for discussion.\nUse logits=True because vocab input is one hot encoded.\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, GRU\nfrom tensorflow.keras.losses import sparse_categorical_crossentropy\n\n# Hyperparameters\nvocab_size = len(vocab_list)\nembed_dim = 64\nrnn_neurons = 1026\n\n# Create model \nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embed_dim, batch_input_shape=[batch_size, None]))\nmodel.add(GRU(rnn_neurons, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'))\nmodel.add(Dense(vocab_size))\nsparse_cat_loss = lambda y_true,y_pred: sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)  # We want a custom loss function with logits=True\nmodel.compile(optimizer='adam', loss=sparse_cat_loss) \nStep 5: Train the model\nepochs = 30\nmodel.fit(dataset,epochs=epochs)\nStep 6: Generate text\nAllow the model to accept a batch size of 1 to allow us to give an input seed text from which to generate text from.\nWe format the seed text so it can be fed into the network, then loop through generating one character at a time and feeding that back into the model.\nmodel.build(tf.TensorShape([1, None]))\n\ndef generate_text(model, input_seed_text, gen_size=100, temperature=1.0):\n    \"\"\"\n    model: tensorflow.model\n    input_seed_text: str\n    gen_size: int\n    temperature: float\n    \"\"\"\n    generated_text_result = []\n    vectorised_seed = tf.expand_dims([char_to_ind[s] for s in input_seed_text], 0)\n    model.reset_states()\n    \n    for k in range(gen_size):\n        raw_predictions = model(vectorised_seed)\n        predictions = tf.squeeze(raw_predictions, 0) / temperature\n        predicted_index = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n        generated_text_result.append(ind_to_char[predicted_index])\n        \n        # Set a new vectorised seed to generate the next character in the loop\n        vectorised_seed = tf.expand_dims([predicted_index], 0)\n        \n    return (input_seed_text + ''.join(generated_text_result))\n\n\n\nThis is an unsupervised learning problem, therefore evaluation metrics are different. Sometimes called semi-supervised as labels may be used in training the model, but not available when it’s used to predict new values.\nApplications:\n\nDimensionality reduction\nNoise removal\n\nDesigned to reproduce its input in the output layer. Number of input neurons is the same as the number of output layers. The encoder reduces dimensionality to the hidden layer. The hidden layer should contain the most relevant information required to reconstruct the input. The decoder reconstructs a high dimensional output from a low dimensional input.\n\nEncoder: Input layer -&gt; Hidden layer\nDecoder: Hidden layer -&gt; Output\n\nStacked autoencoders include multiple hidden layers.\nAutoencoder for dimensionality reduction\nLet’s try to reduce an input dataset from 3 dimensions to 2. We want to train an autoencoder to go from 3 -&gt; 2 -&gt; 3.\nWhen we scale data, we fit and transform on the entire dataset rather than a train/test split, because there is no ground truth for the “correct” dimensionality reduction.\nUsing SGD in Autoencoders allows the learning rate to be varied as a hyperparameter to affect how well the hidden layer learns. Larger values will train quicker but potentially perform worse.\nTo retrieve the lower dimensionality representation, use just the encoder half of the trained autoencoder to predict the input.\n# Scale data\nscaler = MinMaxScaler()\nscaled_data = scaler.fit_transform(input_features)\n\n# Create model\nencoder = Sequential()\nencoder.add(Dense(units=2, acivation='relu', input_shape=[3]))\n\ndecoder = Sequential()\ndecoder.add(Dense(units=3, acivation='relu', input_shape=[2]))\n\nautoencoder = Sequential([encoder, decoder])\nautoencoder.compile(loss='mse', optimizer=SGD(lr=1.5))\n\n# Train the full autoencoder model\nautoencoder.fit(scaled_data, scaled_data, epochs=5)\n\n# Reduce dimensionality by using just the encoder part of the trained model\nencoded_2dim = encoder.predict(scaled_data)\nAutoencoder for noise removal\nStarting with clean images, we want to train an autoencoder to learn to remove noise. We create a noisy dataset by adding noise to our clean images. We then try to reconstruct that input, and compare to the original clean images as the ground truth.\nStarting with 28x28 images (e.g. MNIST dataset), this gives 784 input features when flattened. Use multiple hidden layers as there are a large number of input features that would be difficult to learn in one layer. The numbers of layers and number of neurons in each are hyperparameters to tune; decreasing in powers of 2 is a good rule of thumb.\nAdd a Gaussian noise layer to train the model to remove noise. Without the Gaussian layer, the model would be used for dimensionality reduction rather than noise removal.\nThe final layer uses a sigmoid activation because it is a binary classification problem - does the output match the input?\nencoder = Sequential()\nencoder.add(Flatten(input_shape=[28, 28]))\nencoder.add(Gaussian(0.2))  # Optional layer if training for noise removal rather than dimensionality reduction\nencoder.add(Dense(400, activation='relu'))\nencoder.add(Dense(200, activation='relu'))\nencoder.add(Dense(100, activation='relu'))\nencoder.add(Dense(50, activation='relu'))\nencoder.add(Dense(25, activation='relu'))\n\ndecoder = Sequential()\ndecoder.add(Dense(50, input_shape=[25], activation='relu'))\ndecoder.add(Dense(100, activation='relu'))\ndecoder.add(Dense(200, activation='relu'))\ndecoder.add(Dense(400, activation='relu'))\ndecoder.add(Dense(28*28, activation='sigmoid'))\n\nautoencoder = Sequential([encoder, decoder])\nautoencoder.compile(loss='binary_crossentropy', optimizer=SGD(lr=1.5), metrics=['accuracy'])\nautoencoder.fit(X_train, X_train, epochs=5)\n\n\n\nUse 2 networks competing against each other to generate data - counterfeiter (generator) vs detective (discriminator).\n\nGenerator: recieves random noise\nDiscriminator: receives data set containing real data and fake generated data, and attempts to calssify real vs fake (binary classificaiton).\n\nTwo training phases, each only training one of the networks:\n\nTrain discriminator - real images (labeled 1) and generated images (labeled 0) are fed to the discriminator network.\nTrain generator - only feed generated images to discriminator, ALL labeled 1.\n\nThe generator never sees real images, it creates images based only off of the gradients flowing back from the discriminator.\nDifficulties with GANs:\n\nTraining resources - GPUs generally required\nMode collapse - generator learns an image that fools the discriminator, then only produces this image. Deep convolutional GANs and mini-batch discrimination are two solution to this problem.\nInstability - True performance is hard to measure; just because the discriminator was fooled, it doesn’t mean that the generated image was actually realistic.\n\nCreating the model\nWe create the generator and discriminator as separate models, then join them into a GAN object. This is analogous to how we joined an encoder and decoder into an autoencoder.\nThe discriminator is trained on a binary classification problem, real or fake, so uses a binary cross entropy loss function. Backpropagation only alters the weights of the discriminator in the first phase, not the generator.\ndiscriminator = Sequential()\n\n# Flatten the image\ndiscriminator.add(Flatten(input_shape=[28,28]))\n\n# Some hidden layers\ndiscriminator.add(Dense(150,activation='relu'))\ndiscriminator.add(Dense(100,activation='relu'))\n\n# Binary classification - real vs fake\ndiscriminator.add(Dense(1,activation=\"sigmoid\"))\ndiscriminator.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\nThe generator is trained in the second phase.\nWe set a codings_size, which is the size of the latent representation from which the generator will create a full image. For example, if we want to create a 28x28 image (784 pixels), we may want to generate this from a 100 element input. This is analogous to the decoder part of an autoencoder.\nThe codings_size should be smaller than the total number of features but large enough so that there is something to learn from.\nThe generator is NOT compiled at this step, it is compiled in the combined GAN later so that it is only trained at that step.\ncodings_size = 100\ngenerator = Sequential()\n\n# Input shape of first layer is the codings_size\ngenerator.add(Dense(150, activation=\"relu\", input_shape=[codings_size]))\n\n# Some hidden layers\ngenerator.add(Dense(200,activation='relu'))\n\n# Output is the size of the image we want to create\ngenerator.add(Dense(784, activation=\"sigmoid\"))\ngenerator.add(Reshape([28,28]))\nThe GAN combines the generator and discriminator.\nThe discriminator is only trained in the first phase, not the second phase.\nGAN = Sequential([generator, discriminator])\ndiscriminator.trainable = False\nGAN.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\nTraining the model\nWe first create batches of images from our input data, similarly to previous models.\nbatch_size = 32  # Size of training input batches\nbuffer_size = 1000  # Don't load the entire dataset into memory\nepochs = 1\n\nraw_dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(buffer_size=buffer_size)\nbatched_dataset = raw_dataset.batch(batch_size, drop_remainder=True).prefetch(1)\nThe training loop for each epoch trains the discriminator to distinguish real vs fake images (binary classification). Then it trains the generator to generate fake images using ONLY the gradients learned in the discriminator training step; the generator NEVER sees any real images.\ngenerator, discriminator = GAN.layers\n\nfor epoch in range(epochs):\n    for index, X_batch in enumerate(batched_dataset):\n        # Phase 1: Train the discriminator\n        noise = tf.random.normal(shape=[batch_size, codings_size])\n        generated_images = generator(noise)\n        real_images = tf.dtypes.cast(X_batch,tf.float32)\n        X_train_discriminator = tf.concat([generated_images, real_images], axis=0)\n        y_train_discriminator = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)  # Targets set to zero for fake _images and 1 for real _images\n        discriminator.trainable = True\n        discriminator.train_on_batch(X_train_discriminator, y_train_discriminator)\n\n        # Phase 2: Train the generator\n        noise = tf.random.normal(shape=[batch_size, codings_size])\n        y_train_generator = tf.constant([[1.]] * batch_size)  # We want discriminator to believe that fake _images are real\n        discriminator.trainable = False\n        GAN.train_on_batch(noise, y_train_generator)    \nDeep convolutional GANs\nThese are GANs which use convolutional layers inside the discriminator and generator models.\nThe models are almost identical to the models above, just with additional Conv2D, Conv2DTranspose and BatchNormalization layers.\nChanges compared to regular GANs:\n\nAdditional convolutional layers in model.\nReshape the training set to match the images.\nRescale the training data to be between -1 and 1 so that tanh activation function works.\nActivation of discriminator output layer is tanh rather than sigmoid."
  },
  {
    "objectID": "posts/ml/tensorflow/tensorflow.html#a.-appendix",
    "href": "posts/ml/tensorflow/tensorflow.html#a.-appendix",
    "title": "Tensorflow Notes",
    "section": "",
    "text": "Complete Tensorflow and Keras Udemy course\nIntuition behind neural networks\nThe deep learning bible (with lectures)\nHeuristics for choosing hidden layers\nAlternatives to the deprecated model predict for different classification problems\nMIT course\n\n\n\n\n\nImage kernels explained\nChoosing CNN layers\n\n\n\n\n\nOverview of RNNs\nWikipedia page contains the equations of LSTMs and peepholes\nLSTMs vs GRUs\nWorked example of LSTM\nRNN cheatsheet\n\n\n\n\n\nThe unreasonable effectiveness of RNNs - essay on RNNs applied to NLP\nSparse vs dense categorical crossentropy loss function\n\n\n\n\n\nbuffer_size argument in tensorflow prefetch and shuffle\nMode collapse"
  },
  {
    "objectID": "posts/ml/fastai/lesson1/lesson.html",
    "href": "posts/ml/fastai/lesson1/lesson.html",
    "title": "FastAI Lesson 1: Image Classification Models",
    "section": "",
    "text": "These are notes from lesson 1 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\nTrain an image classifier: see car classification notebook\n\n\n\n\nThere is a companion course focusing on AI ethics here\nThoughts on education: play the whole game first to get an idea of the big picture. Don’t become afflicted with “elementitis” getting too bogged down in details too early.\n\nColoured cups - green, amber, red - for each student to indicate how well they are following\nMeta Learning by Radek Osmulski - a book based on the author’s experience of learning about deep learning (via the fastai course)\nMathematician’s Lament by Paul Lockhart - a book about the state of mathematics education\nMaking Learning Whole by David Perkins - a book about apporaches to holistic learning\n\nRISE is a jupyter notebook extension to turn notebooks into slides. Jeremy uses notebooks for: source code, book, blogging, CI/CD.\n\n\n\nBefore deep learning, the approach to machine learning was to enlist many domain experts to handcraft features and feed this into a constrained linear model (e.g. ridge regression). This is time-consuming, expensive and requires many domain experts.\nNeural networks learn these features. Looking inside a CNN, for example, shows that these learned features match interpretable features that an expert might handcraft. An illustration of the features learned is given in this paper by Zeiler and Fergus.\nFor image classifiers, you don’t need particularly large images as inputs. GPUs are so quick now that if you use large images, most of the time is spent on opening the file rather than computations. So often we resize images down to 400x400 pixels or smaller.\nFor most use cases, there are pre-trained models and sensible default values that we can use. In practice, most of the time is spent on the input layer and output layer. For most models the middle layers are identical.\n\n\n\nData blocks structure the input to learners. An overview of the DataBlock class:\n\nblocks determines the input and output type as a tuple. For multi-target classification this tuple can be arbitrary length.\nget_items is a function that returns a list of all the inputs.\nsplitter defines how to split the training/validation set.\nget_y is a function that returns the label of a given input image.\nitem_tfms defines what transforms to apply to the inputs before training, e.g. resize.\ndataloaders is a method that parallelises loading the data.\n\nA learner combines the model (e.g. resnet or something from timm library) and the data to run that model on (the dataloaders from the DataBlock).\nThe fine_tune method starts with a pretrained model weights rather than randomised weights, and only needs to learn the differences between your data and the original model.\nOther image problems that can utilise deep learning:\n\nImage classification\nImage segmentation\n\nOther problem types use the same process, just with different DataBlock blocks types and the rest is the same. For example, tabular data, collaborative filtering.\n\n\n\n\nTraditional computer programs are essentially:\n\n\n\n\n\n\nFigure 1: Traditional computer programs\n\n\n\nDeep learning models are:\n\n\n\n\n\n\nFigure 2: Deep learning model\n\n\n\n\n\n\n\nCourse lesson page\nVisualizing and Understanding Convolutional Networks\nTIMM library\nStatistical Modeling: The Two Cultures\n\n\n\n\nFigure 1: Traditional computer programs\nFigure 2: Deep learning model"
  },
  {
    "objectID": "posts/ml/fastai/lesson1/lesson.html#notes-on-learning",
    "href": "posts/ml/fastai/lesson1/lesson.html#notes-on-learning",
    "title": "FastAI Lesson 1: Image Classification Models",
    "section": "",
    "text": "There is a companion course focusing on AI ethics here\nThoughts on education: play the whole game first to get an idea of the big picture. Don’t become afflicted with “elementitis” getting too bogged down in details too early.\n\nColoured cups - green, amber, red - for each student to indicate how well they are following\nMeta Learning by Radek Osmulski - a book based on the author’s experience of learning about deep learning (via the fastai course)\nMathematician’s Lament by Paul Lockhart - a book about the state of mathematics education\nMaking Learning Whole by David Perkins - a book about apporaches to holistic learning\n\nRISE is a jupyter notebook extension to turn notebooks into slides. Jeremy uses notebooks for: source code, book, blogging, CI/CD."
  },
  {
    "objectID": "posts/ml/fastai/lesson1/lesson.html#background-on-deep-learning-and-image-classification",
    "href": "posts/ml/fastai/lesson1/lesson.html#background-on-deep-learning-and-image-classification",
    "title": "FastAI Lesson 1: Image Classification Models",
    "section": "",
    "text": "Before deep learning, the approach to machine learning was to enlist many domain experts to handcraft features and feed this into a constrained linear model (e.g. ridge regression). This is time-consuming, expensive and requires many domain experts.\nNeural networks learn these features. Looking inside a CNN, for example, shows that these learned features match interpretable features that an expert might handcraft. An illustration of the features learned is given in this paper by Zeiler and Fergus.\nFor image classifiers, you don’t need particularly large images as inputs. GPUs are so quick now that if you use large images, most of the time is spent on opening the file rather than computations. So often we resize images down to 400x400 pixels or smaller.\nFor most use cases, there are pre-trained models and sensible default values that we can use. In practice, most of the time is spent on the input layer and output layer. For most models the middle layers are identical."
  },
  {
    "objectID": "posts/ml/fastai/lesson1/lesson.html#overview-of-the-fastai-learner",
    "href": "posts/ml/fastai/lesson1/lesson.html#overview-of-the-fastai-learner",
    "title": "FastAI Lesson 1: Image Classification Models",
    "section": "",
    "text": "Data blocks structure the input to learners. An overview of the DataBlock class:\n\nblocks determines the input and output type as a tuple. For multi-target classification this tuple can be arbitrary length.\nget_items is a function that returns a list of all the inputs.\nsplitter defines how to split the training/validation set.\nget_y is a function that returns the label of a given input image.\nitem_tfms defines what transforms to apply to the inputs before training, e.g. resize.\ndataloaders is a method that parallelises loading the data.\n\nA learner combines the model (e.g. resnet or something from timm library) and the data to run that model on (the dataloaders from the DataBlock).\nThe fine_tune method starts with a pretrained model weights rather than randomised weights, and only needs to learn the differences between your data and the original model.\nOther image problems that can utilise deep learning:\n\nImage classification\nImage segmentation\n\nOther problem types use the same process, just with different DataBlock blocks types and the rest is the same. For example, tabular data, collaborative filtering."
  },
  {
    "objectID": "posts/ml/fastai/lesson1/lesson.html#deep-learning-vs-traditional-computer-programs",
    "href": "posts/ml/fastai/lesson1/lesson.html#deep-learning-vs-traditional-computer-programs",
    "title": "FastAI Lesson 1: Image Classification Models",
    "section": "",
    "text": "Traditional computer programs are essentially:\n\n\n\n\n\n\nFigure 1: Traditional computer programs\n\n\n\nDeep learning models are:\n\n\n\n\n\n\nFigure 2: Deep learning model"
  },
  {
    "objectID": "posts/ml/fastai/lesson1/lesson.html#references",
    "href": "posts/ml/fastai/lesson1/lesson.html#references",
    "title": "FastAI Lesson 1: Image Classification Models",
    "section": "",
    "text": "Course lesson page\nVisualizing and Understanding Convolutional Networks\nTIMM library\nStatistical Modeling: The Two Cultures\n\n\n\n\nFigure 1: Traditional computer programs\nFigure 2: Deep learning model"
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html",
    "href": "posts/ml/fastai/lesson7/lesson.html",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "These are notes from lesson 7 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\n\nCreate a collaborative filtering model in a spreadsheet\n\n\n\n\n\nWe have users ratings of movies.\nSay we had “embeddings” of a set of categories for each. So for a given movie, we have a vector of [action, sci-fi, romance] and for a given user we have their preference for [action, sci-fi, romance]. Then we could do the dot product between user embedding and movie embedding to get the probability that the user likes that movie. That is, the predicted user rating.\nSo the problem boils down to:\n\nWhat are the embeddings? i.e. the salient factors ([action, sci-fi, romance] in the example above)\nHow do we get them?\n\nThe answer to both questions is: we just let the model learn them.\nLet’s just pick a randomised embedding for each movie and each user. Then we have a loss function which is the MAE between predicted user rating for a movie and actual rating. Now we can use SGD to optimise those embeddings to find the best values.\n\n\n\nTo gain an intuition behind the calculations behind a collaborative filter, we can work through a (smaller) example in excel. This allows us to see the logic and dig into the calculations before we create them “for real” in Python.\n\n\n\n\n\n\nTip\n\n\n\nThis can be found in this spreadsheet.\n\n\nWe first look at an example where the results are in a cross-table and we can take the dot product of user embeddings and movie embeddings.\nThen we reshape the problem slightly by placing all of the embeddings in a matrix and doing a lookup. This is essentially what pytorch does, although it uses matrix multiplication by one-hot encoded vectors rather than array lookups for computational efficiency.\nWe then add a bias term to account for some users who love all movies, or hate all movies. And also movies that are universally beloved.\n\n\n\nThe broad idea behind collaborative filtering is:\n\nIf we could quantify the most salient “latent factors” about a movie, and…\nQuantify how much a user cares about that factor, then…\nIf we multiplied the two (dot product) it would give a measure of their rating.\n\nBut what are those latent factors? We let the model learn it. 1. We initialise randomised latent factors (called embeddings) 2. We use that to predict the user’s rating for each move. Initially, those randomised weights will give terrible predictions. 3. Our loss function is the MSE of the ground truth actual predictions and the prediction rating. 4. We can optimise the embedding values to minimise this loss function.\n\n\nWe use data on user ratings of movies sourced from MovieLens. The ml-latest-small data set is downloaded and saved in the DATA_DIR folder.\n\nfrom pathlib import Path\n\nfrom fastai.collab import CollabDataLoaders, Module, Embedding, collab_learner\nfrom fastai.tabular.all import one_hot, sigmoid_range, MSELossFlat, Learner, get_emb_sz\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd \nimport torch\n\n\nDATA_DIR = Path(\"/Users/gurpreetjohl/workspace/python/ml-practice/ml-practice/datasets/ml-latest-small\")\n\nLoad the ratings data which we will use for this task:\n\nratings = pd.read_csv(DATA_DIR / 'ratings.csv')\nratings\n\n\n\n\n\n\n\n\nuserId\nmovieId\nrating\ntimestamp\n\n\n\n\n0\n1\n1\n4.0\n964982703\n\n\n1\n1\n3\n4.0\n964981247\n\n\n2\n1\n6\n4.0\n964982224\n\n\n3\n1\n47\n5.0\n964983815\n\n\n4\n1\n50\n5.0\n964982931\n\n\n...\n...\n...\n...\n...\n\n\n100831\n610\n166534\n4.0\n1493848402\n\n\n100832\n610\n168248\n5.0\n1493850091\n\n\n100833\n610\n168250\n5.0\n1494273047\n\n\n100834\n610\n168252\n5.0\n1493846352\n\n\n100835\n610\n170875\n3.0\n1493846415\n\n\n\n\n100836 rows × 4 columns\n\n\n\nThe users and movies are encoded as integers.\nFor reference, we can load the movies data to see what each movieId corresponds to:\n\nmovies = pd.read_csv(DATA_DIR / 'movies.csv')\nmovies\n\n\n\n\n\n\n\n\nmovieId\ntitle\ngenres\n\n\n\n\n0\n1\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n1\n2\nJumanji (1995)\nAdventure|Children|Fantasy\n\n\n2\n3\nGrumpier Old Men (1995)\nComedy|Romance\n\n\n3\n4\nWaiting to Exhale (1995)\nComedy|Drama|Romance\n\n\n4\n5\nFather of the Bride Part II (1995)\nComedy\n\n\n...\n...\n...\n...\n\n\n9737\n193581\nBlack Butler: Book of the Atlantic (2017)\nAction|Animation|Comedy|Fantasy\n\n\n9738\n193583\nNo Game No Life: Zero (2017)\nAnimation|Comedy|Fantasy\n\n\n9739\n193585\nFlint (2017)\nDrama\n\n\n9740\n193587\nBungo Stray Dogs: Dead Apple (2018)\nAction|Animation\n\n\n9741\n193609\nAndrew Dice Clay: Dice Rules (1991)\nComedy\n\n\n\n\n9742 rows × 3 columns\n\n\n\nWe’ll merge the two for easier human readability.\n\nratings = ratings.merge(movies)\nratings\n\n\n\n\n\n\n\n\nuserId\nmovieId\nrating\ntimestamp\ntitle\ngenres\n\n\n\n\n0\n1\n1\n4.0\n964982703\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n1\n1\n3\n4.0\n964981247\nGrumpier Old Men (1995)\nComedy|Romance\n\n\n2\n1\n6\n4.0\n964982224\nHeat (1995)\nAction|Crime|Thriller\n\n\n3\n1\n47\n5.0\n964983815\nSeven (a.k.a. Se7en) (1995)\nMystery|Thriller\n\n\n4\n1\n50\n5.0\n964982931\nUsual Suspects, The (1995)\nCrime|Mystery|Thriller\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n100831\n610\n166534\n4.0\n1493848402\nSplit (2017)\nDrama|Horror|Thriller\n\n\n100832\n610\n168248\n5.0\n1493850091\nJohn Wick: Chapter Two (2017)\nAction|Crime|Thriller\n\n\n100833\n610\n168250\n5.0\n1494273047\nGet Out (2017)\nHorror\n\n\n100834\n610\n168252\n5.0\n1493846352\nLogan (2017)\nAction|Sci-Fi\n\n\n100835\n610\n170875\n3.0\n1493846415\nThe Fate of the Furious (2017)\nAction|Crime|Drama|Thriller\n\n\n\n\n100836 rows × 6 columns\n\n\n\n\n\n\n\ndls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\ndls.show_batch()\n\n\n\n\n\nuserId\ntitle\nrating\n\n\n\n\n0\n4\nMighty Aphrodite (1995)\n3.0\n\n\n1\n573\nDark Knight, The (2008)\n5.0\n\n\n2\n116\nAmadeus (1984)\n3.0\n\n\n3\n380\nAddams Family, The (1991)\n5.0\n\n\n4\n353\nBrothers McMullen, The (1995)\n4.0\n\n\n5\n37\nFugitive, The (1993)\n4.0\n\n\n6\n356\nUnbreakable (2000)\n4.0\n\n\n7\n489\nAlien³ (a.k.a. Alien 3) (1992)\n3.5\n\n\n8\n174\nNell (1994)\n5.0\n\n\n9\n287\nPanic Room (2002)\n2.5\n\n\n\n\n\nInitialise randomised 5-dimensional embeddings.\nHow should we choose the number of latent factors? (5 in the example above). Jeremy wrote down some ballpark values for models of different sizes in excel, then fit a function to it to get a heuristic measure. This is the default used by fast AI.\n\nn_users  = len(dls.classes['userId'])\nn_movies = len(dls.classes['title'])\nn_factors = 5\n\nuser_factors = torch.randn(n_users, n_factors)\nmovie_factors = torch.randn(n_movies, n_factors)\n\nDoing a matrix multiply by a one hot encoded vector is the same as doing a lookup in an array, just in a more computationally efficient way. Recall the softmax example.\nAn embedding is essentially just “look up in an array”.\n\n\n\n\nPutting a sigmoid_range on the final layer to squish ratings to fit 0 to 5 means “the model doesn’t have to work as hard” to get movies in the right range. In practice we use 5.5 as the sigmoid scale value as a sigmoid can never hit 1, but we want ratings to be able to hit 5.\n\nfrom fastai.collab import CollabDataLoaders, Module, Embedding\nfrom fastai.tabular.all import one_hot, sigmoid_range, MSELossFlat, Learner\n\n\nclass DotProduct(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.y_range = y_range\n\n    def forward(self, x):\n        users = self.user_factors(x[:, 0])\n        movies = self.movie_factors(x[:, 1])\n        # Apply a sigmoid to the raw_output\n        raw_output = (users * movies).sum(dim=1)\n        return sigmoid_range(raw_output, *self.y_range)\n\nWe can now fit a model\n\nembedding_dim = 50\nnum_epochs = 5\nmax_learning_rate = 5e-3\n\nmodel = DotProduct(n_users, n_movies, embedding_dim)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(num_epochs, max_learning_rate)\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\nAdding a user bias term and a movie bias term to the prediction call helps account for the fact that some users always rate high (4 or 5) but other users always rate low. And similarly for movies if everyone always rates it a 5 or a 1.\n\nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.user_bias = Embedding(n_users, 1)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.movie_bias = Embedding(n_movies, 1)\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        raw_output = (users * movies).sum(dim=1, keepdim=True)\n        raw_output += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n        return sigmoid_range(raw_output, *self.y_range)\n\n\nmodel = DotProductBias(n_users, n_movies, embedding_dim)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(num_epochs, max_learning_rate)\n\n\n\n\n\n\n    \n      \n      0.00% [0/5 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/1260 00:00&lt;?]\n    \n    \n\n\n\n\n\nThe validation loss in the previous model decreases then icnreases, which is a clear indication of overfitting.\nWe want to avoid overfitting, but data augmentation isn’t possible here. One approach is to use weight decay AKA L2 regularisation. We add sum of weights squared to the loss function.\nHow does this prevent overfitting? The larger the coefficients, the sharper the canyons the model is able to produce, which allows it to fit individual data points. By penalising larger weights, it will only produce sharp changes if this causes the model to fit many points well, so it should generalise better.\nWe essentially want to modify our loss function with an additional term dependent on the magnitude of the weights:\nloss_with_weight_decay = loss + weight_decay * (parameters**2).sum()\nIn practice, these values would be large and numerically unstable. We only actually care about the gradient of the loss, so we can add the gradient of the additional term to the existing gradient.\nparameters.grad += weight_decay * 2 * parameters\nBut weight_decay is just a constant that we choose, so we can fold the 2* term into it.\n\nweight_decay = 0.1\n\nmodel = DotProductBias(n_users, n_movies, embedding_dim)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(num_epochs, max_learning_rate, wd=weight_decay)\n\n\n\n\n\n\n    \n      \n      0.00% [0/5 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n\n\n\n    \n      \n      0.16% [2/1260 00:00&lt;00:20 1.4480]\n    \n    \n\n\n\n\n\nIn the previous section, we used that pytorch (technically the fastai version) Embedding module.\nLet’s briefly take a look at this and create our own Embedding module from scratch.\n\n\nThe way pytorch knows if a tensor is a parameter (and therefore can calculate gradients on it) is if it inherits from nn.Parameter. Then a Module’s .parameters() method will list this tensor.\nAs an example of this behaviour, let’s create a module with some parameters but WITHOUT declaring these as Parameters:\n\nclass MyModule(Module):\n    def __init__(self): \n        self.a = torch.ones(3)\n\nmm = MyModule()\nlist(mm.parameters())\n\n[]\n\n\nWe declared a tensor a in MyModule but we don’t see it! Which means it wouldn’t be trained by Pytorch.\nInstead, let’s declare is as a Parameter:\n\nclass MyModuleWithParams(Module):\n    def __init__(self): \n        self.a = torch.nn.Parameter(torch.ones(3))\n\nmm_params = MyModuleWithParams()\nlist(mm_params.parameters())\n\n[Parameter containing:\n tensor([1., 1., 1.], requires_grad=True)]\n\n\nPytorch’s builtin modules all use Parameter for any trainable parameters, so we haven’t needed to explicitly declare this.\nAs an example, if we use Pytorch’s Linear layer, it will automatically appear as a parameter:\n\nclass MyModuleWithLinear(Module):\n    def __init__(self): \n        self.a = torch.nn.Linear(1, 3, bias=False)\n\nmm_linear = MyModuleWithLinear()\nlist(mm_linear.parameters())\n\n[Parameter containing:\n tensor([[-0.6689],\n         [-0.0181],\n         [ 0.8172]], requires_grad=True)]\n\n\n\ntype(mm_linear.a.weight)\n\ntorch.nn.parameter.Parameter\n\n\n\n\n\nAn Embedding object essentially instantiates a tensor of random weights of the given dimensions and declares this as a Parameter. Pytorch can then modify the weights when training.\n\ndef create_params(tensor_dims):\n    \"\"\"Create a tensor of the required size and fill it with random values.\"\"\"\n    embedding_tensor = torch.zeros(*tensor_dims).normal_(0, 0.01)\n    return torch.nn.Parameter(embedding_tensor)\n\nNow we can replace the import Embedding module with our custom implementation create_params in the DotProductBias module:\n\nclass DotProductBiasCustomEmbedding(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = create_params([n_users, n_factors])\n        self.user_bias = create_params([n_users])\n        self.movie_factors = create_params([n_movies, n_factors])\n        self.movie_bias = create_params([n_movies])\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors[x[:,0]]\n        movies = self.movie_factors[x[:,1]]\n        raw_output = (users * movies).sum(dim=1)\n        raw_output += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]\n        return sigmoid_range(raw_output, *self.y_range)\n\n\nmodel = DotProductBiasCustomEmbedding(n_users, n_movies, embedding_dim)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(num_epochs, max_learning_rate, wd=weight_decay)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.795456\n0.787701\n00:07\n\n\n1\n0.693971\n0.728376\n00:06\n\n\n2\n0.546227\n0.711909\n00:07\n\n\n3\n0.402994\n0.707349\n00:06\n\n\n4\n0.282765\n0.708693\n00:06\n\n\n\n\n\n\n\n\n\nWe can interrogate the model to learn more about these embeddings it has learned.\n\n\nWe can visualise the biases of our collaborative filter model to see:\n\nMovie biases: Which movies are bad even compared to other similar movies of that type? Lawnmower man 2 is crap even compared to similar action movies. But people love titanic even if they don’t normally like romance dramas.\nUser biases: Which users love any and all movies? Users who give a high rating to all movies.\n\nAccording to our biases, these movies are crap even for those who like that style of movie:\n\nmovie_bias = learn.model.movie_bias.squeeze()\nidxs = movie_bias.argsort()[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Karate Kid, Part III, The (1989)',\n 'Catwoman (2004)',\n 'Stuart Saves His Family (1995)',\n 'Speed 2: Cruise Control (1997)',\n 'Dungeons & Dragons (2000)']\n\n\nWhereas these are highly rated, even when users don’t normally like that type of movie:\n\nidxs = movie_bias.argsort()[-5:]\n[dls.classes['title'][i] for i in idxs]\n\n['Star Wars: Episode IV - A New Hope (1977)',\n 'Dark Knight, The (2008)',\n 'Green Mile, The (1999)',\n 'Forrest Gump (1994)',\n 'Shawshank Redemption, The (1994)']\n\n\n\n\n\nWe can visualise the weights to see what human-interpretable features the model is learning.\nWe can condense our embedding to 2 axes with PCA. we get a critically-acclaimed -&gt; popular x-axis and a action-dialog y-axis.\n\n\n\n\n\n\nTip\n\n\n\nFor more details on PCA and related methods, see computational linear algebra fastai course\n\n\n\ng = ratings.groupby('title')['rating'].count()\ntop_movies = g.sort_values(ascending=False).index.values[:1000]\ntop_idxs = torch.tensor([learn.dls.classes['title'].o2i[m] for m in top_movies])\nmovie_w = learn.model.movie_factors[top_idxs].cpu().detach()\nmovie_pca = movie_w.pca(3)\nfac0,fac1,fac2 = movie_pca.t()\nidxs = list(range(50))\nX = fac0[idxs]\nY = fac2[idxs]\nplt.figure(figsize=(12,12))\nplt.scatter(X, Y)\nfor i, x, y in zip(top_movies[idxs], X, Y):\n    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWe can also use the “embedding distance” (distance in the latent space) to see when two movies are similar. We use cosine similarity to determine this distance, which is similar in principle to Euclidean distance but normalised.\nIn the example below, we start with the movie Forrest Gump and find the closest movie to it in our embedding:\n\nmovie_idx = dls.classes['title'].o2i['Forrest Gump (1994)']\n\nmovie_factors = learn.model.movie_factors\ndistances = torch.nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[movie_idx][None])\nclosest_distance_idx = distances.argsort(descending=True)[1]\ndls.classes['title'][closest_distance_idx]\n\n'Beautiful Mind, A (2001)'\n\n\n\n\n\n\n\nWe can repeat the same exercise using the collaborative filter from the fastai library to see how it compares to our from-scratch implementation.\n\nlearn_fast = collab_learner(dls, n_factors=embedding_dim, y_range=(0, 5.5))\nlearn_fast.fit_one_cycle(num_epochs, max_learning_rate, wd=weight_decay)\n\n\n\n\n\n\n    \n      \n      0.00% [0/5 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n\n\n\n    \n      \n      0.40% [5/1260 00:00&lt;00:20 1.5528]\n    \n    \n\n\nWe can repeat any of the analysis of our from-scratch model. For example, the movies with the highest bias:\n\nmovie_bias = learn_fast.model.i_bias.weight.squeeze()\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Shawshank Redemption, The (1994)',\n 'Forrest Gump (1994)',\n 'Green Mile, The (1999)',\n 'Star Wars: Episode IV - A New Hope (1977)',\n 'Dark Knight, The (2008)']\n\n\n\n\n\nHow do you start off a collaborative filtering model? For example, when you first start out, you have no data on users or items.\nOr even for established companies, what happens when we have a new user or a new item, so the entire row or column is null?\nThere is no hard and fast solution, they all boil down to “use common sense”.\n\nA tempting option is to fill NaNs with the median latent vectors. But this might result in an odd combination that doesn’t exist in practice, i.e. the latent space isn’t continuous so this could be where a gap in the latent space lies. For example, a medium action, medium sci-fi film with medium romance and medium comedy that is medium popular and medium critically acclaimed.\nAnother option is to pick a user/item that is representative of the average taste.\nCreate a tabular model using answers to a new user survey. Ask the user some questions when they sign up, then create a model where the user’s embedding vector is the dependent variable and their answers, along with any other relevant signup metadata, are the independent variables.\n\nIt is important to be careful of a small number of extremely enthusiastic users dominating the recommendations. For example, people who watch anime watch a LOT of it, and rate a lot of it highly. So this could end up getting recommended to users outside of this niche.\nThis can create positive feedback loops that change the behaviour of your product in unexpected ways.\n\n\n\nThe matrix completion approach used previously is known as Probabilistic Matrix Factorization (PMF). An alternative approach is to use deep learning.\nIn practice the two approaches are often stacked in an ensemble.\nThis section explores the deep learning collaborative filtering approach from scratch, then recreates it using fastai’s library.\n\n\nWe are concatenating the embedding matrices together, rather than taking the dot product, so that we can pass it through a dense ANN.\nThese matrices can be different sizes, and the size of embedding to use for each depends on the number of classes in the data. Fastai has a heuristic method for this which we use here:\n\n(user_num_classes, user_num_embeddings), (item_num_classes, item_num_embeddings) = get_emb_sz(dls)\n\nWe can then use this in a simple neural network with one hidden layer:\n\nclass CollabNN(Module):\n    def __init__(self, user_embedding_size, item_embedding_size, y_range=(0, 5.5), n_activations=100):\n        self.user_factors = Embedding(*user_embedding_size)\n        self.item_factors = Embedding(*item_embedding_size)\n        self.layers = torch.nn.Sequential(\n            torch.nn.Linear(user_embedding_size[1] + item_embedding_size[1], n_activations),\n            torch.nn.ReLU(),\n            torch.nn.Linear(n_activations, 1))\n        self.y_range = y_range\n        \n    def forward(self, x):\n        embs = self.user_factors(x[:, 0]), self.item_factors(x[:, 1])\n        x = self.layers(torch.cat(embs, dim=1))\n        return sigmoid_range(x, *self.y_range)\n    \ncollab_nn_model = CollabNN(user_embedding_size=(user_num_classes, user_num_embeddings),\n                           item_embedding_size=(item_num_classes, item_num_embeddings))\n\nNow train this model on the data:\n\nlearn_nn = Learner(dls, collab_nn_model, loss_func=MSELossFlat())\nlearn_nn.fit_one_cycle(num_epochs, max_learning_rate, wd=weight_decay)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.799004\n0.792579\n00:10\n\n\n1\n0.747623\n0.755708\n00:10\n\n\n2\n0.706981\n0.723887\n00:10\n\n\n3\n0.650337\n0.719642\n00:10\n\n\n4\n0.569418\n0.734302\n00:10\n\n\n\n\n\n\n\n\nWe can repeat the same exercise using fastai’s implementation.\nThis is almost identical to the PMF approach, simply with an additional argument use_nn=True.\n\nlearn_nn_fast = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100, 50])\nlearn_nn_fast.fit_one_cycle(num_epochs, max_learning_rate, wd=weight_decay)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.838047\n0.801519\n00:14\n\n\n1\n0.761085\n0.744033\n00:12\n\n\n2\n0.709788\n0.734091\n00:13\n\n\n3\n0.653415\n0.728950\n00:13\n\n\n4\n0.545074\n0.743957\n00:12\n\n\n\n\n\n\n\n\n\nThe recommender problem is one where we have some users and their ratings of some items. We want to know which unseen items a user may like.\nWe implemented two approaches to collaborative filtering:\n\nProbabilistic Matrix Factorization (PMF)\nA neural network\n\nFor each approach, we build a model from scratch in Pytorch, then compared that with fastai’s implementation. For the PMF approach, we even gained some intuition by creating a spreadsheet implementation first!\nIn practice, both approaches can be stacked for improved recommendations.\nThe idea we explored here of user some (initially random) embeddings to represent an entity and then letting our model learn them is a powerful one and it is not limited to collaborative learning. NLP uses embeddings to represent each unique token (i.e. each word with word-level tokenisation). It can then understand relationships between similar words, much like we were able to use embedding distances to identify similar movies.\n\n\n\n\nCourse lesson page\nCollaborative filtering notebook\nComputational linear algebra fastai course"
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html#the-intuition-behind-collaborative-filtering",
    "href": "posts/ml/fastai/lesson7/lesson.html#the-intuition-behind-collaborative-filtering",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "We have users ratings of movies.\nSay we had “embeddings” of a set of categories for each. So for a given movie, we have a vector of [action, sci-fi, romance] and for a given user we have their preference for [action, sci-fi, romance]. Then we could do the dot product between user embedding and movie embedding to get the probability that the user likes that movie. That is, the predicted user rating.\nSo the problem boils down to:\n\nWhat are the embeddings? i.e. the salient factors ([action, sci-fi, romance] in the example above)\nHow do we get them?\n\nThe answer to both questions is: we just let the model learn them.\nLet’s just pick a randomised embedding for each movie and each user. Then we have a loss function which is the MAE between predicted user rating for a movie and actual rating. Now we can use SGD to optimise those embeddings to find the best values."
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html#a-deep-learning-spreadsheet",
    "href": "posts/ml/fastai/lesson7/lesson.html#a-deep-learning-spreadsheet",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "To gain an intuition behind the calculations behind a collaborative filter, we can work through a (smaller) example in excel. This allows us to see the logic and dig into the calculations before we create them “for real” in Python.\n\n\n\n\n\n\nTip\n\n\n\nThis can be found in this spreadsheet.\n\n\nWe first look at an example where the results are in a cross-table and we can take the dot product of user embeddings and movie embeddings.\nThen we reshape the problem slightly by placing all of the embeddings in a matrix and doing a lookup. This is essentially what pytorch does, although it uses matrix multiplication by one-hot encoded vectors rather than array lookups for computational efficiency.\nWe then add a bias term to account for some users who love all movies, or hate all movies. And also movies that are universally beloved."
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html#input-data-and-factors",
    "href": "posts/ml/fastai/lesson7/lesson.html#input-data-and-factors",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "The broad idea behind collaborative filtering is:\n\nIf we could quantify the most salient “latent factors” about a movie, and…\nQuantify how much a user cares about that factor, then…\nIf we multiplied the two (dot product) it would give a measure of their rating.\n\nBut what are those latent factors? We let the model learn it. 1. We initialise randomised latent factors (called embeddings) 2. We use that to predict the user’s rating for each move. Initially, those randomised weights will give terrible predictions. 3. Our loss function is the MSE of the ground truth actual predictions and the prediction rating. 4. We can optimise the embedding values to minimise this loss function.\n\n\nWe use data on user ratings of movies sourced from MovieLens. The ml-latest-small data set is downloaded and saved in the DATA_DIR folder.\n\nfrom pathlib import Path\n\nfrom fastai.collab import CollabDataLoaders, Module, Embedding, collab_learner\nfrom fastai.tabular.all import one_hot, sigmoid_range, MSELossFlat, Learner, get_emb_sz\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd \nimport torch\n\n\nDATA_DIR = Path(\"/Users/gurpreetjohl/workspace/python/ml-practice/ml-practice/datasets/ml-latest-small\")\n\nLoad the ratings data which we will use for this task:\n\nratings = pd.read_csv(DATA_DIR / 'ratings.csv')\nratings\n\n\n\n\n\n\n\n\nuserId\nmovieId\nrating\ntimestamp\n\n\n\n\n0\n1\n1\n4.0\n964982703\n\n\n1\n1\n3\n4.0\n964981247\n\n\n2\n1\n6\n4.0\n964982224\n\n\n3\n1\n47\n5.0\n964983815\n\n\n4\n1\n50\n5.0\n964982931\n\n\n...\n...\n...\n...\n...\n\n\n100831\n610\n166534\n4.0\n1493848402\n\n\n100832\n610\n168248\n5.0\n1493850091\n\n\n100833\n610\n168250\n5.0\n1494273047\n\n\n100834\n610\n168252\n5.0\n1493846352\n\n\n100835\n610\n170875\n3.0\n1493846415\n\n\n\n\n100836 rows × 4 columns\n\n\n\nThe users and movies are encoded as integers.\nFor reference, we can load the movies data to see what each movieId corresponds to:\n\nmovies = pd.read_csv(DATA_DIR / 'movies.csv')\nmovies\n\n\n\n\n\n\n\n\nmovieId\ntitle\ngenres\n\n\n\n\n0\n1\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n1\n2\nJumanji (1995)\nAdventure|Children|Fantasy\n\n\n2\n3\nGrumpier Old Men (1995)\nComedy|Romance\n\n\n3\n4\nWaiting to Exhale (1995)\nComedy|Drama|Romance\n\n\n4\n5\nFather of the Bride Part II (1995)\nComedy\n\n\n...\n...\n...\n...\n\n\n9737\n193581\nBlack Butler: Book of the Atlantic (2017)\nAction|Animation|Comedy|Fantasy\n\n\n9738\n193583\nNo Game No Life: Zero (2017)\nAnimation|Comedy|Fantasy\n\n\n9739\n193585\nFlint (2017)\nDrama\n\n\n9740\n193587\nBungo Stray Dogs: Dead Apple (2018)\nAction|Animation\n\n\n9741\n193609\nAndrew Dice Clay: Dice Rules (1991)\nComedy\n\n\n\n\n9742 rows × 3 columns\n\n\n\nWe’ll merge the two for easier human readability.\n\nratings = ratings.merge(movies)\nratings\n\n\n\n\n\n\n\n\nuserId\nmovieId\nrating\ntimestamp\ntitle\ngenres\n\n\n\n\n0\n1\n1\n4.0\n964982703\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n1\n1\n3\n4.0\n964981247\nGrumpier Old Men (1995)\nComedy|Romance\n\n\n2\n1\n6\n4.0\n964982224\nHeat (1995)\nAction|Crime|Thriller\n\n\n3\n1\n47\n5.0\n964983815\nSeven (a.k.a. Se7en) (1995)\nMystery|Thriller\n\n\n4\n1\n50\n5.0\n964982931\nUsual Suspects, The (1995)\nCrime|Mystery|Thriller\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n100831\n610\n166534\n4.0\n1493848402\nSplit (2017)\nDrama|Horror|Thriller\n\n\n100832\n610\n168248\n5.0\n1493850091\nJohn Wick: Chapter Two (2017)\nAction|Crime|Thriller\n\n\n100833\n610\n168250\n5.0\n1494273047\nGet Out (2017)\nHorror\n\n\n100834\n610\n168252\n5.0\n1493846352\nLogan (2017)\nAction|Sci-Fi\n\n\n100835\n610\n170875\n3.0\n1493846415\nThe Fate of the Furious (2017)\nAction|Crime|Drama|Thriller\n\n\n\n\n100836 rows × 6 columns\n\n\n\n\n\n\n\ndls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\ndls.show_batch()\n\n\n\n\n\nuserId\ntitle\nrating\n\n\n\n\n0\n4\nMighty Aphrodite (1995)\n3.0\n\n\n1\n573\nDark Knight, The (2008)\n5.0\n\n\n2\n116\nAmadeus (1984)\n3.0\n\n\n3\n380\nAddams Family, The (1991)\n5.0\n\n\n4\n353\nBrothers McMullen, The (1995)\n4.0\n\n\n5\n37\nFugitive, The (1993)\n4.0\n\n\n6\n356\nUnbreakable (2000)\n4.0\n\n\n7\n489\nAlien³ (a.k.a. Alien 3) (1992)\n3.5\n\n\n8\n174\nNell (1994)\n5.0\n\n\n9\n287\nPanic Room (2002)\n2.5\n\n\n\n\n\nInitialise randomised 5-dimensional embeddings.\nHow should we choose the number of latent factors? (5 in the example above). Jeremy wrote down some ballpark values for models of different sizes in excel, then fit a function to it to get a heuristic measure. This is the default used by fast AI.\n\nn_users  = len(dls.classes['userId'])\nn_movies = len(dls.classes['title'])\nn_factors = 5\n\nuser_factors = torch.randn(n_users, n_factors)\nmovie_factors = torch.randn(n_movies, n_factors)\n\nDoing a matrix multiply by a one hot encoded vector is the same as doing a lookup in an array, just in a more computationally efficient way. Recall the softmax example.\nAn embedding is essentially just “look up in an array”."
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html#building-a-collaborative-filter-from-scratch",
    "href": "posts/ml/fastai/lesson7/lesson.html#building-a-collaborative-filter-from-scratch",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "Putting a sigmoid_range on the final layer to squish ratings to fit 0 to 5 means “the model doesn’t have to work as hard” to get movies in the right range. In practice we use 5.5 as the sigmoid scale value as a sigmoid can never hit 1, but we want ratings to be able to hit 5.\n\nfrom fastai.collab import CollabDataLoaders, Module, Embedding\nfrom fastai.tabular.all import one_hot, sigmoid_range, MSELossFlat, Learner\n\n\nclass DotProduct(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.y_range = y_range\n\n    def forward(self, x):\n        users = self.user_factors(x[:, 0])\n        movies = self.movie_factors(x[:, 1])\n        # Apply a sigmoid to the raw_output\n        raw_output = (users * movies).sum(dim=1)\n        return sigmoid_range(raw_output, *self.y_range)\n\nWe can now fit a model\n\nembedding_dim = 50\nnum_epochs = 5\nmax_learning_rate = 5e-3\n\nmodel = DotProduct(n_users, n_movies, embedding_dim)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(num_epochs, max_learning_rate)\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\nAdding a user bias term and a movie bias term to the prediction call helps account for the fact that some users always rate high (4 or 5) but other users always rate low. And similarly for movies if everyone always rates it a 5 or a 1.\n\nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.user_bias = Embedding(n_users, 1)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.movie_bias = Embedding(n_movies, 1)\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        raw_output = (users * movies).sum(dim=1, keepdim=True)\n        raw_output += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n        return sigmoid_range(raw_output, *self.y_range)\n\n\nmodel = DotProductBias(n_users, n_movies, embedding_dim)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(num_epochs, max_learning_rate)\n\n\n\n\n\n\n    \n      \n      0.00% [0/5 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/1260 00:00&lt;?]\n    \n    \n\n\n\n\n\nThe validation loss in the previous model decreases then icnreases, which is a clear indication of overfitting.\nWe want to avoid overfitting, but data augmentation isn’t possible here. One approach is to use weight decay AKA L2 regularisation. We add sum of weights squared to the loss function.\nHow does this prevent overfitting? The larger the coefficients, the sharper the canyons the model is able to produce, which allows it to fit individual data points. By penalising larger weights, it will only produce sharp changes if this causes the model to fit many points well, so it should generalise better.\nWe essentially want to modify our loss function with an additional term dependent on the magnitude of the weights:\nloss_with_weight_decay = loss + weight_decay * (parameters**2).sum()\nIn practice, these values would be large and numerically unstable. We only actually care about the gradient of the loss, so we can add the gradient of the additional term to the existing gradient.\nparameters.grad += weight_decay * 2 * parameters\nBut weight_decay is just a constant that we choose, so we can fold the 2* term into it.\n\nweight_decay = 0.1\n\nmodel = DotProductBias(n_users, n_movies, embedding_dim)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(num_epochs, max_learning_rate, wd=weight_decay)\n\n\n\n\n\n\n    \n      \n      0.00% [0/5 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n\n\n\n    \n      \n      0.16% [2/1260 00:00&lt;00:20 1.4480]\n    \n    \n\n\n\n\n\nIn the previous section, we used that pytorch (technically the fastai version) Embedding module.\nLet’s briefly take a look at this and create our own Embedding module from scratch.\n\n\nThe way pytorch knows if a tensor is a parameter (and therefore can calculate gradients on it) is if it inherits from nn.Parameter. Then a Module’s .parameters() method will list this tensor.\nAs an example of this behaviour, let’s create a module with some parameters but WITHOUT declaring these as Parameters:\n\nclass MyModule(Module):\n    def __init__(self): \n        self.a = torch.ones(3)\n\nmm = MyModule()\nlist(mm.parameters())\n\n[]\n\n\nWe declared a tensor a in MyModule but we don’t see it! Which means it wouldn’t be trained by Pytorch.\nInstead, let’s declare is as a Parameter:\n\nclass MyModuleWithParams(Module):\n    def __init__(self): \n        self.a = torch.nn.Parameter(torch.ones(3))\n\nmm_params = MyModuleWithParams()\nlist(mm_params.parameters())\n\n[Parameter containing:\n tensor([1., 1., 1.], requires_grad=True)]\n\n\nPytorch’s builtin modules all use Parameter for any trainable parameters, so we haven’t needed to explicitly declare this.\nAs an example, if we use Pytorch’s Linear layer, it will automatically appear as a parameter:\n\nclass MyModuleWithLinear(Module):\n    def __init__(self): \n        self.a = torch.nn.Linear(1, 3, bias=False)\n\nmm_linear = MyModuleWithLinear()\nlist(mm_linear.parameters())\n\n[Parameter containing:\n tensor([[-0.6689],\n         [-0.0181],\n         [ 0.8172]], requires_grad=True)]\n\n\n\ntype(mm_linear.a.weight)\n\ntorch.nn.parameter.Parameter\n\n\n\n\n\nAn Embedding object essentially instantiates a tensor of random weights of the given dimensions and declares this as a Parameter. Pytorch can then modify the weights when training.\n\ndef create_params(tensor_dims):\n    \"\"\"Create a tensor of the required size and fill it with random values.\"\"\"\n    embedding_tensor = torch.zeros(*tensor_dims).normal_(0, 0.01)\n    return torch.nn.Parameter(embedding_tensor)\n\nNow we can replace the import Embedding module with our custom implementation create_params in the DotProductBias module:\n\nclass DotProductBiasCustomEmbedding(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = create_params([n_users, n_factors])\n        self.user_bias = create_params([n_users])\n        self.movie_factors = create_params([n_movies, n_factors])\n        self.movie_bias = create_params([n_movies])\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors[x[:,0]]\n        movies = self.movie_factors[x[:,1]]\n        raw_output = (users * movies).sum(dim=1)\n        raw_output += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]\n        return sigmoid_range(raw_output, *self.y_range)\n\n\nmodel = DotProductBiasCustomEmbedding(n_users, n_movies, embedding_dim)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(num_epochs, max_learning_rate, wd=weight_decay)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.795456\n0.787701\n00:07\n\n\n1\n0.693971\n0.728376\n00:06\n\n\n2\n0.546227\n0.711909\n00:07\n\n\n3\n0.402994\n0.707349\n00:06\n\n\n4\n0.282765\n0.708693\n00:06\n\n\n\n\n\n\n\n\n\nWe can interrogate the model to learn more about these embeddings it has learned.\n\n\nWe can visualise the biases of our collaborative filter model to see:\n\nMovie biases: Which movies are bad even compared to other similar movies of that type? Lawnmower man 2 is crap even compared to similar action movies. But people love titanic even if they don’t normally like romance dramas.\nUser biases: Which users love any and all movies? Users who give a high rating to all movies.\n\nAccording to our biases, these movies are crap even for those who like that style of movie:\n\nmovie_bias = learn.model.movie_bias.squeeze()\nidxs = movie_bias.argsort()[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Karate Kid, Part III, The (1989)',\n 'Catwoman (2004)',\n 'Stuart Saves His Family (1995)',\n 'Speed 2: Cruise Control (1997)',\n 'Dungeons & Dragons (2000)']\n\n\nWhereas these are highly rated, even when users don’t normally like that type of movie:\n\nidxs = movie_bias.argsort()[-5:]\n[dls.classes['title'][i] for i in idxs]\n\n['Star Wars: Episode IV - A New Hope (1977)',\n 'Dark Knight, The (2008)',\n 'Green Mile, The (1999)',\n 'Forrest Gump (1994)',\n 'Shawshank Redemption, The (1994)']\n\n\n\n\n\nWe can visualise the weights to see what human-interpretable features the model is learning.\nWe can condense our embedding to 2 axes with PCA. we get a critically-acclaimed -&gt; popular x-axis and a action-dialog y-axis.\n\n\n\n\n\n\nTip\n\n\n\nFor more details on PCA and related methods, see computational linear algebra fastai course\n\n\n\ng = ratings.groupby('title')['rating'].count()\ntop_movies = g.sort_values(ascending=False).index.values[:1000]\ntop_idxs = torch.tensor([learn.dls.classes['title'].o2i[m] for m in top_movies])\nmovie_w = learn.model.movie_factors[top_idxs].cpu().detach()\nmovie_pca = movie_w.pca(3)\nfac0,fac1,fac2 = movie_pca.t()\nidxs = list(range(50))\nX = fac0[idxs]\nY = fac2[idxs]\nplt.figure(figsize=(12,12))\nplt.scatter(X, Y)\nfor i, x, y in zip(top_movies[idxs], X, Y):\n    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWe can also use the “embedding distance” (distance in the latent space) to see when two movies are similar. We use cosine similarity to determine this distance, which is similar in principle to Euclidean distance but normalised.\nIn the example below, we start with the movie Forrest Gump and find the closest movie to it in our embedding:\n\nmovie_idx = dls.classes['title'].o2i['Forrest Gump (1994)']\n\nmovie_factors = learn.model.movie_factors\ndistances = torch.nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[movie_idx][None])\nclosest_distance_idx = distances.argsort(descending=True)[1]\ndls.classes['title'][closest_distance_idx]\n\n'Beautiful Mind, A (2001)'"
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html#building-a-collaborative-filter-with-fastais-library",
    "href": "posts/ml/fastai/lesson7/lesson.html#building-a-collaborative-filter-with-fastais-library",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "We can repeat the same exercise using the collaborative filter from the fastai library to see how it compares to our from-scratch implementation.\n\nlearn_fast = collab_learner(dls, n_factors=embedding_dim, y_range=(0, 5.5))\nlearn_fast.fit_one_cycle(num_epochs, max_learning_rate, wd=weight_decay)\n\n\n\n\n\n\n    \n      \n      0.00% [0/5 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n\n\n\n    \n      \n      0.40% [5/1260 00:00&lt;00:20 1.5528]\n    \n    \n\n\nWe can repeat any of the analysis of our from-scratch model. For example, the movies with the highest bias:\n\nmovie_bias = learn_fast.model.i_bias.weight.squeeze()\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Shawshank Redemption, The (1994)',\n 'Forrest Gump (1994)',\n 'Green Mile, The (1999)',\n 'Star Wars: Episode IV - A New Hope (1977)',\n 'Dark Knight, The (2008)']"
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html#bootstrapping-a-collaborative-filtering-model",
    "href": "posts/ml/fastai/lesson7/lesson.html#bootstrapping-a-collaborative-filtering-model",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "How do you start off a collaborative filtering model? For example, when you first start out, you have no data on users or items.\nOr even for established companies, what happens when we have a new user or a new item, so the entire row or column is null?\nThere is no hard and fast solution, they all boil down to “use common sense”.\n\nA tempting option is to fill NaNs with the median latent vectors. But this might result in an odd combination that doesn’t exist in practice, i.e. the latent space isn’t continuous so this could be where a gap in the latent space lies. For example, a medium action, medium sci-fi film with medium romance and medium comedy that is medium popular and medium critically acclaimed.\nAnother option is to pick a user/item that is representative of the average taste.\nCreate a tabular model using answers to a new user survey. Ask the user some questions when they sign up, then create a model where the user’s embedding vector is the dependent variable and their answers, along with any other relevant signup metadata, are the independent variables.\n\nIt is important to be careful of a small number of extremely enthusiastic users dominating the recommendations. For example, people who watch anime watch a LOT of it, and rate a lot of it highly. So this could end up getting recommended to users outside of this niche.\nThis can create positive feedback loops that change the behaviour of your product in unexpected ways."
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html#deep-learning-for-collaborative-filtering",
    "href": "posts/ml/fastai/lesson7/lesson.html#deep-learning-for-collaborative-filtering",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "The matrix completion approach used previously is known as Probabilistic Matrix Factorization (PMF). An alternative approach is to use deep learning.\nIn practice the two approaches are often stacked in an ensemble.\nThis section explores the deep learning collaborative filtering approach from scratch, then recreates it using fastai’s library.\n\n\nWe are concatenating the embedding matrices together, rather than taking the dot product, so that we can pass it through a dense ANN.\nThese matrices can be different sizes, and the size of embedding to use for each depends on the number of classes in the data. Fastai has a heuristic method for this which we use here:\n\n(user_num_classes, user_num_embeddings), (item_num_classes, item_num_embeddings) = get_emb_sz(dls)\n\nWe can then use this in a simple neural network with one hidden layer:\n\nclass CollabNN(Module):\n    def __init__(self, user_embedding_size, item_embedding_size, y_range=(0, 5.5), n_activations=100):\n        self.user_factors = Embedding(*user_embedding_size)\n        self.item_factors = Embedding(*item_embedding_size)\n        self.layers = torch.nn.Sequential(\n            torch.nn.Linear(user_embedding_size[1] + item_embedding_size[1], n_activations),\n            torch.nn.ReLU(),\n            torch.nn.Linear(n_activations, 1))\n        self.y_range = y_range\n        \n    def forward(self, x):\n        embs = self.user_factors(x[:, 0]), self.item_factors(x[:, 1])\n        x = self.layers(torch.cat(embs, dim=1))\n        return sigmoid_range(x, *self.y_range)\n    \ncollab_nn_model = CollabNN(user_embedding_size=(user_num_classes, user_num_embeddings),\n                           item_embedding_size=(item_num_classes, item_num_embeddings))\n\nNow train this model on the data:\n\nlearn_nn = Learner(dls, collab_nn_model, loss_func=MSELossFlat())\nlearn_nn.fit_one_cycle(num_epochs, max_learning_rate, wd=weight_decay)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.799004\n0.792579\n00:10\n\n\n1\n0.747623\n0.755708\n00:10\n\n\n2\n0.706981\n0.723887\n00:10\n\n\n3\n0.650337\n0.719642\n00:10\n\n\n4\n0.569418\n0.734302\n00:10\n\n\n\n\n\n\n\n\nWe can repeat the same exercise using fastai’s implementation.\nThis is almost identical to the PMF approach, simply with an additional argument use_nn=True.\n\nlearn_nn_fast = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100, 50])\nlearn_nn_fast.fit_one_cycle(num_epochs, max_learning_rate, wd=weight_decay)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.838047\n0.801519\n00:14\n\n\n1\n0.761085\n0.744033\n00:12\n\n\n2\n0.709788\n0.734091\n00:13\n\n\n3\n0.653415\n0.728950\n00:13\n\n\n4\n0.545074\n0.743957\n00:12"
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html#summary",
    "href": "posts/ml/fastai/lesson7/lesson.html#summary",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "The recommender problem is one where we have some users and their ratings of some items. We want to know which unseen items a user may like.\nWe implemented two approaches to collaborative filtering:\n\nProbabilistic Matrix Factorization (PMF)\nA neural network\n\nFor each approach, we build a model from scratch in Pytorch, then compared that with fastai’s implementation. For the PMF approach, we even gained some intuition by creating a spreadsheet implementation first!\nIn practice, both approaches can be stacked for improved recommendations.\nThe idea we explored here of user some (initially random) embeddings to represent an entity and then letting our model learn them is a powerful one and it is not limited to collaborative learning. NLP uses embeddings to represent each unique token (i.e. each word with word-level tokenisation). It can then understand relationships between similar words, much like we were able to use embedding distances to identify similar movies."
  },
  {
    "objectID": "posts/ml/fastai/lesson7/lesson.html#references",
    "href": "posts/ml/fastai/lesson7/lesson.html#references",
    "title": "FastAI Lesson 7: Collaborative Filtering",
    "section": "",
    "text": "Course lesson page\nCollaborative filtering notebook\nComputational linear algebra fastai course"
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html",
    "href": "posts/ml/fastai/lesson2/lesson.html",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "These are notes from lesson 2 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\n\nDeploy a model to Huggingface Spaces: see car classifier model\nDeploy a model to a Github Pages website: see car classifier website\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nA website for quizzes based on the book: AI quizzes\n\n\n\n\nIt can be useful to train a model on the data BEFORE you clean it\n\nCounterintuitive!\nThe confusion matrix output of the learner gives you a good intuition about which classifications are hard\nplot_top_losses shows which examples were hardest for the model to classify. This can find (1) when the model is correct but not confident, and (2) when the model was confident but incorrect\nImageClassifierCleaner shows the examples in the training and validation set ordered by loss, so we can choose to keep, reclassify or remove them\n\nFor image resizing, random resize crop can often be more effective.\n\nSquishing can result in weird, unrealistic images.\nPadding or mirroring can add false information that the model will erroneously learn.\nRandom crops give different sections of the image which acts as a form of data augmentation.\naug_transforms can be use for more sophisticated data augmentation like warping and recoloring images.\n\n\n\n\nOnce you are happy with the model you’ve trained, you can pickle the learner object and save it.\nlearn.export('model.pkl')\nThen you can add the saved model to the hugging face space. To use the model to make predictions, any external functions you used to create the model will need to be instantiated too.\nlearn = load_learner('model.pkl')\nlearn.predict(image)\n\n\n\nHugging face spaces hosts models with a choice of pre-canned interfaces to quickly deploy a model to the public. Gradio is used in the example in the lecture. Streamlit is an alternative to Gradio that is more flexible.\nGradio requires a dict of classes as keys and probabilities (as floats not tensors) as the values. To go from the Gradio prototype to a production app, you can view the Gradio API from the huggingface space which will show you the API. The API exposes an endpoint which you can then hit from your own frontend app.\nGithub pages is a free and simple way to host a public website. See this fastai repo as an example of a minimal example html website which issues GET requests to the Gradio API\n\n\n\nTo convert a notebook to a python script, you can add #|export to the top of any cells to include in the script, then use:\nfrom nbdev.export import notebook2script\nnotebook2script('name_of_output_file.py')\nUse #|default_exp app in the first cell of the notebook to set the default name of the exported python file.\n\n\n\nHow do we choose the number of epochs to train for? Whenever it is “good enough” for your use case.\nIf you need to train for longer, you may need to use data augmentation to prevent overfitting. Keep an eye on the validation error to check overfitting.\n\n\n\n\nCourse lesson page\nHuggingFace Spaces"
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html#data-pre-processing",
    "href": "posts/ml/fastai/lesson2/lesson.html#data-pre-processing",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "It can be useful to train a model on the data BEFORE you clean it\n\nCounterintuitive!\nThe confusion matrix output of the learner gives you a good intuition about which classifications are hard\nplot_top_losses shows which examples were hardest for the model to classify. This can find (1) when the model is correct but not confident, and (2) when the model was confident but incorrect\nImageClassifierCleaner shows the examples in the training and validation set ordered by loss, so we can choose to keep, reclassify or remove them\n\nFor image resizing, random resize crop can often be more effective.\n\nSquishing can result in weird, unrealistic images.\nPadding or mirroring can add false information that the model will erroneously learn.\nRandom crops give different sections of the image which acts as a form of data augmentation.\naug_transforms can be use for more sophisticated data augmentation like warping and recoloring images."
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html#saving-a-model",
    "href": "posts/ml/fastai/lesson2/lesson.html#saving-a-model",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "Once you are happy with the model you’ve trained, you can pickle the learner object and save it.\nlearn.export('model.pkl')\nThen you can add the saved model to the hugging face space. To use the model to make predictions, any external functions you used to create the model will need to be instantiated too.\nlearn = load_learner('model.pkl')\nlearn.predict(image)"
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html#hosting-a-model",
    "href": "posts/ml/fastai/lesson2/lesson.html#hosting-a-model",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "Hugging face spaces hosts models with a choice of pre-canned interfaces to quickly deploy a model to the public. Gradio is used in the example in the lecture. Streamlit is an alternative to Gradio that is more flexible.\nGradio requires a dict of classes as keys and probabilities (as floats not tensors) as the values. To go from the Gradio prototype to a production app, you can view the Gradio API from the huggingface space which will show you the API. The API exposes an endpoint which you can then hit from your own frontend app.\nGithub pages is a free and simple way to host a public website. See this fastai repo as an example of a minimal example html website which issues GET requests to the Gradio API"
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html#using-nbdev",
    "href": "posts/ml/fastai/lesson2/lesson.html#using-nbdev",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "To convert a notebook to a python script, you can add #|export to the top of any cells to include in the script, then use:\nfrom nbdev.export import notebook2script\nnotebook2script('name_of_output_file.py')\nUse #|default_exp app in the first cell of the notebook to set the default name of the exported python file."
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html#training-the-model-and-when-to-stop",
    "href": "posts/ml/fastai/lesson2/lesson.html#training-the-model-and-when-to-stop",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "How do we choose the number of epochs to train for? Whenever it is “good enough” for your use case.\nIf you need to train for longer, you may need to use data augmentation to prevent overfitting. Keep an eye on the validation error to check overfitting."
  },
  {
    "objectID": "posts/ml/fastai/lesson2/lesson.html#references",
    "href": "posts/ml/fastai/lesson2/lesson.html#references",
    "title": "FastAI Lesson 2: Deployment",
    "section": "",
    "text": "Course lesson page\nHuggingFace Spaces"
  },
  {
    "objectID": "posts/ml/fastai/lesson4/lesson.html",
    "href": "posts/ml/fastai/lesson4/lesson.html",
    "title": "FastAI Lesson 4: Natural Language Processing",
    "section": "",
    "text": "These are notes from lesson 4 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\nKaggle NLP pattern similarity notebook: see notebook\n\n\n\n\nNLP applications: categorising documents, translation, text generation.\nWe are using the Huggingface transformers library for this lesson. It is now incorporated into the fastai library.\nULMFit is an algorithm which uses fine-tuning, in this example to train a positve/negative sentiment classifier in 3 steps: 1. Train an RNN on wikipedia to predict the next word. No labels required. 2. Fine-tune this for IMDb reviews to predict the next word of a movie review. Still no labels required. 3. Fine-tune this to classify the sentiment.\nTransformers have overtaken ULMFit as the state-of-the-art.\nLooking “inside” a CNN, the first layer contains elementary detectors like edge detectors, blob detectors, gradient detectors etc. These get combined in non-linear ways to make increasingly complex detectors. Layer 2 might combine vertical and horizontal edge detectors into a corner detector. By the later layers, it is detecting rich features like lizard eyes, dog faces etc. See Zeiler and Fegus.\nFor the fine-tuning process, the earlier layers are unlikely to need changing because they are more general. So we only need to fine-tune (i.e. re-train) the later layers.\n\n\n\nAs an example of NLP in practice, we walk through this notebook for U.S. Patent Phrase to Phrase Matching.\nReshape the input to fit a standard NLP task\n\nWe want to learn the similarity between two fields and are provided with similarity scores.\nWe concat the fields of interest (with identifiers in between). The identifiers themselves are not important, they just need to be consistent.\nThe NLP model is then a supervised regression task to predict the score given the concatendated string.\n\ndf['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor\n\n\nSplit the text into tokens (words). Tokens are, broadly speaking, words.\nThere are some caveats to that, as some languages like Chinese don’t fit nicely into that model. We don’t want the vocabulary to be too big. Alternative approaches use character tokenization rather than word tokenization.\nIn practice, we tokenize into subwords.\n\n\n\nMap each unique token to a number. One-hot encoding.\nThe choice of tokenization and numericalization depends on the model you use. Whoever trained the model chose a convention for tokenizing. We need to be consistent with that if we want the model to work correctly.\n\n\n\nThe Huggingface model hub contains thousands of pretrained models.\nFor NLP tasks, it is useful to choose a model that was trained on a similar corpus, so you can search the model hub. In this case, we search for “patent”.\nSome models are general purpose, e.g. deberta-v3 used in the lesson.\nULMFit handles large documents better as it can split up the document. Transformer approaches require loading the whole document into GPU memory, so struggle for larger documents.\n\n\n\nIf a model is too simple (i.e. not flexible enough) then it cannot fit the data and be biased, leading to underfitting.\nIf the model fits the data points too closely, it is overfitting.\nA good validation set, and monitoring validation error rather than training error as a metric, is key to avoiding overfitting. See this article for an in-depth discussion on the importance of choosing a good validation set.\nOften people will default to using a random train/test split. This is what scikit-learn uses. This is a BAD idea very often.\nFor time-series data, it’s easier to infer gaps than it is to predict a block in the future. The latter is the more common task but a random split simulates the former, giving unrealistically good performance.\nFor image data, there may be people, boats, etc that are in the training set but not the test set. By failing to have new people in the validation set, the model can learn things about specific people/boats that it can’t rely on in practice.\n\n\n\nMetrics are things that are human-understandable.\nLoss functions should be smooth and differentiable to aid in training.\nThese can sometimes be the same thing, but not in general. For example, accuracy is a good metric in image classification. We could tweak the weights in such a way that it improves the model slightly, but not so much that it now correctly classifies a previously incorrect image. This means the metric function is bumpy, therefore a bad loss function.\nThis article goes into more detail on the choice of metrics.\nAI can be particularly dangerous at confirming systematic biases, because it is so good at optimising metrics, so it will conform to any biases present in the training data. Making decisions based on the model then reinforces those biases.\n\nGoodhart’s law applies: If a metric becomes a target it’s no longer a good metric\n\n\n\n\nThe best way to understand a metric is not to look at the mathematical formula, but to plot some data for which the metric is high, medium and low, then see what that tells you.\nAfter that, look at the equation to see if your intuition matches the logic.\n\n\n\nFast AI has a function to find a good starting point. Otherwise, pick a small value and keep doubling it until it falls apart.\n\n\n\n\n\nCourse lesson page\nHuggingFace Transformers\nVisualizing and Understanding Convolutional Networks\nHuggingFace models\nValidation sets\nModel metrics"
  },
  {
    "objectID": "posts/ml/fastai/lesson4/lesson.html#nlp-background-and-transformers",
    "href": "posts/ml/fastai/lesson4/lesson.html#nlp-background-and-transformers",
    "title": "FastAI Lesson 4: Natural Language Processing",
    "section": "",
    "text": "NLP applications: categorising documents, translation, text generation.\nWe are using the Huggingface transformers library for this lesson. It is now incorporated into the fastai library.\nULMFit is an algorithm which uses fine-tuning, in this example to train a positve/negative sentiment classifier in 3 steps: 1. Train an RNN on wikipedia to predict the next word. No labels required. 2. Fine-tune this for IMDb reviews to predict the next word of a movie review. Still no labels required. 3. Fine-tune this to classify the sentiment.\nTransformers have overtaken ULMFit as the state-of-the-art.\nLooking “inside” a CNN, the first layer contains elementary detectors like edge detectors, blob detectors, gradient detectors etc. These get combined in non-linear ways to make increasingly complex detectors. Layer 2 might combine vertical and horizontal edge detectors into a corner detector. By the later layers, it is detecting rich features like lizard eyes, dog faces etc. See Zeiler and Fegus.\nFor the fine-tuning process, the earlier layers are unlikely to need changing because they are more general. So we only need to fine-tune (i.e. re-train) the later layers."
  },
  {
    "objectID": "posts/ml/fastai/lesson4/lesson.html#kaggle-competition-walkthrough",
    "href": "posts/ml/fastai/lesson4/lesson.html#kaggle-competition-walkthrough",
    "title": "FastAI Lesson 4: Natural Language Processing",
    "section": "",
    "text": "As an example of NLP in practice, we walk through this notebook for U.S. Patent Phrase to Phrase Matching.\nReshape the input to fit a standard NLP task\n\nWe want to learn the similarity between two fields and are provided with similarity scores.\nWe concat the fields of interest (with identifiers in between). The identifiers themselves are not important, they just need to be consistent.\nThe NLP model is then a supervised regression task to predict the score given the concatendated string.\n\ndf['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor\n\n\nSplit the text into tokens (words). Tokens are, broadly speaking, words.\nThere are some caveats to that, as some languages like Chinese don’t fit nicely into that model. We don’t want the vocabulary to be too big. Alternative approaches use character tokenization rather than word tokenization.\nIn practice, we tokenize into subwords.\n\n\n\nMap each unique token to a number. One-hot encoding.\nThe choice of tokenization and numericalization depends on the model you use. Whoever trained the model chose a convention for tokenizing. We need to be consistent with that if we want the model to work correctly.\n\n\n\nThe Huggingface model hub contains thousands of pretrained models.\nFor NLP tasks, it is useful to choose a model that was trained on a similar corpus, so you can search the model hub. In this case, we search for “patent”.\nSome models are general purpose, e.g. deberta-v3 used in the lesson.\nULMFit handles large documents better as it can split up the document. Transformer approaches require loading the whole document into GPU memory, so struggle for larger documents.\n\n\n\nIf a model is too simple (i.e. not flexible enough) then it cannot fit the data and be biased, leading to underfitting.\nIf the model fits the data points too closely, it is overfitting.\nA good validation set, and monitoring validation error rather than training error as a metric, is key to avoiding overfitting. See this article for an in-depth discussion on the importance of choosing a good validation set.\nOften people will default to using a random train/test split. This is what scikit-learn uses. This is a BAD idea very often.\nFor time-series data, it’s easier to infer gaps than it is to predict a block in the future. The latter is the more common task but a random split simulates the former, giving unrealistically good performance.\nFor image data, there may be people, boats, etc that are in the training set but not the test set. By failing to have new people in the validation set, the model can learn things about specific people/boats that it can’t rely on in practice.\n\n\n\nMetrics are things that are human-understandable.\nLoss functions should be smooth and differentiable to aid in training.\nThese can sometimes be the same thing, but not in general. For example, accuracy is a good metric in image classification. We could tweak the weights in such a way that it improves the model slightly, but not so much that it now correctly classifies a previously incorrect image. This means the metric function is bumpy, therefore a bad loss function.\nThis article goes into more detail on the choice of metrics.\nAI can be particularly dangerous at confirming systematic biases, because it is so good at optimising metrics, so it will conform to any biases present in the training data. Making decisions based on the model then reinforces those biases.\n\nGoodhart’s law applies: If a metric becomes a target it’s no longer a good metric\n\n\n\n\nThe best way to understand a metric is not to look at the mathematical formula, but to plot some data for which the metric is high, medium and low, then see what that tells you.\nAfter that, look at the equation to see if your intuition matches the logic.\n\n\n\nFast AI has a function to find a good starting point. Otherwise, pick a small value and keep doubling it until it falls apart."
  },
  {
    "objectID": "posts/ml/fastai/lesson4/lesson.html#references",
    "href": "posts/ml/fastai/lesson4/lesson.html#references",
    "title": "FastAI Lesson 4: Natural Language Processing",
    "section": "",
    "text": "Course lesson page\nHuggingFace Transformers\nVisualizing and Understanding Convolutional Networks\nHuggingFace models\nValidation sets\nModel metrics"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson1/lesson.html",
    "href": "posts/ml/generative_deep_learning/lesson1/lesson.html",
    "title": "Generative AI: Intro",
    "section": "",
    "text": "The 30000ft view of generative AI.\n\n\n\nGenerative modeling is a branch of machine learning that involves training a model to produce new data that is similar ot a given dataset.\n\nThis means we require some training data which we wish to imitate. The model should be probabilistic rather than deterministic, so that we can sample different variations of the output.\nA helpful way to define generative modeling is by contrasting it with its counterpart discriminative modeling:\n\nDiscriminative modeling estimates \\(p(y|x)\\) - the probability of a label \\(y\\) given some observation \\(x\\)\nGenerative modeling estimates \\(p(x)\\) - the probability of seeing an observation \\(x\\). By sampling from this distribution we can generate new observations.\n\nWe can also create a conditional generative model to estimate \\(p(x|y)\\), the probability of seeing an observation \\(x\\) with label \\(y\\). For ample, if we train a model to generate a given type of fruit.\n\n\n\n\n\nThe aim:\n\nWe have training data of observations \\(X\\)\nWe assume the training data has been generated by an unknown distribution \\(p_{data}\\)\nWe want to build a generative model \\(p_{model}\\) that mimics \\(p_{data}\\)\n\nIf we succeed, we can sample from \\(p_{model}\\) to generate synthetic observations\n\n\nDesirable properties of \\(p_{model}\\) are:\n\nAccuracy - If \\(p_{model}\\) is a high value it should look like a real observation, and likewise low values should look fake.\nGeneration - It should be easy to sample from it.\nRepresentation - It should be possible to understand how high-level features are represented by the model.\n\n\n\n\nWe want to be able to describe things in terms of high-level features. For example, when describing appearance, we might talk about hair colour, length, eye colour, etc. Not RGB values pixel by pixel…\nThis is the idea behind representation learning. We describe each training data observation using some lower-dimensional latent space. Then we learn a maaping function from the latent space to the original domain.\nEncoder-decoder techniques try to transform a high-dimensional nonlinear manifold into a simpler latent space.\n\n\n\n\n\nThe complete set of all values that an observation \\(x\\) can take.\n\n\n\nA function \\(p(x)\\) that maps a point \\(x\\) in the sample space to a number between 0 and 1. The integral over the sample space must equal 1 for it to be a well-define probability distribtution.\nThere is one true data distribution \\(p_{data}\\) but infinitely many approximations \\(p_{model}\\) that we can find.\n\n\n\nWe can structure our approach to finding \\(p_{model}\\) by restricting ourselves to a family of density functions \\(p_{\\theta}(x)\\) which can be described with a finite set of parameters \\(\\theta\\).\nFor example, restricting our search to a linear model \\(y = w*x + b\\) where we need to find the parameters \\(w\\) and \\(b\\).\n\n\n\nThe likelihood, \\(L(\\theta|x)\\), of a parameter set \\(\\theta\\) is a function which measures the plausability of the parameters having observed some data point \\(x\\). It is the probability of seeing the data \\(x\\) if the true data-generating distribution was the model parameterized by \\(\\theta\\).\nIt is defined as the parametric model density: \\[\nL(\\theta|x) = p_{\\theta}(x)\n\\]\nIf we have a dataset of independent observations \\(X\\) then the probabilities multiply: \\[\nL(\\theta|X) = \\prod_{x \\in X} p_{\\theta}(x)\n\\]\nThe product of a lot of decimals can get unwieldy, so we often take the log-likelihood \\(l\\) to turn the product into a sum: \\[\nl(\\theta|X) = \\sum_{x \\in X} \\log{p_{\\theta}(x) }\n\\]\nThe focus of parametric modeling is therefore to find the optimal parameter set \\(\\hat{\\theta}\\), which leads to…\n\n\n\nA technique to estimate \\(\\hat{\\theta}\\), the set of parameters that is most likely to explain the observed data \\(X\\): \\[\n\\hat{\\theta} = \\arg\\max_x l(\\theta|X)\n\\]\nNeural networks often work with a loss function, so this is equivalent to minimising the negative log-likelihood: \\[\n\\hat{\\theta} = \\arg\\min_\\theta -l(\\theta|X) = \\arg\\min_\\theta -\\log{p_{\\theta}(X)}\n\\]\n\n\n\nGenerative modeling is a form of MLE where the parameters are the neural network weights. We want to find the weights that maximise the likelihood of observing the training data.\nBut for high-dimensional problems \\(p_{\\theta}(X)\\) is intractable, it cannot be calculated directly.\nThere are different approaches to maing the problem tractable. These are summarised in the following section and explored in more detail in subsequent chapters.\n\n\n\n\nAll types of generative model are seeking to solve the same problem, but they take different approaches to modeling the intractable density function \\(p_{\\theta}(x)\\).\nThere are three general approaches:\n\nExplicitly model the density function but constrain the model in some way.\nExplicitly model a tractable approximation of the density function.\nImplicitly model the density function, using a stochastic process that generates the data directly.\n\n\n\n\n\n\nflowchart LR\n\n\n  A(Generative models) --&gt; B1(Explicit density)\n  A(Generative models) --&gt; B2(Implicit density)\n\n  B1 --&gt; C1(Approximate density)\n  B1 --&gt; C2(Tractable density)\n\n  C1 --&gt; D1(VAE)\n  C1 --&gt; D2(Energy-based model)\n  C1 --&gt; D3(Diffusion model)\n\n  C2 --&gt; D4(Autoregressive model)\n  C2 --&gt; D5(Normalizing flow model)\n\n  B2 --&gt; D6(GAN)\n\n\n\n\n\n\n\nImplicit density models don’t even try to estimate the probability density, instead focusing on producing a stochastic data generation process directly.\nTractable models place constraints on the model architecture (i.e. the parameters \\(\\theta\\)).\n\n\n\n\n\nChapter 1 of Generative Deep Learning by David Foster."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson1/lesson.html#what-is-generative-modeling",
    "href": "posts/ml/generative_deep_learning/lesson1/lesson.html#what-is-generative-modeling",
    "title": "Generative AI: Intro",
    "section": "",
    "text": "Generative modeling is a branch of machine learning that involves training a model to produce new data that is similar ot a given dataset.\n\nThis means we require some training data which we wish to imitate. The model should be probabilistic rather than deterministic, so that we can sample different variations of the output.\nA helpful way to define generative modeling is by contrasting it with its counterpart discriminative modeling:\n\nDiscriminative modeling estimates \\(p(y|x)\\) - the probability of a label \\(y\\) given some observation \\(x\\)\nGenerative modeling estimates \\(p(x)\\) - the probability of seeing an observation \\(x\\). By sampling from this distribution we can generate new observations.\n\nWe can also create a conditional generative model to estimate \\(p(x|y)\\), the probability of seeing an observation \\(x\\) with label \\(y\\). For ample, if we train a model to generate a given type of fruit."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson1/lesson.html#the-generative-modeling-framework",
    "href": "posts/ml/generative_deep_learning/lesson1/lesson.html#the-generative-modeling-framework",
    "title": "Generative AI: Intro",
    "section": "",
    "text": "The aim:\n\nWe have training data of observations \\(X\\)\nWe assume the training data has been generated by an unknown distribution \\(p_{data}\\)\nWe want to build a generative model \\(p_{model}\\) that mimics \\(p_{data}\\)\n\nIf we succeed, we can sample from \\(p_{model}\\) to generate synthetic observations\n\n\nDesirable properties of \\(p_{model}\\) are:\n\nAccuracy - If \\(p_{model}\\) is a high value it should look like a real observation, and likewise low values should look fake.\nGeneration - It should be easy to sample from it.\nRepresentation - It should be possible to understand how high-level features are represented by the model."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson1/lesson.html#representation-learning",
    "href": "posts/ml/generative_deep_learning/lesson1/lesson.html#representation-learning",
    "title": "Generative AI: Intro",
    "section": "",
    "text": "We want to be able to describe things in terms of high-level features. For example, when describing appearance, we might talk about hair colour, length, eye colour, etc. Not RGB values pixel by pixel…\nThis is the idea behind representation learning. We describe each training data observation using some lower-dimensional latent space. Then we learn a maaping function from the latent space to the original domain.\nEncoder-decoder techniques try to transform a high-dimensional nonlinear manifold into a simpler latent space."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson1/lesson.html#core-probability-theory",
    "href": "posts/ml/generative_deep_learning/lesson1/lesson.html#core-probability-theory",
    "title": "Generative AI: Intro",
    "section": "",
    "text": "The complete set of all values that an observation \\(x\\) can take.\n\n\n\nA function \\(p(x)\\) that maps a point \\(x\\) in the sample space to a number between 0 and 1. The integral over the sample space must equal 1 for it to be a well-define probability distribtution.\nThere is one true data distribution \\(p_{data}\\) but infinitely many approximations \\(p_{model}\\) that we can find.\n\n\n\nWe can structure our approach to finding \\(p_{model}\\) by restricting ourselves to a family of density functions \\(p_{\\theta}(x)\\) which can be described with a finite set of parameters \\(\\theta\\).\nFor example, restricting our search to a linear model \\(y = w*x + b\\) where we need to find the parameters \\(w\\) and \\(b\\).\n\n\n\nThe likelihood, \\(L(\\theta|x)\\), of a parameter set \\(\\theta\\) is a function which measures the plausability of the parameters having observed some data point \\(x\\). It is the probability of seeing the data \\(x\\) if the true data-generating distribution was the model parameterized by \\(\\theta\\).\nIt is defined as the parametric model density: \\[\nL(\\theta|x) = p_{\\theta}(x)\n\\]\nIf we have a dataset of independent observations \\(X\\) then the probabilities multiply: \\[\nL(\\theta|X) = \\prod_{x \\in X} p_{\\theta}(x)\n\\]\nThe product of a lot of decimals can get unwieldy, so we often take the log-likelihood \\(l\\) to turn the product into a sum: \\[\nl(\\theta|X) = \\sum_{x \\in X} \\log{p_{\\theta}(x) }\n\\]\nThe focus of parametric modeling is therefore to find the optimal parameter set \\(\\hat{\\theta}\\), which leads to…\n\n\n\nA technique to estimate \\(\\hat{\\theta}\\), the set of parameters that is most likely to explain the observed data \\(X\\): \\[\n\\hat{\\theta} = \\arg\\max_x l(\\theta|X)\n\\]\nNeural networks often work with a loss function, so this is equivalent to minimising the negative log-likelihood: \\[\n\\hat{\\theta} = \\arg\\min_\\theta -l(\\theta|X) = \\arg\\min_\\theta -\\log{p_{\\theta}(X)}\n\\]\n\n\n\nGenerative modeling is a form of MLE where the parameters are the neural network weights. We want to find the weights that maximise the likelihood of observing the training data.\nBut for high-dimensional problems \\(p_{\\theta}(X)\\) is intractable, it cannot be calculated directly.\nThere are different approaches to maing the problem tractable. These are summarised in the following section and explored in more detail in subsequent chapters."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson1/lesson.html#generative-model-taxonomy",
    "href": "posts/ml/generative_deep_learning/lesson1/lesson.html#generative-model-taxonomy",
    "title": "Generative AI: Intro",
    "section": "",
    "text": "All types of generative model are seeking to solve the same problem, but they take different approaches to modeling the intractable density function \\(p_{\\theta}(x)\\).\nThere are three general approaches:\n\nExplicitly model the density function but constrain the model in some way.\nExplicitly model a tractable approximation of the density function.\nImplicitly model the density function, using a stochastic process that generates the data directly.\n\n\n\n\n\n\nflowchart LR\n\n\n  A(Generative models) --&gt; B1(Explicit density)\n  A(Generative models) --&gt; B2(Implicit density)\n\n  B1 --&gt; C1(Approximate density)\n  B1 --&gt; C2(Tractable density)\n\n  C1 --&gt; D1(VAE)\n  C1 --&gt; D2(Energy-based model)\n  C1 --&gt; D3(Diffusion model)\n\n  C2 --&gt; D4(Autoregressive model)\n  C2 --&gt; D5(Normalizing flow model)\n\n  B2 --&gt; D6(GAN)\n\n\n\n\n\n\n\nImplicit density models don’t even try to estimate the probability density, instead focusing on producing a stochastic data generation process directly.\nTractable models place constraints on the model architecture (i.e. the parameters \\(\\theta\\))."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson1/lesson.html#references",
    "href": "posts/ml/generative_deep_learning/lesson1/lesson.html#references",
    "title": "Generative AI: Intro",
    "section": "",
    "text": "Chapter 1 of Generative Deep Learning by David Foster."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson4/chapter4.html",
    "href": "posts/ml/generative_deep_learning/lesson4/chapter4.html",
    "title": "Generative AI: GANs",
    "section": "",
    "text": "Notes on Generative Adversarial Networks (GANs).\n\n\n\n\n\n\nStory Time\n\n\n\nImagine a forger trying to forge £20 notes and the popo trying to stop them.\nThe popo learn to spot the fakes. But then the forger learns to improve their forging skills to make better fakes.\nThis goes back and forth. With each iteration, the forger keeps getting better but then the popo learn to spot these more sophisticated fakes.\nThe results in a forger (generator) learning to create convincing fakes and the popo (discriminator) learning to spot fakes.\n\n\n\n\nThe idea of GANs is that we can train two competing models:\n\nThe generator tries to convert random noise into convincing observations.\nThe discriminator tries to predict whether an observation came from the original training dataset or is a “fake”.\n\nWe initialise both as random models; the generator outputs noise and the discriminator predicts randomly. We then alternate the training of the two networks so that the generator gets incrementally better at fooling the discriminator, then the discriminator gets incrementally better at spotting fakes.\n\n\n\n\n\nflowchart LR\n\n  A([Random noise]) --&gt; B[Generator] --&gt; C([Generated image]) \n\n  D([Image]) --&gt; E[Discriminator] --&gt; F([Prediction of realness probability])\n\n\n\n\n\n\n\n\n\nWe will implement a GAN to generate pictures of bricks.\n\n\nLoad image data of lego bricks. We will train a model that can generate novel lego brick images.\n\n\nThe original data is scaled from [0, 255].\nOften we will rescale this to [0, 1] so that we can use sigmoid activation functions.\nIn this case we will scale to [-1, 1] so that we can use tanh activation functions, which tend to give stronger gradients than sigmoid.\n\n\nCode\nfrom pathlib import Path\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras import (\n    layers,\n    models,\n    callbacks,\n    losses,\n    utils,\n    metrics,\n    optimizers,\n)\n\n\ndef sample_batch(dataset):\n    batch = dataset.take(1).get_single_element()\n    if isinstance(batch, tuple):\n        batch = batch[0]\n    return batch.numpy()\n\n\ndef display_images(\n    images, n=10, size=(20, 3), cmap=\"gray_r\", as_type=\"float32\", save_to=None\n):\n    \"\"\"Displays n random images from each one of the supplied arrays.\"\"\"\n    if images.max() &gt; 1.0:\n        images = images / 255.0\n    elif images.min() &lt; 0.0:\n        images = (images + 1.0) / 2.0\n\n    plt.figure(figsize=size)\n    for i in range(n):\n        _ = plt.subplot(1, n, i + 1)\n        plt.imshow(images[i].astype(as_type), cmap=cmap)\n        plt.axis(\"off\")\n\n    if save_to:\n        plt.savefig(save_to)\n        print(f\"\\nSaved to {save_to}\")\n\n    plt.show()\n\n\nModel and data parameters:\n\n\nCode\nDATA_DIR = Path(\"/Users/gurpreetjohl/workspace/datasets/lego-brick-images\")\n\nIMAGE_SIZE = 64\nCHANNELS = 1\nBATCH_SIZE = 128\nZ_DIM = 100\nEPOCHS = 100\nLOAD_MODEL = False\nADAM_BETA_1 = 0.5\nADAM_BETA_2 = 0.999\nLEARNING_RATE = 0.0002\nNOISE_PARAM = 0.1\n\n\nLoad and pre-process the training data:\n\n\nCode\ndef preprocess(img):\n    \"\"\"Normalize and reshape the images.\"\"\"\n    img = (tf.cast(img, \"float32\") - 127.5) / 127.5\n    return img\n\n\ntraining_data = utils.image_dataset_from_directory(\n    DATA_DIR / \"dataset\",\n    labels=None,\n    color_mode=\"grayscale\",\n    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    seed=42,\n    interpolation=\"bilinear\",\n)\ntrain = training_data.map(lambda x: preprocess(x))\n\n\nFound 40000 files belonging to 1 classes.\n\n\nSome sample input images:\n\n\nCode\ndisplay_images(sample_batch(train))\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe goal of the discriminator is to predict whether an image is real or fake.\nThis is a supervised binary classification problem, so we can use CNN architecture with a single output node. We stack Conv2D layers with BatchNormalization, LeakyReLU and Dropout layers sandwiched between.\n\n\nCode\ndiscriminator_input = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS))\n\nx = layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(discriminator_input)\nx = layers.LeakyReLU(0.2)(x)\nx = layers.Dropout(0.3)(x)\n\nx = layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\nx = layers.Dropout(0.3)(x)\n\nx = layers.Conv2D(256, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\nx = layers.Dropout(0.3)(x)\n\nx = layers.Conv2D(512, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\nx = layers.Dropout(0.3)(x)\n\nx = layers.Conv2D(1, kernel_size=4, strides=1, padding=\"valid\", use_bias=False, activation=\"sigmoid\")(x)\ndiscriminator_output = layers.Flatten()(x)  # The shape is already 1x1 so no need for a Dense layer after this\n\ndiscriminator = models.Model(discriminator_input, discriminator_output)\ndiscriminator.summary()\n\n\nModel: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 64, 64, 1)]       0         \n                                                                 \n conv2d_5 (Conv2D)           (None, 32, 32, 64)        1024      \n                                                                 \n leaky_re_lu_4 (LeakyReLU)   (None, 32, 32, 64)        0         \n                                                                 \n dropout_4 (Dropout)         (None, 32, 32, 64)        0         \n                                                                 \n conv2d_6 (Conv2D)           (None, 16, 16, 128)       131072    \n                                                                 \n batch_normalization_3 (Bat  (None, 16, 16, 128)       512       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_5 (LeakyReLU)   (None, 16, 16, 128)       0         \n                                                                 \n dropout_5 (Dropout)         (None, 16, 16, 128)       0         \n                                                                 \n conv2d_7 (Conv2D)           (None, 8, 8, 256)         524288    \n                                                                 \n batch_normalization_4 (Bat  (None, 8, 8, 256)         1024      \n chNormalization)                                                \n                                                                 \n leaky_re_lu_6 (LeakyReLU)   (None, 8, 8, 256)         0         \n                                                                 \n dropout_6 (Dropout)         (None, 8, 8, 256)         0         \n                                                                 \n conv2d_8 (Conv2D)           (None, 4, 4, 512)         2097152   \n                                                                 \n batch_normalization_5 (Bat  (None, 4, 4, 512)         2048      \n chNormalization)                                                \n                                                                 \n leaky_re_lu_7 (LeakyReLU)   (None, 4, 4, 512)         0         \n                                                                 \n dropout_7 (Dropout)         (None, 4, 4, 512)         0         \n                                                                 \n conv2d_9 (Conv2D)           (None, 1, 1, 1)           8192      \n                                                                 \n flatten_1 (Flatten)         (None, 1)                 0         \n                                                                 \n=================================================================\nTotal params: 2765312 (10.55 MB)\nTrainable params: 2763520 (10.54 MB)\nNon-trainable params: 1792 (7.00 KB)\n_________________________________________________________________\n\n\n\n\n\nThe purpose of the generator is to turn random noise into convincing images.\nThe input is a vector sampled from a multivariate Normal distribution, and the output is an image of the same size as the training data.\nThe discriminator-generator relationship in a GAN is similar to that of the encoder-decoder relations in a VAE.\nThe architecture of the discriminator is similar to the discriminator but in reverse (like a decoder). We pass stack Conv2DTranspose layers with BatchNormalization and LeakyReLU layers sandwiched in between.\n\n\nWe use Conv2DTranspose layers to scale the image size up.\nAn alternative would be to use stacks of Upsampling2D and Conv2D layers, i.e. the following serves the same purpose as a Conv2DTranspose layer:\nx = layers.Upsampling2D(size=2)(x)\nx = layers.Conv2D(256, kernel_size=4, strides=1, padding=\"same\")(x)\nThe Upsampling2D layer simply repeats each row and column to double its size, then Conv2D applies a convolution.\nThe idea is similar with Conv2DTranspose, but the extra rows and columns are filled with zeros rather than repeated existing values.\nConv2DTranspose layers can result in checkerboard pattern artifacts. Both options are used in practice, so it is often helpful to experiment and see which gives better results.\n\n\nCode\ngenerator_input = layers.Input(shape=(Z_DIM,))\n\nx = layers.Reshape((1, 1, Z_DIM))(generator_input)  # Reshape the input vector so we can apply conv transpose operations to it\n\nx = layers.Conv2DTranspose(512, kernel_size=4, strides=1, padding=\"valid\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\n\nx = layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\n\nx = layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\n\nx = layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\n\ngenerator_output = layers.Conv2DTranspose(\n    CHANNELS,\n    kernel_size=4,\n    strides=2,\n    padding=\"same\",\n    use_bias=False,\n    activation=\"tanh\",\n)(x)\n\ngenerator = models.Model(generator_input, generator_output)\ngenerator.summary()\n\n\nModel: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_3 (InputLayer)        [(None, 100)]             0         \n                                                                 \n reshape (Reshape)           (None, 1, 1, 100)         0         \n                                                                 \n conv2d_transpose (Conv2DTr  (None, 4, 4, 512)         819200    \n anspose)                                                        \n                                                                 \n batch_normalization_6 (Bat  (None, 4, 4, 512)         2048      \n chNormalization)                                                \n                                                                 \n leaky_re_lu_8 (LeakyReLU)   (None, 4, 4, 512)         0         \n                                                                 \n conv2d_transpose_1 (Conv2D  (None, 8, 8, 256)         2097152   \n Transpose)                                                      \n                                                                 \n batch_normalization_7 (Bat  (None, 8, 8, 256)         1024      \n chNormalization)                                                \n                                                                 \n leaky_re_lu_9 (LeakyReLU)   (None, 8, 8, 256)         0         \n                                                                 \n conv2d_transpose_2 (Conv2D  (None, 16, 16, 128)       524288    \n Transpose)                                                      \n                                                                 \n batch_normalization_8 (Bat  (None, 16, 16, 128)       512       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_10 (LeakyReLU)  (None, 16, 16, 128)       0         \n                                                                 \n conv2d_transpose_3 (Conv2D  (None, 32, 32, 64)        131072    \n Transpose)                                                      \n                                                                 \n batch_normalization_9 (Bat  (None, 32, 32, 64)        256       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_11 (LeakyReLU)  (None, 32, 32, 64)        0         \n                                                                 \n conv2d_transpose_4 (Conv2D  (None, 64, 64, 1)         1024      \n Transpose)                                                      \n                                                                 \n=================================================================\nTotal params: 3576576 (13.64 MB)\nTrainable params: 3574656 (13.64 MB)\nNon-trainable params: 1920 (7.50 KB)\n_________________________________________________________________\n\n\n\n\n\n\nWe alternate between training the discriminator and generator. They are not trained simultaneously. We want the generated images to be predicted close to 1 because the generator is good, not because the discriminator is weak.\nFor the discriminator, we create a training set where some images are real images from the training data and some are outputs from the generator. This is then a supervised binary classification problem.\nFor the generator, we want a way of scoring each generated image on its realness so that we can optimise this. The discriminator provides exactly this. We pass the generated images through the discriminator to get probabilities. The generator wants to fool the discriminator, so ideally this would be a vector of 1s. So the loss function is the binary crossentropy between these probabilities and a vector of 1s.\n\n\nCode\nclass DCGAN(models.Model):\n    def __init__(self, discriminator, generator, latent_dim):\n        super(DCGAN, self).__init__()\n        self.discriminator = discriminator\n        self.generator = generator\n        self.latent_dim = latent_dim\n\n    def compile(self, d_optimizer, g_optimizer):\n        super(DCGAN, self).compile()\n        self.loss_fn = losses.BinaryCrossentropy()\n        self.d_optimizer = d_optimizer\n        self.g_optimizer = g_optimizer\n        self.d_loss_metric = metrics.Mean(name=\"d_loss\")\n        self.d_real_acc_metric = metrics.BinaryAccuracy(name=\"d_real_acc\")\n        self.d_fake_acc_metric = metrics.BinaryAccuracy(name=\"d_fake_acc\")\n        self.d_acc_metric = metrics.BinaryAccuracy(name=\"d_acc\")\n        self.g_loss_metric = metrics.Mean(name=\"g_loss\")\n        self.g_acc_metric = metrics.BinaryAccuracy(name=\"g_acc\")\n\n    @property\n    def metrics(self):\n        return [\n            self.d_loss_metric,\n            self.d_real_acc_metric,\n            self.d_fake_acc_metric,\n            self.d_acc_metric,\n            self.g_loss_metric,\n            self.g_acc_metric,\n        ]\n\n    def train_step(self, real_images):\n        # Sample random points in the latent space\n        batch_size = tf.shape(real_images)[0]\n        random_latent_vectors = tf.random.normal(\n            shape=(batch_size, self.latent_dim)\n        )\n\n        # Train the discriminator on fake images\n        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n            generated_images = self.generator(\n                random_latent_vectors, training=True\n            )\n\n            # Evaluate the discriminator on the real and fake images\n            real_predictions = self.discriminator(real_images, training=True)\n            fake_predictions = self.discriminator(generated_images, training=True)\n\n            real_labels = tf.ones_like(real_predictions)\n            real_noisy_labels = real_labels + NOISE_PARAM * tf.random.uniform(\n                tf.shape(real_predictions)\n            )\n            fake_labels = tf.zeros_like(fake_predictions)\n            fake_noisy_labels = fake_labels - NOISE_PARAM * tf.random.uniform(\n                tf.shape(fake_predictions)\n            )\n\n            # Calculate the losses\n            d_real_loss = self.loss_fn(real_noisy_labels, real_predictions)\n            d_fake_loss = self.loss_fn(fake_noisy_labels, fake_predictions)\n            d_loss = (d_real_loss + d_fake_loss) / 2.0\n\n            g_loss = self.loss_fn(real_labels, fake_predictions)\n\n        # Update gradients\n        gradients_of_discriminator = disc_tape.gradient(\n            d_loss, self.discriminator.trainable_variables\n        )\n        gradients_of_generator = gen_tape.gradient(\n            g_loss, self.generator.trainable_variables\n        )\n\n        self.d_optimizer.apply_gradients(\n            zip(gradients_of_discriminator, discriminator.trainable_variables)\n        )\n        self.g_optimizer.apply_gradients(\n            zip(gradients_of_generator, generator.trainable_variables)\n        )\n\n        # Update metrics\n        self.d_loss_metric.update_state(d_loss)\n        self.d_real_acc_metric.update_state(real_labels, real_predictions)\n        self.d_fake_acc_metric.update_state(fake_labels, fake_predictions)\n        self.d_acc_metric.update_state(\n            [real_labels, fake_labels], [real_predictions, fake_predictions]\n        )\n        self.g_loss_metric.update_state(g_loss)\n        self.g_acc_metric.update_state(real_labels, fake_predictions)\n\n        return {m.name: m.result() for m in self.metrics}\n\n\n\n\nCode\n# Create a DCGAN\ndcgan = DCGAN(\n    discriminator=discriminator, generator=generator, latent_dim=Z_DIM\n)\n\n\n\n\nCode\ndcgan.compile(\n    d_optimizer=optimizers.legacy.Adam(\n        learning_rate=LEARNING_RATE, beta_1=ADAM_BETA_1, beta_2=ADAM_BETA_2\n    ),\n    g_optimizer=optimizers.legacy.Adam(\n        learning_rate=LEARNING_RATE, beta_1=ADAM_BETA_1, beta_2=ADAM_BETA_2\n    ),\n)\ndcgan.fit(train,  epochs=EPOCHS)\n\n\nEpoch 1/100\n313/313 [==============================] - 459s 1s/step - d_loss: 0.0252 - d_real_acc: 0.9011 - d_fake_acc: 0.9013 - d_acc: 0.9012 - g_loss: 5.3464 - g_acc: 0.0987\nEpoch 2/100\n313/313 [==============================] - 459s 1s/step - d_loss: 0.0454 - d_real_acc: 0.8986 - d_fake_acc: 0.8997 - d_acc: 0.8992 - g_loss: 5.2642 - g_acc: 0.1002\nEpoch 3/100\n313/313 [==============================] - 458s 1s/step - d_loss: 0.0556 - d_real_acc: 0.8958 - d_fake_acc: 0.8975 - d_acc: 0.8967 - g_loss: 4.9679 - g_acc: 0.1025\nEpoch 4/100\n313/313 [==============================] - 467s 1s/step - d_loss: 0.0246 - d_real_acc: 0.9065 - d_fake_acc: 0.9091 - d_acc: 0.9078 - g_loss: 5.1611 - g_acc: 0.0909\nEpoch 5/100\n313/313 [==============================] - 470s 1s/step - d_loss: 0.0178 - d_real_acc: 0.9067 - d_fake_acc: 0.9088 - d_acc: 0.9078 - g_loss: 5.1731 - g_acc: 0.0912\nEpoch 6/100\n313/313 [==============================] - 463s 1s/step - d_loss: 0.0314 - d_real_acc: 0.9116 - d_fake_acc: 0.9105 - d_acc: 0.9110 - g_loss: 5.2774 - g_acc: 0.0895\nEpoch 7/100\n313/313 [==============================] - 559s 2s/step - d_loss: 0.0229 - d_real_acc: 0.9085 - d_fake_acc: 0.9079 - d_acc: 0.9082 - g_loss: 5.3445 - g_acc: 0.0921\nEpoch 8/100\n313/313 [==============================] - 467s 1s/step - d_loss: -0.0155 - d_real_acc: 0.9161 - d_fake_acc: 0.9161 - d_acc: 0.9161 - g_loss: 5.7091 - g_acc: 0.0839\nEpoch 9/100\n313/313 [==============================] - 438s 1s/step - d_loss: -0.0077 - d_real_acc: 0.9220 - d_fake_acc: 0.9224 - d_acc: 0.9222 - g_loss: 5.8731 - g_acc: 0.0776\nEpoch 10/100\n313/313 [==============================] - 468s 1s/step - d_loss: -0.0472 - d_real_acc: 0.9228 - d_fake_acc: 0.9241 - d_acc: 0.9234 - g_loss: 5.9693 - g_acc: 0.0759\nEpoch 11/100\n313/313 [==============================] - 430s 1s/step - d_loss: -0.0839 - d_real_acc: 0.9404 - d_fake_acc: 0.9424 - d_acc: 0.9414 - g_loss: 6.1212 - g_acc: 0.0576\nEpoch 12/100\n313/313 [==============================] - 457s 1s/step - d_loss: 0.0431 - d_real_acc: 0.9053 - d_fake_acc: 0.9046 - d_acc: 0.9049 - g_loss: 6.0708 - g_acc: 0.0954\nEpoch 13/100\n313/313 [==============================] - 448s 1s/step - d_loss: -0.0154 - d_real_acc: 0.9236 - d_fake_acc: 0.9244 - d_acc: 0.9240 - g_loss: 6.3106 - g_acc: 0.0756\nEpoch 14/100\n313/313 [==============================] - 432s 1s/step - d_loss: -0.0720 - d_real_acc: 0.9320 - d_fake_acc: 0.9342 - d_acc: 0.9331 - g_loss: 6.6509 - g_acc: 0.0658\nEpoch 15/100\n313/313 [==============================] - 443s 1s/step - d_loss: 0.0057 - d_real_acc: 0.9072 - d_fake_acc: 0.9097 - d_acc: 0.9085 - g_loss: 6.1399 - g_acc: 0.0903\nEpoch 16/100\n313/313 [==============================] - 427s 1s/step - d_loss: 0.0069 - d_real_acc: 0.9185 - d_fake_acc: 0.9167 - d_acc: 0.9176 - g_loss: 6.3255 - g_acc: 0.0833\nEpoch 17/100\n313/313 [==============================] - 439s 1s/step - d_loss: -0.0709 - d_real_acc: 0.9220 - d_fake_acc: 0.9239 - d_acc: 0.9230 - g_loss: 6.8108 - g_acc: 0.0761\nEpoch 18/100\n313/313 [==============================] - 437s 1s/step - d_loss: 0.0373 - d_real_acc: 0.9288 - d_fake_acc: 0.9512 - d_acc: 0.9400 - g_loss: 8.1066 - g_acc: 0.0488\nEpoch 19/100\n313/313 [==============================] - 1029s 3s/step - d_loss: -0.1154 - d_real_acc: 0.9408 - d_fake_acc: 0.9420 - d_acc: 0.9414 - g_loss: 7.6274 - g_acc: 0.0580\nEpoch 20/100\n313/313 [==============================] - 5781s 19s/step - d_loss: -0.0431 - d_real_acc: 0.9222 - d_fake_acc: 0.9231 - d_acc: 0.9227 - g_loss: 7.1953 - g_acc: 0.0769\nEpoch 21/100\n313/313 [==============================] - 2696s 9s/step - d_loss: -0.0542 - d_real_acc: 0.9176 - d_fake_acc: 0.9205 - d_acc: 0.9191 - g_loss: 7.1675 - g_acc: 0.0794\nEpoch 22/100\n313/313 [==============================] - 1481s 5s/step - d_loss: -0.1424 - d_real_acc: 0.9398 - d_fake_acc: 0.9400 - d_acc: 0.9399 - g_loss: 7.7399 - g_acc: 0.0600\nEpoch 23/100\n313/313 [==============================] - 439s 1s/step - d_loss: -0.0263 - d_real_acc: 0.9154 - d_fake_acc: 0.9148 - d_acc: 0.9151 - g_loss: 7.3322 - g_acc: 0.0852\nEpoch 24/100\n313/313 [==============================] - 443s 1s/step - d_loss: -0.1420 - d_real_acc: 0.9406 - d_fake_acc: 0.9430 - d_acc: 0.9418 - g_loss: 8.1502 - g_acc: 0.0570\nEpoch 25/100\n313/313 [==============================] - 475s 2s/step - d_loss: -0.1650 - d_real_acc: 0.9393 - d_fake_acc: 0.9395 - d_acc: 0.9394 - g_loss: 7.9602 - g_acc: 0.0605\nEpoch 26/100\n313/313 [==============================] - 447s 1s/step - d_loss: -0.1247 - d_real_acc: 0.9419 - d_fake_acc: 0.9437 - d_acc: 0.9428 - g_loss: 7.7939 - g_acc: 0.0563\nEpoch 27/100\n313/313 [==============================] - 1356s 4s/step - d_loss: -0.0182 - d_real_acc: 0.9160 - d_fake_acc: 0.9212 - d_acc: 0.9186 - g_loss: 6.9180 - g_acc: 0.0787\nEpoch 28/100\n313/313 [==============================] - 2219s 7s/step - d_loss: -0.2227 - d_real_acc: 0.9511 - d_fake_acc: 0.9519 - d_acc: 0.9515 - g_loss: 8.1970 - g_acc: 0.0481\nEpoch 29/100\n313/313 [==============================] - 5807s 19s/step - d_loss: -0.1091 - d_real_acc: 0.9318 - d_fake_acc: 0.9320 - d_acc: 0.9319 - g_loss: 7.5829 - g_acc: 0.0680\nEpoch 30/100\n313/313 [==============================] - 2511s 8s/step - d_loss: -0.3131 - d_real_acc: 0.9571 - d_fake_acc: 0.9604 - d_acc: 0.9588 - g_loss: 9.8839 - g_acc: 0.0395\nEpoch 31/100\n313/313 [==============================] - 2768s 9s/step - d_loss: -0.0996 - d_real_acc: 0.9269 - d_fake_acc: 0.9286 - d_acc: 0.9277 - g_loss: 8.3337 - g_acc: 0.0714\nEpoch 32/100\n313/313 [==============================] - 3046s 10s/step - d_loss: -0.1619 - d_real_acc: 0.9423 - d_fake_acc: 0.9482 - d_acc: 0.9453 - g_loss: 8.2435 - g_acc: 0.0518\nEpoch 33/100\n313/313 [==============================] - 3478s 11s/step - d_loss: -0.1182 - d_real_acc: 0.9284 - d_fake_acc: 0.9304 - d_acc: 0.9294 - g_loss: 8.1681 - g_acc: 0.0696\nEpoch 34/100\n313/313 [==============================] - 2776s 9s/step - d_loss: -0.2214 - d_real_acc: 0.9459 - d_fake_acc: 0.9582 - d_acc: 0.9520 - g_loss: 9.9168 - g_acc: 0.0417\nEpoch 35/100\n313/313 [==============================] - 2724s 9s/step - d_loss: -0.3101 - d_real_acc: 0.9421 - d_fake_acc: 0.9293 - d_acc: 0.9357 - g_loss: 13.2857 - g_acc: 0.0707\nEpoch 36/100\n313/313 [==============================] - 2648s 8s/step - d_loss: -0.0441 - d_real_acc: 0.8963 - d_fake_acc: 0.8961 - d_acc: 0.8962 - g_loss: 7.5664 - g_acc: 0.1038\nEpoch 37/100\n313/313 [==============================] - 3262s 10s/step - d_loss: -0.0859 - d_real_acc: 0.9314 - d_fake_acc: 0.9402 - d_acc: 0.9358 - g_loss: 8.3591 - g_acc: 0.0598\nEpoch 38/100\n313/313 [==============================] - 2612s 8s/step - d_loss: -0.2979 - d_real_acc: 0.9554 - d_fake_acc: 0.9577 - d_acc: 0.9566 - g_loss: 9.2534 - g_acc: 0.0423\nEpoch 39/100\n313/313 [==============================] - 2235s 7s/step - d_loss: -0.3387 - d_real_acc: 0.9607 - d_fake_acc: 0.9622 - d_acc: 0.9615 - g_loss: 9.9397 - g_acc: 0.0378\nEpoch 40/100\n313/313 [==============================] - 3453s 11s/step - d_loss: -0.1056 - d_real_acc: 0.9279 - d_fake_acc: 0.9310 - d_acc: 0.9294 - g_loss: 8.9394 - g_acc: 0.0690\nEpoch 41/100\n313/313 [==============================] - 2316s 7s/step - d_loss: -0.2147 - d_real_acc: 0.9318 - d_fake_acc: 0.9327 - d_acc: 0.9323 - g_loss: 9.0337 - g_acc: 0.0673\nEpoch 42/100\n313/313 [==============================] - 3134s 10s/step - d_loss: -0.2554 - d_real_acc: 0.9511 - d_fake_acc: 0.9540 - d_acc: 0.9526 - g_loss: 9.5571 - g_acc: 0.0460\nEpoch 43/100\n313/313 [==============================] - 3933s 13s/step - d_loss: -0.2871 - d_real_acc: 0.9490 - d_fake_acc: 0.9526 - d_acc: 0.9508 - g_loss: 10.3316 - g_acc: 0.0474\nEpoch 44/100\n313/313 [==============================] - 3248s 10s/step - d_loss: -0.3456 - d_real_acc: 0.9635 - d_fake_acc: 0.9635 - d_acc: 0.9635 - g_loss: 9.8675 - g_acc: 0.0364\nEpoch 45/100\n313/313 [==============================] - 3043s 10s/step - d_loss: -0.3274 - d_real_acc: 0.9603 - d_fake_acc: 0.9618 - d_acc: 0.9610 - g_loss: 10.4185 - g_acc: 0.0382\nEpoch 46/100\n313/313 [==============================] - 2706s 9s/step - d_loss: -0.6160 - d_real_acc: 0.9902 - d_fake_acc: 0.9908 - d_acc: 0.9905 - g_loss: 13.0574 - g_acc: 0.0092\nEpoch 47/100\n313/313 [==============================] - 2453s 8s/step - d_loss: 0.3413 - d_real_acc: 0.8073 - d_fake_acc: 0.8054 - d_acc: 0.8064 - g_loss: 6.5391 - g_acc: 0.1946\nEpoch 48/100\n313/313 [==============================] - 2898s 9s/step - d_loss: -0.4416 - d_real_acc: 0.9764 - d_fake_acc: 0.9784 - d_acc: 0.9774 - g_loss: 10.8318 - g_acc: 0.0216\nEpoch 49/100\n313/313 [==============================] - 3358s 11s/step - d_loss: 6.8776 - d_real_acc: 0.1058 - d_fake_acc: 0.9910 - d_acc: 0.5484 - g_loss: 14.8921 - g_acc: 0.0090\nEpoch 50/100\n313/313 [==============================] - 2940s 9s/step - d_loss: 7.7113 - d_real_acc: 0.0000e+00 - d_fake_acc: 1.0000 - d_acc: 0.5000 - g_loss: 15.4250 - g_acc: 0.0000e+00\nEpoch 51/100\n313/313 [==============================] - 2983s 10s/step - d_loss: 7.7121 - d_real_acc: 0.0000e+00 - d_fake_acc: 1.0000 - d_acc: 0.5000 - g_loss: 15.4250 - g_acc: 0.0000e+00\nEpoch 52/100\n313/313 [==============================] - 3458s 11s/step - d_loss: 7.7149 - d_real_acc: 0.0000e+00 - d_fake_acc: 1.0000 - d_acc: 0.5000 - g_loss: 15.4250 - g_acc: 0.0000e+00\nEpoch 53/100\n313/313 [==============================] - 2404s 8s/step - d_loss: 4.4950 - d_real_acc: 0.3543 - d_fake_acc: 0.8865 - d_acc: 0.6204 - g_loss: 10.6772 - g_acc: 0.1135\nEpoch 54/100\n313/313 [==============================] - 3297s 11s/step - d_loss: -0.0132 - d_real_acc: 0.9068 - d_fake_acc: 0.9010 - d_acc: 0.9039 - g_loss: 7.9660 - g_acc: 0.0990\nEpoch 55/100\n313/313 [==============================] - 2486s 8s/step - d_loss: -0.3508 - d_real_acc: 0.9615 - d_fake_acc: 0.9612 - d_acc: 0.9614 - g_loss: 10.2242 - g_acc: 0.0388\nEpoch 56/100\n313/313 [==============================] - 2995s 10s/step - d_loss: -0.3125 - d_real_acc: 0.9525 - d_fake_acc: 0.9533 - d_acc: 0.9529 - g_loss: 10.4182 - g_acc: 0.0467\nEpoch 57/100\n313/313 [==============================] - 1791s 6s/step - d_loss: -0.3201 - d_real_acc: 0.9532 - d_fake_acc: 0.9560 - d_acc: 0.9546 - g_loss: 10.4752 - g_acc: 0.0441\nEpoch 58/100\n313/313 [==============================] - 2792s 9s/step - d_loss: -0.2649 - d_real_acc: 0.9509 - d_fake_acc: 0.9532 - d_acc: 0.9520 - g_loss: 9.3587 - g_acc: 0.0468\nEpoch 59/100\n313/313 [==============================] - 3665s 12s/step - d_loss: -0.1747 - d_real_acc: 0.9413 - d_fake_acc: 0.9584 - d_acc: 0.9499 - g_loss: 10.1369 - g_acc: 0.0416\nEpoch 60/100\n313/313 [==============================] - 2493s 8s/step - d_loss: -0.2692 - d_real_acc: 0.9499 - d_fake_acc: 0.9534 - d_acc: 0.9517 - g_loss: 9.7124 - g_acc: 0.0466\nEpoch 61/100\n313/313 [==============================] - 2293s 7s/step - d_loss: -0.2869 - d_real_acc: 0.9520 - d_fake_acc: 0.9556 - d_acc: 0.9538 - g_loss: 10.2684 - g_acc: 0.0444\nEpoch 62/100\n313/313 [==============================] - 2865s 9s/step - d_loss: -0.6188 - d_real_acc: 0.9900 - d_fake_acc: 0.9901 - d_acc: 0.9901 - g_loss: 12.9584 - g_acc: 0.0099\nEpoch 63/100\n313/313 [==============================] - 2301s 7s/step - d_loss: -0.7197 - d_real_acc: 0.9984 - d_fake_acc: 0.9985 - d_acc: 0.9985 - g_loss: 14.5762 - g_acc: 0.0015\nEpoch 64/100\n313/313 [==============================] - 2404s 8s/step - d_loss: -0.4320 - d_real_acc: 0.9665 - d_fake_acc: 0.9702 - d_acc: 0.9683 - g_loss: 12.0177 - g_acc: 0.0298\nEpoch 65/100\n313/313 [==============================] - 4723s 15s/step - d_loss: 6.7591 - d_real_acc: 0.9940 - d_fake_acc: 0.1077 - d_acc: 0.5509 - g_loss: 1.4402 - g_acc: 0.8923\nEpoch 66/100\n313/313 [==============================] - 2726s 9s/step - d_loss: 7.6259 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 67/100\n313/313 [==============================] - 3325s 11s/step - d_loss: 7.6250 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 68/100\n313/313 [==============================] - 2513s 8s/step - d_loss: 7.6251 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 7.4387e-12 - g_acc: 1.0000\nEpoch 69/100\n313/313 [==============================] - 3304s 11s/step - d_loss: 3.4076 - d_real_acc: 0.8552 - d_fake_acc: 0.5472 - d_acc: 0.7012 - g_loss: 5.2072 - g_acc: 0.4528\nEpoch 70/100\n313/313 [==============================] - 2276s 7s/step - d_loss: -0.2424 - d_real_acc: 0.9448 - d_fake_acc: 0.9581 - d_acc: 0.9514 - g_loss: 10.5206 - g_acc: 0.0419\nEpoch 71/100\n313/313 [==============================] - 3315s 11s/step - d_loss: -0.3093 - d_real_acc: 0.9563 - d_fake_acc: 0.9640 - d_acc: 0.9602 - g_loss: 10.6076 - g_acc: 0.0360\nEpoch 72/100\n313/313 [==============================] - 2306s 7s/step - d_loss: -0.2440 - d_real_acc: 0.9466 - d_fake_acc: 0.9581 - d_acc: 0.9523 - g_loss: 10.1996 - g_acc: 0.0419\nEpoch 73/100\n313/313 [==============================] - 3218s 10s/step - d_loss: -0.7206 - d_real_acc: 0.9985 - d_fake_acc: 0.9983 - d_acc: 0.9984 - g_loss: 14.6350 - g_acc: 0.0017\nEpoch 74/100\n313/313 [==============================] - 3258s 10s/step - d_loss: -0.6281 - d_real_acc: 0.9828 - d_fake_acc: 0.9841 - d_acc: 0.9834 - g_loss: 14.5219 - g_acc: 0.0159\nEpoch 75/100\n313/313 [==============================] - 2874s 9s/step - d_loss: -0.0555 - d_real_acc: 0.9254 - d_fake_acc: 0.9371 - d_acc: 0.9312 - g_loss: 9.3443 - g_acc: 0.0629\nEpoch 76/100\n313/313 [==============================] - 2559s 8s/step - d_loss: -0.2825 - d_real_acc: 0.9515 - d_fake_acc: 0.9611 - d_acc: 0.9563 - g_loss: 10.7583 - g_acc: 0.0388\nEpoch 77/100\n313/313 [==============================] - 3663s 12s/step - d_loss: -0.3945 - d_real_acc: 0.9667 - d_fake_acc: 0.9691 - d_acc: 0.9679 - g_loss: 10.8566 - g_acc: 0.0309\nEpoch 78/100\n313/313 [==============================] - 2314s 7s/step - d_loss: -0.3953 - d_real_acc: 0.9508 - d_fake_acc: 0.9529 - d_acc: 0.9519 - g_loss: 12.0037 - g_acc: 0.0471\nEpoch 79/100\n313/313 [==============================] - 2816s 9s/step - d_loss: -0.6059 - d_real_acc: 0.9841 - d_fake_acc: 0.9838 - d_acc: 0.9840 - g_loss: 13.3516 - g_acc: 0.0162\nEpoch 80/100\n313/313 [==============================] - 3232s 10s/step - d_loss: -0.3555 - d_real_acc: 0.9587 - d_fake_acc: 0.9649 - d_acc: 0.9618 - g_loss: 11.2453 - g_acc: 0.0351\nEpoch 81/100\n313/313 [==============================] - 3705s 12s/step - d_loss: -0.4501 - d_real_acc: 0.9731 - d_fake_acc: 0.9743 - d_acc: 0.9737 - g_loss: 11.4553 - g_acc: 0.0258\nEpoch 82/100\n313/313 [==============================] - 2364s 8s/step - d_loss: -0.3827 - d_real_acc: 0.9588 - d_fake_acc: 0.9639 - d_acc: 0.9613 - g_loss: 11.6275 - g_acc: 0.0361\nEpoch 83/100\n313/313 [==============================] - 1122s 4s/step - d_loss: -0.4355 - d_real_acc: 0.9642 - d_fake_acc: 0.9674 - d_acc: 0.9658 - g_loss: 12.1025 - g_acc: 0.0326\nEpoch 84/100\n313/313 [==============================] - 4065s 13s/step - d_loss: -0.4456 - d_real_acc: 0.9695 - d_fake_acc: 0.9714 - d_acc: 0.9704 - g_loss: 11.6065 - g_acc: 0.0287\nEpoch 85/100\n313/313 [==============================] - 4461s 14s/step - d_loss: -0.6405 - d_real_acc: 0.9901 - d_fake_acc: 0.9899 - d_acc: 0.9900 - g_loss: 13.4694 - g_acc: 0.0101\nEpoch 86/100\n313/313 [==============================] - 2630s 8s/step - d_loss: -0.6431 - d_real_acc: 0.9857 - d_fake_acc: 0.9856 - d_acc: 0.9857 - g_loss: 14.3623 - g_acc: 0.0144\nEpoch 87/100\n313/313 [==============================] - 2567s 8s/step - d_loss: -0.3870 - d_real_acc: 0.9534 - d_fake_acc: 0.9578 - d_acc: 0.9556 - g_loss: 12.0201 - g_acc: 0.0422\nEpoch 88/100\n313/313 [==============================] - 2597s 8s/step - d_loss: -0.7624 - d_real_acc: 0.9999 - d_fake_acc: 0.9998 - d_acc: 0.9998 - g_loss: 15.3547 - g_acc: 2.5000e-04\nEpoch 89/100\n313/313 [==============================] - 1477s 5s/step - d_loss: -0.5787 - d_real_acc: 0.9764 - d_fake_acc: 0.9759 - d_acc: 0.9762 - g_loss: 13.8500 - g_acc: 0.0241\nEpoch 90/100\n313/313 [==============================] - 522s 2s/step - d_loss: -0.6747 - d_real_acc: 0.9885 - d_fake_acc: 0.9897 - d_acc: 0.9891 - g_loss: 14.5329 - g_acc: 0.0104\nEpoch 91/100\n313/313 [==============================] - 512s 2s/step - d_loss: 6.4703 - d_real_acc: 0.9892 - d_fake_acc: 0.1438 - d_acc: 0.5665 - g_loss: 2.1184 - g_acc: 0.8562\nEpoch 92/100\n313/313 [==============================] - 514s 2s/step - d_loss: 7.6245 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 93/100\n313/313 [==============================] - 533s 2s/step - d_loss: 7.6249 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 94/100\n313/313 [==============================] - 499s 2s/step - d_loss: 7.6236 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 95/100\n313/313 [==============================] - 483s 2s/step - d_loss: 7.6240 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 96/100\n313/313 [==============================] - 481s 2s/step - d_loss: 7.6248 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 97/100\n313/313 [==============================] - 488s 2s/step - d_loss: 7.6247 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 98/100\n313/313 [==============================] - 481s 2s/step - d_loss: 7.6263 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 99/100\n313/313 [==============================] - 459s 1s/step - d_loss: 7.6235 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 100/100\n313/313 [==============================] - 4669s 15s/step - d_loss: 7.6231 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\n\n\n&lt;keras.src.callbacks.History at 0x10f686690&gt;\n\n\n\n\nCode\n# Save the final models\ngenerator.save(\"./models/generator\")\ndiscriminator.save(\"./models/discriminator\")\n\n\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nINFO:tensorflow:Assets written to: ./models/generator/assets\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nINFO:tensorflow:Assets written to: ./models/discriminator/assets\n\n\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nINFO:tensorflow:Assets written to: ./models/generator/assets\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nINFO:tensorflow:Assets written to: ./models/discriminator/assets\n\n\n\n\n\n\nWe can see some examples of images produced by the GAN.\n(I don’t have a GPU so training is slow and I only trained 100 epochs… they’re a bit crap)\n\n\nCode\n# Sample some points in the latent space, from the standard normal distribution\ngrid_width, grid_height = (10, 3)\nz_sample = np.random.normal(size=(grid_width * grid_height, Z_DIM))\n\n# Decode the sampled points\nreconstructions = generator.predict(z_sample)\n\n# Draw a plot of decoded images\nfig = plt.figure(figsize=(18, 5))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\n# Output the grid of faces\nfor i in range(grid_width * grid_height):\n    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n    ax.axis(\"off\")\n    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")\n\n\n\n\n\n\n\n\n\nWe also want to make sure a generative model doesn’t simply recreate images that are already in the training set.\nAs a sanity check, we plot some generated images and the closest training images (using the L1 distance). This confirms that the generator is able to understand high-level features, even though we didn’t provide anything other than raw pixels, and it can generate examples distinct from those encountered before.\n\n\nCode\ndef compare_images(img1, img2):\n    return np.mean(np.abs(img1 - img2))\n\nall_data = []\nfor i in train.as_numpy_iterator():\n    all_data.extend(i)\nall_data = np.array(all_data)\n\n# Plot the images\nr, c = 3, 5\nfig, axs = plt.subplots(r, c, figsize=(10, 6))\nfig.suptitle(\"Generated images\", fontsize=20)\n\nnoise = np.random.normal(size=(r * c, Z_DIM))\ngen_imgs = generator.predict(noise)\n\ncnt = 0\nfor i in range(r):\n    for j in range(c):\n        axs[i, j].imshow(gen_imgs[cnt], cmap=\"gray_r\")\n        axs[i, j].axis(\"off\")\n        cnt += 1\n\nplt.show()\n\n\n1/1 [==============================] - 0s 80ms/step\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig, axs = plt.subplots(r, c, figsize=(10, 6))\nfig.suptitle(\"Closest images in the training set\", fontsize=20)\n\ncnt = 0\nfor i in range(r):\n    for j in range(c):\n        c_diff = 99999\n        c_img = None\n        for k_idx, k in enumerate(all_data):\n            diff = compare_images(gen_imgs[cnt], k)\n            if diff &lt; c_diff:\n                c_img = np.copy(k)\n                c_diff = diff\n        axs[i, j].imshow(c_img, cmap=\"gray_r\")\n        axs[i, j].axis(\"off\")\n        cnt += 1\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nGANs are notoriously difficult to train because there is a balancing act between the generator and discriminator; neither should grow so strong that it overpowers the other.\n\n\nThe discriminator can always spot the fakes, so the signal from the loss function becomes too weak to cause any meaningful improvements in the generator.\nIn the extreme case, the discriminator distinguishes fakes perfectly, so gradients vanish and no training takes place.\nWe need to weaken the discriminator in this case. Some possible options are:\n\nMore dropout\nLower learning rate\nSimplify the discriminator architecture - use fewer layers\nAdd noise to the labels when training the discriminator\nAdd intentional labelling errors - randomly flip the labels of some images when training the discriminator\n\n\n\n\nIf the discriminator is too weak, the generator will learn that it can trick the discriminator using a small sample of nearly identical images. This is known as mode collapse. The generator would map every point in the latent space to this image, so the gradients of the loss function would vanish and it would not be able to recover.\nStrengthening the discriminator would not help because the generator would just learn to find a different mode that fools the discriminator with no diversity; it is numb to its input.\nSome possible options are:\n\nStrengthen the discriminator - do the opposite of the previous section\nReduce the learning rate of both generator and discriminator\nIncrease the batch size\n\n\n\n\n\nThe value of the loss is not meaningful as a measure of the generator’s strength when training.\nThe loss function is relative to the discriminator, and since the discriminator is also being trained, the goalposts are constantly shifting. Also, we don’t want the loss function to reach 0 or else we may reach mode collapse as described above.\nThis makes GAN training difficult to monitor.\n\n\n\nThere are a lot of hyperparameters involved with GANs because we are now training two networks.\nThe performance is highly sensitive to these hyperparameter choices, and involves a lot of trial and error.\n\n\n\nThe Wasserstein GAN replaces the binary crossentropy loss function with the Wassserstein lss function in both the discriminator and generator.\nThis results in two desirable properties:\n\nA meaningful loss metric that correlates with generator convergence. This allows for better monitoring of training.\nMore stable optimisation process.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 4 of Generative Deep Learning by David Foster."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson4/chapter4.html#gans-1",
    "href": "posts/ml/generative_deep_learning/lesson4/chapter4.html#gans-1",
    "title": "Generative AI: GANs",
    "section": "",
    "text": "The idea of GANs is that we can train two competing models:\n\nThe generator tries to convert random noise into convincing observations.\nThe discriminator tries to predict whether an observation came from the original training dataset or is a “fake”.\n\nWe initialise both as random models; the generator outputs noise and the discriminator predicts randomly. We then alternate the training of the two networks so that the generator gets incrementally better at fooling the discriminator, then the discriminator gets incrementally better at spotting fakes.\n\n\n\n\n\nflowchart LR\n\n  A([Random noise]) --&gt; B[Generator] --&gt; C([Generated image]) \n\n  D([Image]) --&gt; E[Discriminator] --&gt; F([Prediction of realness probability])"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson4/chapter4.html#building-a-deep-convolutional-gan-dcgan",
    "href": "posts/ml/generative_deep_learning/lesson4/chapter4.html#building-a-deep-convolutional-gan-dcgan",
    "title": "Generative AI: GANs",
    "section": "",
    "text": "We will implement a GAN to generate pictures of bricks.\n\n\nLoad image data of lego bricks. We will train a model that can generate novel lego brick images.\n\n\nThe original data is scaled from [0, 255].\nOften we will rescale this to [0, 1] so that we can use sigmoid activation functions.\nIn this case we will scale to [-1, 1] so that we can use tanh activation functions, which tend to give stronger gradients than sigmoid.\n\n\nCode\nfrom pathlib import Path\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras import (\n    layers,\n    models,\n    callbacks,\n    losses,\n    utils,\n    metrics,\n    optimizers,\n)\n\n\ndef sample_batch(dataset):\n    batch = dataset.take(1).get_single_element()\n    if isinstance(batch, tuple):\n        batch = batch[0]\n    return batch.numpy()\n\n\ndef display_images(\n    images, n=10, size=(20, 3), cmap=\"gray_r\", as_type=\"float32\", save_to=None\n):\n    \"\"\"Displays n random images from each one of the supplied arrays.\"\"\"\n    if images.max() &gt; 1.0:\n        images = images / 255.0\n    elif images.min() &lt; 0.0:\n        images = (images + 1.0) / 2.0\n\n    plt.figure(figsize=size)\n    for i in range(n):\n        _ = plt.subplot(1, n, i + 1)\n        plt.imshow(images[i].astype(as_type), cmap=cmap)\n        plt.axis(\"off\")\n\n    if save_to:\n        plt.savefig(save_to)\n        print(f\"\\nSaved to {save_to}\")\n\n    plt.show()\n\n\nModel and data parameters:\n\n\nCode\nDATA_DIR = Path(\"/Users/gurpreetjohl/workspace/datasets/lego-brick-images\")\n\nIMAGE_SIZE = 64\nCHANNELS = 1\nBATCH_SIZE = 128\nZ_DIM = 100\nEPOCHS = 100\nLOAD_MODEL = False\nADAM_BETA_1 = 0.5\nADAM_BETA_2 = 0.999\nLEARNING_RATE = 0.0002\nNOISE_PARAM = 0.1\n\n\nLoad and pre-process the training data:\n\n\nCode\ndef preprocess(img):\n    \"\"\"Normalize and reshape the images.\"\"\"\n    img = (tf.cast(img, \"float32\") - 127.5) / 127.5\n    return img\n\n\ntraining_data = utils.image_dataset_from_directory(\n    DATA_DIR / \"dataset\",\n    labels=None,\n    color_mode=\"grayscale\",\n    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    seed=42,\n    interpolation=\"bilinear\",\n)\ntrain = training_data.map(lambda x: preprocess(x))\n\n\nFound 40000 files belonging to 1 classes.\n\n\nSome sample input images:\n\n\nCode\ndisplay_images(sample_batch(train))\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe goal of the discriminator is to predict whether an image is real or fake.\nThis is a supervised binary classification problem, so we can use CNN architecture with a single output node. We stack Conv2D layers with BatchNormalization, LeakyReLU and Dropout layers sandwiched between.\n\n\nCode\ndiscriminator_input = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS))\n\nx = layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(discriminator_input)\nx = layers.LeakyReLU(0.2)(x)\nx = layers.Dropout(0.3)(x)\n\nx = layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\nx = layers.Dropout(0.3)(x)\n\nx = layers.Conv2D(256, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\nx = layers.Dropout(0.3)(x)\n\nx = layers.Conv2D(512, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\nx = layers.Dropout(0.3)(x)\n\nx = layers.Conv2D(1, kernel_size=4, strides=1, padding=\"valid\", use_bias=False, activation=\"sigmoid\")(x)\ndiscriminator_output = layers.Flatten()(x)  # The shape is already 1x1 so no need for a Dense layer after this\n\ndiscriminator = models.Model(discriminator_input, discriminator_output)\ndiscriminator.summary()\n\n\nModel: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 64, 64, 1)]       0         \n                                                                 \n conv2d_5 (Conv2D)           (None, 32, 32, 64)        1024      \n                                                                 \n leaky_re_lu_4 (LeakyReLU)   (None, 32, 32, 64)        0         \n                                                                 \n dropout_4 (Dropout)         (None, 32, 32, 64)        0         \n                                                                 \n conv2d_6 (Conv2D)           (None, 16, 16, 128)       131072    \n                                                                 \n batch_normalization_3 (Bat  (None, 16, 16, 128)       512       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_5 (LeakyReLU)   (None, 16, 16, 128)       0         \n                                                                 \n dropout_5 (Dropout)         (None, 16, 16, 128)       0         \n                                                                 \n conv2d_7 (Conv2D)           (None, 8, 8, 256)         524288    \n                                                                 \n batch_normalization_4 (Bat  (None, 8, 8, 256)         1024      \n chNormalization)                                                \n                                                                 \n leaky_re_lu_6 (LeakyReLU)   (None, 8, 8, 256)         0         \n                                                                 \n dropout_6 (Dropout)         (None, 8, 8, 256)         0         \n                                                                 \n conv2d_8 (Conv2D)           (None, 4, 4, 512)         2097152   \n                                                                 \n batch_normalization_5 (Bat  (None, 4, 4, 512)         2048      \n chNormalization)                                                \n                                                                 \n leaky_re_lu_7 (LeakyReLU)   (None, 4, 4, 512)         0         \n                                                                 \n dropout_7 (Dropout)         (None, 4, 4, 512)         0         \n                                                                 \n conv2d_9 (Conv2D)           (None, 1, 1, 1)           8192      \n                                                                 \n flatten_1 (Flatten)         (None, 1)                 0         \n                                                                 \n=================================================================\nTotal params: 2765312 (10.55 MB)\nTrainable params: 2763520 (10.54 MB)\nNon-trainable params: 1792 (7.00 KB)\n_________________________________________________________________\n\n\n\n\n\nThe purpose of the generator is to turn random noise into convincing images.\nThe input is a vector sampled from a multivariate Normal distribution, and the output is an image of the same size as the training data.\nThe discriminator-generator relationship in a GAN is similar to that of the encoder-decoder relations in a VAE.\nThe architecture of the discriminator is similar to the discriminator but in reverse (like a decoder). We pass stack Conv2DTranspose layers with BatchNormalization and LeakyReLU layers sandwiched in between.\n\n\nWe use Conv2DTranspose layers to scale the image size up.\nAn alternative would be to use stacks of Upsampling2D and Conv2D layers, i.e. the following serves the same purpose as a Conv2DTranspose layer:\nx = layers.Upsampling2D(size=2)(x)\nx = layers.Conv2D(256, kernel_size=4, strides=1, padding=\"same\")(x)\nThe Upsampling2D layer simply repeats each row and column to double its size, then Conv2D applies a convolution.\nThe idea is similar with Conv2DTranspose, but the extra rows and columns are filled with zeros rather than repeated existing values.\nConv2DTranspose layers can result in checkerboard pattern artifacts. Both options are used in practice, so it is often helpful to experiment and see which gives better results.\n\n\nCode\ngenerator_input = layers.Input(shape=(Z_DIM,))\n\nx = layers.Reshape((1, 1, Z_DIM))(generator_input)  # Reshape the input vector so we can apply conv transpose operations to it\n\nx = layers.Conv2DTranspose(512, kernel_size=4, strides=1, padding=\"valid\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\n\nx = layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\n\nx = layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\n\nx = layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\nx = layers.BatchNormalization(momentum=0.9)(x)\nx = layers.LeakyReLU(0.2)(x)\n\ngenerator_output = layers.Conv2DTranspose(\n    CHANNELS,\n    kernel_size=4,\n    strides=2,\n    padding=\"same\",\n    use_bias=False,\n    activation=\"tanh\",\n)(x)\n\ngenerator = models.Model(generator_input, generator_output)\ngenerator.summary()\n\n\nModel: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_3 (InputLayer)        [(None, 100)]             0         \n                                                                 \n reshape (Reshape)           (None, 1, 1, 100)         0         \n                                                                 \n conv2d_transpose (Conv2DTr  (None, 4, 4, 512)         819200    \n anspose)                                                        \n                                                                 \n batch_normalization_6 (Bat  (None, 4, 4, 512)         2048      \n chNormalization)                                                \n                                                                 \n leaky_re_lu_8 (LeakyReLU)   (None, 4, 4, 512)         0         \n                                                                 \n conv2d_transpose_1 (Conv2D  (None, 8, 8, 256)         2097152   \n Transpose)                                                      \n                                                                 \n batch_normalization_7 (Bat  (None, 8, 8, 256)         1024      \n chNormalization)                                                \n                                                                 \n leaky_re_lu_9 (LeakyReLU)   (None, 8, 8, 256)         0         \n                                                                 \n conv2d_transpose_2 (Conv2D  (None, 16, 16, 128)       524288    \n Transpose)                                                      \n                                                                 \n batch_normalization_8 (Bat  (None, 16, 16, 128)       512       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_10 (LeakyReLU)  (None, 16, 16, 128)       0         \n                                                                 \n conv2d_transpose_3 (Conv2D  (None, 32, 32, 64)        131072    \n Transpose)                                                      \n                                                                 \n batch_normalization_9 (Bat  (None, 32, 32, 64)        256       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_11 (LeakyReLU)  (None, 32, 32, 64)        0         \n                                                                 \n conv2d_transpose_4 (Conv2D  (None, 64, 64, 1)         1024      \n Transpose)                                                      \n                                                                 \n=================================================================\nTotal params: 3576576 (13.64 MB)\nTrainable params: 3574656 (13.64 MB)\nNon-trainable params: 1920 (7.50 KB)\n_________________________________________________________________\n\n\n\n\n\n\nWe alternate between training the discriminator and generator. They are not trained simultaneously. We want the generated images to be predicted close to 1 because the generator is good, not because the discriminator is weak.\nFor the discriminator, we create a training set where some images are real images from the training data and some are outputs from the generator. This is then a supervised binary classification problem.\nFor the generator, we want a way of scoring each generated image on its realness so that we can optimise this. The discriminator provides exactly this. We pass the generated images through the discriminator to get probabilities. The generator wants to fool the discriminator, so ideally this would be a vector of 1s. So the loss function is the binary crossentropy between these probabilities and a vector of 1s.\n\n\nCode\nclass DCGAN(models.Model):\n    def __init__(self, discriminator, generator, latent_dim):\n        super(DCGAN, self).__init__()\n        self.discriminator = discriminator\n        self.generator = generator\n        self.latent_dim = latent_dim\n\n    def compile(self, d_optimizer, g_optimizer):\n        super(DCGAN, self).compile()\n        self.loss_fn = losses.BinaryCrossentropy()\n        self.d_optimizer = d_optimizer\n        self.g_optimizer = g_optimizer\n        self.d_loss_metric = metrics.Mean(name=\"d_loss\")\n        self.d_real_acc_metric = metrics.BinaryAccuracy(name=\"d_real_acc\")\n        self.d_fake_acc_metric = metrics.BinaryAccuracy(name=\"d_fake_acc\")\n        self.d_acc_metric = metrics.BinaryAccuracy(name=\"d_acc\")\n        self.g_loss_metric = metrics.Mean(name=\"g_loss\")\n        self.g_acc_metric = metrics.BinaryAccuracy(name=\"g_acc\")\n\n    @property\n    def metrics(self):\n        return [\n            self.d_loss_metric,\n            self.d_real_acc_metric,\n            self.d_fake_acc_metric,\n            self.d_acc_metric,\n            self.g_loss_metric,\n            self.g_acc_metric,\n        ]\n\n    def train_step(self, real_images):\n        # Sample random points in the latent space\n        batch_size = tf.shape(real_images)[0]\n        random_latent_vectors = tf.random.normal(\n            shape=(batch_size, self.latent_dim)\n        )\n\n        # Train the discriminator on fake images\n        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n            generated_images = self.generator(\n                random_latent_vectors, training=True\n            )\n\n            # Evaluate the discriminator on the real and fake images\n            real_predictions = self.discriminator(real_images, training=True)\n            fake_predictions = self.discriminator(generated_images, training=True)\n\n            real_labels = tf.ones_like(real_predictions)\n            real_noisy_labels = real_labels + NOISE_PARAM * tf.random.uniform(\n                tf.shape(real_predictions)\n            )\n            fake_labels = tf.zeros_like(fake_predictions)\n            fake_noisy_labels = fake_labels - NOISE_PARAM * tf.random.uniform(\n                tf.shape(fake_predictions)\n            )\n\n            # Calculate the losses\n            d_real_loss = self.loss_fn(real_noisy_labels, real_predictions)\n            d_fake_loss = self.loss_fn(fake_noisy_labels, fake_predictions)\n            d_loss = (d_real_loss + d_fake_loss) / 2.0\n\n            g_loss = self.loss_fn(real_labels, fake_predictions)\n\n        # Update gradients\n        gradients_of_discriminator = disc_tape.gradient(\n            d_loss, self.discriminator.trainable_variables\n        )\n        gradients_of_generator = gen_tape.gradient(\n            g_loss, self.generator.trainable_variables\n        )\n\n        self.d_optimizer.apply_gradients(\n            zip(gradients_of_discriminator, discriminator.trainable_variables)\n        )\n        self.g_optimizer.apply_gradients(\n            zip(gradients_of_generator, generator.trainable_variables)\n        )\n\n        # Update metrics\n        self.d_loss_metric.update_state(d_loss)\n        self.d_real_acc_metric.update_state(real_labels, real_predictions)\n        self.d_fake_acc_metric.update_state(fake_labels, fake_predictions)\n        self.d_acc_metric.update_state(\n            [real_labels, fake_labels], [real_predictions, fake_predictions]\n        )\n        self.g_loss_metric.update_state(g_loss)\n        self.g_acc_metric.update_state(real_labels, fake_predictions)\n\n        return {m.name: m.result() for m in self.metrics}\n\n\n\n\nCode\n# Create a DCGAN\ndcgan = DCGAN(\n    discriminator=discriminator, generator=generator, latent_dim=Z_DIM\n)\n\n\n\n\nCode\ndcgan.compile(\n    d_optimizer=optimizers.legacy.Adam(\n        learning_rate=LEARNING_RATE, beta_1=ADAM_BETA_1, beta_2=ADAM_BETA_2\n    ),\n    g_optimizer=optimizers.legacy.Adam(\n        learning_rate=LEARNING_RATE, beta_1=ADAM_BETA_1, beta_2=ADAM_BETA_2\n    ),\n)\ndcgan.fit(train,  epochs=EPOCHS)\n\n\nEpoch 1/100\n313/313 [==============================] - 459s 1s/step - d_loss: 0.0252 - d_real_acc: 0.9011 - d_fake_acc: 0.9013 - d_acc: 0.9012 - g_loss: 5.3464 - g_acc: 0.0987\nEpoch 2/100\n313/313 [==============================] - 459s 1s/step - d_loss: 0.0454 - d_real_acc: 0.8986 - d_fake_acc: 0.8997 - d_acc: 0.8992 - g_loss: 5.2642 - g_acc: 0.1002\nEpoch 3/100\n313/313 [==============================] - 458s 1s/step - d_loss: 0.0556 - d_real_acc: 0.8958 - d_fake_acc: 0.8975 - d_acc: 0.8967 - g_loss: 4.9679 - g_acc: 0.1025\nEpoch 4/100\n313/313 [==============================] - 467s 1s/step - d_loss: 0.0246 - d_real_acc: 0.9065 - d_fake_acc: 0.9091 - d_acc: 0.9078 - g_loss: 5.1611 - g_acc: 0.0909\nEpoch 5/100\n313/313 [==============================] - 470s 1s/step - d_loss: 0.0178 - d_real_acc: 0.9067 - d_fake_acc: 0.9088 - d_acc: 0.9078 - g_loss: 5.1731 - g_acc: 0.0912\nEpoch 6/100\n313/313 [==============================] - 463s 1s/step - d_loss: 0.0314 - d_real_acc: 0.9116 - d_fake_acc: 0.9105 - d_acc: 0.9110 - g_loss: 5.2774 - g_acc: 0.0895\nEpoch 7/100\n313/313 [==============================] - 559s 2s/step - d_loss: 0.0229 - d_real_acc: 0.9085 - d_fake_acc: 0.9079 - d_acc: 0.9082 - g_loss: 5.3445 - g_acc: 0.0921\nEpoch 8/100\n313/313 [==============================] - 467s 1s/step - d_loss: -0.0155 - d_real_acc: 0.9161 - d_fake_acc: 0.9161 - d_acc: 0.9161 - g_loss: 5.7091 - g_acc: 0.0839\nEpoch 9/100\n313/313 [==============================] - 438s 1s/step - d_loss: -0.0077 - d_real_acc: 0.9220 - d_fake_acc: 0.9224 - d_acc: 0.9222 - g_loss: 5.8731 - g_acc: 0.0776\nEpoch 10/100\n313/313 [==============================] - 468s 1s/step - d_loss: -0.0472 - d_real_acc: 0.9228 - d_fake_acc: 0.9241 - d_acc: 0.9234 - g_loss: 5.9693 - g_acc: 0.0759\nEpoch 11/100\n313/313 [==============================] - 430s 1s/step - d_loss: -0.0839 - d_real_acc: 0.9404 - d_fake_acc: 0.9424 - d_acc: 0.9414 - g_loss: 6.1212 - g_acc: 0.0576\nEpoch 12/100\n313/313 [==============================] - 457s 1s/step - d_loss: 0.0431 - d_real_acc: 0.9053 - d_fake_acc: 0.9046 - d_acc: 0.9049 - g_loss: 6.0708 - g_acc: 0.0954\nEpoch 13/100\n313/313 [==============================] - 448s 1s/step - d_loss: -0.0154 - d_real_acc: 0.9236 - d_fake_acc: 0.9244 - d_acc: 0.9240 - g_loss: 6.3106 - g_acc: 0.0756\nEpoch 14/100\n313/313 [==============================] - 432s 1s/step - d_loss: -0.0720 - d_real_acc: 0.9320 - d_fake_acc: 0.9342 - d_acc: 0.9331 - g_loss: 6.6509 - g_acc: 0.0658\nEpoch 15/100\n313/313 [==============================] - 443s 1s/step - d_loss: 0.0057 - d_real_acc: 0.9072 - d_fake_acc: 0.9097 - d_acc: 0.9085 - g_loss: 6.1399 - g_acc: 0.0903\nEpoch 16/100\n313/313 [==============================] - 427s 1s/step - d_loss: 0.0069 - d_real_acc: 0.9185 - d_fake_acc: 0.9167 - d_acc: 0.9176 - g_loss: 6.3255 - g_acc: 0.0833\nEpoch 17/100\n313/313 [==============================] - 439s 1s/step - d_loss: -0.0709 - d_real_acc: 0.9220 - d_fake_acc: 0.9239 - d_acc: 0.9230 - g_loss: 6.8108 - g_acc: 0.0761\nEpoch 18/100\n313/313 [==============================] - 437s 1s/step - d_loss: 0.0373 - d_real_acc: 0.9288 - d_fake_acc: 0.9512 - d_acc: 0.9400 - g_loss: 8.1066 - g_acc: 0.0488\nEpoch 19/100\n313/313 [==============================] - 1029s 3s/step - d_loss: -0.1154 - d_real_acc: 0.9408 - d_fake_acc: 0.9420 - d_acc: 0.9414 - g_loss: 7.6274 - g_acc: 0.0580\nEpoch 20/100\n313/313 [==============================] - 5781s 19s/step - d_loss: -0.0431 - d_real_acc: 0.9222 - d_fake_acc: 0.9231 - d_acc: 0.9227 - g_loss: 7.1953 - g_acc: 0.0769\nEpoch 21/100\n313/313 [==============================] - 2696s 9s/step - d_loss: -0.0542 - d_real_acc: 0.9176 - d_fake_acc: 0.9205 - d_acc: 0.9191 - g_loss: 7.1675 - g_acc: 0.0794\nEpoch 22/100\n313/313 [==============================] - 1481s 5s/step - d_loss: -0.1424 - d_real_acc: 0.9398 - d_fake_acc: 0.9400 - d_acc: 0.9399 - g_loss: 7.7399 - g_acc: 0.0600\nEpoch 23/100\n313/313 [==============================] - 439s 1s/step - d_loss: -0.0263 - d_real_acc: 0.9154 - d_fake_acc: 0.9148 - d_acc: 0.9151 - g_loss: 7.3322 - g_acc: 0.0852\nEpoch 24/100\n313/313 [==============================] - 443s 1s/step - d_loss: -0.1420 - d_real_acc: 0.9406 - d_fake_acc: 0.9430 - d_acc: 0.9418 - g_loss: 8.1502 - g_acc: 0.0570\nEpoch 25/100\n313/313 [==============================] - 475s 2s/step - d_loss: -0.1650 - d_real_acc: 0.9393 - d_fake_acc: 0.9395 - d_acc: 0.9394 - g_loss: 7.9602 - g_acc: 0.0605\nEpoch 26/100\n313/313 [==============================] - 447s 1s/step - d_loss: -0.1247 - d_real_acc: 0.9419 - d_fake_acc: 0.9437 - d_acc: 0.9428 - g_loss: 7.7939 - g_acc: 0.0563\nEpoch 27/100\n313/313 [==============================] - 1356s 4s/step - d_loss: -0.0182 - d_real_acc: 0.9160 - d_fake_acc: 0.9212 - d_acc: 0.9186 - g_loss: 6.9180 - g_acc: 0.0787\nEpoch 28/100\n313/313 [==============================] - 2219s 7s/step - d_loss: -0.2227 - d_real_acc: 0.9511 - d_fake_acc: 0.9519 - d_acc: 0.9515 - g_loss: 8.1970 - g_acc: 0.0481\nEpoch 29/100\n313/313 [==============================] - 5807s 19s/step - d_loss: -0.1091 - d_real_acc: 0.9318 - d_fake_acc: 0.9320 - d_acc: 0.9319 - g_loss: 7.5829 - g_acc: 0.0680\nEpoch 30/100\n313/313 [==============================] - 2511s 8s/step - d_loss: -0.3131 - d_real_acc: 0.9571 - d_fake_acc: 0.9604 - d_acc: 0.9588 - g_loss: 9.8839 - g_acc: 0.0395\nEpoch 31/100\n313/313 [==============================] - 2768s 9s/step - d_loss: -0.0996 - d_real_acc: 0.9269 - d_fake_acc: 0.9286 - d_acc: 0.9277 - g_loss: 8.3337 - g_acc: 0.0714\nEpoch 32/100\n313/313 [==============================] - 3046s 10s/step - d_loss: -0.1619 - d_real_acc: 0.9423 - d_fake_acc: 0.9482 - d_acc: 0.9453 - g_loss: 8.2435 - g_acc: 0.0518\nEpoch 33/100\n313/313 [==============================] - 3478s 11s/step - d_loss: -0.1182 - d_real_acc: 0.9284 - d_fake_acc: 0.9304 - d_acc: 0.9294 - g_loss: 8.1681 - g_acc: 0.0696\nEpoch 34/100\n313/313 [==============================] - 2776s 9s/step - d_loss: -0.2214 - d_real_acc: 0.9459 - d_fake_acc: 0.9582 - d_acc: 0.9520 - g_loss: 9.9168 - g_acc: 0.0417\nEpoch 35/100\n313/313 [==============================] - 2724s 9s/step - d_loss: -0.3101 - d_real_acc: 0.9421 - d_fake_acc: 0.9293 - d_acc: 0.9357 - g_loss: 13.2857 - g_acc: 0.0707\nEpoch 36/100\n313/313 [==============================] - 2648s 8s/step - d_loss: -0.0441 - d_real_acc: 0.8963 - d_fake_acc: 0.8961 - d_acc: 0.8962 - g_loss: 7.5664 - g_acc: 0.1038\nEpoch 37/100\n313/313 [==============================] - 3262s 10s/step - d_loss: -0.0859 - d_real_acc: 0.9314 - d_fake_acc: 0.9402 - d_acc: 0.9358 - g_loss: 8.3591 - g_acc: 0.0598\nEpoch 38/100\n313/313 [==============================] - 2612s 8s/step - d_loss: -0.2979 - d_real_acc: 0.9554 - d_fake_acc: 0.9577 - d_acc: 0.9566 - g_loss: 9.2534 - g_acc: 0.0423\nEpoch 39/100\n313/313 [==============================] - 2235s 7s/step - d_loss: -0.3387 - d_real_acc: 0.9607 - d_fake_acc: 0.9622 - d_acc: 0.9615 - g_loss: 9.9397 - g_acc: 0.0378\nEpoch 40/100\n313/313 [==============================] - 3453s 11s/step - d_loss: -0.1056 - d_real_acc: 0.9279 - d_fake_acc: 0.9310 - d_acc: 0.9294 - g_loss: 8.9394 - g_acc: 0.0690\nEpoch 41/100\n313/313 [==============================] - 2316s 7s/step - d_loss: -0.2147 - d_real_acc: 0.9318 - d_fake_acc: 0.9327 - d_acc: 0.9323 - g_loss: 9.0337 - g_acc: 0.0673\nEpoch 42/100\n313/313 [==============================] - 3134s 10s/step - d_loss: -0.2554 - d_real_acc: 0.9511 - d_fake_acc: 0.9540 - d_acc: 0.9526 - g_loss: 9.5571 - g_acc: 0.0460\nEpoch 43/100\n313/313 [==============================] - 3933s 13s/step - d_loss: -0.2871 - d_real_acc: 0.9490 - d_fake_acc: 0.9526 - d_acc: 0.9508 - g_loss: 10.3316 - g_acc: 0.0474\nEpoch 44/100\n313/313 [==============================] - 3248s 10s/step - d_loss: -0.3456 - d_real_acc: 0.9635 - d_fake_acc: 0.9635 - d_acc: 0.9635 - g_loss: 9.8675 - g_acc: 0.0364\nEpoch 45/100\n313/313 [==============================] - 3043s 10s/step - d_loss: -0.3274 - d_real_acc: 0.9603 - d_fake_acc: 0.9618 - d_acc: 0.9610 - g_loss: 10.4185 - g_acc: 0.0382\nEpoch 46/100\n313/313 [==============================] - 2706s 9s/step - d_loss: -0.6160 - d_real_acc: 0.9902 - d_fake_acc: 0.9908 - d_acc: 0.9905 - g_loss: 13.0574 - g_acc: 0.0092\nEpoch 47/100\n313/313 [==============================] - 2453s 8s/step - d_loss: 0.3413 - d_real_acc: 0.8073 - d_fake_acc: 0.8054 - d_acc: 0.8064 - g_loss: 6.5391 - g_acc: 0.1946\nEpoch 48/100\n313/313 [==============================] - 2898s 9s/step - d_loss: -0.4416 - d_real_acc: 0.9764 - d_fake_acc: 0.9784 - d_acc: 0.9774 - g_loss: 10.8318 - g_acc: 0.0216\nEpoch 49/100\n313/313 [==============================] - 3358s 11s/step - d_loss: 6.8776 - d_real_acc: 0.1058 - d_fake_acc: 0.9910 - d_acc: 0.5484 - g_loss: 14.8921 - g_acc: 0.0090\nEpoch 50/100\n313/313 [==============================] - 2940s 9s/step - d_loss: 7.7113 - d_real_acc: 0.0000e+00 - d_fake_acc: 1.0000 - d_acc: 0.5000 - g_loss: 15.4250 - g_acc: 0.0000e+00\nEpoch 51/100\n313/313 [==============================] - 2983s 10s/step - d_loss: 7.7121 - d_real_acc: 0.0000e+00 - d_fake_acc: 1.0000 - d_acc: 0.5000 - g_loss: 15.4250 - g_acc: 0.0000e+00\nEpoch 52/100\n313/313 [==============================] - 3458s 11s/step - d_loss: 7.7149 - d_real_acc: 0.0000e+00 - d_fake_acc: 1.0000 - d_acc: 0.5000 - g_loss: 15.4250 - g_acc: 0.0000e+00\nEpoch 53/100\n313/313 [==============================] - 2404s 8s/step - d_loss: 4.4950 - d_real_acc: 0.3543 - d_fake_acc: 0.8865 - d_acc: 0.6204 - g_loss: 10.6772 - g_acc: 0.1135\nEpoch 54/100\n313/313 [==============================] - 3297s 11s/step - d_loss: -0.0132 - d_real_acc: 0.9068 - d_fake_acc: 0.9010 - d_acc: 0.9039 - g_loss: 7.9660 - g_acc: 0.0990\nEpoch 55/100\n313/313 [==============================] - 2486s 8s/step - d_loss: -0.3508 - d_real_acc: 0.9615 - d_fake_acc: 0.9612 - d_acc: 0.9614 - g_loss: 10.2242 - g_acc: 0.0388\nEpoch 56/100\n313/313 [==============================] - 2995s 10s/step - d_loss: -0.3125 - d_real_acc: 0.9525 - d_fake_acc: 0.9533 - d_acc: 0.9529 - g_loss: 10.4182 - g_acc: 0.0467\nEpoch 57/100\n313/313 [==============================] - 1791s 6s/step - d_loss: -0.3201 - d_real_acc: 0.9532 - d_fake_acc: 0.9560 - d_acc: 0.9546 - g_loss: 10.4752 - g_acc: 0.0441\nEpoch 58/100\n313/313 [==============================] - 2792s 9s/step - d_loss: -0.2649 - d_real_acc: 0.9509 - d_fake_acc: 0.9532 - d_acc: 0.9520 - g_loss: 9.3587 - g_acc: 0.0468\nEpoch 59/100\n313/313 [==============================] - 3665s 12s/step - d_loss: -0.1747 - d_real_acc: 0.9413 - d_fake_acc: 0.9584 - d_acc: 0.9499 - g_loss: 10.1369 - g_acc: 0.0416\nEpoch 60/100\n313/313 [==============================] - 2493s 8s/step - d_loss: -0.2692 - d_real_acc: 0.9499 - d_fake_acc: 0.9534 - d_acc: 0.9517 - g_loss: 9.7124 - g_acc: 0.0466\nEpoch 61/100\n313/313 [==============================] - 2293s 7s/step - d_loss: -0.2869 - d_real_acc: 0.9520 - d_fake_acc: 0.9556 - d_acc: 0.9538 - g_loss: 10.2684 - g_acc: 0.0444\nEpoch 62/100\n313/313 [==============================] - 2865s 9s/step - d_loss: -0.6188 - d_real_acc: 0.9900 - d_fake_acc: 0.9901 - d_acc: 0.9901 - g_loss: 12.9584 - g_acc: 0.0099\nEpoch 63/100\n313/313 [==============================] - 2301s 7s/step - d_loss: -0.7197 - d_real_acc: 0.9984 - d_fake_acc: 0.9985 - d_acc: 0.9985 - g_loss: 14.5762 - g_acc: 0.0015\nEpoch 64/100\n313/313 [==============================] - 2404s 8s/step - d_loss: -0.4320 - d_real_acc: 0.9665 - d_fake_acc: 0.9702 - d_acc: 0.9683 - g_loss: 12.0177 - g_acc: 0.0298\nEpoch 65/100\n313/313 [==============================] - 4723s 15s/step - d_loss: 6.7591 - d_real_acc: 0.9940 - d_fake_acc: 0.1077 - d_acc: 0.5509 - g_loss: 1.4402 - g_acc: 0.8923\nEpoch 66/100\n313/313 [==============================] - 2726s 9s/step - d_loss: 7.6259 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 67/100\n313/313 [==============================] - 3325s 11s/step - d_loss: 7.6250 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 68/100\n313/313 [==============================] - 2513s 8s/step - d_loss: 7.6251 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 7.4387e-12 - g_acc: 1.0000\nEpoch 69/100\n313/313 [==============================] - 3304s 11s/step - d_loss: 3.4076 - d_real_acc: 0.8552 - d_fake_acc: 0.5472 - d_acc: 0.7012 - g_loss: 5.2072 - g_acc: 0.4528\nEpoch 70/100\n313/313 [==============================] - 2276s 7s/step - d_loss: -0.2424 - d_real_acc: 0.9448 - d_fake_acc: 0.9581 - d_acc: 0.9514 - g_loss: 10.5206 - g_acc: 0.0419\nEpoch 71/100\n313/313 [==============================] - 3315s 11s/step - d_loss: -0.3093 - d_real_acc: 0.9563 - d_fake_acc: 0.9640 - d_acc: 0.9602 - g_loss: 10.6076 - g_acc: 0.0360\nEpoch 72/100\n313/313 [==============================] - 2306s 7s/step - d_loss: -0.2440 - d_real_acc: 0.9466 - d_fake_acc: 0.9581 - d_acc: 0.9523 - g_loss: 10.1996 - g_acc: 0.0419\nEpoch 73/100\n313/313 [==============================] - 3218s 10s/step - d_loss: -0.7206 - d_real_acc: 0.9985 - d_fake_acc: 0.9983 - d_acc: 0.9984 - g_loss: 14.6350 - g_acc: 0.0017\nEpoch 74/100\n313/313 [==============================] - 3258s 10s/step - d_loss: -0.6281 - d_real_acc: 0.9828 - d_fake_acc: 0.9841 - d_acc: 0.9834 - g_loss: 14.5219 - g_acc: 0.0159\nEpoch 75/100\n313/313 [==============================] - 2874s 9s/step - d_loss: -0.0555 - d_real_acc: 0.9254 - d_fake_acc: 0.9371 - d_acc: 0.9312 - g_loss: 9.3443 - g_acc: 0.0629\nEpoch 76/100\n313/313 [==============================] - 2559s 8s/step - d_loss: -0.2825 - d_real_acc: 0.9515 - d_fake_acc: 0.9611 - d_acc: 0.9563 - g_loss: 10.7583 - g_acc: 0.0388\nEpoch 77/100\n313/313 [==============================] - 3663s 12s/step - d_loss: -0.3945 - d_real_acc: 0.9667 - d_fake_acc: 0.9691 - d_acc: 0.9679 - g_loss: 10.8566 - g_acc: 0.0309\nEpoch 78/100\n313/313 [==============================] - 2314s 7s/step - d_loss: -0.3953 - d_real_acc: 0.9508 - d_fake_acc: 0.9529 - d_acc: 0.9519 - g_loss: 12.0037 - g_acc: 0.0471\nEpoch 79/100\n313/313 [==============================] - 2816s 9s/step - d_loss: -0.6059 - d_real_acc: 0.9841 - d_fake_acc: 0.9838 - d_acc: 0.9840 - g_loss: 13.3516 - g_acc: 0.0162\nEpoch 80/100\n313/313 [==============================] - 3232s 10s/step - d_loss: -0.3555 - d_real_acc: 0.9587 - d_fake_acc: 0.9649 - d_acc: 0.9618 - g_loss: 11.2453 - g_acc: 0.0351\nEpoch 81/100\n313/313 [==============================] - 3705s 12s/step - d_loss: -0.4501 - d_real_acc: 0.9731 - d_fake_acc: 0.9743 - d_acc: 0.9737 - g_loss: 11.4553 - g_acc: 0.0258\nEpoch 82/100\n313/313 [==============================] - 2364s 8s/step - d_loss: -0.3827 - d_real_acc: 0.9588 - d_fake_acc: 0.9639 - d_acc: 0.9613 - g_loss: 11.6275 - g_acc: 0.0361\nEpoch 83/100\n313/313 [==============================] - 1122s 4s/step - d_loss: -0.4355 - d_real_acc: 0.9642 - d_fake_acc: 0.9674 - d_acc: 0.9658 - g_loss: 12.1025 - g_acc: 0.0326\nEpoch 84/100\n313/313 [==============================] - 4065s 13s/step - d_loss: -0.4456 - d_real_acc: 0.9695 - d_fake_acc: 0.9714 - d_acc: 0.9704 - g_loss: 11.6065 - g_acc: 0.0287\nEpoch 85/100\n313/313 [==============================] - 4461s 14s/step - d_loss: -0.6405 - d_real_acc: 0.9901 - d_fake_acc: 0.9899 - d_acc: 0.9900 - g_loss: 13.4694 - g_acc: 0.0101\nEpoch 86/100\n313/313 [==============================] - 2630s 8s/step - d_loss: -0.6431 - d_real_acc: 0.9857 - d_fake_acc: 0.9856 - d_acc: 0.9857 - g_loss: 14.3623 - g_acc: 0.0144\nEpoch 87/100\n313/313 [==============================] - 2567s 8s/step - d_loss: -0.3870 - d_real_acc: 0.9534 - d_fake_acc: 0.9578 - d_acc: 0.9556 - g_loss: 12.0201 - g_acc: 0.0422\nEpoch 88/100\n313/313 [==============================] - 2597s 8s/step - d_loss: -0.7624 - d_real_acc: 0.9999 - d_fake_acc: 0.9998 - d_acc: 0.9998 - g_loss: 15.3547 - g_acc: 2.5000e-04\nEpoch 89/100\n313/313 [==============================] - 1477s 5s/step - d_loss: -0.5787 - d_real_acc: 0.9764 - d_fake_acc: 0.9759 - d_acc: 0.9762 - g_loss: 13.8500 - g_acc: 0.0241\nEpoch 90/100\n313/313 [==============================] - 522s 2s/step - d_loss: -0.6747 - d_real_acc: 0.9885 - d_fake_acc: 0.9897 - d_acc: 0.9891 - g_loss: 14.5329 - g_acc: 0.0104\nEpoch 91/100\n313/313 [==============================] - 512s 2s/step - d_loss: 6.4703 - d_real_acc: 0.9892 - d_fake_acc: 0.1438 - d_acc: 0.5665 - g_loss: 2.1184 - g_acc: 0.8562\nEpoch 92/100\n313/313 [==============================] - 514s 2s/step - d_loss: 7.6245 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 93/100\n313/313 [==============================] - 533s 2s/step - d_loss: 7.6249 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 94/100\n313/313 [==============================] - 499s 2s/step - d_loss: 7.6236 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 95/100\n313/313 [==============================] - 483s 2s/step - d_loss: 7.6240 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 96/100\n313/313 [==============================] - 481s 2s/step - d_loss: 7.6248 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 97/100\n313/313 [==============================] - 488s 2s/step - d_loss: 7.6247 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 98/100\n313/313 [==============================] - 481s 2s/step - d_loss: 7.6263 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 99/100\n313/313 [==============================] - 459s 1s/step - d_loss: 7.6235 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\nEpoch 100/100\n313/313 [==============================] - 4669s 15s/step - d_loss: 7.6231 - d_real_acc: 1.0000 - d_fake_acc: 0.0000e+00 - d_acc: 0.5000 - g_loss: 0.0000e+00 - g_acc: 1.0000\n\n\n&lt;keras.src.callbacks.History at 0x10f686690&gt;\n\n\n\n\nCode\n# Save the final models\ngenerator.save(\"./models/generator\")\ndiscriminator.save(\"./models/discriminator\")\n\n\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nINFO:tensorflow:Assets written to: ./models/generator/assets\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nINFO:tensorflow:Assets written to: ./models/discriminator/assets\n\n\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nINFO:tensorflow:Assets written to: ./models/generator/assets\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nINFO:tensorflow:Assets written to: ./models/discriminator/assets"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson4/chapter4.html#analysing-the-gan",
    "href": "posts/ml/generative_deep_learning/lesson4/chapter4.html#analysing-the-gan",
    "title": "Generative AI: GANs",
    "section": "",
    "text": "We can see some examples of images produced by the GAN.\n(I don’t have a GPU so training is slow and I only trained 100 epochs… they’re a bit crap)\n\n\nCode\n# Sample some points in the latent space, from the standard normal distribution\ngrid_width, grid_height = (10, 3)\nz_sample = np.random.normal(size=(grid_width * grid_height, Z_DIM))\n\n# Decode the sampled points\nreconstructions = generator.predict(z_sample)\n\n# Draw a plot of decoded images\nfig = plt.figure(figsize=(18, 5))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\n# Output the grid of faces\nfor i in range(grid_width * grid_height):\n    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n    ax.axis(\"off\")\n    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")\n\n\n\n\n\n\n\n\n\nWe also want to make sure a generative model doesn’t simply recreate images that are already in the training set.\nAs a sanity check, we plot some generated images and the closest training images (using the L1 distance). This confirms that the generator is able to understand high-level features, even though we didn’t provide anything other than raw pixels, and it can generate examples distinct from those encountered before.\n\n\nCode\ndef compare_images(img1, img2):\n    return np.mean(np.abs(img1 - img2))\n\nall_data = []\nfor i in train.as_numpy_iterator():\n    all_data.extend(i)\nall_data = np.array(all_data)\n\n# Plot the images\nr, c = 3, 5\nfig, axs = plt.subplots(r, c, figsize=(10, 6))\nfig.suptitle(\"Generated images\", fontsize=20)\n\nnoise = np.random.normal(size=(r * c, Z_DIM))\ngen_imgs = generator.predict(noise)\n\ncnt = 0\nfor i in range(r):\n    for j in range(c):\n        axs[i, j].imshow(gen_imgs[cnt], cmap=\"gray_r\")\n        axs[i, j].axis(\"off\")\n        cnt += 1\n\nplt.show()\n\n\n1/1 [==============================] - 0s 80ms/step\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig, axs = plt.subplots(r, c, figsize=(10, 6))\nfig.suptitle(\"Closest images in the training set\", fontsize=20)\n\ncnt = 0\nfor i in range(r):\n    for j in range(c):\n        c_diff = 99999\n        c_img = None\n        for k_idx, k in enumerate(all_data):\n            diff = compare_images(gen_imgs[cnt], k)\n            if diff &lt; c_diff:\n                c_img = np.copy(k)\n                c_diff = diff\n        axs[i, j].imshow(c_img, cmap=\"gray_r\")\n        axs[i, j].axis(\"off\")\n        cnt += 1\n\nplt.show()"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson4/chapter4.html#gan-training-tips",
    "href": "posts/ml/generative_deep_learning/lesson4/chapter4.html#gan-training-tips",
    "title": "Generative AI: GANs",
    "section": "",
    "text": "GANs are notoriously difficult to train because there is a balancing act between the generator and discriminator; neither should grow so strong that it overpowers the other.\n\n\nThe discriminator can always spot the fakes, so the signal from the loss function becomes too weak to cause any meaningful improvements in the generator.\nIn the extreme case, the discriminator distinguishes fakes perfectly, so gradients vanish and no training takes place.\nWe need to weaken the discriminator in this case. Some possible options are:\n\nMore dropout\nLower learning rate\nSimplify the discriminator architecture - use fewer layers\nAdd noise to the labels when training the discriminator\nAdd intentional labelling errors - randomly flip the labels of some images when training the discriminator\n\n\n\n\nIf the discriminator is too weak, the generator will learn that it can trick the discriminator using a small sample of nearly identical images. This is known as mode collapse. The generator would map every point in the latent space to this image, so the gradients of the loss function would vanish and it would not be able to recover.\nStrengthening the discriminator would not help because the generator would just learn to find a different mode that fools the discriminator with no diversity; it is numb to its input.\nSome possible options are:\n\nStrengthen the discriminator - do the opposite of the previous section\nReduce the learning rate of both generator and discriminator\nIncrease the batch size"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson4/chapter4.html#uninformative-loss",
    "href": "posts/ml/generative_deep_learning/lesson4/chapter4.html#uninformative-loss",
    "title": "Generative AI: GANs",
    "section": "",
    "text": "The value of the loss is not meaningful as a measure of the generator’s strength when training.\nThe loss function is relative to the discriminator, and since the discriminator is also being trained, the goalposts are constantly shifting. Also, we don’t want the loss function to reach 0 or else we may reach mode collapse as described above.\nThis makes GAN training difficult to monitor."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson4/chapter4.html#hyperparameters",
    "href": "posts/ml/generative_deep_learning/lesson4/chapter4.html#hyperparameters",
    "title": "Generative AI: GANs",
    "section": "",
    "text": "There are a lot of hyperparameters involved with GANs because we are now training two networks.\nThe performance is highly sensitive to these hyperparameter choices, and involves a lot of trial and error."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson4/chapter4.html#wasserstein-gan-with-gradient-penalty-wgan-gp",
    "href": "posts/ml/generative_deep_learning/lesson4/chapter4.html#wasserstein-gan-with-gradient-penalty-wgan-gp",
    "title": "Generative AI: GANs",
    "section": "",
    "text": "The Wasserstein GAN replaces the binary crossentropy loss function with the Wassserstein lss function in both the discriminator and generator.\nThis results in two desirable properties:\n\nA meaningful loss metric that correlates with generator convergence. This allows for better monitoring of training.\nMore stable optimisation process."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson4/chapter4.html#references",
    "href": "posts/ml/generative_deep_learning/lesson4/chapter4.html#references",
    "title": "Generative AI: GANs",
    "section": "",
    "text": "Chapter 4 of Generative Deep Learning by David Foster."
  },
  {
    "objectID": "posts/software/react/23_deployment/post.html",
    "href": "posts/software/react/23_deployment/post.html",
    "title": "React: Deployment",
    "section": "",
    "text": "We should think about the following when we’re ready to deploy our app:\n\nTest code\nOptimise code\nBuild app\nUpload app\nConfigure server\n\nTesting is handled in a separate post.\nThis post contains some optimisation, build and configuration considerations.\n\n\nLoad code only when it’s needed.\nWhen we import files into other files, they are immediately resolved. This means that we need to load everything before any part of the site loads.\nWith lazy loading, we only load each file as it is needed by the site.\nInstead of:\nimport BlogPage from './pages/Blog';\nWe can use:\nimport { lazy, Suspense } from 'react';\n\nconst BlogPage = lazy(() =&gt; import('./pages/Blog'));\n\n// Then wherever the BlogPage component is used, wrap it with a Suspense component\n&lt;Suspense fallback={&lt;p&gt;Loading...&lt;/p&gt;}&gt;\n  &lt;BlogPost /&gt;\n&lt;/Suspense&gt;\nYou can verify how lazy loading works by looking at the network tab of the browser while navigating the website.\nIt should only load pages as required while you navigate.\n\n\n\nRunning npm run build will create an optimised build which transforms React code to Javascript, CSS and HTML, which are supported natively by browsers.\nThe contents of this build directory should be uploaded to the hosting server.\nA React Single-Page Application (SPA) is a static website. It does not require code to be executed by the server.\nThere are many static site hosts, e.g. Github Pages, Firebase.\n\n\n\nClient-side routing is commonly used in smaller apps to keep it as an SPA.\nThere are multiple “pages” which are handled by react-router. So the server only ever actually returns a single page, index.html, regardless of the URL and the page routing is handled in JavaScript on the client side.\nThis is in contrast to server-side routing, where each page is a separate html file, so different URLs return different HTML pages.\nIf creating the website as an SPA, the deployment job should be configured for this so that the server correctly resolves the different URLs internally rather than trying to serve different HTML files.\n\n\n\n\nSection 23 of “React: The Complete Guide” Udemy course"
  },
  {
    "objectID": "posts/software/react/23_deployment/post.html#lazy-loading",
    "href": "posts/software/react/23_deployment/post.html#lazy-loading",
    "title": "React: Deployment",
    "section": "",
    "text": "Load code only when it’s needed.\nWhen we import files into other files, they are immediately resolved. This means that we need to load everything before any part of the site loads.\nWith lazy loading, we only load each file as it is needed by the site.\nInstead of:\nimport BlogPage from './pages/Blog';\nWe can use:\nimport { lazy, Suspense } from 'react';\n\nconst BlogPage = lazy(() =&gt; import('./pages/Blog'));\n\n// Then wherever the BlogPage component is used, wrap it with a Suspense component\n&lt;Suspense fallback={&lt;p&gt;Loading...&lt;/p&gt;}&gt;\n  &lt;BlogPost /&gt;\n&lt;/Suspense&gt;\nYou can verify how lazy loading works by looking at the network tab of the browser while navigating the website.\nIt should only load pages as required while you navigate."
  },
  {
    "objectID": "posts/software/react/23_deployment/post.html#building-code-for-production",
    "href": "posts/software/react/23_deployment/post.html#building-code-for-production",
    "title": "React: Deployment",
    "section": "",
    "text": "Running npm run build will create an optimised build which transforms React code to Javascript, CSS and HTML, which are supported natively by browsers.\nThe contents of this build directory should be uploaded to the hosting server.\nA React Single-Page Application (SPA) is a static website. It does not require code to be executed by the server.\nThere are many static site hosts, e.g. Github Pages, Firebase."
  },
  {
    "objectID": "posts/software/react/23_deployment/post.html#server-side-routing",
    "href": "posts/software/react/23_deployment/post.html#server-side-routing",
    "title": "React: Deployment",
    "section": "",
    "text": "Client-side routing is commonly used in smaller apps to keep it as an SPA.\nThere are multiple “pages” which are handled by react-router. So the server only ever actually returns a single page, index.html, regardless of the URL and the page routing is handled in JavaScript on the client side.\nThis is in contrast to server-side routing, where each page is a separate html file, so different URLs return different HTML pages.\nIf creating the website as an SPA, the deployment job should be configured for this so that the server correctly resolves the different URLs internally rather than trying to serve different HTML files."
  },
  {
    "objectID": "posts/software/react/23_deployment/post.html#references",
    "href": "posts/software/react/23_deployment/post.html#references",
    "title": "React: Deployment",
    "section": "",
    "text": "Section 23 of “React: The Complete Guide” Udemy course"
  },
  {
    "objectID": "posts/software/react/30_typescript/post.html",
    "href": "posts/software/react/30_typescript/post.html",
    "title": "React: TypeScript Essentials",
    "section": "",
    "text": "TypeScript (TS) is a superset of JavaScript (JS) which adds static typing.\nVanilla JS is dynamically typed. Types do not have to be specified ahead of time. This can result in funky errors like the classic “1” + “2”= “12”.\n\n\n\nIt can be installed like any other package and managed in the project dependencies package.json\nnpm install typescript \nTypescript is compiled. It does not run in the browser directly. Instead, there is a compilation step which converts TS to JS which can run in the browser.\nThis compile step is where we will find type errors, before they hit production.\nnpx tsc\n\n\n\n\n\nThe primitives are number, string, boolean. We also have null and undefined.\nNote that primitive types are lowercase, e.g. number. The object itself, e.g. Number, is not what we want here.\nThere is also an any type which is a catch all. We generally avoid this, as it defeats the purpose of using TS.\n\n\n\nWe have built-in complex types: objects and arrays.\nDefine an array of strings like:\nlet myArray: string[] = [“this”, “is”, “an”, “array”];\nDefine an object type by specifying the keys and their types:\nlet person: {name: string, age: number} = {name: “Gurp”, age: 30}\n\n\n\nUse a pipe to denote where multiple types are allowed, e.g.\nlet val: string|number = 69;\n\n\n\nDefine a type with the type keyword.\nThis allows the type to be reusable if it’s used in multiple places.\n\n\n\nTS infers the output type based on the arguments.\nIf this is correct, it’s common practice to not override this, let it infer. But you may want to override if you want it to output a union of types which it hasn’t inferred.\nFunctions also have a special void return type if they do not return anything.\n\n\n\nIf you have a utility function that can accept any input type, but the output should be the same type as the input, you can denote this using generic types with angled brackets.\nconst myFunc&lt;T&gt;(inputArray: T[], inputValue: T) {\n    return [inputValue, …inputArray]\n}\nThis can be called with strings and would return an array of strings. Or called with numbers and return an array of numbers. Rather than having to use any, we can use the generic (T is arbitrary and just stands for Type) then when we call the function TypeScript will infer the output type correctly.\n\n\n\nThese are often interchangeable. The key difference is that interfaces can be extended, whereas type cannot.\nSee here\n\n\n\nWhen defining a class, you can specify the types of each attribute.\nclass Todo {\n    id: string;\n    text: string;\n\n    constructor(inputText: string) {\n        this.text = inputText;\n        this.id = new Date().toISOString();\n    }\n}\nWhen instances of this class are used, you can simply use the class itself as the type.\nThis is useful for defining data models.\n\n\n\n\nCreating a react project using TypeScript is largely the same, but you will have .tsx files rather than .jsx.\nCertain packages that you install may have additional type annotations packages if they were written in JS, to make them they play nicely with TS. Some packages don’t need it if they were written in TS to begin with.\n\n\nWhen passing props in React, it automatically passes certain default props like children. It would be cumbersome if we had to manually define the types of those default props on every component.\nInstead, we can set the output type of our component as React.FC (Functional Component) and this will handle the default props.\nIf we then want to define our custom prop types, we can do so in angle brackets after:\nReact.FC&lt;{prop1: string, prop2: number}&gt;\nHere we are using a generic type, React.FC. The angled brackets are defining what types are being used in this particular case for this generic type.\nProps are marked as optional by adding a ? after the variable name, i.e. the key in the object.\n\n\n\nThe form submit outputs an event object which can be used by other functions.\nThe type of this can be encapsulated by the React.FormEvent type. Similarly, there is a React.MouseEvent for the onClick listener.\n\n\n\nWe create a ref with useRef then attach it to a component (can be built-in or custom).\nTypeScript doesn’t know which component you intend to attach the ref to, so you need to specify this when creating the ref.\nBy default, useRef returns a generic type, so we need to set the specific type when we call it. We also need to provide a starting value (null) to convince the TypeScript compiler that the ref isn’t already assigned to something else.\nconst inputRef = useRef&lt;HTMLInputElement&gt;(null);\nThen use this in an input element\n&lt;input ref={inputRef} /&gt;\nWhen working with ref.current TypeScript will often demand a ? to indicate that this is possibly null. The resulting value’s type will then be, for example, string or null. If you know it will never be null, you can replace with the ! operator. This means the resulting value will have type string only.\n\n\n\nWhere we pass a function as a prop we define its type as an arrow function specifying inputs and outputs.\nmyFunc: (text: string) =&gt; void\nNote that this is similar to how object types look like an object but don’t actually create an object. Function types look like a function but don’t actually create a function.\nThe .bind method is similar to partial in Python. This is useful when we are passing a function down a prop chain and it will always have a certain argument. Bind saves us having to pass the value and declare its type and every stage of the prop chain.\n\n\n\nThe compilerOptions.target value defines which version of JavaScript the TypeScript compiler will transform the code to.\nThe compilerOptions.lib value defines which TypeScript default types are included out of the box. For example, “DOM” gives support for built-in html types like HTMLInputType.\nIf we want to allow plain JavaScript files in the project alongside TypeScript, we can set compilerOptions.allowJs to True. If False, everything must strictly be TypeScript.\nWe can set a strict compile with compilerOptions.strict. This will forbid implicit any types etc.\n\n\n\nWhen we create a state, we often initialise it with an empty value, e.g. null or an empty array. But then TypeScript does not know what type is going to go in that state later.\nThe useState function returns a generic type so we can overwrite it with our type.\nconst [myState, setMyState] = useState&lt;string[]&gt;([])\n\n\n\n\n\nSection 30 of “React: The Complete Guide” Udemy course\nTypes vs Interfaces"
  },
  {
    "objectID": "posts/software/react/30_typescript/post.html#wtf-is-ts",
    "href": "posts/software/react/30_typescript/post.html#wtf-is-ts",
    "title": "React: TypeScript Essentials",
    "section": "",
    "text": "TypeScript (TS) is a superset of JavaScript (JS) which adds static typing.\nVanilla JS is dynamically typed. Types do not have to be specified ahead of time. This can result in funky errors like the classic “1” + “2”= “12”."
  },
  {
    "objectID": "posts/software/react/30_typescript/post.html#installing-and-using-ts.",
    "href": "posts/software/react/30_typescript/post.html#installing-and-using-ts.",
    "title": "React: TypeScript Essentials",
    "section": "",
    "text": "It can be installed like any other package and managed in the project dependencies package.json\nnpm install typescript \nTypescript is compiled. It does not run in the browser directly. Instead, there is a compilation step which converts TS to JS which can run in the browser.\nThis compile step is where we will find type errors, before they hit production.\nnpx tsc"
  },
  {
    "objectID": "posts/software/react/30_typescript/post.html#types",
    "href": "posts/software/react/30_typescript/post.html#types",
    "title": "React: TypeScript Essentials",
    "section": "",
    "text": "The primitives are number, string, boolean. We also have null and undefined.\nNote that primitive types are lowercase, e.g. number. The object itself, e.g. Number, is not what we want here.\nThere is also an any type which is a catch all. We generally avoid this, as it defeats the purpose of using TS.\n\n\n\nWe have built-in complex types: objects and arrays.\nDefine an array of strings like:\nlet myArray: string[] = [“this”, “is”, “an”, “array”];\nDefine an object type by specifying the keys and their types:\nlet person: {name: string, age: number} = {name: “Gurp”, age: 30}\n\n\n\nUse a pipe to denote where multiple types are allowed, e.g.\nlet val: string|number = 69;\n\n\n\nDefine a type with the type keyword.\nThis allows the type to be reusable if it’s used in multiple places.\n\n\n\nTS infers the output type based on the arguments.\nIf this is correct, it’s common practice to not override this, let it infer. But you may want to override if you want it to output a union of types which it hasn’t inferred.\nFunctions also have a special void return type if they do not return anything.\n\n\n\nIf you have a utility function that can accept any input type, but the output should be the same type as the input, you can denote this using generic types with angled brackets.\nconst myFunc&lt;T&gt;(inputArray: T[], inputValue: T) {\n    return [inputValue, …inputArray]\n}\nThis can be called with strings and would return an array of strings. Or called with numbers and return an array of numbers. Rather than having to use any, we can use the generic (T is arbitrary and just stands for Type) then when we call the function TypeScript will infer the output type correctly.\n\n\n\nThese are often interchangeable. The key difference is that interfaces can be extended, whereas type cannot.\nSee here\n\n\n\nWhen defining a class, you can specify the types of each attribute.\nclass Todo {\n    id: string;\n    text: string;\n\n    constructor(inputText: string) {\n        this.text = inputText;\n        this.id = new Date().toISOString();\n    }\n}\nWhen instances of this class are used, you can simply use the class itself as the type.\nThis is useful for defining data models."
  },
  {
    "objectID": "posts/software/react/30_typescript/post.html#typescript-with-react",
    "href": "posts/software/react/30_typescript/post.html#typescript-with-react",
    "title": "React: TypeScript Essentials",
    "section": "",
    "text": "Creating a react project using TypeScript is largely the same, but you will have .tsx files rather than .jsx.\nCertain packages that you install may have additional type annotations packages if they were written in JS, to make them they play nicely with TS. Some packages don’t need it if they were written in TS to begin with.\n\n\nWhen passing props in React, it automatically passes certain default props like children. It would be cumbersome if we had to manually define the types of those default props on every component.\nInstead, we can set the output type of our component as React.FC (Functional Component) and this will handle the default props.\nIf we then want to define our custom prop types, we can do so in angle brackets after:\nReact.FC&lt;{prop1: string, prop2: number}&gt;\nHere we are using a generic type, React.FC. The angled brackets are defining what types are being used in this particular case for this generic type.\nProps are marked as optional by adding a ? after the variable name, i.e. the key in the object.\n\n\n\nThe form submit outputs an event object which can be used by other functions.\nThe type of this can be encapsulated by the React.FormEvent type. Similarly, there is a React.MouseEvent for the onClick listener.\n\n\n\nWe create a ref with useRef then attach it to a component (can be built-in or custom).\nTypeScript doesn’t know which component you intend to attach the ref to, so you need to specify this when creating the ref.\nBy default, useRef returns a generic type, so we need to set the specific type when we call it. We also need to provide a starting value (null) to convince the TypeScript compiler that the ref isn’t already assigned to something else.\nconst inputRef = useRef&lt;HTMLInputElement&gt;(null);\nThen use this in an input element\n&lt;input ref={inputRef} /&gt;\nWhen working with ref.current TypeScript will often demand a ? to indicate that this is possibly null. The resulting value’s type will then be, for example, string or null. If you know it will never be null, you can replace with the ! operator. This means the resulting value will have type string only.\n\n\n\nWhere we pass a function as a prop we define its type as an arrow function specifying inputs and outputs.\nmyFunc: (text: string) =&gt; void\nNote that this is similar to how object types look like an object but don’t actually create an object. Function types look like a function but don’t actually create a function.\nThe .bind method is similar to partial in Python. This is useful when we are passing a function down a prop chain and it will always have a certain argument. Bind saves us having to pass the value and declare its type and every stage of the prop chain.\n\n\n\nThe compilerOptions.target value defines which version of JavaScript the TypeScript compiler will transform the code to.\nThe compilerOptions.lib value defines which TypeScript default types are included out of the box. For example, “DOM” gives support for built-in html types like HTMLInputType.\nIf we want to allow plain JavaScript files in the project alongside TypeScript, we can set compilerOptions.allowJs to True. If False, everything must strictly be TypeScript.\nWe can set a strict compile with compilerOptions.strict. This will forbid implicit any types etc.\n\n\n\nWhen we create a state, we often initialise it with an empty value, e.g. null or an empty array. But then TypeScript does not know what type is going to go in that state later.\nThe useState function returns a generic type so we can overwrite it with our type.\nconst [myState, setMyState] = useState&lt;string[]&gt;([])"
  },
  {
    "objectID": "posts/software/react/30_typescript/post.html#references",
    "href": "posts/software/react/30_typescript/post.html#references",
    "title": "React: TypeScript Essentials",
    "section": "",
    "text": "Section 30 of “React: The Complete Guide” Udemy course\nTypes vs Interfaces"
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html",
    "href": "posts/software/react/2_js_essentials/post.html",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "These notes serve as a JavaScript refresher, and for me a gentle introduction to JavaScript coming from Python.\n\n\n\n\n\nJavascript is supported natively by the browser, so we can add it directly to the html of a webpage.\nWe can add it either in the body of the &lt;script&gt; tag, or preferably as a separate .js file that’s then called as the src parameter of the script tag.\nThe defer parameter means the script won’t be called until the rest of the body is loaded.\nThe type=module parameter means the JavaScript file will be treated as a module rather than executed as a script.\n\n\n\nJSX is not natively supported by the browser, so a build tool transforms it to regular JavaScript.\nIt also minifies the project to optimise the size and loading times.\n\n\n\nWe need to use the export keyword to make a function or variable available outside of that file. Each file can have at most one default export.\nThe import keyword then lets us use this.\nUse curly braces for the import unless it is a default export. If it is a default export, you assign your own name to the imported variable. The path to import from is in single or double quotes, with the file extension in plain JS. In React, some build tools automatically populate the file extension so you don’t need it.\nWe can group the imports if there are many by using starred imports.\nimport * as utils from “./utils.js”;\nThen use utils.blah to use those imported values.\nWe can also alias individual imported variables with the as keyword.\n\n\n\nThe primitives in JavaScript are: string, number, boolean, null, undefined.\nThe are also complex types built in: object, array.\nVariables are defined with the let keyword. Camel case is most common in JS.\nConstants are defined with the const keyword. They cannot be reassigned. Prefer const where it is appropriate, to be clear about your intentions that this should not be reassigned.\nOlder versions of JavaScript did not make this distinction and used var in all cases. This is discouraged now.\n\n\n\nThese include add, subtract, divide, multiply.\nThese can be defined on any types, not just numbers.\nTriple equals === is used to compare values.\n\n\n\nFunctions can be defined using “regular” syntax or “arrow” syntax.\nRegular syntax:\nfunction sum(a, b) {\n    return a + b;\n}\nArrow function syntax:\nconst sum = (a, b) =&gt; {a + b};\nThe function can then be invoked as\nsum(1, 3)\nWe can set default values of variables as\nconst sum = (a, b = 1) =&gt; {a + b};\nFunctions can be passed as props to other functions. This is helpful when defining components which we want to pass state setters or other handler functions to (functional components are ultimately just functions themselves).\nWe can also define functions inside of other functions. This is helpful when we want the function to be scoped only to the outer function, not defined globally. This is again used a lot in React since we may want to define functions with our (functional) components.\n\n\n\nObjects are key-value pairs. The value for a given key can be accessed with ., for example:\nobj.key1\nObjects can also have methods. These are functions defined inside the object.\nconst obj = {\n    name: “Gurp”,\n    method1: greet() {return “Hello “ + this.name}\n}\nobj.greet()\nClass instances are essentially objects like above. If we want to create a reusable class, we can formally define a class.\nclass User (\n    constructor(name, age) {\n        this.name = name;\n        this.age = age;\n    }\n\n    greet() {\n        return “Hello “ + this.name;\n    }\n)\n\nconst user1 = new User(\"Gurp\", 30);\nObjects (and, by extension, arrays) are passed by reference. So when we modify the object, it does not create a new object, it mutates the original. The memory address is stored as a constant, not the value. So if we create an object as a const, we can modify it without reassigning it.\n\n\n\nArrays are technically a special case of object.\nconst array1 = [1, 2, 3, 4];\nThere are some built-in utility methods of arrays that are particularly helpful/common:\n\n\n\nMethod\nExample\nDocs\n\n\n\n\npush\narray1.push(5);\npush docs\n\n\nmap\nconst squares = array1.map((item) =&gt; item * 2);\nmap docs\n\n\nfind\nconst found = array1.find((element) =&gt; element &gt; 3);\nfind docs\n\n\nfindIndex\narr.findIndex((item) =&gt; item===2)\nfindIndex docs\n\n\nfilter\nconst result = array1.filter((item) =&gt; item &gt; 2);\nfilter docs\n\n\nreduce\nconst summedArray = array1.reduce((accumulator, currentValue) =&gt; accumulator + currentValue);\nreduce docs\n\n\nconcat\nconst array3 = array1.concat(array2);\nconcat docs\n\n\nslice\narray1.slice(1,3)  // returns [2, 3]\nslice docs\n\n\nsplice\nmonths.splice(4, 1, 'May');  // Replaces 1 element at index 4\nsplice docs\n\n\n\n\n\n\nArray destructuring allows us to pick out the values of an array rather than assigning them one-by-one.\nconst [firstName, lastName] = [\"Gurp\", \"Johl\"]\ninstead of\nconst nameArray = [\"Gurp\", \"Johl\"]\nconst firstName = nameArray[0]\nconst lastName =  nameArray[1]\nSimilarly, we can destructure objects too. We can also alias the keys with a :, as in the example below where the name key is aliased to userName.\nconst {name: userName, age} = {name: 'Gurp', age: 30}\ninstead of\nconst userObj = {name: 'Gurp', age: 30}\nconst userName = userObj.name\nconst age = userObj.age\n\n\n\nThe spread operator pulls out values of arrays and objects. This is useful for merging multiple arrays, e.g.\nconst arrayA = [1, 2, 3]\nconst arrayB = [4, 5, 6]\nconst mergedArray = [...arrayA, ...arrayB]\nThe same applies to merging objects.\n\n\n\nIf-else clauses work similarly to other languages:\nif userName === \"Gurp\" {\n    // Do something\n} else {\n    // Do something else\n}\nFor loops again are similar, although the syntax looks a bit janky at first compared to Python:\nconst hobbies = [\"Sports\", \"Music\"];\n\nfor (const hobby of hobbies) {\n    console.log(hobby);\n}\n\n\n\n\nMDN docs\nSection 2 of “React: The Complete Guide” Udemy course"
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#adding-javascript-to-a-page",
    "href": "posts/software/react/2_js_essentials/post.html#adding-javascript-to-a-page",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "Javascript is supported natively by the browser, so we can add it directly to the html of a webpage.\nWe can add it either in the body of the &lt;script&gt; tag, or preferably as a separate .js file that’s then called as the src parameter of the script tag.\nThe defer parameter means the script won’t be called until the rest of the body is loaded.\nThe type=module parameter means the JavaScript file will be treated as a module rather than executed as a script."
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#react-projects-use-a-build-process.",
    "href": "posts/software/react/2_js_essentials/post.html#react-projects-use-a-build-process.",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "JSX is not natively supported by the browser, so a build tool transforms it to regular JavaScript.\nIt also minifies the project to optimise the size and loading times."
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#imports-and-exports",
    "href": "posts/software/react/2_js_essentials/post.html#imports-and-exports",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "We need to use the export keyword to make a function or variable available outside of that file. Each file can have at most one default export.\nThe import keyword then lets us use this.\nUse curly braces for the import unless it is a default export. If it is a default export, you assign your own name to the imported variable. The path to import from is in single or double quotes, with the file extension in plain JS. In React, some build tools automatically populate the file extension so you don’t need it.\nWe can group the imports if there are many by using starred imports.\nimport * as utils from “./utils.js”;\nThen use utils.blah to use those imported values.\nWe can also alias individual imported variables with the as keyword."
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#variables",
    "href": "posts/software/react/2_js_essentials/post.html#variables",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "The primitives in JavaScript are: string, number, boolean, null, undefined.\nThe are also complex types built in: object, array.\nVariables are defined with the let keyword. Camel case is most common in JS.\nConstants are defined with the const keyword. They cannot be reassigned. Prefer const where it is appropriate, to be clear about your intentions that this should not be reassigned.\nOlder versions of JavaScript did not make this distinction and used var in all cases. This is discouraged now."
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#operators",
    "href": "posts/software/react/2_js_essentials/post.html#operators",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "These include add, subtract, divide, multiply.\nThese can be defined on any types, not just numbers.\nTriple equals === is used to compare values."
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#functions",
    "href": "posts/software/react/2_js_essentials/post.html#functions",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "Functions can be defined using “regular” syntax or “arrow” syntax.\nRegular syntax:\nfunction sum(a, b) {\n    return a + b;\n}\nArrow function syntax:\nconst sum = (a, b) =&gt; {a + b};\nThe function can then be invoked as\nsum(1, 3)\nWe can set default values of variables as\nconst sum = (a, b = 1) =&gt; {a + b};\nFunctions can be passed as props to other functions. This is helpful when defining components which we want to pass state setters or other handler functions to (functional components are ultimately just functions themselves).\nWe can also define functions inside of other functions. This is helpful when we want the function to be scoped only to the outer function, not defined globally. This is again used a lot in React since we may want to define functions with our (functional) components."
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#objects",
    "href": "posts/software/react/2_js_essentials/post.html#objects",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "Objects are key-value pairs. The value for a given key can be accessed with ., for example:\nobj.key1\nObjects can also have methods. These are functions defined inside the object.\nconst obj = {\n    name: “Gurp”,\n    method1: greet() {return “Hello “ + this.name}\n}\nobj.greet()\nClass instances are essentially objects like above. If we want to create a reusable class, we can formally define a class.\nclass User (\n    constructor(name, age) {\n        this.name = name;\n        this.age = age;\n    }\n\n    greet() {\n        return “Hello “ + this.name;\n    }\n)\n\nconst user1 = new User(\"Gurp\", 30);\nObjects (and, by extension, arrays) are passed by reference. So when we modify the object, it does not create a new object, it mutates the original. The memory address is stored as a constant, not the value. So if we create an object as a const, we can modify it without reassigning it."
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#arrays",
    "href": "posts/software/react/2_js_essentials/post.html#arrays",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "Arrays are technically a special case of object.\nconst array1 = [1, 2, 3, 4];\nThere are some built-in utility methods of arrays that are particularly helpful/common:\n\n\n\nMethod\nExample\nDocs\n\n\n\n\npush\narray1.push(5);\npush docs\n\n\nmap\nconst squares = array1.map((item) =&gt; item * 2);\nmap docs\n\n\nfind\nconst found = array1.find((element) =&gt; element &gt; 3);\nfind docs\n\n\nfindIndex\narr.findIndex((item) =&gt; item===2)\nfindIndex docs\n\n\nfilter\nconst result = array1.filter((item) =&gt; item &gt; 2);\nfilter docs\n\n\nreduce\nconst summedArray = array1.reduce((accumulator, currentValue) =&gt; accumulator + currentValue);\nreduce docs\n\n\nconcat\nconst array3 = array1.concat(array2);\nconcat docs\n\n\nslice\narray1.slice(1,3)  // returns [2, 3]\nslice docs\n\n\nsplice\nmonths.splice(4, 1, 'May');  // Replaces 1 element at index 4\nsplice docs"
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#destructuring",
    "href": "posts/software/react/2_js_essentials/post.html#destructuring",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "Array destructuring allows us to pick out the values of an array rather than assigning them one-by-one.\nconst [firstName, lastName] = [\"Gurp\", \"Johl\"]\ninstead of\nconst nameArray = [\"Gurp\", \"Johl\"]\nconst firstName = nameArray[0]\nconst lastName =  nameArray[1]\nSimilarly, we can destructure objects too. We can also alias the keys with a :, as in the example below where the name key is aliased to userName.\nconst {name: userName, age} = {name: 'Gurp', age: 30}\ninstead of\nconst userObj = {name: 'Gurp', age: 30}\nconst userName = userObj.name\nconst age = userObj.age"
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#spread-operator",
    "href": "posts/software/react/2_js_essentials/post.html#spread-operator",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "The spread operator pulls out values of arrays and objects. This is useful for merging multiple arrays, e.g.\nconst arrayA = [1, 2, 3]\nconst arrayB = [4, 5, 6]\nconst mergedArray = [...arrayA, ...arrayB]\nThe same applies to merging objects."
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#control-structures",
    "href": "posts/software/react/2_js_essentials/post.html#control-structures",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "If-else clauses work similarly to other languages:\nif userName === \"Gurp\" {\n    // Do something\n} else {\n    // Do something else\n}\nFor loops again are similar, although the syntax looks a bit janky at first compared to Python:\nconst hobbies = [\"Sports\", \"Music\"];\n\nfor (const hobby of hobbies) {\n    console.log(hobby);\n}"
  },
  {
    "objectID": "posts/software/react/2_js_essentials/post.html#references",
    "href": "posts/software/react/2_js_essentials/post.html#references",
    "title": "React: JavaScript Essentials",
    "section": "",
    "text": "MDN docs\nSection 2 of “React: The Complete Guide” Udemy course"
  },
  {
    "objectID": "posts/software/react/6_styling/post.html",
    "href": "posts/software/react/6_styling/post.html",
    "title": "React: Styling",
    "section": "",
    "text": "First stylin’, then profilin’. Woo!\n\n\nThere are several options of styling in React:\n\nVanilla CSS with separate modules\nIn-line CSS styles\nScoped CSS with modules\nCSS-in-React with styled components\nTailwind CSS\n\nWe will look at each in turn, along with their relative merits.\nIn all cases, styling can be static or dynamic.\n\n\n\nIn plain CSS we can use:\n\nElement selectors - header h1\nID selectors - #auth-inputs\nClass selectors - .controls\n\nImport the .css file into the JSX file you want to style. You can have multiple css files, placing each next to the component JSX file it relates to.\nAdvantages:\n\nDecouples styling from JSX.\nCSS can be modified independently of the code logic, when working with multiple developers .\nMore developers will be familiar with plain CSS.\n\nDisadvantages:\n\nStyles are not scoped to components, which can lead to clashes and unexpected behaviour. The CSS just gets injected into the styles part of the page by the build process, so they apply globally.\n\nDynamic styling can be achieved by having two different class names and conditionally switching between them with a ternary expression.\n\n\n\nYou can set the style prop of each component directly.\nAdvantages:\n\nQuick to add\nStyles are scoped to the component\nDynamic styling is easy\n\nDisadvantages:\n\nYou have to style each component individually\nNo separation between CSS and JSX code\n\n\n\n\nFile-specific scoping for CSS classes.\nUsing .module in the css file name, e.g. Header.module.css will scope the styles to the file that it’s imported into.\nThe import is done slightly differently as it now returns a JavaScript object.\nimport { styles } from ‘Header.module.css’\nThis module approach is not supported natively by browsers. Instead, the build tool takes each of your classes and renames it to ensure it is unique per file. These transformed styles are what you see in the rendered DOM.\nConditional switching for dynamic styles works the same as vanilla CSS.\nAdvantages:\n\nDecouple CSS from JS\nCSS classes are scoped to file\n\nDisadvantages:\n\nBigger projects end up with many small CSS module files\n\n\n\n\nInstall this with\nnpm install styled-components\nThis keeps components and styles linked as a combined object that can be reused in multiple places.\nUse backticks to define a style template :\nconst Container = styled.div`\n    Styling goes here\n`\nThis creates a regular div under the hood. Any props passed to it will get forwarded to the underlying component so we can use it like normal.\nWe can pass functions in between the backticks for dynamic styling. Any props passed to the component are forwarded, so we can use these in functions to set styles.\nconst Label = styled.label`\n    display: block;\n    margin-bottom: 0.5rem;\n    color: ${({invalid}) =&gt; invalid ? 'red' : 'green'}\n`\nA common convention is to name any props used only for styling with a $ at the start to ensure they don’t clash with any other built-in props of the component.\n\n\n\nThis is another 3rd-party framework. The idea is you don’t need to know CSS. Instead you apply small, pre-defined utility classes to each component to achieve a style, and these abstract away a lot of the CSS styles.\nUse a VSCode tailwind plugin to get autocomplete suggestions.\nMedia queries can be used to apply different styles depending on the screen size. E.g. md:, hover:, etc\nAdvantages:\n\nDon’t need to know CSS.\nStyles are scoped to the component.\nConfigurable and extensible.\nRapid development (once you know the built-in class names).\n\nDisadvantages:\n\nLong class names.\nChanging styling requires modifying JSX.\nYou end up with lots of small wrapper components or lots of copy paste.\nYou need to learn the built-in class names.\n\n\n\n\n\nSection 6 of “React: The Complete Guide” Udemy course"
  },
  {
    "objectID": "posts/software/react/6_styling/post.html#styling-options-in-react",
    "href": "posts/software/react/6_styling/post.html#styling-options-in-react",
    "title": "React: Styling",
    "section": "",
    "text": "There are several options of styling in React:\n\nVanilla CSS with separate modules\nIn-line CSS styles\nScoped CSS with modules\nCSS-in-React with styled components\nTailwind CSS\n\nWe will look at each in turn, along with their relative merits.\nIn all cases, styling can be static or dynamic."
  },
  {
    "objectID": "posts/software/react/6_styling/post.html#vanilla-css",
    "href": "posts/software/react/6_styling/post.html#vanilla-css",
    "title": "React: Styling",
    "section": "",
    "text": "In plain CSS we can use:\n\nElement selectors - header h1\nID selectors - #auth-inputs\nClass selectors - .controls\n\nImport the .css file into the JSX file you want to style. You can have multiple css files, placing each next to the component JSX file it relates to.\nAdvantages:\n\nDecouples styling from JSX.\nCSS can be modified independently of the code logic, when working with multiple developers .\nMore developers will be familiar with plain CSS.\n\nDisadvantages:\n\nStyles are not scoped to components, which can lead to clashes and unexpected behaviour. The CSS just gets injected into the styles part of the page by the build process, so they apply globally.\n\nDynamic styling can be achieved by having two different class names and conditionally switching between them with a ternary expression."
  },
  {
    "objectID": "posts/software/react/6_styling/post.html#inline-styles",
    "href": "posts/software/react/6_styling/post.html#inline-styles",
    "title": "React: Styling",
    "section": "",
    "text": "You can set the style prop of each component directly.\nAdvantages:\n\nQuick to add\nStyles are scoped to the component\nDynamic styling is easy\n\nDisadvantages:\n\nYou have to style each component individually\nNo separation between CSS and JSX code"
  },
  {
    "objectID": "posts/software/react/6_styling/post.html#css-modules",
    "href": "posts/software/react/6_styling/post.html#css-modules",
    "title": "React: Styling",
    "section": "",
    "text": "File-specific scoping for CSS classes.\nUsing .module in the css file name, e.g. Header.module.css will scope the styles to the file that it’s imported into.\nThe import is done slightly differently as it now returns a JavaScript object.\nimport { styles } from ‘Header.module.css’\nThis module approach is not supported natively by browsers. Instead, the build tool takes each of your classes and renames it to ensure it is unique per file. These transformed styles are what you see in the rendered DOM.\nConditional switching for dynamic styles works the same as vanilla CSS.\nAdvantages:\n\nDecouple CSS from JS\nCSS classes are scoped to file\n\nDisadvantages:\n\nBigger projects end up with many small CSS module files"
  },
  {
    "objectID": "posts/software/react/6_styling/post.html#styled-components-third-party-library",
    "href": "posts/software/react/6_styling/post.html#styled-components-third-party-library",
    "title": "React: Styling",
    "section": "",
    "text": "Install this with\nnpm install styled-components\nThis keeps components and styles linked as a combined object that can be reused in multiple places.\nUse backticks to define a style template :\nconst Container = styled.div`\n    Styling goes here\n`\nThis creates a regular div under the hood. Any props passed to it will get forwarded to the underlying component so we can use it like normal.\nWe can pass functions in between the backticks for dynamic styling. Any props passed to the component are forwarded, so we can use these in functions to set styles.\nconst Label = styled.label`\n    display: block;\n    margin-bottom: 0.5rem;\n    color: ${({invalid}) =&gt; invalid ? 'red' : 'green'}\n`\nA common convention is to name any props used only for styling with a $ at the start to ensure they don’t clash with any other built-in props of the component."
  },
  {
    "objectID": "posts/software/react/6_styling/post.html#tailwind-css",
    "href": "posts/software/react/6_styling/post.html#tailwind-css",
    "title": "React: Styling",
    "section": "",
    "text": "This is another 3rd-party framework. The idea is you don’t need to know CSS. Instead you apply small, pre-defined utility classes to each component to achieve a style, and these abstract away a lot of the CSS styles.\nUse a VSCode tailwind plugin to get autocomplete suggestions.\nMedia queries can be used to apply different styles depending on the screen size. E.g. md:, hover:, etc\nAdvantages:\n\nDon’t need to know CSS.\nStyles are scoped to the component.\nConfigurable and extensible.\nRapid development (once you know the built-in class names).\n\nDisadvantages:\n\nLong class names.\nChanging styling requires modifying JSX.\nYou end up with lots of small wrapper components or lots of copy paste.\nYou need to learn the built-in class names."
  },
  {
    "objectID": "posts/software/react/6_styling/post.html#references",
    "href": "posts/software/react/6_styling/post.html#references",
    "title": "React: Styling",
    "section": "",
    "text": "Section 6 of “React: The Complete Guide” Udemy course"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html",
    "href": "posts/software/system_design/system_design_notes.html",
    "title": "System Design Notes",
    "section": "",
    "text": "Steps:\n\nRequirements engineering\nCapacity estimation\nData modeling\nAPI design\nSystem design\nDesign discussion\n\n\n\nFunctional and non-functional requirements. Scale of the system.\n\n\nWhat the system should do.\n\nWhich problem does the system solve\nWhich features are essential to solve these problems\n\nCore features: bare minimum required to solve the user’s problem. Support features: features that help the user solve their problem more conveniently.\n\n\n\nWhat the system should handle.\n\n\n\n\n\n\nInterview Tips\n\n\n\n\nIdentify the non-functional requirements the interviewer cares most about\nShow awareness of system attributes, trade-offs and user experience\n\n\n\nSystem analysis:\n\nRead or write heavy?\n\nImpacts requirements for database scaling, redundancy of services, effectiveness of caching.\nSystem priorities - is it worse if the system fails to read or to write?\n\nMonolithic or distributed architecture?\n\nScale is the key consideration. Large scale applications must be distributed, smaller scale can be single server.\n\nData availability vs consistency\n\nCAP trilemma - A system can have only two of: Consistency, Availability, Partition tolerance\nChoice determines the impact of a network failure\n\n\nNon-functional requirements:\n\nAvailability - How long is the system up and running per year?\n\nAvailability of a system is the product of its components’ availability\nReliability, redundancy, fault tolerance\n\nConsistency - Data appears the same on all nodes regardless of the user and location.\n\nLinearizability\n\nScalability - Ability of a system to handle fast-growing user base and temporary load spikes\n\nVertical scaling - single server that we add more CPU/RAM to\n\nPros: fast inter-process communication, data consistency\nCons: single point of failure, hardware limits on maximum scale\n\nHorizontal scaling - cluster of servers in parallel\n\nPros: redundancy improves availability, linear cost of scaling\nCons: complex architecture, data inconsistency\n\n\nLatency - Amount of time taken for a single message to be delivered\n\nExperienced latency = network latency + system latency\nDatabase and data model choices affect latency\nCaching can be effective\n\nCompatibility - Ability of a system to operate seamlessly with other software, hardware and systems\nSecurity\n\nBasic security measures: TLS encrypted network traffic, API keys for rate limiting\n\n\nNon-functional requirements depend heavily on expected scale. The following help quantify expected scale, and relate to capacity estimation in the next step.\n\nDaily active users (DAU)\nPeak active users\nInteractions per user - including read/write ratio\nRequest size\nReplication factor - determines the storage requirement (typically 3x)\n\n\n\n\n\n\n\n\n\n\n\nInterview Tips\n\n\n\n\nSimplify wherever you can - simple assumptions and round heavily\nConvert all numbers to scientific notation\nKnow powers of 2\n\n\n\n\n\nRequests per second (RPS) is the key measure.\nRequests per day = Daily active users * Activities per user\nRequests per second = RPD / 10^5\nPeak load = RPS * Peak load factor\nRead/write ratio is important to get the total requests. Usually, information on either reads or writes is missing and must be inferred using the other.\nTotal RPS = Read RPS + Write RPS\n\n\n\nThe amount of data that can be transmitted between two nodes in a fixed period of time. Bits per second.\nThroughput, bandwidth and latency are all related and can be thought of with the motorway analogy.\n\nThroughput is the total number of vehicles that must pass through that road per day.\nBandwidth is the number of lanes.\nLatency is the time taken to get from one end of the road to another.\n\nThe bandwidth must be large enough to support the required throughput.\nBandwidth = Total RPS * Request size\nEven high bandwidth can result in low latency if there is an unexpectedly high peak throughput. Rush hour traffic jam.\nRequest size can be varied according to bandwidth, e.g. YouTube lowers the resolution if the connection is slow.\n\n\n\nMeasured in bits per second using the Write RPS. Long term storage (assuming the same user base) is also helpful.\nStorage capacity per second = Write RPS * Request size * Replication factor\nStorage capacity in 5 years = Storage capacity per second * 2*10^8 \n\n\n\n\nKey concepts are: entities, attributes, relations.\nOptimisation discussions are anchored by the data model.\nRelational databases: denormalization, SQL tuning, sharding, federation\nNon-relational databases: indexing, caching\nThe data model is not a full-blown data schema (which only applies to relational databases) but a more informal, high-level version.\nMust haves:\n\nDerive entities and attributes.\nDraw connections between entities. Then:\nSelect a database based on the requirements\nExtend data model to a proper data schema\nIdentify bottlenecks and apply quick fixes Later:\nDefer any detailed discussions to the design discussion section.\n\n\n\nEntities are “things” with a distinct and independent existence, e.g. users, files, tweets, posts, channels. Create a unique table for each entity.\nAttributes describe properties of an entity, e.g. a user’s name, date of birth. These are the fields in the table of that entity.\n\n\n\n\n\n\nInterview Tips\n\n\n\n\nDo not focus on irrelevant details\nDo not miss any critical functionality\nDouble check all tables after you extract all info from requirements\n\n\n\n\n\n\nExtract connections between entities from the requirements. Relationships can be one-to-one, one-to-many or many-to-many.\n\n\n\n\nSpecify the API endpoints with:\n\nFunction signature\nParameters\nResponse and status codes\n\nThis specifies a binding contract between the client and the application server. There can be APIs between every system component, but just focus on the client/server interface.\nProcess:\n\nRevisit the functional requirements\nDerive the goal of each endpoint\nDerive the signature from the goal and the inputs from the data model\nDefine the response outputs\n\nNaming conventions. Use HTTP request types in the call signature where appropriate: GET, POST, PUT, PATCH, DELETE\n\n\n\n\n\n\nInterview Tips\n\n\n\n\nAvoid vague inputs\nDon’t add extra parameters that are out of scope\nAvoid redundant parameters\nDon’t miss any parameters\nConfirm the output repsonse satisfies the requirements\nIdentify inefficiencies of the data structure\n\n\n\n\n\n\nSystem components:\n\nRepresentation layer\n\nWeb app\nMobile app\nConsole\n\nService (and load balancer)\nData store\n\nRelational database\n\nPros: linked tables reduce duplicate data; flexible queries\n\nKey-value store\n\nUseful for lots of small continuous reads and writes\nPros: Fast access, lightweight, highly scalable\nCons: Cannot query by value, no standard query language so data access in written in the application layer no data normalisation\n\n\n\nScaling:\n\nScale services by having multiple instances of the service with a load balancer in front\nScale database with federation (functional partitioning) - split up data by function\n\nSystem diagrams: arrows point in direction of user flow (not data flow)\n\n\n\nTypes of questions:\n\nNFR questions\n\nHow to achieve scalability\n\nIdentify each bottleneck and remedy it\n\nHow to improve resiliency\n\nJustification question\n\nChoice of components, e.g. what would change if you changed block storage for object storage or relational database\nAdditional components, e.g. is there a use case for a message queue and where\n\nExtension questions\n\nAdditional functional requirements not in the original scope\n\n\n\n\n\n\n\nThere are two parameters we can vary:\n\nLength of the key - should be limited so the URL is short enough\nRange of allowed characters - should be URL-safe\n\nUsing Base-64 encoding as the character range, then a 6-digit key length gives enough unique URLs; 64^6 = 68 billion\nEncoding options: 1. MD5 hash: not collision resistant and too long 2. Encoded counter, each URL is just an index int, and its value is the base-64 encoding of that int: length is not fixed and increases over time 3. Key range, generate all unique keys beforehand: no collisions, but storage intensive 4. Key range modified, generate 5-digit keys beforehand, then add the 6th digit on the fly.\n\n\n\n\n\n4 main operations (CRUD): Create, Read, Update, Delete.\nTransactions - group several operations together\n\nEither the entire transaction succeeds or it fails and rolls back to the initial state. Commit or abort on error.\nWithout transactions, if an individual operation fails, it can be complex to unwind the related transactions to return to the initial state.\nError handling is simpler as manual rollback of operations is not required.\nTransactions provide guarantees so we can reason about the database state before and after the transaction.\n\nTransactions enforce “ACID” guarantees:\n\nAtomicity - Transactions cannot be broken down into smaller parts.\nConsistency - All data points within the database must align to be properly read and accepted. Raise a consistency error if this is not the case. Database consistency is different from system consistency which states the data should be the same across all nodes.\nIsolation - Concurrently executing transactions are isolated from each other. Avoids race conditions if multiple clients are accessing the same database record.\nDurability - Once a transaction is committed to the database, it is permanently preserved. Backups and transaction logs can restore committed transactions in the case of a failure.\n\nRelational databases are a good fit when:\n\nData is well-structured\nUse case requires complex querying\nData consistency is important\n\nLimitations of relational databases:\n\nHorizontal scaling is complex\nDistributed databases are more complex to keep transaction guarantees\n\nTwo phase commit protocol (2PC):\n\nPrepare - Ask each node if it;s able to promise to carry out the transaction\nCommit - Block the nodes and do the commit\n\nBlocking causes problems on distributed systems where it may cause unexpected consequences if the database is unavailable for this short time.\n\n\n\n\n\nExamples of non-relational databases:\n\nKey-value store\nDocument store\nWide-column store\nGraph store\n\nThese vary a lot, and in general NoSQL simply means “no ACID guarantees”.\nTransactions were seen as the enemy of scalability, so needed to be abandoned completely for performance and scalability. ACID enforces consistency for relational databases; for non-relational databases there is eventual consistency.\nBASE:\n\nBasically Available - Always possible to read and write data even though it may not be consistent. E.g. reads may not reflect latest changes, writes may not be persisted.\nSoft state - Lack of consistency guarantees mean data state may change without any interactions with the application as the database reaches eventual consistency.\nEventual consistency - Data will eventually become consistent once inputs stops.\n\nBenefits:\n\nWithout atomicity constraint, overheads like 2PC are not required\nWithout consistency constraint, horizontal scaling is trivial\nWithout isolation constraint, no blocking is required which improves availability.\n\nNon-relational databases are a good fit when:\n\nLarge data volume that isn’t tabular\nHigh availability requirement\nLack of consistency across nodes is acceptable\n\nLimitations:\n\nConsistency is necessary for some use cases\nLack of standardisation\n\n\n\n\n\nProblem: if we have large files with small changes, we don’t want to have to re-upload and re-download the whole file every time something changes.\nSolution: identify the changes and only push/pull these. Similar to git.\nThe rsync algorithm makes it easier to compare changes by:\n\nSplit the file into blocks\nCalculate a checksum for each block (using MD5 hashing algorithm)\nTo compare files between the client and the server, we only need to send the hashes back and forth\nFor the mismatching hashes, transfer the corresponding blocks\n\n\n\n\nOne-way communication that broadcasts file updates one-to-many.\n\n\nThese are synchronous.\nPolling is an example of a pull API. We periodically query the server to see if there are any updates. If not, the server responds immediately saying there is no new data. This may result in delayed updates and can overwhelm the server with too many requests.\n\n\n\n\n\n\nFigure 1: Polling\n\n\n\n\n\n\nThese are asynchronous.\nLong-polling. The client connects to the server and makes a request. Instead of replying immediately, the server waits until there is an update and then sends the response. If there is no update for a long time, the connection times out and the client must reconnect. Server resources are tied up until the connection is closed, even if there is no update.\n\n\n\n\n\n\nFigure 2: LongPolling\n\n\n\nWebsockets. The client establishes a connection using an HTTP request and response. This establishes a TCP/IP connection for 2-way communication. This allows for real-time applications without long-polling.\n\n\n\n\n\n\nFigure 3: Web Sockets\n\n\n\nServer sent events. Event-based approach. EventSource API supported by all browsers. The connection is 1-directional; the client can only pass data to the server at the point of connection. After that, only the server can send data to the client. The server doesn’t know if the client loses connection.\n\n\n\n\n\n\nFigure 4: SSE\n\n\n\n\n\n\n\nA system component that implements asynchronous communication between services, decoupling them. An independent server which producers and consumers connect to. AKA message queues.\nMessage brokers persist the messages until there are consumers ready to receive them.\nMessage brokers are similar in principle to databases since they persist data. The differences are they: automatically delete messages after delivery, do not support queries, typically have a small working set, and notify clients about changes.\nRabbitMQ, Kafka and Redis are open source message brokers. Amazon, GCP and Azure have managed service implementations.\n\n\nA one-to-one relationship between sender and receiver. Each message is consumed by exactly one receiver.\nThe message broker serves as an abstraction layer. The producer only sends to the broker. The receiver only receives from the broker. The services do not need to know about one another. The broker also guarantees delivery, so the message does not get lost if the consumer is unavailable, it will retry.\n\n\n\n\n\n\nFigure 5: Point-to-Point\n\n\n\n\n\n\nA one-to-many relationship between sender and receiver. The publisher publishes to a certain topic. Any consumer who is interested can then subscribe to receive that message.\n\n\n\n\n\n\nFigure 6: Pub-Sub\n\n\n\n\n\n\n\n\n\nFiles are stored in folders, with metadata about creation time and modification time.\nBenefits:\n\nSimplicity - simple and well-known pattern\nCompatibility - works with most applications and OSes\nLimitations:\nPerformance degradation - as size increases, the resource demands increase.\nExpensive - although cheap in principle, they can get expensive when trying to work around the performance issues.\n\nUse cases: data protection, local archiving, data retrieval done by users.\n\n\n\nA system component that manages data as objects in a flat structure. An object contains the file and its metadata.\nBenefits:\n\nHorizontal scalability\n\nLimitations:\n\nObjects must be edited as a unit - degrades performance if only a small part of the file needs to be updated.\n\nUse cases: large amounts of unstructured data.\n\n\n\nA system component that breaks up and then stores data as fixed-size blocks, each with a unique identifier.\nBenefits:\n\nHighly structured - easy to index, search and retrieve blocks\nLow data transfer overheads\nSupports frequent data writes without performance degradation\nLow latency\n\nLimitations:\n\nNo metadata, so metadata must be handled manually\nExpensive\n\n\n\n\nA subclass of key-value stores, which hold computer-readable documents, like XML or JSON objects.\n\n\n\n\nIssues:\n\nMassive files: 4TB for 4 hours of 4K video\n\nHow can we reliably upload very large files?\n\nLots of different end-user clients/devices/connection speeds\n\nHow can we process and store a range of versions/resolutions?\n\n\nProcessing pipeline:\n\n\n\n\n\n\nFigure 7: Video Processing Pipeline\n\n\n\n\nFile chunker\n\nSame principle as the file-sharing problem.\nSplit the file into chunks, then use checksums to determine that files are present and correct.\nIf there is a network outage, we only need to send the remaining chunks.\n\nContent filter\n\nCheck if the video complies with the platform’s content policy wrt copyright, piracy, NSFW\n\nTranscoder\n\nData is decoded to an uncompressed format\nThis will fan out in the next step to create multiple optimised versions of the file\n\nQuality conversion\n\nConvert the uncompressed file into multiple different resolution options.\n\n\nArchitecture considerations:\n\nWait for the full file to upload before processing? Or process each chunk as it is uploaded?\nAn upload service persists chunks to a database. Object store is a good choice as in the file-sharing example.\nOnce a chunk is ready to be processed, it can be read and placed in the message queue for the processing pipeline.\nThe processing pipeline handles fixed-size chunks, so the hardware requirements are known ahead of time. A chunk can be processed as soon as a hardware unit becomes available.\n\n\n\n\n\n\n\nFigure 8: Video Upload Architecture\n\n\n\n\n\n\nThe challenges are similar to that of file-sharing, as this is essentially large files being transferred. Latency and processing power of the end-user device.\nTwo main transfer protocols:\n\nUDP\nTCP\n\n\n\nA connection-less protocol. Nodes do NOT establish a stable end-to-end connection before sending data. Instead, each packet has its destination address attached so the network can route it to the correct place.\nAdvantages:\n\nFast\nPackets are routed independently so if some are lost the others can still reach their destination\n\nDisadvantages:\n\nNo guarantee that data will arrive intact.\n\nUse cases:\n\nLow-latency applications like video conferencing or gaming.\nNot suitable for movie or audio streaming, as missing bits cannot be tolerated.\n\n\n\n\nA connection is established between 2 nodes. The nodes agree on certain parameters before any data is transferred. Parameters: IP addresses of source and destination, port numbers of source and destination.\nThree-way handshake establishes the connection:\n\nSender transfers their sequence number to the Receiver.\nReceiver acknowledges and sends its own sequence number.\nSender acknowledges.\n\nAdvantages:\n\nReliability; guarantees delivery and receives acknowledgements before further packets are sent.\nReceiver acknowledgements ensure the receiver is able to process/buffer the data in time before more packets are sent.\n\nDisadvantages:\n\nSlower due to error checking and resending lost packets.\nRequires three-way handshake to establish connection which is slower.\n\nUse cases:\n\nMedia streaming (HTTP live-streaming or MPEG-DASH)\n\nAdaptive bitrate streaming\n\nTCP adjusts transmission speed in response to network conditions\nWhen packet loss is detected, smaller chunks are sent at a lower transmission speed. The lower resolution file can be sent in this case.\n\n\n\n\n\nPerceived latency = Network latency + System latency\n\n\nCaching is the process of storing copies of frequently accessed data in a temporary storage location.\nCaches utilise the difference in read performance between memory and storage. Times to fetch 1MB from:\n\nHDD: 3 ms\nSSD: 0.2 ms\nRAM: 0.01 ms\n\nWhen a user requests data it first requests from the cache\n\nCache HIT: If the data is in the cache, return it\nCache MISS: If the data is not in the cache, pass the request to the database, update the cache and return the data\n\nA cache can grow stale over time. There are 2 common approaches to mitigate this:\n\nSet up a time-to-live (TTL) policy within the cache\n\n\nTrade off between short TTL (fresh data but worse performance) vs long TTL (data can be stale)\n\n\nImplement active cache invalidation mechanism\n\n\nComplicated to implement. Requires an “invalidation service” component to monitor and read from the database, then update the cache when necessary.\n\nApplication-level caching. Insert caching logic into the application’s source code to temporarily store data in memory.\nTwo approaches to application-level caching:\n\nQuery-based implementation\n\nHash the query as a key and store the value against the key in a key-value store.\nLimitations: hard to delete cached result with a complex query; if a cell changes then all cached query which might include it need updating.\n\nObject-based implementation\n\nThe result of a query is stored as an object\nBenefits: only one serialisation/deserialisation overhead when reading/writing; complexity of object doesn’t matter, serialized objects in cache can be used by multiple applications.\n\n\nPopular implementations: Redis, Memcache, Hazlecast\n\n\n\nA network of caching layer in different locations (Points of Presence, PoPs). Full data is stored on SSD and most frequently accessed data is stored in RAM.\nDon’t cache dynamic or time-sensitive data.\nInvalidating the cache. You can manually update the CDN, but there may be additional caches at the ISP- and browser-level which would still serve stale data.\n\n\n\n\n\n\nFigure 9: CDN Invalidation\n\n\n\nApproaches to invalidating cache:\n\nCaching headers - set a time when an object should be cleared, e.g. max-age=86400 caches for a day.\nCache busting - change the link to all files and assets, so the CDN treats them like new files.\n\n\n\n\n\nThese are specialised NoSQL databases with the following benefits:\n\nCan match even with typos or non-exact matches\nFull-text search means you can suggest autocomplete results and related queries as the user types\nIndexing allows faster search performance on big data.\n\nThe database is structures as a hashmap where the inverted index points at documents.\n\nDocument - Represents a specific entity, like a row in a relational database\nInverted index - Maps from content to the documents that include it. The inverted index splits each document into individual search terms and makes each point to the document itself.\nRanking algorithm - Produces a list of possibly relevant documents. Additional factors may impact this, like a user’s previous search history.\n\nThe database is a dictionary of {search_term_id: [document_ids, …]}\n\n\n\n\n\n\nFigure 10: Inverted Index\n\n\n\nPopular implementations: Lucene, Elastic search, Apache solr, Atas search (MongoDB), Redis search.\nBenefits of search engine databases:\n\nScalable - NoSQL so scale horizontally\nSchemaless\nWorks out of the box - just need ot decide upfront what attributes should be searchable.\n\nLimitations:\n\nNo ACID guarantees.\nNot efficient at reading/writing data, so requires another database to manage state.\n\nMaintaining consistency between the main database and the search engine database can be tricky.\n\nSome SQL databases have full-text search so may not require a dedicated search engine database.\n\n\n\n\n\n\n\nAn app that lets a user add and delete items from a todo list.\n\nRepresentation layer is a web app\nMicroservices handle the todo CRUD operations and the user details separately\n\nEach service is scaled horizontally, so a load balancer is placed in front of it\n\nRelational database stores data\n\nThere are two databases, one per service, which is an example of functional partitioning. This means todo service I/O does not interfere with user service I/O and vice versa.\n\n\n\n\n\n\n\n\nFigure 11: To-do App System Design\n\n\n\n\n\n\nTake an input URL and return a unique, shortened URL that redirects to the original.\nPlanned system architecture:\n\nPre-generate all 5 digit keys (1 billion entries rather than 64 billion for 6 digits)\nSystem retrieves 5-difit keys from database\nSystem appends 1 out of 64 characters\nThe system is extendable because if we run out of keys we can append 2 more characters rather than 1\n\n\n\nSystem analysis:\n\nSystem is read heavy - Once a short URL is created it will be read multiple times\nDistributed system as this has to scale\nAvailability &gt; Consistency - not so much of a concern if a link isn’t available to all users at the same time, but they must be unique and lead to their destination.\n\nRequirements engineering:-\nCore feature:\n\nA user can input a URL of arbitrary length and receive a unique short URL of fixed size.\nA user can navigate to a short link and be redirected to the original URL.\n\nSupport features:\n\nA user can see their link history of all created short URLs.\nLifecycle policy - links should expire after a default time span.\n\nNon-functional requirements:\n\nAvailability - the system should be available 99% of the time.\nScalability - the system should support billions of short URLs, and thousands of concurrent users.\nLatency - the system should return redirect from a short URL to the original in under 1 second.\n\nQuestions to capture scale:\n\nDaily active users, and how often do users interact per day\n\n100 million, 1 interaction per day\n\nPeak active users - are there events that lead to traffic spikes?\n\nNo spikes\n\nRead/write ratio\n\n10 to 1\n\nRequest size - how long are the originals URLs typically\n\n200 characters =&gt; 200 Bytes\n\nReplication factor\n\n1x - ignore replication\n\n\nCapacity estimation:-\n\nRequests per second\n\nReads per second = DAU * interactions per day / seconds in day = 10^8 * 1 / 10^5 = 1000 reads/s\nWrites per second = Reads per second / read write ratio = 1000 / 10 = 100 writes/s\nRequests per second = Reads per second + Writes per second = 1000 + 100 = 1100 requests/s\nNo peak loads to consider\n\nBandwidth\n\nBandwidth = Requests per second * Message size\nRead bandwidth = 1000 * 200 Bytes = 200kB/s\nWrite bandwidth = 100 * 200 Bytes = 20 kB/s\n\nStorage\n\nStorage per year = Write bandwidth * seconds per year * Replication factor = 20000 Bytes * (360024365) * 1 = 630 GB\n\n\nData model:-\nIdentify entities, attributes and relationships.\nEntities and attributes:\n\nLinks: Key (used to create short URL), Original URL, Expiry date\nUsers: UserID, Links\nKey ranges: Key range, In use (bool)\n\nRelationships:\n\nUsers own Links\nLinks belong to Key ranges\n\nData stores:\n\nUsers\n\nUser data is typically relational and we rarely want it all returned at once.\nConsistency is important as we want the user to have the same experience regardless of which server handles their log in, and don’t want userIds to clash.\nRelational database.\n\nLinks\n\nNon-functional requirements of low latency and high availability.\nData and relationships are not complex.\nKey-value store.\n\nKey ranges\n\nFavour data consistency as we don’t want to accidentally reuse the same key range.\nFiltering keys by status would help to find available key ranges.\nRelational database.\n\n\nAPI design:-\nEndpoints:\n\ncreateUrl(originalUrl: str) -&gt; shortUrl\n\nresponse code 200, {shortURL: str}\n\ngetLinkHistory(userId: str, sorting: {‘asc’, ‘desc’})\n\nresponse code 200, {links: [str], order: ‘asc’}\n\nredirectURL(shortUrl: str) -&gt; originalUrl\n\nresponse code 200, {originalUrl: str}\n\n\nSystem design:-\nUser flows for each of the required functional requirements.\n\n\n\n\n\n\nFigure 12: URL Shortener System Design\n\n\n\n\n\n\n\nRequirements engineering\nRequirements gathering:\n\nRead-heavy\nDistributed\nData consistency is more important than availability - files should be consistent for all users\n\nCore functional requirements:\n\nA user can upload a file to the server\nA user can download their files from the server\n\nSecondary functional requirements:\n\nA user can see a history of files uploaded and downloaded\nA user can see who has downloaded a specific file and when\n\nNon-functional requirements:\n\nConsistency - data should be consistent across users and devices\nResilience - customer data should never be lost\nMinimal latency\nCompatibility across different devices\nSecurity - multi-tenancy; customer data should be separate of one another\n\nQuestions to determine scale:\n\nDaily active users\nPeak active users - what events lead to a spike?\nInteractions per user - how many file syncs, how many files does a user store\nRequest size - how large are files?\nRead/write ratio\nReplication factor\n\nCapacity estimation\nThroughput\n\nPeak write RPS = 2 peak load ratio * 10^8 users * 2 files/day / 10^5 seconds = 4000 RPS\nPeak read RPS = 10 read/write ration * 4000 write RPS = 40000 RPS\nPeak total RPS = 44000 RPS\n\nBandwidth\n\nWrite bandwidth = 4000 Write RPS * 100 kB Request size = 400 MB/s\nRead bandwidth = 40000 Read RPS * 100 kB request size = 4 GB/s\nTotal bandwidth = Write bandwidth + Read bandwidth = 4.4 GB/s\n\nStorage\n\nStorage capacity = 5 * 10^8 Total users * 100 files per user * 1MB File Size * 3 Replication factor = 15*10^10 MB = 15000 TB\n\nData model\nUsers: Store Ids for the files that belong to them\nFiles: Metadata on the owner, file edit history, filename, size, chunks that comprise the file, version history\nAPI Design\nEndpoints:\n\ncompareHashes(fileId: str, chunkHashes: list)\n\nTakes a file ID and a list of chunk hashes and returns a list of the hashes that need resyncing .\nresponse code 200, {syncChunks: [Hash,…]}\n\nuploadChange(fileId: str, chunks: list)\n\nUpload the chunks to resync the file\nresponse code 200, {message: “Chunks uploaded successfully”}\n\nrequestUpdate(fileId: str, chunkHashes: list)\n\nUpdate the local version of the file to match that on the server.\nresponse code 200, {chunks: [Chunk,…]}\n\n\nSystem Design\n\n\n\n\n\n\nFigure 13: Dropbox System Design\n\n\n\nDesign Discussion\n\nWhat can we do to achieve a scalable design?\n\nIdentify bottlenecks and remedy each.\n\nWhat can we do to achieve resiliency?\n\n\n\n\n\n\nUdemy course\nExcalidraw session\nCapacity estimation cheat sheet\n“Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems” by Martin Kleppmann\nRsync algorithm\nSearch Engines Information Retrieval in Practice book\n\n\n\n\nFigure 1: Polling\nFigure 2: LongPolling\nFigure 3: Web Sockets\nFigure 4: SSE\nFigure 5: Point-to-Point\nFigure 6: Pub-Sub\nFigure 7: Video Processing Pipeline\nFigure 8: Video Upload Architecture\nFigure 9: CDN Invalidation\nFigure 10: Inverted Index\nFigure 11: To-do App System Design\nFigure 12: URL Shortener System Design\nFigure 13: Dropbox System Design"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#requirements-engineering",
    "href": "posts/software/system_design/system_design_notes.html#requirements-engineering",
    "title": "System Design Notes",
    "section": "",
    "text": "Functional and non-functional requirements. Scale of the system.\n\n\nWhat the system should do.\n\nWhich problem does the system solve\nWhich features are essential to solve these problems\n\nCore features: bare minimum required to solve the user’s problem. Support features: features that help the user solve their problem more conveniently.\n\n\n\nWhat the system should handle.\n\n\n\n\n\n\nInterview Tips\n\n\n\n\nIdentify the non-functional requirements the interviewer cares most about\nShow awareness of system attributes, trade-offs and user experience\n\n\n\nSystem analysis:\n\nRead or write heavy?\n\nImpacts requirements for database scaling, redundancy of services, effectiveness of caching.\nSystem priorities - is it worse if the system fails to read or to write?\n\nMonolithic or distributed architecture?\n\nScale is the key consideration. Large scale applications must be distributed, smaller scale can be single server.\n\nData availability vs consistency\n\nCAP trilemma - A system can have only two of: Consistency, Availability, Partition tolerance\nChoice determines the impact of a network failure\n\n\nNon-functional requirements:\n\nAvailability - How long is the system up and running per year?\n\nAvailability of a system is the product of its components’ availability\nReliability, redundancy, fault tolerance\n\nConsistency - Data appears the same on all nodes regardless of the user and location.\n\nLinearizability\n\nScalability - Ability of a system to handle fast-growing user base and temporary load spikes\n\nVertical scaling - single server that we add more CPU/RAM to\n\nPros: fast inter-process communication, data consistency\nCons: single point of failure, hardware limits on maximum scale\n\nHorizontal scaling - cluster of servers in parallel\n\nPros: redundancy improves availability, linear cost of scaling\nCons: complex architecture, data inconsistency\n\n\nLatency - Amount of time taken for a single message to be delivered\n\nExperienced latency = network latency + system latency\nDatabase and data model choices affect latency\nCaching can be effective\n\nCompatibility - Ability of a system to operate seamlessly with other software, hardware and systems\nSecurity\n\nBasic security measures: TLS encrypted network traffic, API keys for rate limiting\n\n\nNon-functional requirements depend heavily on expected scale. The following help quantify expected scale, and relate to capacity estimation in the next step.\n\nDaily active users (DAU)\nPeak active users\nInteractions per user - including read/write ratio\nRequest size\nReplication factor - determines the storage requirement (typically 3x)"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#capacity-estimation",
    "href": "posts/software/system_design/system_design_notes.html#capacity-estimation",
    "title": "System Design Notes",
    "section": "",
    "text": "Interview Tips\n\n\n\n\nSimplify wherever you can - simple assumptions and round heavily\nConvert all numbers to scientific notation\nKnow powers of 2\n\n\n\n\n\nRequests per second (RPS) is the key measure.\nRequests per day = Daily active users * Activities per user\nRequests per second = RPD / 10^5\nPeak load = RPS * Peak load factor\nRead/write ratio is important to get the total requests. Usually, information on either reads or writes is missing and must be inferred using the other.\nTotal RPS = Read RPS + Write RPS\n\n\n\nThe amount of data that can be transmitted between two nodes in a fixed period of time. Bits per second.\nThroughput, bandwidth and latency are all related and can be thought of with the motorway analogy.\n\nThroughput is the total number of vehicles that must pass through that road per day.\nBandwidth is the number of lanes.\nLatency is the time taken to get from one end of the road to another.\n\nThe bandwidth must be large enough to support the required throughput.\nBandwidth = Total RPS * Request size\nEven high bandwidth can result in low latency if there is an unexpectedly high peak throughput. Rush hour traffic jam.\nRequest size can be varied according to bandwidth, e.g. YouTube lowers the resolution if the connection is slow.\n\n\n\nMeasured in bits per second using the Write RPS. Long term storage (assuming the same user base) is also helpful.\nStorage capacity per second = Write RPS * Request size * Replication factor\nStorage capacity in 5 years = Storage capacity per second * 2*10^8"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#data-modeling",
    "href": "posts/software/system_design/system_design_notes.html#data-modeling",
    "title": "System Design Notes",
    "section": "",
    "text": "Key concepts are: entities, attributes, relations.\nOptimisation discussions are anchored by the data model.\nRelational databases: denormalization, SQL tuning, sharding, federation\nNon-relational databases: indexing, caching\nThe data model is not a full-blown data schema (which only applies to relational databases) but a more informal, high-level version.\nMust haves:\n\nDerive entities and attributes.\nDraw connections between entities. Then:\nSelect a database based on the requirements\nExtend data model to a proper data schema\nIdentify bottlenecks and apply quick fixes Later:\nDefer any detailed discussions to the design discussion section.\n\n\n\nEntities are “things” with a distinct and independent existence, e.g. users, files, tweets, posts, channels. Create a unique table for each entity.\nAttributes describe properties of an entity, e.g. a user’s name, date of birth. These are the fields in the table of that entity.\n\n\n\n\n\n\nInterview Tips\n\n\n\n\nDo not focus on irrelevant details\nDo not miss any critical functionality\nDouble check all tables after you extract all info from requirements\n\n\n\n\n\n\nExtract connections between entities from the requirements. Relationships can be one-to-one, one-to-many or many-to-many."
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#api-design",
    "href": "posts/software/system_design/system_design_notes.html#api-design",
    "title": "System Design Notes",
    "section": "",
    "text": "Specify the API endpoints with:\n\nFunction signature\nParameters\nResponse and status codes\n\nThis specifies a binding contract between the client and the application server. There can be APIs between every system component, but just focus on the client/server interface.\nProcess:\n\nRevisit the functional requirements\nDerive the goal of each endpoint\nDerive the signature from the goal and the inputs from the data model\nDefine the response outputs\n\nNaming conventions. Use HTTP request types in the call signature where appropriate: GET, POST, PUT, PATCH, DELETE\n\n\n\n\n\n\nInterview Tips\n\n\n\n\nAvoid vague inputs\nDon’t add extra parameters that are out of scope\nAvoid redundant parameters\nDon’t miss any parameters\nConfirm the output repsonse satisfies the requirements\nIdentify inefficiencies of the data structure"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#system-design-1",
    "href": "posts/software/system_design/system_design_notes.html#system-design-1",
    "title": "System Design Notes",
    "section": "",
    "text": "System components:\n\nRepresentation layer\n\nWeb app\nMobile app\nConsole\n\nService (and load balancer)\nData store\n\nRelational database\n\nPros: linked tables reduce duplicate data; flexible queries\n\nKey-value store\n\nUseful for lots of small continuous reads and writes\nPros: Fast access, lightweight, highly scalable\nCons: Cannot query by value, no standard query language so data access in written in the application layer no data normalisation\n\n\n\nScaling:\n\nScale services by having multiple instances of the service with a load balancer in front\nScale database with federation (functional partitioning) - split up data by function\n\nSystem diagrams: arrows point in direction of user flow (not data flow)"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#design-discussion",
    "href": "posts/software/system_design/system_design_notes.html#design-discussion",
    "title": "System Design Notes",
    "section": "",
    "text": "Types of questions:\n\nNFR questions\n\nHow to achieve scalability\n\nIdentify each bottleneck and remedy it\n\nHow to improve resiliency\n\nJustification question\n\nChoice of components, e.g. what would change if you changed block storage for object storage or relational database\nAdditional components, e.g. is there a use case for a message queue and where\n\nExtension questions\n\nAdditional functional requirements not in the original scope"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#system-components-deep-dive",
    "href": "posts/software/system_design/system_design_notes.html#system-components-deep-dive",
    "title": "System Design Notes",
    "section": "",
    "text": "There are two parameters we can vary:\n\nLength of the key - should be limited so the URL is short enough\nRange of allowed characters - should be URL-safe\n\nUsing Base-64 encoding as the character range, then a 6-digit key length gives enough unique URLs; 64^6 = 68 billion\nEncoding options: 1. MD5 hash: not collision resistant and too long 2. Encoded counter, each URL is just an index int, and its value is the base-64 encoding of that int: length is not fixed and increases over time 3. Key range, generate all unique keys beforehand: no collisions, but storage intensive 4. Key range modified, generate 5-digit keys beforehand, then add the 6th digit on the fly.\n\n\n\n\n\n4 main operations (CRUD): Create, Read, Update, Delete.\nTransactions - group several operations together\n\nEither the entire transaction succeeds or it fails and rolls back to the initial state. Commit or abort on error.\nWithout transactions, if an individual operation fails, it can be complex to unwind the related transactions to return to the initial state.\nError handling is simpler as manual rollback of operations is not required.\nTransactions provide guarantees so we can reason about the database state before and after the transaction.\n\nTransactions enforce “ACID” guarantees:\n\nAtomicity - Transactions cannot be broken down into smaller parts.\nConsistency - All data points within the database must align to be properly read and accepted. Raise a consistency error if this is not the case. Database consistency is different from system consistency which states the data should be the same across all nodes.\nIsolation - Concurrently executing transactions are isolated from each other. Avoids race conditions if multiple clients are accessing the same database record.\nDurability - Once a transaction is committed to the database, it is permanently preserved. Backups and transaction logs can restore committed transactions in the case of a failure.\n\nRelational databases are a good fit when:\n\nData is well-structured\nUse case requires complex querying\nData consistency is important\n\nLimitations of relational databases:\n\nHorizontal scaling is complex\nDistributed databases are more complex to keep transaction guarantees\n\nTwo phase commit protocol (2PC):\n\nPrepare - Ask each node if it;s able to promise to carry out the transaction\nCommit - Block the nodes and do the commit\n\nBlocking causes problems on distributed systems where it may cause unexpected consequences if the database is unavailable for this short time.\n\n\n\n\n\nExamples of non-relational databases:\n\nKey-value store\nDocument store\nWide-column store\nGraph store\n\nThese vary a lot, and in general NoSQL simply means “no ACID guarantees”.\nTransactions were seen as the enemy of scalability, so needed to be abandoned completely for performance and scalability. ACID enforces consistency for relational databases; for non-relational databases there is eventual consistency.\nBASE:\n\nBasically Available - Always possible to read and write data even though it may not be consistent. E.g. reads may not reflect latest changes, writes may not be persisted.\nSoft state - Lack of consistency guarantees mean data state may change without any interactions with the application as the database reaches eventual consistency.\nEventual consistency - Data will eventually become consistent once inputs stops.\n\nBenefits:\n\nWithout atomicity constraint, overheads like 2PC are not required\nWithout consistency constraint, horizontal scaling is trivial\nWithout isolation constraint, no blocking is required which improves availability.\n\nNon-relational databases are a good fit when:\n\nLarge data volume that isn’t tabular\nHigh availability requirement\nLack of consistency across nodes is acceptable\n\nLimitations:\n\nConsistency is necessary for some use cases\nLack of standardisation\n\n\n\n\n\nProblem: if we have large files with small changes, we don’t want to have to re-upload and re-download the whole file every time something changes.\nSolution: identify the changes and only push/pull these. Similar to git.\nThe rsync algorithm makes it easier to compare changes by:\n\nSplit the file into blocks\nCalculate a checksum for each block (using MD5 hashing algorithm)\nTo compare files between the client and the server, we only need to send the hashes back and forth\nFor the mismatching hashes, transfer the corresponding blocks\n\n\n\n\nOne-way communication that broadcasts file updates one-to-many.\n\n\nThese are synchronous.\nPolling is an example of a pull API. We periodically query the server to see if there are any updates. If not, the server responds immediately saying there is no new data. This may result in delayed updates and can overwhelm the server with too many requests.\n\n\n\n\n\n\nFigure 1: Polling\n\n\n\n\n\n\nThese are asynchronous.\nLong-polling. The client connects to the server and makes a request. Instead of replying immediately, the server waits until there is an update and then sends the response. If there is no update for a long time, the connection times out and the client must reconnect. Server resources are tied up until the connection is closed, even if there is no update.\n\n\n\n\n\n\nFigure 2: LongPolling\n\n\n\nWebsockets. The client establishes a connection using an HTTP request and response. This establishes a TCP/IP connection for 2-way communication. This allows for real-time applications without long-polling.\n\n\n\n\n\n\nFigure 3: Web Sockets\n\n\n\nServer sent events. Event-based approach. EventSource API supported by all browsers. The connection is 1-directional; the client can only pass data to the server at the point of connection. After that, only the server can send data to the client. The server doesn’t know if the client loses connection.\n\n\n\n\n\n\nFigure 4: SSE\n\n\n\n\n\n\n\nA system component that implements asynchronous communication between services, decoupling them. An independent server which producers and consumers connect to. AKA message queues.\nMessage brokers persist the messages until there are consumers ready to receive them.\nMessage brokers are similar in principle to databases since they persist data. The differences are they: automatically delete messages after delivery, do not support queries, typically have a small working set, and notify clients about changes.\nRabbitMQ, Kafka and Redis are open source message brokers. Amazon, GCP and Azure have managed service implementations.\n\n\nA one-to-one relationship between sender and receiver. Each message is consumed by exactly one receiver.\nThe message broker serves as an abstraction layer. The producer only sends to the broker. The receiver only receives from the broker. The services do not need to know about one another. The broker also guarantees delivery, so the message does not get lost if the consumer is unavailable, it will retry.\n\n\n\n\n\n\nFigure 5: Point-to-Point\n\n\n\n\n\n\nA one-to-many relationship between sender and receiver. The publisher publishes to a certain topic. Any consumer who is interested can then subscribe to receive that message.\n\n\n\n\n\n\nFigure 6: Pub-Sub\n\n\n\n\n\n\n\n\n\nFiles are stored in folders, with metadata about creation time and modification time.\nBenefits:\n\nSimplicity - simple and well-known pattern\nCompatibility - works with most applications and OSes\nLimitations:\nPerformance degradation - as size increases, the resource demands increase.\nExpensive - although cheap in principle, they can get expensive when trying to work around the performance issues.\n\nUse cases: data protection, local archiving, data retrieval done by users.\n\n\n\nA system component that manages data as objects in a flat structure. An object contains the file and its metadata.\nBenefits:\n\nHorizontal scalability\n\nLimitations:\n\nObjects must be edited as a unit - degrades performance if only a small part of the file needs to be updated.\n\nUse cases: large amounts of unstructured data.\n\n\n\nA system component that breaks up and then stores data as fixed-size blocks, each with a unique identifier.\nBenefits:\n\nHighly structured - easy to index, search and retrieve blocks\nLow data transfer overheads\nSupports frequent data writes without performance degradation\nLow latency\n\nLimitations:\n\nNo metadata, so metadata must be handled manually\nExpensive\n\n\n\n\nA subclass of key-value stores, which hold computer-readable documents, like XML or JSON objects.\n\n\n\n\nIssues:\n\nMassive files: 4TB for 4 hours of 4K video\n\nHow can we reliably upload very large files?\n\nLots of different end-user clients/devices/connection speeds\n\nHow can we process and store a range of versions/resolutions?\n\n\nProcessing pipeline:\n\n\n\n\n\n\nFigure 7: Video Processing Pipeline\n\n\n\n\nFile chunker\n\nSame principle as the file-sharing problem.\nSplit the file into chunks, then use checksums to determine that files are present and correct.\nIf there is a network outage, we only need to send the remaining chunks.\n\nContent filter\n\nCheck if the video complies with the platform’s content policy wrt copyright, piracy, NSFW\n\nTranscoder\n\nData is decoded to an uncompressed format\nThis will fan out in the next step to create multiple optimised versions of the file\n\nQuality conversion\n\nConvert the uncompressed file into multiple different resolution options.\n\n\nArchitecture considerations:\n\nWait for the full file to upload before processing? Or process each chunk as it is uploaded?\nAn upload service persists chunks to a database. Object store is a good choice as in the file-sharing example.\nOnce a chunk is ready to be processed, it can be read and placed in the message queue for the processing pipeline.\nThe processing pipeline handles fixed-size chunks, so the hardware requirements are known ahead of time. A chunk can be processed as soon as a hardware unit becomes available.\n\n\n\n\n\n\n\nFigure 8: Video Upload Architecture\n\n\n\n\n\n\nThe challenges are similar to that of file-sharing, as this is essentially large files being transferred. Latency and processing power of the end-user device.\nTwo main transfer protocols:\n\nUDP\nTCP\n\n\n\nA connection-less protocol. Nodes do NOT establish a stable end-to-end connection before sending data. Instead, each packet has its destination address attached so the network can route it to the correct place.\nAdvantages:\n\nFast\nPackets are routed independently so if some are lost the others can still reach their destination\n\nDisadvantages:\n\nNo guarantee that data will arrive intact.\n\nUse cases:\n\nLow-latency applications like video conferencing or gaming.\nNot suitable for movie or audio streaming, as missing bits cannot be tolerated.\n\n\n\n\nA connection is established between 2 nodes. The nodes agree on certain parameters before any data is transferred. Parameters: IP addresses of source and destination, port numbers of source and destination.\nThree-way handshake establishes the connection:\n\nSender transfers their sequence number to the Receiver.\nReceiver acknowledges and sends its own sequence number.\nSender acknowledges.\n\nAdvantages:\n\nReliability; guarantees delivery and receives acknowledgements before further packets are sent.\nReceiver acknowledgements ensure the receiver is able to process/buffer the data in time before more packets are sent.\n\nDisadvantages:\n\nSlower due to error checking and resending lost packets.\nRequires three-way handshake to establish connection which is slower.\n\nUse cases:\n\nMedia streaming (HTTP live-streaming or MPEG-DASH)\n\nAdaptive bitrate streaming\n\nTCP adjusts transmission speed in response to network conditions\nWhen packet loss is detected, smaller chunks are sent at a lower transmission speed. The lower resolution file can be sent in this case.\n\n\n\n\n\nPerceived latency = Network latency + System latency\n\n\nCaching is the process of storing copies of frequently accessed data in a temporary storage location.\nCaches utilise the difference in read performance between memory and storage. Times to fetch 1MB from:\n\nHDD: 3 ms\nSSD: 0.2 ms\nRAM: 0.01 ms\n\nWhen a user requests data it first requests from the cache\n\nCache HIT: If the data is in the cache, return it\nCache MISS: If the data is not in the cache, pass the request to the database, update the cache and return the data\n\nA cache can grow stale over time. There are 2 common approaches to mitigate this:\n\nSet up a time-to-live (TTL) policy within the cache\n\n\nTrade off between short TTL (fresh data but worse performance) vs long TTL (data can be stale)\n\n\nImplement active cache invalidation mechanism\n\n\nComplicated to implement. Requires an “invalidation service” component to monitor and read from the database, then update the cache when necessary.\n\nApplication-level caching. Insert caching logic into the application’s source code to temporarily store data in memory.\nTwo approaches to application-level caching:\n\nQuery-based implementation\n\nHash the query as a key and store the value against the key in a key-value store.\nLimitations: hard to delete cached result with a complex query; if a cell changes then all cached query which might include it need updating.\n\nObject-based implementation\n\nThe result of a query is stored as an object\nBenefits: only one serialisation/deserialisation overhead when reading/writing; complexity of object doesn’t matter, serialized objects in cache can be used by multiple applications.\n\n\nPopular implementations: Redis, Memcache, Hazlecast\n\n\n\nA network of caching layer in different locations (Points of Presence, PoPs). Full data is stored on SSD and most frequently accessed data is stored in RAM.\nDon’t cache dynamic or time-sensitive data.\nInvalidating the cache. You can manually update the CDN, but there may be additional caches at the ISP- and browser-level which would still serve stale data.\n\n\n\n\n\n\nFigure 9: CDN Invalidation\n\n\n\nApproaches to invalidating cache:\n\nCaching headers - set a time when an object should be cleared, e.g. max-age=86400 caches for a day.\nCache busting - change the link to all files and assets, so the CDN treats them like new files.\n\n\n\n\n\nThese are specialised NoSQL databases with the following benefits:\n\nCan match even with typos or non-exact matches\nFull-text search means you can suggest autocomplete results and related queries as the user types\nIndexing allows faster search performance on big data.\n\nThe database is structures as a hashmap where the inverted index points at documents.\n\nDocument - Represents a specific entity, like a row in a relational database\nInverted index - Maps from content to the documents that include it. The inverted index splits each document into individual search terms and makes each point to the document itself.\nRanking algorithm - Produces a list of possibly relevant documents. Additional factors may impact this, like a user’s previous search history.\n\nThe database is a dictionary of {search_term_id: [document_ids, …]}\n\n\n\n\n\n\nFigure 10: Inverted Index\n\n\n\nPopular implementations: Lucene, Elastic search, Apache solr, Atas search (MongoDB), Redis search.\nBenefits of search engine databases:\n\nScalable - NoSQL so scale horizontally\nSchemaless\nWorks out of the box - just need ot decide upfront what attributes should be searchable.\n\nLimitations:\n\nNo ACID guarantees.\nNot efficient at reading/writing data, so requires another database to manage state.\n\nMaintaining consistency between the main database and the search engine database can be tricky.\n\nSome SQL databases have full-text search so may not require a dedicated search engine database."
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#system-design-examples-and-discussion",
    "href": "posts/software/system_design/system_design_notes.html#system-design-examples-and-discussion",
    "title": "System Design Notes",
    "section": "",
    "text": "An app that lets a user add and delete items from a todo list.\n\nRepresentation layer is a web app\nMicroservices handle the todo CRUD operations and the user details separately\n\nEach service is scaled horizontally, so a load balancer is placed in front of it\n\nRelational database stores data\n\nThere are two databases, one per service, which is an example of functional partitioning. This means todo service I/O does not interfere with user service I/O and vice versa.\n\n\n\n\n\n\n\n\nFigure 11: To-do App System Design\n\n\n\n\n\n\nTake an input URL and return a unique, shortened URL that redirects to the original.\nPlanned system architecture:\n\nPre-generate all 5 digit keys (1 billion entries rather than 64 billion for 6 digits)\nSystem retrieves 5-difit keys from database\nSystem appends 1 out of 64 characters\nThe system is extendable because if we run out of keys we can append 2 more characters rather than 1\n\n\n\nSystem analysis:\n\nSystem is read heavy - Once a short URL is created it will be read multiple times\nDistributed system as this has to scale\nAvailability &gt; Consistency - not so much of a concern if a link isn’t available to all users at the same time, but they must be unique and lead to their destination.\n\nRequirements engineering:-\nCore feature:\n\nA user can input a URL of arbitrary length and receive a unique short URL of fixed size.\nA user can navigate to a short link and be redirected to the original URL.\n\nSupport features:\n\nA user can see their link history of all created short URLs.\nLifecycle policy - links should expire after a default time span.\n\nNon-functional requirements:\n\nAvailability - the system should be available 99% of the time.\nScalability - the system should support billions of short URLs, and thousands of concurrent users.\nLatency - the system should return redirect from a short URL to the original in under 1 second.\n\nQuestions to capture scale:\n\nDaily active users, and how often do users interact per day\n\n100 million, 1 interaction per day\n\nPeak active users - are there events that lead to traffic spikes?\n\nNo spikes\n\nRead/write ratio\n\n10 to 1\n\nRequest size - how long are the originals URLs typically\n\n200 characters =&gt; 200 Bytes\n\nReplication factor\n\n1x - ignore replication\n\n\nCapacity estimation:-\n\nRequests per second\n\nReads per second = DAU * interactions per day / seconds in day = 10^8 * 1 / 10^5 = 1000 reads/s\nWrites per second = Reads per second / read write ratio = 1000 / 10 = 100 writes/s\nRequests per second = Reads per second + Writes per second = 1000 + 100 = 1100 requests/s\nNo peak loads to consider\n\nBandwidth\n\nBandwidth = Requests per second * Message size\nRead bandwidth = 1000 * 200 Bytes = 200kB/s\nWrite bandwidth = 100 * 200 Bytes = 20 kB/s\n\nStorage\n\nStorage per year = Write bandwidth * seconds per year * Replication factor = 20000 Bytes * (360024365) * 1 = 630 GB\n\n\nData model:-\nIdentify entities, attributes and relationships.\nEntities and attributes:\n\nLinks: Key (used to create short URL), Original URL, Expiry date\nUsers: UserID, Links\nKey ranges: Key range, In use (bool)\n\nRelationships:\n\nUsers own Links\nLinks belong to Key ranges\n\nData stores:\n\nUsers\n\nUser data is typically relational and we rarely want it all returned at once.\nConsistency is important as we want the user to have the same experience regardless of which server handles their log in, and don’t want userIds to clash.\nRelational database.\n\nLinks\n\nNon-functional requirements of low latency and high availability.\nData and relationships are not complex.\nKey-value store.\n\nKey ranges\n\nFavour data consistency as we don’t want to accidentally reuse the same key range.\nFiltering keys by status would help to find available key ranges.\nRelational database.\n\n\nAPI design:-\nEndpoints:\n\ncreateUrl(originalUrl: str) -&gt; shortUrl\n\nresponse code 200, {shortURL: str}\n\ngetLinkHistory(userId: str, sorting: {‘asc’, ‘desc’})\n\nresponse code 200, {links: [str], order: ‘asc’}\n\nredirectURL(shortUrl: str) -&gt; originalUrl\n\nresponse code 200, {originalUrl: str}\n\n\nSystem design:-\nUser flows for each of the required functional requirements.\n\n\n\n\n\n\nFigure 12: URL Shortener System Design\n\n\n\n\n\n\n\nRequirements engineering\nRequirements gathering:\n\nRead-heavy\nDistributed\nData consistency is more important than availability - files should be consistent for all users\n\nCore functional requirements:\n\nA user can upload a file to the server\nA user can download their files from the server\n\nSecondary functional requirements:\n\nA user can see a history of files uploaded and downloaded\nA user can see who has downloaded a specific file and when\n\nNon-functional requirements:\n\nConsistency - data should be consistent across users and devices\nResilience - customer data should never be lost\nMinimal latency\nCompatibility across different devices\nSecurity - multi-tenancy; customer data should be separate of one another\n\nQuestions to determine scale:\n\nDaily active users\nPeak active users - what events lead to a spike?\nInteractions per user - how many file syncs, how many files does a user store\nRequest size - how large are files?\nRead/write ratio\nReplication factor\n\nCapacity estimation\nThroughput\n\nPeak write RPS = 2 peak load ratio * 10^8 users * 2 files/day / 10^5 seconds = 4000 RPS\nPeak read RPS = 10 read/write ration * 4000 write RPS = 40000 RPS\nPeak total RPS = 44000 RPS\n\nBandwidth\n\nWrite bandwidth = 4000 Write RPS * 100 kB Request size = 400 MB/s\nRead bandwidth = 40000 Read RPS * 100 kB request size = 4 GB/s\nTotal bandwidth = Write bandwidth + Read bandwidth = 4.4 GB/s\n\nStorage\n\nStorage capacity = 5 * 10^8 Total users * 100 files per user * 1MB File Size * 3 Replication factor = 15*10^10 MB = 15000 TB\n\nData model\nUsers: Store Ids for the files that belong to them\nFiles: Metadata on the owner, file edit history, filename, size, chunks that comprise the file, version history\nAPI Design\nEndpoints:\n\ncompareHashes(fileId: str, chunkHashes: list)\n\nTakes a file ID and a list of chunk hashes and returns a list of the hashes that need resyncing .\nresponse code 200, {syncChunks: [Hash,…]}\n\nuploadChange(fileId: str, chunks: list)\n\nUpload the chunks to resync the file\nresponse code 200, {message: “Chunks uploaded successfully”}\n\nrequestUpdate(fileId: str, chunkHashes: list)\n\nUpdate the local version of the file to match that on the server.\nresponse code 200, {chunks: [Chunk,…]}\n\n\nSystem Design\n\n\n\n\n\n\nFigure 13: Dropbox System Design\n\n\n\nDesign Discussion\n\nWhat can we do to achieve a scalable design?\n\nIdentify bottlenecks and remedy each.\n\nWhat can we do to achieve resiliency?"
  },
  {
    "objectID": "posts/software/system_design/system_design_notes.html#references",
    "href": "posts/software/system_design/system_design_notes.html#references",
    "title": "System Design Notes",
    "section": "",
    "text": "Udemy course\nExcalidraw session\nCapacity estimation cheat sheet\n“Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems” by Martin Kleppmann\nRsync algorithm\nSearch Engines Information Retrieval in Practice book\n\n\n\n\nFigure 1: Polling\nFigure 2: LongPolling\nFigure 3: Web Sockets\nFigure 4: SSE\nFigure 5: Point-to-Point\nFigure 6: Pub-Sub\nFigure 7: Video Processing Pipeline\nFigure 8: Video Upload Architecture\nFigure 9: CDN Invalidation\nFigure 10: Inverted Index\nFigure 11: To-do App System Design\nFigure 12: URL Shortener System Design\nFigure 13: Dropbox System Design"
  },
  {
    "objectID": "posts/business/pitching/pitching.html",
    "href": "posts/business/pitching/pitching.html",
    "title": "Pitching Notes",
    "section": "",
    "text": "Notes from “Pitch Anything” by Oren Klaff.\nSome of the book comes across as a bit incel sigma vibes, talking about alphas and betas. I don’t agree with it, but summarising it here for the parts that are interesting.\n\n\nFrames, AKA perspective AKA point of view, describe a person’s mental model of controlling the world. The person who owns the frame owns the conversation.\nInformation must penetrate through the following layers of the brain in order.\n\nSurvival - “crocodile brain”. Ignore anything that isn’t new, exciting, dangerous, unexpected. Needs big picture, high contrast points.\nSocial\nReason\n\nLogic won’t reach the “reason” part of the brain until the croc brain and social brain are satiated. Data is perceived as a threat by layer 1. the croc brain needs unexpected information that is concrete not abstract.\nSTRONG process:\n\nSet the frame\nTell the story\nReveal the intrigue\nOffer the prize\nNail the hook point\nGet the deal\n\n\n\n\nFrames are the mental structure that shape how you see the world. When frames collide, the stronger frame absorbs the weaker frame. If you have to explain your authority, power, position, leverage then you have the weaker frame.\nOpposing frames and how to counter them.\n\nPower frame - fuelled by status and ego\n\nDon’t react to our strengthen their status\nActs of denial/defiance - establish your own rituals of power rather than abiding by theirs.\nMildly shocking but not unfriendly. Humour is key.\n\nPrize frame\n\nMake yourself the prize. Make them qualify themselves to you. The money has to earn you, not the other way around.\nMake statements not “trial close” questions\n\nTime frame - occurs later in the interaction\n\nStop as soon as you are done OR have lost their attention. Continuing further signals desperation.\n\nAnalyst frame - stats, technical details\n\nIntrigue frame counters this\nThe brain can’t perform cold cognitions (analysis) at the same time as hot cognitions (excitement)\nTell a brief, relevant, intriguing story with tension involving you at the centre of it to break the cold cognition.\n\n\n\n\n\nGlobal status is a function of wealth, power and status.\nBeta traps enforce your lower status. E.g. making you wait, big dogs not showing up to meetings, small talk before meetings.\nSituational status is the temporary shift of status based on the situation. Create local star power to increase your situational status.\n\nIgnore power rituals, avoid beta traps, ignore their global status\nFrame control - small, friendly acts of defiance\nLocal star power - shift conversation to a topic where you are the domain expert\nPrize frame - position yourself as the reward, e.g. leave after a hard cut off time\nConfirm your status - make them say you’re the alpha\n\nShock -&gt; frame control -&gt; local star power -&gt; hook point -&gt; leave\n\n\n\nEvery pitch should tell a story.\nFour sections of the pitch:\n\nIntroduce yourself and the big idea (5 mins)\nExplain the budget and secret sauce (10 mins)\nOffer the deal (2 mins)\nStack hot cognition frames (3 mins)\n\n\n\nKeep the pitch short - 20 mins.\n\nAnnounce the time constraint at the start to appease the croc brain. Otherwise people don’t know how long they are trapped for and panic.\nDNA presentation by Crick and Watson was 5 mins\n\nIntro should be highlights of successes NOT a life story.\n\nThe impression you leave is based the average not the sum, so don’t dilute it.\n\nThe “why now?” frame. 3 market forces pattern:\n\nEconomic\nSocial\nTechnology\n\nTell the story of how your idea came to be.\n\nMovement is key to overcoming change blindness. Don’t just show 2 states, describe the transition between them.\nTrends, impacts of those trends, how have these opened a (temporary) market window (time frame).\nThis story places you as the hero at the centre of solving this problem (prize frame).\n\nIdea introduction pattern:\nFor [target customers]\n\nWho are dissatisfied with [current offerings in the market]\n \nMy idea/product is a [new idea or product category]\n \nThat provides [key problem/solution features]\n \nUnlike [the competing product]\n \nMy idea/product is [describe key features]\n\n\n\nTune the message to the croc brain of the target.\n\nSimplicity can come across as naive, but focus on describing concrete things like relationships between people. This is high-level enough that it will not engage cold cognitions or threaten the croc brain.\n\nAttention = Desire (anticipation of reward) + Tension (fear of loss, introduce consequences)\n\nAttention is high when novelty is high.\nPush/pull patterns to increase tension and desire\nThe pitch narrative is a series of tension loops.\nWhen the tension is released and attention is lost the pitch is over.\nDesire eventually becomes fear, so keep the pitch short.\n\nMust haves:\n\nNumbers and projections\n\nThis should demonstrate your skill at budgeting (a difficult, precise skill), not your skill at projecting (a simple, inaccurate skill).\n\nCompetition\n\nHow easy is it for new competitors to enter?\nHow easy is it for customers to switch TO you and AWAY from you?\n\nSecret sauce\n\nYour competitive advantage.\nPhrasing it this way frames you as the prize.\n\n\n\n\n\nDescribe what they get if they do business with you\n\nWhat you will deliver, when, and how.\nWhat are their roles, responsibilities and next steps.\n\n\n\n\n\nPropose something concrete and actionable in such a compelling way that the target wants to chase it. Don’t wait for the target to evaluate you on the spot at the end of the pitch as that triggers cold cognitions. Pile up the hot cognitions instead.\nA hot cognition is emotional rather than analytical.\n\nYou decide that you like something before you understand it.\nData is used to justify decisions after the fact.\nHot cognitions are generally unavoidable; you may be able to control the expression of emotion, but the initial experience is still there.\nHot cognitions tend to be instant and enduring.\nHot vs cold cognitions can be characterised by spinach vs chocolate - you know one is good for you but you want the other anyway.\nHot cognitions are “knowing” something through feeling it, cold cognitions are “knowing something through evaluating it.”\n\nStacking frames.\n\nWe want to stack frames to create “wanting”, an emotional response\nThis is not the same as getting the target to “like” us, as that is a slow, intellectual cold cognition.\nAppealing to the analytical brain can only trigger a “like”; we want to trigger a “want” in the croc brain.\n\nThe 4 frame stack of hot cognitions:\n\nThe intrigue frame\n\nA story where most important things are who it happened to and how the characters reacted to their situation.\nThe events don’t need to be extreme but the characters’ reactions should be.\nA narrative that feels correct in time with convey a strong sense of truth and accuracy.\n“Man in the jungle” formula - Put a man in the jungle; have beasts attack him; will he get to safety?; get the man to the edge of the jungle but don’t get him out of it (cliffhanger)\n\nThe prize frame\n\nPosition yourself as the most important party in the deal. Conviction is key.\n“I am the prize. You are trying to impress me. You are trying to win my approval.”\n\nThe time frame\n\nThis creates a scarcity bias that triggers fear that triggers action.\nExtreme time pressure feels forced and cutrate. There is a balance between pressure and fairness; there should be a real-world time constraint.\n\nThe moral authority frame\n\nOnce the frame stacking is complete, you have the target’s attention for about 30 seconds.\n\n\n\nValidation-seeking behaviour (i.e. neediness) is the number one deal killer.\n\nNeediness is a threat signal, it triggers fear and uncertainty in the target which triggers self-protection.\n\nAvoid “soft closes” aiming to seek validation, e.g. “So what do you think so far?”, “Do you still think this is a good deal?”\nCauses of neediness:\n\nAnxiety and insecurity turn to fear, so you resort to acceptance-seeking behaviour to confirm that you’re still “part of the pack”\nResponse to disappointment is to seek validation\nWe want something only the target can give us\nWe need cooperation from the target, and the lack of cooperation causes anxiety\nWe believe the target can make us feel good by accepting our offer\n\nCounteracting validation-seeking behaviours:\n\nWant nothing\nFocus only on things you do well\nAnnounce your intention to leave - time frame. When you finish the pitch, withdraw and deny your audience; this creates a prize frame.\n\nMantra: \"I don't need these people, they need me. I am the prize\".\nExample:\n\nTime frame - “The deal will be fully subscribed in the next 10 days”.\nPower frame - “We don’t need VC money but want a big name on cap sheet for later IPO”\nPrize frame - “Are you really the right investor for this? We need to know more about the relationships and value you can bring.”\n\n\n\n\nYou are not pushing, you are interacting with people using basic rules of social dynamics.\nCroc brain:\n\nBoring: Ignore it\nDangerous? Fight it or run\nComplicated? Radically summarise\n\nThe presenter’s problem boils down to deciding which information to include. It is not like an engineering problem where more information is better.\nThree key insights:\n\nAppeal to the croc brain\nControl frames\nUse humour - this is not to relieve tension but to signal that you are so confident that you’re able to play around a little\n\nIf the pitch stops being fun, you’re doing it wrong.\nSteps to learning the method:\n\nRecognise beta traps; anything designed to control your behaviour\nStep around beta traps\nIdentify social frames\nInitiate frame collisions with safe targets - always with good humour and “a twinkle in your eye”\nPractice the push/pull of small acts of defiance and denial\n\nThe only rule is that you make the rules that others follow."
  },
  {
    "objectID": "posts/business/pitching/pitching.html#the-method",
    "href": "posts/business/pitching/pitching.html#the-method",
    "title": "Pitching Notes",
    "section": "",
    "text": "Frames, AKA perspective AKA point of view, describe a person’s mental model of controlling the world. The person who owns the frame owns the conversation.\nInformation must penetrate through the following layers of the brain in order.\n\nSurvival - “crocodile brain”. Ignore anything that isn’t new, exciting, dangerous, unexpected. Needs big picture, high contrast points.\nSocial\nReason\n\nLogic won’t reach the “reason” part of the brain until the croc brain and social brain are satiated. Data is perceived as a threat by layer 1. the croc brain needs unexpected information that is concrete not abstract.\nSTRONG process:\n\nSet the frame\nTell the story\nReveal the intrigue\nOffer the prize\nNail the hook point\nGet the deal"
  },
  {
    "objectID": "posts/business/pitching/pitching.html#frame-control",
    "href": "posts/business/pitching/pitching.html#frame-control",
    "title": "Pitching Notes",
    "section": "",
    "text": "Frames are the mental structure that shape how you see the world. When frames collide, the stronger frame absorbs the weaker frame. If you have to explain your authority, power, position, leverage then you have the weaker frame.\nOpposing frames and how to counter them.\n\nPower frame - fuelled by status and ego\n\nDon’t react to our strengthen their status\nActs of denial/defiance - establish your own rituals of power rather than abiding by theirs.\nMildly shocking but not unfriendly. Humour is key.\n\nPrize frame\n\nMake yourself the prize. Make them qualify themselves to you. The money has to earn you, not the other way around.\nMake statements not “trial close” questions\n\nTime frame - occurs later in the interaction\n\nStop as soon as you are done OR have lost their attention. Continuing further signals desperation.\n\nAnalyst frame - stats, technical details\n\nIntrigue frame counters this\nThe brain can’t perform cold cognitions (analysis) at the same time as hot cognitions (excitement)\nTell a brief, relevant, intriguing story with tension involving you at the centre of it to break the cold cognition."
  },
  {
    "objectID": "posts/business/pitching/pitching.html#status",
    "href": "posts/business/pitching/pitching.html#status",
    "title": "Pitching Notes",
    "section": "",
    "text": "Global status is a function of wealth, power and status.\nBeta traps enforce your lower status. E.g. making you wait, big dogs not showing up to meetings, small talk before meetings.\nSituational status is the temporary shift of status based on the situation. Create local star power to increase your situational status.\n\nIgnore power rituals, avoid beta traps, ignore their global status\nFrame control - small, friendly acts of defiance\nLocal star power - shift conversation to a topic where you are the domain expert\nPrize frame - position yourself as the reward, e.g. leave after a hard cut off time\nConfirm your status - make them say you’re the alpha\n\nShock -&gt; frame control -&gt; local star power -&gt; hook point -&gt; leave"
  },
  {
    "objectID": "posts/business/pitching/pitching.html#pitching-the-big-idea",
    "href": "posts/business/pitching/pitching.html#pitching-the-big-idea",
    "title": "Pitching Notes",
    "section": "",
    "text": "Every pitch should tell a story.\nFour sections of the pitch:\n\nIntroduce yourself and the big idea (5 mins)\nExplain the budget and secret sauce (10 mins)\nOffer the deal (2 mins)\nStack hot cognition frames (3 mins)\n\n\n\nKeep the pitch short - 20 mins.\n\nAnnounce the time constraint at the start to appease the croc brain. Otherwise people don’t know how long they are trapped for and panic.\nDNA presentation by Crick and Watson was 5 mins\n\nIntro should be highlights of successes NOT a life story.\n\nThe impression you leave is based the average not the sum, so don’t dilute it.\n\nThe “why now?” frame. 3 market forces pattern:\n\nEconomic\nSocial\nTechnology\n\nTell the story of how your idea came to be.\n\nMovement is key to overcoming change blindness. Don’t just show 2 states, describe the transition between them.\nTrends, impacts of those trends, how have these opened a (temporary) market window (time frame).\nThis story places you as the hero at the centre of solving this problem (prize frame).\n\nIdea introduction pattern:\nFor [target customers]\n\nWho are dissatisfied with [current offerings in the market]\n \nMy idea/product is a [new idea or product category]\n \nThat provides [key problem/solution features]\n \nUnlike [the competing product]\n \nMy idea/product is [describe key features]\n\n\n\nTune the message to the croc brain of the target.\n\nSimplicity can come across as naive, but focus on describing concrete things like relationships between people. This is high-level enough that it will not engage cold cognitions or threaten the croc brain.\n\nAttention = Desire (anticipation of reward) + Tension (fear of loss, introduce consequences)\n\nAttention is high when novelty is high.\nPush/pull patterns to increase tension and desire\nThe pitch narrative is a series of tension loops.\nWhen the tension is released and attention is lost the pitch is over.\nDesire eventually becomes fear, so keep the pitch short.\n\nMust haves:\n\nNumbers and projections\n\nThis should demonstrate your skill at budgeting (a difficult, precise skill), not your skill at projecting (a simple, inaccurate skill).\n\nCompetition\n\nHow easy is it for new competitors to enter?\nHow easy is it for customers to switch TO you and AWAY from you?\n\nSecret sauce\n\nYour competitive advantage.\nPhrasing it this way frames you as the prize.\n\n\n\n\n\nDescribe what they get if they do business with you\n\nWhat you will deliver, when, and how.\nWhat are their roles, responsibilities and next steps."
  },
  {
    "objectID": "posts/business/pitching/pitching.html#stack-hot-cognition-frames-3-mins",
    "href": "posts/business/pitching/pitching.html#stack-hot-cognition-frames-3-mins",
    "title": "Pitching Notes",
    "section": "",
    "text": "Propose something concrete and actionable in such a compelling way that the target wants to chase it. Don’t wait for the target to evaluate you on the spot at the end of the pitch as that triggers cold cognitions. Pile up the hot cognitions instead.\nA hot cognition is emotional rather than analytical.\n\nYou decide that you like something before you understand it.\nData is used to justify decisions after the fact.\nHot cognitions are generally unavoidable; you may be able to control the expression of emotion, but the initial experience is still there.\nHot cognitions tend to be instant and enduring.\nHot vs cold cognitions can be characterised by spinach vs chocolate - you know one is good for you but you want the other anyway.\nHot cognitions are “knowing” something through feeling it, cold cognitions are “knowing something through evaluating it.”\n\nStacking frames.\n\nWe want to stack frames to create “wanting”, an emotional response\nThis is not the same as getting the target to “like” us, as that is a slow, intellectual cold cognition.\nAppealing to the analytical brain can only trigger a “like”; we want to trigger a “want” in the croc brain.\n\nThe 4 frame stack of hot cognitions:\n\nThe intrigue frame\n\nA story where most important things are who it happened to and how the characters reacted to their situation.\nThe events don’t need to be extreme but the characters’ reactions should be.\nA narrative that feels correct in time with convey a strong sense of truth and accuracy.\n“Man in the jungle” formula - Put a man in the jungle; have beasts attack him; will he get to safety?; get the man to the edge of the jungle but don’t get him out of it (cliffhanger)\n\nThe prize frame\n\nPosition yourself as the most important party in the deal. Conviction is key.\n“I am the prize. You are trying to impress me. You are trying to win my approval.”\n\nThe time frame\n\nThis creates a scarcity bias that triggers fear that triggers action.\nExtreme time pressure feels forced and cutrate. There is a balance between pressure and fairness; there should be a real-world time constraint.\n\nThe moral authority frame\n\nOnce the frame stacking is complete, you have the target’s attention for about 30 seconds."
  },
  {
    "objectID": "posts/business/pitching/pitching.html#eradicating-neediness",
    "href": "posts/business/pitching/pitching.html#eradicating-neediness",
    "title": "Pitching Notes",
    "section": "",
    "text": "Validation-seeking behaviour (i.e. neediness) is the number one deal killer.\n\nNeediness is a threat signal, it triggers fear and uncertainty in the target which triggers self-protection.\n\nAvoid “soft closes” aiming to seek validation, e.g. “So what do you think so far?”, “Do you still think this is a good deal?”\nCauses of neediness:\n\nAnxiety and insecurity turn to fear, so you resort to acceptance-seeking behaviour to confirm that you’re still “part of the pack”\nResponse to disappointment is to seek validation\nWe want something only the target can give us\nWe need cooperation from the target, and the lack of cooperation causes anxiety\nWe believe the target can make us feel good by accepting our offer\n\nCounteracting validation-seeking behaviours:\n\nWant nothing\nFocus only on things you do well\nAnnounce your intention to leave - time frame. When you finish the pitch, withdraw and deny your audience; this creates a prize frame.\n\nMantra: \"I don't need these people, they need me. I am the prize\".\nExample:\n\nTime frame - “The deal will be fully subscribed in the next 10 days”.\nPower frame - “We don’t need VC money but want a big name on cap sheet for later IPO”\nPrize frame - “Are you really the right investor for this? We need to know more about the relationships and value you can bring.”"
  },
  {
    "objectID": "posts/business/pitching/pitching.html#closing-thoughts",
    "href": "posts/business/pitching/pitching.html#closing-thoughts",
    "title": "Pitching Notes",
    "section": "",
    "text": "You are not pushing, you are interacting with people using basic rules of social dynamics.\nCroc brain:\n\nBoring: Ignore it\nDangerous? Fight it or run\nComplicated? Radically summarise\n\nThe presenter’s problem boils down to deciding which information to include. It is not like an engineering problem where more information is better.\nThree key insights:\n\nAppeal to the croc brain\nControl frames\nUse humour - this is not to relieve tension but to signal that you are so confident that you’re able to play around a little\n\nIf the pitch stops being fun, you’re doing it wrong.\nSteps to learning the method:\n\nRecognise beta traps; anything designed to control your behaviour\nStep around beta traps\nIdentify social frames\nInitiate frame collisions with safe targets - always with good humour and “a twinkle in your eye”\nPractice the push/pull of small acts of defiance and denial\n\nThe only rule is that you make the rules that others follow."
  },
  {
    "objectID": "projects_section/xai_research/xai.html",
    "href": "projects_section/xai_research/xai.html",
    "title": "Explainable AI in Healthcare",
    "section": "",
    "text": "This is a research project I completed which aimed to quantify the confidence we should have in a trained model applied ot our data set.\nThe full paper is available here.\nSlides from a presentation I gave at the Nuffield Department of Orthopaedics, Rheumatology and Musculoskeletal Sciences (NDORMS) at the University of Oxford are available here.\n\n\n\n Back to top"
  },
  {
    "objectID": "projects_section/tradeintel/tradeintel.html",
    "href": "projects_section/tradeintel/tradeintel.html",
    "title": "TradeIntel",
    "section": "",
    "text": "This is a robo-advisor app to give tailored stock portfolio recommendations.\nThe user can input high-level preferences like their risk tolerance, industry preferences, and how closely they would like to follow the broader market. We then perform a portfolio optimisation process to recommend a robust portfolio based on those criteria.\nThis is available on the App Store\n\n\n\n\n Back to top"
  },
  {
    "objectID": "projects_section/bjj/bjj.html",
    "href": "projects_section/bjj/bjj.html",
    "title": "Brazilian Jiu Jitsu Taxonomy",
    "section": "",
    "text": "This is a (work-in-progress) interactive React app I made to:\n\nKeep track of my BJJ notes\nPractice some web development\n\nCheck it out here.\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html",
    "href": "posts/business/public_speaking/public_speaking.html",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "Notes from reading “Ted Talks: The offical TED guide to public speaking” by Chris Anderson.\n\n\n\n\nPresentation literacy is its own skill. Your goal is to be you, not a version of someone you like.\n\n\n\nIdeas are all that matter in the talk. This can be a novel idea, or a novel story or presentation of an existing idea.\nYour goal is to recreate the core idea inside your audience’s mind. The language you use must be shared by the speaker and listener for it to have this effect. Focus on what you can give to the audience.\nVisualise the talk as a journey that you guide the user through. Start where the audience starts. Don’t make any impossible leaps or unexpected changes of direction.\n\nWhat issues matter the most\nHow are they related?\nHow can they be easily explained?\nWhat are the key open questions? Controversies?\n\n\n\n\nFour talk styles to AVOID.\n\nSales Pitch\n\nYou should give not take.\nGenerosity evokes a response.\n\nRamble\n\nInsults the audience; suggests that their time doesn’t matter to you.\n\nOrganisation bore\n\nYour company’s structure is boring.\nFocus on the work and ideas, not the company, products or individuals.\n\nInspiration tryhard\n\nAll style and no substance.\nComes across as manipulative.\nInspiration has to be earned. “Like love you don’t get it by pursuing it”.\nInspiration is the response to authenticity, courage, wisdom, selflessness.\n\n\n\n\n\nThe throughline is a connecting theme that ties the narrative elements\n\nA focused, precise, unexpected idea\nNot the same as a theme\nYou should be able to encapsulate it in &lt;15 words\n\nTrace the path the journey will take. If the audience knows where you’re going, it will be easier to follow.\nCut down the talk to keep the throughline focused.\n\nDON’T just keep all the content but say it in less detail. Overstuffed = underexplained\nCut back the range of topics. Say less with more impact.\n“The power of the deleted word”.\nAn 18 minute time limit is short enough to keep an audience’s attention, long enough to cover enough ground and precise enough to be taken seriously.\n\nThe overall structure can be thought of in 3 parts: What? So what? What now? In more detail, this is:\n\nIntro - what will be covered\nContext - why this issue matters\nMain concepts\nPractical implications\n\nTalk about ideas not issues. Write for an audience of one.\nChecklist:\n\nIs this a topic I’m passionate about? Do I know enough to be worth the audience’s time? Do I have credibility?\nDoes it inspire curiosity?\nWill knowing this make a difference to the audience?\nIs the talk a gift or an ask?\nIs the information fresh?\nCan the topic be covered in enough detail with examples in the time slot?\nWhat are the 15 words that encapsulate the talk?\nWould those 15 words persuade someone they’re interested in hearing the talk?\n\n\n\n\n\n\n\nA human connection is required before you can inform, persuade or inspire. Knowledge has to be pulled in, not pushed in. Be AUTHENTIC.\nDos\n\nEye contact.\nVulnerability, but authentic.\nHumour signals to the group that you’ve all connected.\n\nDon’ts\n\nEgo breeds contempt.\nAvoid tribal topics/language, e.g. politics.\n\n\n\n\nStorytelling helps people to imagine and dream -&gt; empathise -&gt; care\nKey elements:\n\nCharacter\nTension\nRight level of detail\nSatisfying resolution\n\nIf you’re telling a story, know why you’re telling it. The goal is to give, not boost your ego.\nConnect the dots enough, but don’t force-feed the conclusion.\n\n\n\nSteps:\n\nStart where the audience is. No assumed knowledge or jargon.\nSpark curiosity. Create a knowledge gap that the listener wants to fill.\nIntroduce new concepts one-by-one. Name each concept.\nMetaphors and analogies. Reveal the “shape” of new concepts.\nExamples confirm understanding.\n\nKnowledge is built hierarchically. Explain the structure and where each concept fits. Convert a web of ideas into a string of words.\nCurse of knowledge. Revisit assumed knowledge.\nMake clear what the concept isn’t. An explanation is a small mental model in a large space of possibilities. Reduce the size of that space.\n\n\n\nDemolish the listener’s existing knowledge and rebuild it; replace it with something better. Genius is something that comes to you, not something you are.\nAn “intuition pump” is an example or metaphor that makes the conclusion more plausible. Not a rigorous argument, but nudges the listener in the right direction. Prime with emotion or story, then persuade with reason.\nTurn the audience into detectives. Tell a story. Start with a big mystery, traverse the possible solutions until only one plausible conclusion remains.\n\n\n\nThree approaches to revelatory talks:\n\nThe wonder walk\n\nReveal a succession of inspiration images\nObvious throughline with accessible language and silence\nThe message should be “how intriguing is this” not “look what I achieved”\n\nDemo\n\nInitial tease, background context, demo, implications\n\nDreamscape\n\nCreate a world that doesn’t exist but someday might\nPaint a bold picture of an alternative future\nMake others desire that future\nEmphasise human values not just clever tech, otherwise the audience might freak out at the possible negative implications of the technology\n\n\n\n\n\n\n\n\nSlides can distract. If you do use them, make sure they add something.\nVisuals should:\n\nReveal - set context, prime, BAM!\nExplain - one idea per slide with a clickbait headline not a summary. Highlight the point you’re making.\nDelight - show impressive things and let them speak for themselves. Don’t need to explain every image/slide.\n\nDon’t let the slides be spoilers for what you’re about to say. The message loses its impact, it’s not news anymore.\n\n\n\nThe goal is to have a set structure that you speak naturally and authentically about.\nBoth approaches have the same result:\n\nScript and memorise until natural, or\nFreestyle around bullet points and rehearse until structure is set\n\nBeing read to and being talked to are very different experiences\n\nDictating the speech rather than writing it can help make sure it comes across how you would speak rather than how you would write.\nDon’t end up in the uncanny valley between reading and speaking\nIn some cases, reading is powerful if it’s clear that this is a poetic, written piece. Abandon the script for an impactful finale.\n\nWhat you are saying matters more than the exact word choice.\nRehearse your impromptu remarks.\n\n\n\nPractice speaking by speaking!\nRehearse in phases:\n\nEditing and cutting\nPacing and timing\nDelivery\n\nAim to use &lt;90% of the time limit. results in tighter writing, and time to riff and enjoy the reaction.\n\n\n\nYou have the audience’s attention at the start, then it’s yours to lose. Opener has to capture attention and keep it. 2 stages:\n\n10 seconds to capture attention\n1 minute to hold it\n\nOpening techniques:\n\nDrama\nCuriosity - more specific questions are more intriguing\nCompelling slide/image - “the next image changed my life”\nTense but don’t give away - show where you’re going but save the reveal\n\nThe closer dictates how the whole talk will be remembered.\nClosing techniques:\n\nCamera pullback - big picture and implications\nCall to action\nPersonal commitment\nInspiring values/vision\nEncapsulation - neatly reframe the main idea\nNarrative symmetry - callback to opener\nLyrical inspiration - poetic conclusion\n\n\n\n\n\n\n\n\nChoose an outfit early\nDress like the audience but smarter\nDress for the people in the back row\n\n\n\n\n\nEmbrace the nerves and draw attention to it. Vulnerability humanises you.\nPick friendly faces in the audience and speak to them.\nBackup plan - have a story ready to fill in during unexpected technical issues\nFocus on the message - “THIS MATTERS”\n\n\n\n\nVisual barriers between the speaker and audience can create a sense of authority but at the expense of a human connection. If they can see you, you’re vulnerable. If you’re vulnerable, they can connect.\nReferring to notes is fine if done sparingly and unambiguously. It humanises you if done honestly. It appears sneaky and dishonest if you try to do it secretly.\n\n\n\nSpeaking style adds another stream of input parallel to the words themselves. It tells the listener how they should interpret the words.\nSix voice tools. Vary each of them depending on the meaning and emotion.\n\nVolume\nPitch\nPace\nTimbre\nTone\nProsody (singsong rise and fall)\n\nSpeed should be a conversational pace, approx 130-170 words per minute.\n\nPeople often worry about speaking too fast\nSpeaking too slow is a more common problem (overcorrection?)\nUnderstanding outpaces articulation, the listener is “dying of word starvation”\n\nSpeak, don’t orate.\nBody language\n\nStand and move intentionally.\nStop to make a point then walk to the next point.\n\n\n\n\nAdd too many ingredients and you risk losing attention. What you say can just be signposts for what you show - set it up, show it, shut up.\n\n\n\n\n\n\nKnowledge of specific facts is becoming more specialised and commoditised. Understanding how everything fits together is becoming broader, more unified.\nConcepts of specialised knowledge are outdated, stemming from industrial age thinknig. Modern thinking values:\n\nContextual knowledge\nCreating knowledge\nUnderstanding of humanity\n\nAs people become more interconnected, innovation becomes crowd-accelerated.\nHappiness is finding something bigger than you and dedicating your life to it. Nudge the world - give it questions to spark conversations. “The future is not yet written, we are all collectively in the process of writing it”"
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html#foundation",
    "href": "posts/business/public_speaking/public_speaking.html#foundation",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "Presentation literacy is its own skill. Your goal is to be you, not a version of someone you like.\n\n\n\nIdeas are all that matter in the talk. This can be a novel idea, or a novel story or presentation of an existing idea.\nYour goal is to recreate the core idea inside your audience’s mind. The language you use must be shared by the speaker and listener for it to have this effect. Focus on what you can give to the audience.\nVisualise the talk as a journey that you guide the user through. Start where the audience starts. Don’t make any impossible leaps or unexpected changes of direction.\n\nWhat issues matter the most\nHow are they related?\nHow can they be easily explained?\nWhat are the key open questions? Controversies?\n\n\n\n\nFour talk styles to AVOID.\n\nSales Pitch\n\nYou should give not take.\nGenerosity evokes a response.\n\nRamble\n\nInsults the audience; suggests that their time doesn’t matter to you.\n\nOrganisation bore\n\nYour company’s structure is boring.\nFocus on the work and ideas, not the company, products or individuals.\n\nInspiration tryhard\n\nAll style and no substance.\nComes across as manipulative.\nInspiration has to be earned. “Like love you don’t get it by pursuing it”.\nInspiration is the response to authenticity, courage, wisdom, selflessness.\n\n\n\n\n\nThe throughline is a connecting theme that ties the narrative elements\n\nA focused, precise, unexpected idea\nNot the same as a theme\nYou should be able to encapsulate it in &lt;15 words\n\nTrace the path the journey will take. If the audience knows where you’re going, it will be easier to follow.\nCut down the talk to keep the throughline focused.\n\nDON’T just keep all the content but say it in less detail. Overstuffed = underexplained\nCut back the range of topics. Say less with more impact.\n“The power of the deleted word”.\nAn 18 minute time limit is short enough to keep an audience’s attention, long enough to cover enough ground and precise enough to be taken seriously.\n\nThe overall structure can be thought of in 3 parts: What? So what? What now? In more detail, this is:\n\nIntro - what will be covered\nContext - why this issue matters\nMain concepts\nPractical implications\n\nTalk about ideas not issues. Write for an audience of one.\nChecklist:\n\nIs this a topic I’m passionate about? Do I know enough to be worth the audience’s time? Do I have credibility?\nDoes it inspire curiosity?\nWill knowing this make a difference to the audience?\nIs the talk a gift or an ask?\nIs the information fresh?\nCan the topic be covered in enough detail with examples in the time slot?\nWhat are the 15 words that encapsulate the talk?\nWould those 15 words persuade someone they’re interested in hearing the talk?"
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html#talk-tools",
    "href": "posts/business/public_speaking/public_speaking.html#talk-tools",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "A human connection is required before you can inform, persuade or inspire. Knowledge has to be pulled in, not pushed in. Be AUTHENTIC.\nDos\n\nEye contact.\nVulnerability, but authentic.\nHumour signals to the group that you’ve all connected.\n\nDon’ts\n\nEgo breeds contempt.\nAvoid tribal topics/language, e.g. politics.\n\n\n\n\nStorytelling helps people to imagine and dream -&gt; empathise -&gt; care\nKey elements:\n\nCharacter\nTension\nRight level of detail\nSatisfying resolution\n\nIf you’re telling a story, know why you’re telling it. The goal is to give, not boost your ego.\nConnect the dots enough, but don’t force-feed the conclusion.\n\n\n\nSteps:\n\nStart where the audience is. No assumed knowledge or jargon.\nSpark curiosity. Create a knowledge gap that the listener wants to fill.\nIntroduce new concepts one-by-one. Name each concept.\nMetaphors and analogies. Reveal the “shape” of new concepts.\nExamples confirm understanding.\n\nKnowledge is built hierarchically. Explain the structure and where each concept fits. Convert a web of ideas into a string of words.\nCurse of knowledge. Revisit assumed knowledge.\nMake clear what the concept isn’t. An explanation is a small mental model in a large space of possibilities. Reduce the size of that space.\n\n\n\nDemolish the listener’s existing knowledge and rebuild it; replace it with something better. Genius is something that comes to you, not something you are.\nAn “intuition pump” is an example or metaphor that makes the conclusion more plausible. Not a rigorous argument, but nudges the listener in the right direction. Prime with emotion or story, then persuade with reason.\nTurn the audience into detectives. Tell a story. Start with a big mystery, traverse the possible solutions until only one plausible conclusion remains.\n\n\n\nThree approaches to revelatory talks:\n\nThe wonder walk\n\nReveal a succession of inspiration images\nObvious throughline with accessible language and silence\nThe message should be “how intriguing is this” not “look what I achieved”\n\nDemo\n\nInitial tease, background context, demo, implications\n\nDreamscape\n\nCreate a world that doesn’t exist but someday might\nPaint a bold picture of an alternative future\nMake others desire that future\nEmphasise human values not just clever tech, otherwise the audience might freak out at the possible negative implications of the technology"
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html#preparation-process",
    "href": "posts/business/public_speaking/public_speaking.html#preparation-process",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "Slides can distract. If you do use them, make sure they add something.\nVisuals should:\n\nReveal - set context, prime, BAM!\nExplain - one idea per slide with a clickbait headline not a summary. Highlight the point you’re making.\nDelight - show impressive things and let them speak for themselves. Don’t need to explain every image/slide.\n\nDon’t let the slides be spoilers for what you’re about to say. The message loses its impact, it’s not news anymore.\n\n\n\nThe goal is to have a set structure that you speak naturally and authentically about.\nBoth approaches have the same result:\n\nScript and memorise until natural, or\nFreestyle around bullet points and rehearse until structure is set\n\nBeing read to and being talked to are very different experiences\n\nDictating the speech rather than writing it can help make sure it comes across how you would speak rather than how you would write.\nDon’t end up in the uncanny valley between reading and speaking\nIn some cases, reading is powerful if it’s clear that this is a poetic, written piece. Abandon the script for an impactful finale.\n\nWhat you are saying matters more than the exact word choice.\nRehearse your impromptu remarks.\n\n\n\nPractice speaking by speaking!\nRehearse in phases:\n\nEditing and cutting\nPacing and timing\nDelivery\n\nAim to use &lt;90% of the time limit. results in tighter writing, and time to riff and enjoy the reaction.\n\n\n\nYou have the audience’s attention at the start, then it’s yours to lose. Opener has to capture attention and keep it. 2 stages:\n\n10 seconds to capture attention\n1 minute to hold it\n\nOpening techniques:\n\nDrama\nCuriosity - more specific questions are more intriguing\nCompelling slide/image - “the next image changed my life”\nTense but don’t give away - show where you’re going but save the reveal\n\nThe closer dictates how the whole talk will be remembered.\nClosing techniques:\n\nCamera pullback - big picture and implications\nCall to action\nPersonal commitment\nInspiring values/vision\nEncapsulation - neatly reframe the main idea\nNarrative symmetry - callback to opener\nLyrical inspiration - poetic conclusion"
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html#on-stage",
    "href": "posts/business/public_speaking/public_speaking.html#on-stage",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "Choose an outfit early\nDress like the audience but smarter\nDress for the people in the back row\n\n\n\n\n\nEmbrace the nerves and draw attention to it. Vulnerability humanises you.\nPick friendly faces in the audience and speak to them.\nBackup plan - have a story ready to fill in during unexpected technical issues\nFocus on the message - “THIS MATTERS”\n\n\n\n\nVisual barriers between the speaker and audience can create a sense of authority but at the expense of a human connection. If they can see you, you’re vulnerable. If you’re vulnerable, they can connect.\nReferring to notes is fine if done sparingly and unambiguously. It humanises you if done honestly. It appears sneaky and dishonest if you try to do it secretly.\n\n\n\nSpeaking style adds another stream of input parallel to the words themselves. It tells the listener how they should interpret the words.\nSix voice tools. Vary each of them depending on the meaning and emotion.\n\nVolume\nPitch\nPace\nTimbre\nTone\nProsody (singsong rise and fall)\n\nSpeed should be a conversational pace, approx 130-170 words per minute.\n\nPeople often worry about speaking too fast\nSpeaking too slow is a more common problem (overcorrection?)\nUnderstanding outpaces articulation, the listener is “dying of word starvation”\n\nSpeak, don’t orate.\nBody language\n\nStand and move intentionally.\nStop to make a point then walk to the next point.\n\n\n\n\nAdd too many ingredients and you risk losing attention. What you say can just be signposts for what you show - set it up, show it, shut up."
  },
  {
    "objectID": "posts/business/public_speaking/public_speaking.html#reflection",
    "href": "posts/business/public_speaking/public_speaking.html#reflection",
    "title": "Public Speaking Notes",
    "section": "",
    "text": "Knowledge of specific facts is becoming more specialised and commoditised. Understanding how everything fits together is becoming broader, more unified.\nConcepts of specialised knowledge are outdated, stemming from industrial age thinknig. Modern thinking values:\n\nContextual knowledge\nCreating knowledge\nUnderstanding of humanity\n\nAs people become more interconnected, innovation becomes crowd-accelerated.\nHappiness is finding something bigger than you and dedicating your life to it. Nudge the world - give it questions to spark conversations. “The future is not yet written, we are all collectively in the process of writing it”"
  },
  {
    "objectID": "posts/business/marketing/marketing.html",
    "href": "posts/business/marketing/marketing.html",
    "title": "Marketing Notes",
    "section": "",
    "text": "Notes from “The 1-Page Marketing Plan” by Allan Dib.\n\n\n\n\nThe overall phases of the direct response marketing strategy are:\n\n\n\nPhase\nStatus\nGoal\n\n\n\n\nBefore\nProspect\nGet them to know you\n\n\nDuring\nLead\nGet them to like you\n\n\nAfter\nCustomer\nGet them to trust you\n\n\n\nDirect response marketing should be:\n\nSpecific\nTrackable\nInclude an offer\nFollow up\n\nA marketing plan encompasses:\n\n\n\n\n\n\n\nStage\nExample\n\n\n\n\nAdvertising\nSign for the circus\n\n\nPromotion\nPut the sign on an elephant and walk it into town\n\n\nPublicity\nElephant tramples the mayor’s flowers and it makes the news\n\n\nPR\nYou get the mayor to laugh it off\n\n\nSales\nPeople come to the circus and spend money\n\n\n\nStrategy changes with scale\n\nDon’t blindly copy large companies. “Branding” to “get your name out there” is expensive and ineffective.\nThe one and only objective of marketing for a small company is “make a profit” For large companies it may be: appease shareholders, board, managers, existing clients, win advertising awards. Making a profit is a low priority for them.\nStrategy without tactics = paralysis by analysis\nTactics without strategy = bright shiny object syndrome\n\nProduct quality is a customer retention tool, not a customer acquisition tool.\n\nPeople don’t know how good the product is until they’ve already bought it\n“If you build it, they will come” doesn’t work\n\nThe 80/20 rule (Pareto principle) states that 80% of the outputs come from 20% of the inputs. Applying the 80/20 rule to the 80/20 rule gives the 64/4 rule: 64% of the outputs come from 4% of the inputs. Identify this 4% and focus on it. Often this is marketing.\nSpend money to make time, not vice versa. Money is a renewable resource, time is finite.\n\n\n\nThe niche should be as specific as possible - an inch wide and a mile deep.\n\nToo broad a target market is useless as the message gets diluted\nMass marketing/branding can work for larger companies but not small. Large companies have enough firepower to fire arrows in all directions. Small companies need to fire in a specific direction.\n\nBecome a specialist in your field to make price irrelevant\n\nBecome an educator and build trust\nYou don’t want to be a commodity shopped on price. A customer won on price will be lost on price.\n\nUse the PVP index to identify the ideal niche. Assign values to each possible segment for:\n\nPersonal fulfillment\nValue to the marketplace\nProfitability\n\nQuestions for the target prospect:\n\nWhat keeps them up at night?\nWhat are they afraid of?\nWhat are they angry about? Who are they angry at?\nWhat trends affect them?\nWhat do they secretly desire most? What one thing do they crave above all else?\nHow do they make decisions? Analytical, emotional, social proof\nWhat language/jargon do they use?\nWhat magazines do they read? What websites do they visit?\nWhat is a typical day like?\nWhat is the single most dominant emotion this market feels?\n\nCreate an avatar for each prospect. Put a name and photo against each market segment and describe a specific person in detail. Describe their life, job, activities, relationships. Answer each of the questions above from their POV.\n\n\n\nThere are two key elements of the message:\n\nPurpose of the ad\nWhat it focuses on\n\nKeep the message focused: 1 ad = 1 objective. “Marketing on purpose” rather than “marketing by accident”. There should be a clear call to action. You should know the exact action you want the prospect to take next.\nThe following two questions help refine the USP:\n\nWhy should they buy?\nWhy should they buy from me?\n\nKey takeaways for USP:\n\nWhy should they buy from you rather than competitor\nIf you remove the company name and logo, would people still know it’s you?\nQuality and service are not USPs. The customer only finds these out after they buy from you. USPs should attract customers before they buy.\nLowest price is not a USP. There’s always someone willing to go out of business quicker than you.\n\nPeople rarely want what you’re selling, they want the result of what you’re selling. Sell the problem. People don’t know what they want until they’re presented with it. Purchasing is done with emotions and justified with logic after the fact. “If I had asked people what they wanted, they’d have said faster horses” - Henry Ford.\nWhen you confuse them, you lose them.\n\nExplain the product and its benefit in one sentence.\nThe biggest threat isn’t that someone buys from a competitor, it’s that they do nothing.\nElevator pitch of the form: “You know ? What we do is ? In fact .”\n\nBe remarkable\n\nTransform something ordinary into something that makes the customer smile.\nUnique is dangerous, you just need to be “different enough”.\n\nCraft the offer\n\nIf you don’t give a compelling reason to buy, customers will default to price, and you become a commodity.\nDon’t just offer a discount, think about which product will definitely solve their biggest problem.\nAdd value rather than discounting, through bundles and customisation.\nCreate “apples to oranges” comparisons to your competitors to get out of the commmodity game.\nTarget pain points. Sell the problem, not a feature list. Customers in pain are price insensitive.\n\nCreate the offer:\n\nValue to customer\nLanguage and jargon they use\nReason why this offer is here. People are skeptical if it is too good to be true.\nValue stacking. Add in high-margin bonuses.\nUpsells\nOutrageous guarantee\nCreate scarcity. People react more to fear of loss than prospect of gain.\n\nWriting copy:\n\nBe yourself and be authentic. “Professional” = boring.\n“The enemy in common”. People buy with emotions: fear, love, greed, guilt, pride.\nCopy should address the bad parts of the product, not just the good. Who is this not for?\n\nNaming:\n\nTitle=content\nChoose clarity over cleverness\n\n\n\n\n\n“Half the money I spend on advertising is wasted; the trouble is I don’t know which half” - John Wanamaker\n\nMeasure ROI of ad campaign\n\nIf it made more money that it lost, it was a success. The only metric that matters.\nCustomer acquisition cost vs lifetime value of customer\nOther metrics like response rate are only useful to the extent that they help you calculate the above.\nSpecific target market is key. The goal of the ad is to make they prospect say “Hey, that’s for me”.\nWhat gets measured gets managed. Conversely, what you don’t know will hurt you.\n\nLifetime value of customer = Frontend (initial purchase) + Backend (subsequent purchases)\n\nIdeally the frontend alone exceeds the customer acquisition cost to be sustainable. Then the cost of the ad is immediately recouped and the ad campaign can be reupped for another cycle.\n\nSuccessful marketing has three components:\n\nMarket\nMessage\nMedia\n\nSocial media is a media not a strategy and it’s not free. It’s only free if your time is worthless.\nEmail is preferable to social media because you own the mailing list. The customer database which is a valuable asset in its own right.\n\nEmail regularly - weekly at least\nGive value - 3 value-building emails for each offer email\nSnail mail complements email - less cluttered and less competitive because fewer people still use it.\n\nMarketing budget\n\nThe only time for a budget is during the testing phase when your finding what works (what delivers a positive ROI)\nOnce you have a campaign that works, go all in. It makes money, it’s not an expense so don’t limit it.\nIf you could buy £10 notes for £5 you’d buy as many as possible, you wouldn’t limit it with a budget. The same applies for a marketing campaign with a £5 cost of acquisition and £10 lifetime value of customer.\n\nAvoid single points of failure\n\nOne customer, supplier, ad medium, lead generation method.\nPaid media is reliable and allows ROI to be tracked and measured.\n\n\n\n\n\n\n\nTreat marketing like farming not hunting. Avoid “random acts of marketing” that hunters do. Build a CRM system and infrastructure for new leads, follow-ups, conversions, etc.\nThe first goal is to generate leads - find people who are interested.\n\nAvoid trying to sell from the ad immediately\nCapture all who could be interested, not just those ready to buy immediately\nBuild a customer database, then sell to them later\n\nThe market for your product might break down something like:\n\n3% - ready to buy now\n7% - open to buying\n30% - interested but not right now\n60% - not interested\n\nBy selling directly from the ad you limit yourself just to the 3%. Capture leads first and nurture them to appeal to the 40% who are interested.\nEducate customers to offer value, then you become a trusted advisor rather than a pest salesman.\n\n\n\nThe process of taking people from being vaguely interested to desiring it. The nurturing process means they should be predisposed to buying from you before you ever try to sell to them.\nStay in contact.\n\nMost stop after 1 or 2 follow-ups. The most effective salespeople do 10+ follow-ups.\nThe money is in the follow-up\n\nBuild trust and add value. Don’t pester to close.\n\nWhen the customer is ready to buy, you will be top of mind and they are predisposed to buy. It should be when they are ready to buy, not when you are ready to sell.\nDo not pressure prospects to close. Don’t be a pest, be a welcome guest.\nContact regularly - “market until they buy or die”.\nMost contact should be value add, with occasional pitch/offer emails. 3:1 ratio of value add:pitch.\n\nShock and awe package.\n\nPhysical parcel mailed to high probability prospects.\n“Lumpy” mail grabs attention - snail mail with something physical inside.\nNeed numbers on conversion probabilities and lifetime value of customer for this approach to be viable. Good marketing can’t beat bad math.\nHigh value contents should:\n\nProvide value\nPosition you as a trusted expert\nMove prospect further down the buying cycle.\n\n\nMake lots of offers. A/B test with small segments. Take risks, write compelling copy and make outrageous guarantees.\nThree major types of people:\n\n\n\nType\nRole\nResponsibility\n\n\n\n\nEntrepreneur\n“Make it up”\nVision\n\n\nSpecialist\n“Make it real”\nImplement\n\n\nManager\n“Make it recur”\nDaily BAU\n\n\n\nMarketing calendar.\n\nWhat needs to be done and when\nDaily, weekly, monthly, quarterly, annually\nEvent-triggers tasks, e.g. on signup, on complaint\n\nDelegate.\n\nIf someone else can do the job 80% as good as you, delegate it.\nSeparate majors and minors. Focus on the majors, dump or delegate the minors.\nDon’t mistake movement for achievement.\nDays are expensive. You can get more money but not more time.\n\n\n\n\nOnce you reach a certain level of expertise, the real profit is in how you market yourself. A concert violinist makes more money performing concerts than busking.\nTrust is key in sales.\n\nDifficult for unknown businesses.\nPeople are wary because of dodgy sellers. Assume every dog bites.\nEducate -&gt; Position as expert -&gt; Trust\nDelay the sale to show:\n\nYou are willing to give before you take\nYou are an educator, so can be trusted\n\nStop selling and start educating, consulting, advising.\nEntrepreneur is someone who solves people’s problems for profit.\n\nOutrageous guarantees.\n\nReverse the risk from the customer to you.\nGuarantee needs to be specific and address the prospect’s reservations directly. Not just a generic money-back guarantee.\nIf you’re an ethical operator, you’re already implicitly providing a guarantee, just not marketing it. Use it!\n\nPricing.\n\nThe marketing potential of price is often underutilised.\nDon’t just naively undercut the market leader or use cost-plus pricing.\nSmall number of options/tiers\n\nMore choice means you are more likely to pique interest but less likely to keep interest.\nFear of a suboptimal choice; the paradox of choice.\nStandard and premium tiers and a good rule of thumb.\n\n“Unlimited” option can often be profitable because customers overestimate how much they will use, so overpay for unlimited use.\nUltra high ticket item\n\nMakes other tiers seem more reasonably priced\n10% of customers would pay 10x more. 1% of customers would pay 100x more.\n\n\nOther price considerations:\n\nDon’t discount, unless specifically using a loss-leader strategy.\nTry before you buy (the puppy dog close)\n\nThe sale feels less permanent so more palatable for the customer.\nThe onus is on the customer to return - shifts the power balance.\n\nDon’t segregate departments - everyone can warm up a prospect.\nPayment plans - people are less attached to future money than present money.\n\n\n\n\n\n\n\nThe goal is to turn customers into a tribe of raving fans. Most companies stop marketing at the sell, but this is where the process begins. Most of the money is in the backend not the frontend.\nSell them what they want, give them what they need.\n\nThese don’t always overlap\nIf they buy a treadmill and don’t use it, they’ll blame you when they don’t lose weight. You need to make sure they use your product correctly after they buy it.\nSplit the process up for them to make it less daunting.\n\nCreate a sense of theatre\n\nPeople don’t just want ot be serviced, they want to be entertained.\nInnovate on: pricing, financing, packaging, support, delivery etc, not just product. For example, “will it blend” YouTube videos to sell ordinary blenders.\n\nUse technology to reduce friction. Treat tech like an employee. Why am I hiring it? What are its KPIs?\nBecome a voice of value to your tribe.\n\nEducation-based marketing leads to trust and becoming an expert in the field.\nTell your audience the effort that goes into the product. For example, shampoo bottle that says how many mint leaves went into it.\n\nCreate replicable systems\n\nSystems ensure you have a business, rather than you are the business\nOften overlooked because it is “back office” or “not urgent”\nBenefits: build valuable asset; leverage and scalability; consistency; lower labour costs\nAreas:\n\nMarketing (leads)\nSales (follow-up)\nFulfillment\nAdmin\n\nCreate an operations manual:\n\nImagine the business is 10x the size\nIdentify all roles\nIdentify all tasks - what does each role do\nChecklist for each task\n\n\nExit strategy. Start with the end in mind. If the goal is to sell for $50 million, any decision can be framed as “will this help me get to $50 million?”. Who will buy the company? Why? Customer base, revenue or IP?\n\n\n\nPeople are 21x more likely to buy from a company they’ve previously bought from. Increase revenue by selling more to existing customers.\n\nRaise prices\n\n\nPeople are less price sensitive than you’d expect.\nSome may leave but these were the lowest-value customers - this may be polluted revenue you want to fire anyway. A customer won on price can be lost on price.\nGive a reason why prices increased. Explain the benefits they’ve received and future innovations.\nGrandfather existing customers on old price as a loyalty bonus.\n\n\nUpselling\n\n\n“Contrast principle” - people are less price sensitive to add-ons. If they’ve already bought the suit then the shirt seems cheap.\n“Customers who bought X also bought Y” - people want to fit social norms.\nSell more when the client is “hot and heavy” in the buying mood. Some people naively think you should overwhelm them with more selling.\n\n\nAscension\n\n\nMove customers up to a higher-priced tier.\nProduct mix - standard, premium, ultra-high.\n\n\nFrequency of purchases\n\n\nReminders, subscriptions\nReasons to come back; voucher for next purchase.\n\n\nReactivation\n\n\nLure back lapsed customers with strong offer and call to action.\nAsk why they haven’t returned.\nApologise and make corrections if necessary. Make the feel special if they do return.\n\nKey numbers - what gets measured gets managed\n\nLeads\nConversion rate\nAverage transaction value\nBreak even point\n\nFor subscription models measure:\n\nMonthly recurring revenue\nChurn rate\nCustomer lifetime value\n\nPolluted revenue - dollars from toxic customers != dollars from raving fans. Types of customers:\n\nTribe\nChurners - can’t afford you so drop you on price or intro offer ending\nVampires - suck up all your time, you can’t afford them\nSnow leopard - big customer but not replicable\n\nNet promoter score\n\n“How likely are you to recommend us to a friend?”\nDetractors 0-6; Passive 7-8; Promoters 9-10\nNPS=+100 if everyone is a promoter, -100 if everyone is a detractor\n\nFire problem customers\n\n“The customer is always right” is naive. It should be “the right customer is always right”.\nGenuine customer complaints are valuable and may help retain other customers who had the same issue but didn’t speak up.\nLow-value, price-sensitive customers complain the most. They take up your time and energy at the expense of your tribe.\nFiring detractors creates scarcity and frees your time to focus on the tribe. With limited supply, customers have to play and pay by your rules.\n\n\n\n\nOrchestrating referrals is an active task, not needy or desperate.\n\nDon’t just deliver a good product and hope for the best\nPeople refer because it makes them feel good, helpful or knowledgeable, not to do you a favour.\nAppeal to their ego, offer them something valuable, give them a reason to refer.\n\nAsk (for a referral) and ye shall receive.\n\n“Law of 250” - most people know 250 people well enough to invite to a wedding or funeral. This is 250 potential referrals per customer.\nSet expectation of referrals beforehand - “We’re going to do a great job and we need your help… Most people refer 3 others.”\n\nBe specific about who you ask and what kind of referral you want. Conquer the bystander effect.\nMake referral profiles for each customer group in your database.\n\nWho do they know?\nHow can you make them remember you?\nWhat will make them look good?\nHow will they provide value to their referral target\n\nCustomer buying behaviour\n\nWho has your customers before you?\nJoint venture for referrals\nSell/exchange leads, resell complementary products, affiliate referral partner\n\nBrand = personality of the business. Think of the business as a person:\n\nName\nDesign (what they wear)\nPositioning (how they communicate)\nBrand promise (core values)\nTarget market (who they associate with)\nBrand awareness (how well known / popular)\n\nBest form of brand building is selling\n\nBranding is what you do after someone has bought from you.\nBrand equity - customers crossing the road to buy from you and walk past competitors.\n\n\n\n\n\nImplementation is crucial\n\nKnowing and not doing is the same as not knowing.\nCommon pitfalls:\n\nParalysis by analysis - 80% out the door is better than 100% in the drawer; money loves speed.\nFail to delegate\n“My business is different” - Spend your effort figuring out how to make it work rather than why it won’t work.\n\n\nTime is not money\n\nEntrepreneurs get paid for the value they create, not the time or effort they put in.\nMaking money is a side effect of creating value.\n\nThe value of a business is the eyeballs it has access to.\nQuestions to ponder:\n\nWhat business should I be in?\nWhat technologies will disrupt it?\nHow can I take advantage of tech rather than fight it?"
  },
  {
    "objectID": "posts/business/marketing/marketing.html#the-before-phase",
    "href": "posts/business/marketing/marketing.html#the-before-phase",
    "title": "Marketing Notes",
    "section": "",
    "text": "The overall phases of the direct response marketing strategy are:\n\n\n\nPhase\nStatus\nGoal\n\n\n\n\nBefore\nProspect\nGet them to know you\n\n\nDuring\nLead\nGet them to like you\n\n\nAfter\nCustomer\nGet them to trust you\n\n\n\nDirect response marketing should be:\n\nSpecific\nTrackable\nInclude an offer\nFollow up\n\nA marketing plan encompasses:\n\n\n\n\n\n\n\nStage\nExample\n\n\n\n\nAdvertising\nSign for the circus\n\n\nPromotion\nPut the sign on an elephant and walk it into town\n\n\nPublicity\nElephant tramples the mayor’s flowers and it makes the news\n\n\nPR\nYou get the mayor to laugh it off\n\n\nSales\nPeople come to the circus and spend money\n\n\n\nStrategy changes with scale\n\nDon’t blindly copy large companies. “Branding” to “get your name out there” is expensive and ineffective.\nThe one and only objective of marketing for a small company is “make a profit” For large companies it may be: appease shareholders, board, managers, existing clients, win advertising awards. Making a profit is a low priority for them.\nStrategy without tactics = paralysis by analysis\nTactics without strategy = bright shiny object syndrome\n\nProduct quality is a customer retention tool, not a customer acquisition tool.\n\nPeople don’t know how good the product is until they’ve already bought it\n“If you build it, they will come” doesn’t work\n\nThe 80/20 rule (Pareto principle) states that 80% of the outputs come from 20% of the inputs. Applying the 80/20 rule to the 80/20 rule gives the 64/4 rule: 64% of the outputs come from 4% of the inputs. Identify this 4% and focus on it. Often this is marketing.\nSpend money to make time, not vice versa. Money is a renewable resource, time is finite.\n\n\n\nThe niche should be as specific as possible - an inch wide and a mile deep.\n\nToo broad a target market is useless as the message gets diluted\nMass marketing/branding can work for larger companies but not small. Large companies have enough firepower to fire arrows in all directions. Small companies need to fire in a specific direction.\n\nBecome a specialist in your field to make price irrelevant\n\nBecome an educator and build trust\nYou don’t want to be a commodity shopped on price. A customer won on price will be lost on price.\n\nUse the PVP index to identify the ideal niche. Assign values to each possible segment for:\n\nPersonal fulfillment\nValue to the marketplace\nProfitability\n\nQuestions for the target prospect:\n\nWhat keeps them up at night?\nWhat are they afraid of?\nWhat are they angry about? Who are they angry at?\nWhat trends affect them?\nWhat do they secretly desire most? What one thing do they crave above all else?\nHow do they make decisions? Analytical, emotional, social proof\nWhat language/jargon do they use?\nWhat magazines do they read? What websites do they visit?\nWhat is a typical day like?\nWhat is the single most dominant emotion this market feels?\n\nCreate an avatar for each prospect. Put a name and photo against each market segment and describe a specific person in detail. Describe their life, job, activities, relationships. Answer each of the questions above from their POV.\n\n\n\nThere are two key elements of the message:\n\nPurpose of the ad\nWhat it focuses on\n\nKeep the message focused: 1 ad = 1 objective. “Marketing on purpose” rather than “marketing by accident”. There should be a clear call to action. You should know the exact action you want the prospect to take next.\nThe following two questions help refine the USP:\n\nWhy should they buy?\nWhy should they buy from me?\n\nKey takeaways for USP:\n\nWhy should they buy from you rather than competitor\nIf you remove the company name and logo, would people still know it’s you?\nQuality and service are not USPs. The customer only finds these out after they buy from you. USPs should attract customers before they buy.\nLowest price is not a USP. There’s always someone willing to go out of business quicker than you.\n\nPeople rarely want what you’re selling, they want the result of what you’re selling. Sell the problem. People don’t know what they want until they’re presented with it. Purchasing is done with emotions and justified with logic after the fact. “If I had asked people what they wanted, they’d have said faster horses” - Henry Ford.\nWhen you confuse them, you lose them.\n\nExplain the product and its benefit in one sentence.\nThe biggest threat isn’t that someone buys from a competitor, it’s that they do nothing.\nElevator pitch of the form: “You know ? What we do is ? In fact .”\n\nBe remarkable\n\nTransform something ordinary into something that makes the customer smile.\nUnique is dangerous, you just need to be “different enough”.\n\nCraft the offer\n\nIf you don’t give a compelling reason to buy, customers will default to price, and you become a commodity.\nDon’t just offer a discount, think about which product will definitely solve their biggest problem.\nAdd value rather than discounting, through bundles and customisation.\nCreate “apples to oranges” comparisons to your competitors to get out of the commmodity game.\nTarget pain points. Sell the problem, not a feature list. Customers in pain are price insensitive.\n\nCreate the offer:\n\nValue to customer\nLanguage and jargon they use\nReason why this offer is here. People are skeptical if it is too good to be true.\nValue stacking. Add in high-margin bonuses.\nUpsells\nOutrageous guarantee\nCreate scarcity. People react more to fear of loss than prospect of gain.\n\nWriting copy:\n\nBe yourself and be authentic. “Professional” = boring.\n“The enemy in common”. People buy with emotions: fear, love, greed, guilt, pride.\nCopy should address the bad parts of the product, not just the good. Who is this not for?\n\nNaming:\n\nTitle=content\nChoose clarity over cleverness\n\n\n\n\n\n“Half the money I spend on advertising is wasted; the trouble is I don’t know which half” - John Wanamaker\n\nMeasure ROI of ad campaign\n\nIf it made more money that it lost, it was a success. The only metric that matters.\nCustomer acquisition cost vs lifetime value of customer\nOther metrics like response rate are only useful to the extent that they help you calculate the above.\nSpecific target market is key. The goal of the ad is to make they prospect say “Hey, that’s for me”.\nWhat gets measured gets managed. Conversely, what you don’t know will hurt you.\n\nLifetime value of customer = Frontend (initial purchase) + Backend (subsequent purchases)\n\nIdeally the frontend alone exceeds the customer acquisition cost to be sustainable. Then the cost of the ad is immediately recouped and the ad campaign can be reupped for another cycle.\n\nSuccessful marketing has three components:\n\nMarket\nMessage\nMedia\n\nSocial media is a media not a strategy and it’s not free. It’s only free if your time is worthless.\nEmail is preferable to social media because you own the mailing list. The customer database which is a valuable asset in its own right.\n\nEmail regularly - weekly at least\nGive value - 3 value-building emails for each offer email\nSnail mail complements email - less cluttered and less competitive because fewer people still use it.\n\nMarketing budget\n\nThe only time for a budget is during the testing phase when your finding what works (what delivers a positive ROI)\nOnce you have a campaign that works, go all in. It makes money, it’s not an expense so don’t limit it.\nIf you could buy £10 notes for £5 you’d buy as many as possible, you wouldn’t limit it with a budget. The same applies for a marketing campaign with a £5 cost of acquisition and £10 lifetime value of customer.\n\nAvoid single points of failure\n\nOne customer, supplier, ad medium, lead generation method.\nPaid media is reliable and allows ROI to be tracked and measured."
  },
  {
    "objectID": "posts/business/marketing/marketing.html#the-during-phase",
    "href": "posts/business/marketing/marketing.html#the-during-phase",
    "title": "Marketing Notes",
    "section": "",
    "text": "Treat marketing like farming not hunting. Avoid “random acts of marketing” that hunters do. Build a CRM system and infrastructure for new leads, follow-ups, conversions, etc.\nThe first goal is to generate leads - find people who are interested.\n\nAvoid trying to sell from the ad immediately\nCapture all who could be interested, not just those ready to buy immediately\nBuild a customer database, then sell to them later\n\nThe market for your product might break down something like:\n\n3% - ready to buy now\n7% - open to buying\n30% - interested but not right now\n60% - not interested\n\nBy selling directly from the ad you limit yourself just to the 3%. Capture leads first and nurture them to appeal to the 40% who are interested.\nEducate customers to offer value, then you become a trusted advisor rather than a pest salesman.\n\n\n\nThe process of taking people from being vaguely interested to desiring it. The nurturing process means they should be predisposed to buying from you before you ever try to sell to them.\nStay in contact.\n\nMost stop after 1 or 2 follow-ups. The most effective salespeople do 10+ follow-ups.\nThe money is in the follow-up\n\nBuild trust and add value. Don’t pester to close.\n\nWhen the customer is ready to buy, you will be top of mind and they are predisposed to buy. It should be when they are ready to buy, not when you are ready to sell.\nDo not pressure prospects to close. Don’t be a pest, be a welcome guest.\nContact regularly - “market until they buy or die”.\nMost contact should be value add, with occasional pitch/offer emails. 3:1 ratio of value add:pitch.\n\nShock and awe package.\n\nPhysical parcel mailed to high probability prospects.\n“Lumpy” mail grabs attention - snail mail with something physical inside.\nNeed numbers on conversion probabilities and lifetime value of customer for this approach to be viable. Good marketing can’t beat bad math.\nHigh value contents should:\n\nProvide value\nPosition you as a trusted expert\nMove prospect further down the buying cycle.\n\n\nMake lots of offers. A/B test with small segments. Take risks, write compelling copy and make outrageous guarantees.\nThree major types of people:\n\n\n\nType\nRole\nResponsibility\n\n\n\n\nEntrepreneur\n“Make it up”\nVision\n\n\nSpecialist\n“Make it real”\nImplement\n\n\nManager\n“Make it recur”\nDaily BAU\n\n\n\nMarketing calendar.\n\nWhat needs to be done and when\nDaily, weekly, monthly, quarterly, annually\nEvent-triggers tasks, e.g. on signup, on complaint\n\nDelegate.\n\nIf someone else can do the job 80% as good as you, delegate it.\nSeparate majors and minors. Focus on the majors, dump or delegate the minors.\nDon’t mistake movement for achievement.\nDays are expensive. You can get more money but not more time.\n\n\n\n\nOnce you reach a certain level of expertise, the real profit is in how you market yourself. A concert violinist makes more money performing concerts than busking.\nTrust is key in sales.\n\nDifficult for unknown businesses.\nPeople are wary because of dodgy sellers. Assume every dog bites.\nEducate -&gt; Position as expert -&gt; Trust\nDelay the sale to show:\n\nYou are willing to give before you take\nYou are an educator, so can be trusted\n\nStop selling and start educating, consulting, advising.\nEntrepreneur is someone who solves people’s problems for profit.\n\nOutrageous guarantees.\n\nReverse the risk from the customer to you.\nGuarantee needs to be specific and address the prospect’s reservations directly. Not just a generic money-back guarantee.\nIf you’re an ethical operator, you’re already implicitly providing a guarantee, just not marketing it. Use it!\n\nPricing.\n\nThe marketing potential of price is often underutilised.\nDon’t just naively undercut the market leader or use cost-plus pricing.\nSmall number of options/tiers\n\nMore choice means you are more likely to pique interest but less likely to keep interest.\nFear of a suboptimal choice; the paradox of choice.\nStandard and premium tiers and a good rule of thumb.\n\n“Unlimited” option can often be profitable because customers overestimate how much they will use, so overpay for unlimited use.\nUltra high ticket item\n\nMakes other tiers seem more reasonably priced\n10% of customers would pay 10x more. 1% of customers would pay 100x more.\n\n\nOther price considerations:\n\nDon’t discount, unless specifically using a loss-leader strategy.\nTry before you buy (the puppy dog close)\n\nThe sale feels less permanent so more palatable for the customer.\nThe onus is on the customer to return - shifts the power balance.\n\nDon’t segregate departments - everyone can warm up a prospect.\nPayment plans - people are less attached to future money than present money."
  },
  {
    "objectID": "posts/business/marketing/marketing.html#the-after-phase",
    "href": "posts/business/marketing/marketing.html#the-after-phase",
    "title": "Marketing Notes",
    "section": "",
    "text": "The goal is to turn customers into a tribe of raving fans. Most companies stop marketing at the sell, but this is where the process begins. Most of the money is in the backend not the frontend.\nSell them what they want, give them what they need.\n\nThese don’t always overlap\nIf they buy a treadmill and don’t use it, they’ll blame you when they don’t lose weight. You need to make sure they use your product correctly after they buy it.\nSplit the process up for them to make it less daunting.\n\nCreate a sense of theatre\n\nPeople don’t just want ot be serviced, they want to be entertained.\nInnovate on: pricing, financing, packaging, support, delivery etc, not just product. For example, “will it blend” YouTube videos to sell ordinary blenders.\n\nUse technology to reduce friction. Treat tech like an employee. Why am I hiring it? What are its KPIs?\nBecome a voice of value to your tribe.\n\nEducation-based marketing leads to trust and becoming an expert in the field.\nTell your audience the effort that goes into the product. For example, shampoo bottle that says how many mint leaves went into it.\n\nCreate replicable systems\n\nSystems ensure you have a business, rather than you are the business\nOften overlooked because it is “back office” or “not urgent”\nBenefits: build valuable asset; leverage and scalability; consistency; lower labour costs\nAreas:\n\nMarketing (leads)\nSales (follow-up)\nFulfillment\nAdmin\n\nCreate an operations manual:\n\nImagine the business is 10x the size\nIdentify all roles\nIdentify all tasks - what does each role do\nChecklist for each task\n\n\nExit strategy. Start with the end in mind. If the goal is to sell for $50 million, any decision can be framed as “will this help me get to $50 million?”. Who will buy the company? Why? Customer base, revenue or IP?\n\n\n\nPeople are 21x more likely to buy from a company they’ve previously bought from. Increase revenue by selling more to existing customers.\n\nRaise prices\n\n\nPeople are less price sensitive than you’d expect.\nSome may leave but these were the lowest-value customers - this may be polluted revenue you want to fire anyway. A customer won on price can be lost on price.\nGive a reason why prices increased. Explain the benefits they’ve received and future innovations.\nGrandfather existing customers on old price as a loyalty bonus.\n\n\nUpselling\n\n\n“Contrast principle” - people are less price sensitive to add-ons. If they’ve already bought the suit then the shirt seems cheap.\n“Customers who bought X also bought Y” - people want to fit social norms.\nSell more when the client is “hot and heavy” in the buying mood. Some people naively think you should overwhelm them with more selling.\n\n\nAscension\n\n\nMove customers up to a higher-priced tier.\nProduct mix - standard, premium, ultra-high.\n\n\nFrequency of purchases\n\n\nReminders, subscriptions\nReasons to come back; voucher for next purchase.\n\n\nReactivation\n\n\nLure back lapsed customers with strong offer and call to action.\nAsk why they haven’t returned.\nApologise and make corrections if necessary. Make the feel special if they do return.\n\nKey numbers - what gets measured gets managed\n\nLeads\nConversion rate\nAverage transaction value\nBreak even point\n\nFor subscription models measure:\n\nMonthly recurring revenue\nChurn rate\nCustomer lifetime value\n\nPolluted revenue - dollars from toxic customers != dollars from raving fans. Types of customers:\n\nTribe\nChurners - can’t afford you so drop you on price or intro offer ending\nVampires - suck up all your time, you can’t afford them\nSnow leopard - big customer but not replicable\n\nNet promoter score\n\n“How likely are you to recommend us to a friend?”\nDetractors 0-6; Passive 7-8; Promoters 9-10\nNPS=+100 if everyone is a promoter, -100 if everyone is a detractor\n\nFire problem customers\n\n“The customer is always right” is naive. It should be “the right customer is always right”.\nGenuine customer complaints are valuable and may help retain other customers who had the same issue but didn’t speak up.\nLow-value, price-sensitive customers complain the most. They take up your time and energy at the expense of your tribe.\nFiring detractors creates scarcity and frees your time to focus on the tribe. With limited supply, customers have to play and pay by your rules.\n\n\n\n\nOrchestrating referrals is an active task, not needy or desperate.\n\nDon’t just deliver a good product and hope for the best\nPeople refer because it makes them feel good, helpful or knowledgeable, not to do you a favour.\nAppeal to their ego, offer them something valuable, give them a reason to refer.\n\nAsk (for a referral) and ye shall receive.\n\n“Law of 250” - most people know 250 people well enough to invite to a wedding or funeral. This is 250 potential referrals per customer.\nSet expectation of referrals beforehand - “We’re going to do a great job and we need your help… Most people refer 3 others.”\n\nBe specific about who you ask and what kind of referral you want. Conquer the bystander effect.\nMake referral profiles for each customer group in your database.\n\nWho do they know?\nHow can you make them remember you?\nWhat will make them look good?\nHow will they provide value to their referral target\n\nCustomer buying behaviour\n\nWho has your customers before you?\nJoint venture for referrals\nSell/exchange leads, resell complementary products, affiliate referral partner\n\nBrand = personality of the business. Think of the business as a person:\n\nName\nDesign (what they wear)\nPositioning (how they communicate)\nBrand promise (core values)\nTarget market (who they associate with)\nBrand awareness (how well known / popular)\n\nBest form of brand building is selling\n\nBranding is what you do after someone has bought from you.\nBrand equity - customers crossing the road to buy from you and walk past competitors."
  },
  {
    "objectID": "posts/business/marketing/marketing.html#conclusion",
    "href": "posts/business/marketing/marketing.html#conclusion",
    "title": "Marketing Notes",
    "section": "",
    "text": "Implementation is crucial\n\nKnowing and not doing is the same as not knowing.\nCommon pitfalls:\n\nParalysis by analysis - 80% out the door is better than 100% in the drawer; money loves speed.\nFail to delegate\n“My business is different” - Spend your effort figuring out how to make it work rather than why it won’t work.\n\n\nTime is not money\n\nEntrepreneurs get paid for the value they create, not the time or effort they put in.\nMaking money is a side effect of creating value.\n\nThe value of a business is the eyeballs it has access to.\nQuestions to ponder:\n\nWhat business should I be in?\nWhat technologies will disrupt it?\nHow can I take advantage of tech rather than fight it?"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html",
    "href": "posts/software/software_architecture/software_architect_notes.html",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "Notes from “The Complete Guide to Becoming a Software Architect” Udemy course\n\n\nA developer knows what can be done, an architect knows what should be done. How do we use technology to meet business requirements.\nGeneral system requirements:\n\nFast\nSecure\nReliable\nEasy to maintain\n\nArchitect’s need to know how to code for:\n\nArchitecture’s trustworthiness\nSupport developers\nRespect of developers\n\n\n\n\n\nUnderstand the business - Strengths, weaknesses, compettions, growth strategy.\nDefine the system’s goals - Goals are not requirements. Goals describe the effect on the organisation, requirements describe what the system should do.\nWork for your client’s clients - Prioritise the end user.\nTalk to the right people with the right language - What is the thing that really matters to the person I’m talking to?\n\nProject managers care about how things affect deadlines, developers care about the technologies used, CEOs care about business continuity and bottom line.\n\n\n\n\nUnderstand the system requirements - What the system should do.\nUnderstand the non-functional requirements - Technical and service level attributes, e.g. number of users, loads, volumes, performance.\nMap the components - Understand the system functionality and communicate this to your client. Completely non-technical at this point, no mention of specific technologies. A diagram of high level components helps.\nSelect the technology stack - Backend, frontend, data store.\nDesign the architecture\nWrite the architecture document\nSupport the team\n\nInclude developers in non-functional requirements and architecture design for two reasons:\n\nLearn about unknown scenarios early\nCreate ambassadors\n\n\n\n\nThe 2 types of requirements: functional and non-functional.\n\n\n“What the system should do”\n\nBusiness flows\nBusiness services\nUser interfaces\n\n\n\n\n“What the system should deal with”\n\nPerformance - latency and throughput\n\nLatency - How long does it take to perform a single task?\nThroughput - How many tasks can be performed in a given time unit?\n\nLoad - Quantity of work without crashing. Determines the availability of the system. Always look at peak (worst case) numbers.\nData volume - How much data will the system accumulate over time?\n\nData required on day one\nData growth (say, annually)\n\nConcurrent users - How many users will be using the system? This includes “dead times” which differentiates it from the load requirement. Rule of thumb is concurrent_users = load * 10\nSLA - Service Level Agreement for uptime.\n\nThe non-functional requirements are generally the more important in determining the architecture. The client will generally need guiding towards sensible values, otherwise they just want as much load as possible, as much uptime as possible etc.\n\n\n\n\nThe application type should be established early based on the use case and expected user interaction.\n\nWeb apps\n\nServe html pages.\nUI; user-initiated actions; large scale; short, focused actions.\nRequest-response model.\n\nWeb API\n\nServe data (often JSON).\nData retrieval and storage; client-initiated actions; large scale; short, focused actions.\nRequest-response model.\n\nMobile\n\nRequire user interaction; frontend for web API; location-based.\n\nConsole\n\nNo UI; limited interation; long-running processes; short actions for power users.\nRequire technical knowledge.\n\nService\n\nNo UI; managed by service manager; long-running processes.\n\nDesktop\n\nAll resources on the PC; UI; user-centric actions.\n\nFunction-as-a-service\n\nAWS lambda\n\n\n\n\n\nConsiderations:\n\nAppropriate for the task\nCommunity - e.g. stack overflow activity\nPopularity - google trends over 2 years\n\n\n\nCovers web app, web API, console, service.\nOptions: .NET, Java, node.js, PHP, Python\n\n\n\n\n\n\nFigure 1: Backend Tech\n\n\n\n\n\n\nCovers:\n\nWeb app - Angular, React\nMobile - Native (Swift, Java/Kotlin), Xamarin, React Native\nDesktop - depends on target OS\n\n\n\n\nSQL - small, structured data\n\nRelational tables\nTransactions\n\nAtomicity\nConsistency\nIsolation\nDurability\n\nQuerying language is universal\n\nNoSQL - huge, unstructured or semi-structured data\n\nEmphasis on scale and performance.\nSchema-less, with entities stored as JSON.\nEventual consistency - data can be temporarily inconsistent\nNo universal querying language\n\n\n\n\n\nQuality attributes that describe technical capabilities to fulfill the non-functional requirements. Non-functional requirements map to quality attributes, which are designed in the architecture.\n\nScalability - Adding computing resources without any interruption. Scale out (add more compute instances) is preferred over scale up (increase CPU, RAM on the existing instance). Scaling out introduces redundancy and is not limited by hardware.\nManageability - Know what’s going on and take action accordingly. The question “who reports the problems” determines if a system is manageable. The system should flag problems, not the suer.\nModularity - A system that is built from blcoks that can be changed or replaced without affecting the whole system.\nExtensibility - Functionality of the system can be extended without modifying existing code.\nTestability - How easy is it to test the application. Independent modules and methods and single responsibility principle aid testability.\n\n\n\n\nA software component is a piece of code that runs in a single process. Distributed systems are composed of independent software components deployed on separate processes/servers/containers.\nComponent architecture relates to the lower level details, whereas system architecture is the higher level view of how the components fit together to achieve the non-functional requirements.\n\n\nLayers represent horizontal functionality:\n\nUI/SI - user interface or service interface (i.e. an API), authentication\nBusiness logic - validation, enrichment, computation\nData access layer - connection handling, transaction handling, querying/saving data\n\nCode can flow downwards by one layer only. It can never skip a layer and can never go up a layer. This enforces modularity.\nA service might sit across layers, for example logging sits across the 3 layers described above. In this case, the logging service is called a “sross-cutting concern”.\nEach layer should be independent of the implementation of other layers. Layers should handle exceptions from those below them, log the error and then throw a more generic exception, so that there is no reference to other layers’ implementation.\nLayers are part of the same process. This is different from tiers, which are processes that are distributed across a network.\n\n\n\nAn interface declares the signature of an implementation. This means that each implementation must implement the methods described. The abstract base class in Python is an example of this.\nClose coupling should be avoided; “new is glue”.\n\n\n\n\nSingle responsibility principle: Each class, module or method should have exactly one responsibility.\nOpen/closed principle: Software should be open for extension but closed for modification. Can be implemented using class inheritance.\nLiskov substitution principle: If S is a subtype of T, then objects of type T can be replaced with objects of type S without altering the program. This looks similar to polymorphism but is stricter; not only should the classes implement the same methods, but the behaviour of those methods should be the same too, there should be no hidden functionality performed by S that is not performed by T or vice versa.\nInterface segregation principle: Many client-specific interfaces are preferable to one general purpose interface.\nDependency inversion principle: High-level modules should depend on abstractions rather than concrete implementations. Relies on dependency injection, where one object supplies the dependencies of another object. A factory method determines which class to load and returns an instance of that class. This also makes testing easier, as you can inject a mock class rather than having to mock out specific functionality.\n\n\n\n\n\nStructure - case, underscores\nContent - class names should be nouns, methods should be imperative verbs\n\n\n\n\nSome best practices:\n\nOnly catch exceptions if you are going to do something with it\nCatch specific exceptions\nUse try-catch on the smallest code fragments possible\nLayers should handle exceptions raised by layers below them, without exposing the specific implementation of the other layer. Catch, log, re-raise a more generic error.\n\nPurposes of logging:\n\nTrack errors\nGather data\n\n\n\n\n\nA collection of general, reusable solutions to common software design problems. Popularised in the book “Design patterns: elements of reusable object-oriented software”.\n\nThe factory pattern: Creating objects without specifying the exact class of the object. Avoids strong coupling between classes. Create an interface, then a factory method returns an instance of a class which implements that interface. If the implementation needs to change, we only need to change the factory method.\nThe repository pattern: Modules which don’t work with the data store should be oblivious to the type of data store. This is similar to the concept of the Data Access Layer. Layers are for architects, design patterns are for developers. They both seek to solve the same issue.\nThe facade pattern: Creating a layer of abstraction to mask complex actions. The facade does not create any new functionality, it is just a wrapper integrating existing modular functions together.\nThe command pattern: All the action’s information is encapsulated within an object.\n\n\n\n\nThe architecture design is the big picture that should answer the following:\n\nHow will the system work under heavy load?\nWhat will happen if the system crashes at a critical moment?\nHow complicated is it to update?\n\nThe architecture should:\n\nDefine components\nDefine how components communicate\nDefine the system’s quality attributes\n\nMake the correct choice as early as possible\n\n\nEnsuring the services are not strongly tied to other services. Avoid the “spiderweb” - when the graph of connections between services is densely connected.\nPrevents coupling of platforms and URLs.\nTo avoid URL coupling between services, two options are:\n\n“Yellow pages” directory\nGateway\n\nThe “yellow pages” contains a directory of all service URLs. If service A wants to query service D, A queries the yellow pages for D’s URL, then uses that URL to query D. This means that service A does not hardcode any information about service D. If D’s URLs change, then only the yellow pages need to be updated. Services only need to know the yellow pages directory’s URL.\n\n\n\n\n\n\nFigure 2: Yellow Pages\n\n\n\nThe gateway acts as a middleman. It holds a mapping table of all URLs Service A queries the gateway, which in turn queries service E. Service A doesn’t need to know about E or any other services. Services only need to know the gateway’s URL.\n\n\n\n\n\n\nFigure 3: Gateway\n\n\n\n\n\n\nThe application’s state is stored in only 2 places:\n\nThe data store\nThe user interface\n\nStateless architecture is preferred in virtually all circumstances.\nIn a stateful architecture, when a user logs in, the login service retrieves the user details from the database and stored them for future use. Data is stored in code.\nIf the login request was routed to server A, the user’s details are stored there. If the user later tried to add itemsto the cart and their cart service request is routed to server B, then their user details will not exist as those are stored on server A.\nDisadvantages of stateful:\n\nLack of scalability\nLack of redundancy\n\n\n\n\n\n\n\nFigure 4: Stateful\n\n\n\nIn a stateless architecture, no data is stored in the service itself. This means the behaviour will be the same regardless of which server a request was routed to.\n\n\n\n\n\n\nFigure 5: Stateless\n\n\n\n\n\n\nCaches store data locally to avoid retrieving the same data from the database multiple times. It trades reliability of data (it is stored in volatile memory) for improved performance.\nA cache should store data that is frequently accessed and rarely modified.\nTwo types of cache:\n\nIn-memory cache - Cache stored in memory on a single service\n\nPros:\n\nBest performance\nCan store any objects\nEasy to use\n\nCons:\n\nSize is limited by the process’s memory\nCan grow stale/inconsistent if the service is scaled out to multiple servers\n\n\nDistributed cache - Cache is independent of the services and can be accessed by all servers\n\nPros:\n\nSupports scaled out servers\nFailover capabilities\nStorage is unlimited\n\nCons:\n\nSetup is more complex\nOften only support primitive data types\nWorse performance than in-memory cache\n\n\n\n\n\n\nMessaging methods can be evaluated on these criteria:\n\nPerformance\nMessage size\nExecution model\nFeedback (handshaking) and reliability\nComplexity\n\nMessaging methods include:\n\nREST API\nHTTP Push\nQueue\nFile-based and database-based methods\n\n\n\nUniversal standard for HTTP-based systems.\nUseful for traditional web apps.\n\n\n\n\n\n\nFigure 6: REST API\n\n\n\n\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nVery fast\n\n\nMessage size\nSame as HTTP protocol limitations - GET 8KB, POST ~10MB\n\n\nExecution model\nRequest/response - ideal for quick, short actions\n\n\nFeedback\nImmediate feedback via response codes\n\n\nComplexity\nExtremely easy\n\n\n\n\n\n\nA client subscribes to the service waiting for an event. When that event occurs, the server notifies the client.\nFor real-time messaging, these often use web sockets to maintain the subscription connection, rather than a traditional request/response model.\nUseful for chat or monitoring.\n\n\n\n\n\n\nFigure 7: HTTP Push\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nVery fast\n\n\nMessage size\nLimited, few KB\n\n\nExecution model\nWeb socket connection / long polling\n\n\nFeedback\nNone (fire and forget)\n\n\nComplexity\nExtremely easy\n\n\n\n\n\n\nThe queue sits between two (or more) services. If service A wants to send a message to service B, A places the message in the queue and B periodically pulls from the queue.\nThis ensures messages will be handled exactly once and in the order received.\nUseful for complex systems with lots of data, when order and reliability are important.\n\n\n\n\n\n\nFigure 8: Queue\n\n\n\n\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nSlow - push/poll time and database persistence\n\n\nMessage size\nUnlimited but best practice to use small messages\n\n\nExecution model\nPolling\n\n\nFeedback\nVery reliable but feedback depends on the monitoring in place to ensure messages make it to the queue and get executed\n\n\nComplexity\nRequires training and setup to maintain a queue engine\n\n\n\n\n\n\nSimilar to queue, but rather han placing messages in a queue it places them in a file directory or database. There is no guarantee that messages are processed once and only once.\nIf multiple services are polling the same file folder, then when a new file is added we can get two issues:\n\nFile locked\nDuplicate processing\n\nSimilar use case to queues, but queues are generally preferred.\n\n\n\n\n\n\nFigure 9: File-based Messaging\n\n\n\n\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nSlow - push/poll time and database persistence\n\n\nMessage size\nUnlimited\n\n\nExecution model\nPolling\n\n\nFeedback\nVery reliable but feedback depends on the monitoring\n\n\nComplexity\nRequires training and setup to maintain the filestore or database\n\n\n\n\n\n\n\n\n\nCreate a central logging service that all other services call to write to a central database. This solves the problem where each microservice has its own log format, data and location. For example, some might be in files, SQL database, NoSQL database, etc.\nImplementation can be via an API or polling folders that each of the services write to.\n\n\n\n\n\n\nFigure 10: Central Logging Service\n\n\n\n\n\n\nCorrelation ID is an identifier attached to the beginning of a user flow and is attached to any action taken by that user, so that if there is an error at any point the user flow can be traced back through the different services’ logs.\n\n\n\n\n\nExternal considerations can affect architecture and design decisions.\n\nDeadlines\nDev team skills - New technologies can introduce uncertainty, delays and low quality\nIT support - Assign who will support the product from the outset. This should not be developers.\nCost - Build vs buy. Estimate cost vs value. Spend money to save time, don’t spend time to save money.\n\n\n\n\nThis should describe the basic elements of the system:\n\nTechnology stack\nComponents\nServices\nCommunication between components and services\n\nNo development should begin before the document is complete.\nGoals of the document:\n\nDescribe what should be developed and how\nList the functional and non-functional requirements\n\nAudience:\n\nEveryone involved with the system: project manager, CTO, QA leader, developers\nSections for management appear first as they are unlikely to read the whole document\nQA lead can begin preparing test infrastructure ahead of time\n\nFormat: UML (universal modeling language) is often used. It visualises the system’s design with concepts and diagrams. However, this assumes the audience is familiar with UML which is often not the case.\n\nKeep the contents as simple as possible\nUse plain English\nVisualise using whatever software is comfortable and appropriate\n\nStructure:\n\nBackground\nRequirements\nExecutive summary\nArchitecture overview\nComponents drill-down\n\n\n\nThis section validates your point of view and instils confidence that you understand the project.\nOne page for all team and management.\nDescribe the system from a business POV:\n\nThe system’s role\nReasons for replacing the old system\nExpected business impact\n\n\n\nFunctional and non-functional requirements - what should the system do and deal with? This section validates your understadning of the requirements. The requirements dictate the architecture so this section sets the scene for the chosen architecture.\nOne page for all team and management.\nThis should be a brief, bullet point list of requirements with no more than 3 lines on each.\n\n\n\n\nProvide a high-level view of the architecture.\nManagers will not read the whole document; &lt;3 pages for management.\n\nUse charts and diagrams\nWrite this after the rest of the document\nUse technical terms sparsely and only well known ones\nBe concise and don’t repeat yourself\n\n\n\n\nProvide a high-level view of the architecture in technical terms. Do not deep dive into specific components\n&lt;10 pages for developers and QA lead.\n\nGeneral description: type and major non-functional requirements\nHigh-level diagram of logical components, no specifics about hardware or technologies\nDiagram walkthrough: describe the various parts and their roles\nTechnology stack: iff there is a single stack include it here, otherwise include it in the components drill down section\n\n\n\n\nDetailed description of each component.\nUnlimited length, for developers and QA lead.\nFor each component:\n\nComponent’s role\nTechnology stack: Data store, backend, frontend\nComponent’s architecture: Describe the API (URL, logic, response code, comments). Describe the layers. Mention design patterns here.\nDevelopment instructions: Specific development guidelines\n\nBe specific and include rationale behind decisions.\n\n\n\n\nThese architecture patterns solve specific problems but add complexity to the system, so the costs and benefits should be compared.\n\n\nAn architecture in which functionalities are implemented as separate, loosely coupled services that interact with each other using a standard lightweight protocol.\nProblems with monolithic services:\n\nA single exception can crash the whole process\nUpdates impact all components\nLimited to one development platform/language\nUnoptimised compute resources\n\nWith microservices, each service is independent of others so can be updated separately, use a different platform, and be optimised separately.\nAn example of splitting a monolithic architecture into microservices:\n\n\n\n\n\n\nFigure 11: Monolith Example\n\n\n\nAs a microservice architecture, this becomes:\n\n\n\n\n\n\nFigure 12: Microservice Example\n\n\n\nProblems with microservices:\n\nComplex monitoring of all services and their interactions\nComplex architecture\nComplex testing\n\n\n\n\nStoring the deltas to each entity rather than updating it. The events can then be “rebuilt” from the start to give a view of the state at any given point in time.\nUse when history matters.\n\n\n\n\n\n\nFigure 13: Event Sourcing\n\n\n\nPros:\n\nTracing history\nSimple data model\nPerformance\nReporting\n\nCons:\n\nNo unified view - need to rebuild all events from the start to see the current state\nStorage usage\n\n\n\n\nCommand query responsibility segregation. Data storage and data retrieval are two separate databases, with a sync service between the two.\nThis integrates nicely with event sourcing, where events (deltas) are stored in one database and the current state is periodically built and stored in the retrieval database.\n\n\n\n\n\n\nFigure 14: CQRS\n\n\n\nPros:\n\nUseful with high-frequency updates that require near real-time querying\n\nCons:\n\nComplexity - need 2 databases, a sync service, ETL between the storage and retrieval database\n\n\n\n\n\nThe architect is often not the direct line manager of the developers implementing the software. You ultimately work with people, not software. Influence without authority is key.\n\nListening - assume you are not the smartest person in the room, collective wisdom is always better.\nDealing with criticism\n\nGenuine questioning: Provide facts and logic, be willing to go back and check again\nMocking: Don’t attack back, provide facts and logic\n\nBe smart not right\nOrganisational politics - be aware of it but don’t engage in it\nPublic speaking - define a goal, know your audience, be confident, don’t read, maintain eye contact\nLearning - blogs (DZone, InfoQ, O’Reilly), articles, conferences\n\n\n\n\n\n“The Complete Guide to Becoming a Software Architect” Udemy course\nDesign patterns\n\n\n\n\nFigure 1: Backend Tech\nFigure 2: Yellow Pages\nFigure 3: Gateway\nFigure 4: Stateful\nFigure 5: Stateless\nFigure 6: REST API\nFigure 7: HTTP Push\nFigure 8: Queue\nFigure 9: File-based Messaging\nFigure 10: Central Logging Service\nFigure 11: Monolith Example\nFigure 12: Microservice Example\nFigure 13: Event Sourcing\nFigure 14: CQRS"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#what-is-a-software-architect",
    "href": "posts/software/software_architecture/software_architect_notes.html#what-is-a-software-architect",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "A developer knows what can be done, an architect knows what should be done. How do we use technology to meet business requirements.\nGeneral system requirements:\n\nFast\nSecure\nReliable\nEasy to maintain\n\nArchitect’s need to know how to code for:\n\nArchitecture’s trustworthiness\nSupport developers\nRespect of developers"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#the-architects-mindset",
    "href": "posts/software/software_architecture/software_architect_notes.html#the-architects-mindset",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "Understand the business - Strengths, weaknesses, compettions, growth strategy.\nDefine the system’s goals - Goals are not requirements. Goals describe the effect on the organisation, requirements describe what the system should do.\nWork for your client’s clients - Prioritise the end user.\nTalk to the right people with the right language - What is the thing that really matters to the person I’m talking to?\n\nProject managers care about how things affect deadlines, developers care about the technologies used, CEOs care about business continuity and bottom line."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#the-architecture-process",
    "href": "posts/software/software_architecture/software_architect_notes.html#the-architecture-process",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "Understand the system requirements - What the system should do.\nUnderstand the non-functional requirements - Technical and service level attributes, e.g. number of users, loads, volumes, performance.\nMap the components - Understand the system functionality and communicate this to your client. Completely non-technical at this point, no mention of specific technologies. A diagram of high level components helps.\nSelect the technology stack - Backend, frontend, data store.\nDesign the architecture\nWrite the architecture document\nSupport the team\n\nInclude developers in non-functional requirements and architecture design for two reasons:\n\nLearn about unknown scenarios early\nCreate ambassadors"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#system-requirements",
    "href": "posts/software/software_architecture/software_architect_notes.html#system-requirements",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "The 2 types of requirements: functional and non-functional.\n\n\n“What the system should do”\n\nBusiness flows\nBusiness services\nUser interfaces\n\n\n\n\n“What the system should deal with”\n\nPerformance - latency and throughput\n\nLatency - How long does it take to perform a single task?\nThroughput - How many tasks can be performed in a given time unit?\n\nLoad - Quantity of work without crashing. Determines the availability of the system. Always look at peak (worst case) numbers.\nData volume - How much data will the system accumulate over time?\n\nData required on day one\nData growth (say, annually)\n\nConcurrent users - How many users will be using the system? This includes “dead times” which differentiates it from the load requirement. Rule of thumb is concurrent_users = load * 10\nSLA - Service Level Agreement for uptime.\n\nThe non-functional requirements are generally the more important in determining the architecture. The client will generally need guiding towards sensible values, otherwise they just want as much load as possible, as much uptime as possible etc."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#application-types",
    "href": "posts/software/software_architecture/software_architect_notes.html#application-types",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "The application type should be established early based on the use case and expected user interaction.\n\nWeb apps\n\nServe html pages.\nUI; user-initiated actions; large scale; short, focused actions.\nRequest-response model.\n\nWeb API\n\nServe data (often JSON).\nData retrieval and storage; client-initiated actions; large scale; short, focused actions.\nRequest-response model.\n\nMobile\n\nRequire user interaction; frontend for web API; location-based.\n\nConsole\n\nNo UI; limited interation; long-running processes; short actions for power users.\nRequire technical knowledge.\n\nService\n\nNo UI; managed by service manager; long-running processes.\n\nDesktop\n\nAll resources on the PC; UI; user-centric actions.\n\nFunction-as-a-service\n\nAWS lambda"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#select-technology-stack",
    "href": "posts/software/software_architecture/software_architect_notes.html#select-technology-stack",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "Considerations:\n\nAppropriate for the task\nCommunity - e.g. stack overflow activity\nPopularity - google trends over 2 years\n\n\n\nCovers web app, web API, console, service.\nOptions: .NET, Java, node.js, PHP, Python\n\n\n\n\n\n\nFigure 1: Backend Tech\n\n\n\n\n\n\nCovers:\n\nWeb app - Angular, React\nMobile - Native (Swift, Java/Kotlin), Xamarin, React Native\nDesktop - depends on target OS\n\n\n\n\nSQL - small, structured data\n\nRelational tables\nTransactions\n\nAtomicity\nConsistency\nIsolation\nDurability\n\nQuerying language is universal\n\nNoSQL - huge, unstructured or semi-structured data\n\nEmphasis on scale and performance.\nSchema-less, with entities stored as JSON.\nEventual consistency - data can be temporarily inconsistent\nNo universal querying language"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#the--ilities",
    "href": "posts/software/software_architecture/software_architect_notes.html#the--ilities",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "Quality attributes that describe technical capabilities to fulfill the non-functional requirements. Non-functional requirements map to quality attributes, which are designed in the architecture.\n\nScalability - Adding computing resources without any interruption. Scale out (add more compute instances) is preferred over scale up (increase CPU, RAM on the existing instance). Scaling out introduces redundancy and is not limited by hardware.\nManageability - Know what’s going on and take action accordingly. The question “who reports the problems” determines if a system is manageable. The system should flag problems, not the suer.\nModularity - A system that is built from blcoks that can be changed or replaced without affecting the whole system.\nExtensibility - Functionality of the system can be extended without modifying existing code.\nTestability - How easy is it to test the application. Independent modules and methods and single responsibility principle aid testability."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#components-architecture",
    "href": "posts/software/software_architecture/software_architect_notes.html#components-architecture",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "A software component is a piece of code that runs in a single process. Distributed systems are composed of independent software components deployed on separate processes/servers/containers.\nComponent architecture relates to the lower level details, whereas system architecture is the higher level view of how the components fit together to achieve the non-functional requirements.\n\n\nLayers represent horizontal functionality:\n\nUI/SI - user interface or service interface (i.e. an API), authentication\nBusiness logic - validation, enrichment, computation\nData access layer - connection handling, transaction handling, querying/saving data\n\nCode can flow downwards by one layer only. It can never skip a layer and can never go up a layer. This enforces modularity.\nA service might sit across layers, for example logging sits across the 3 layers described above. In this case, the logging service is called a “sross-cutting concern”.\nEach layer should be independent of the implementation of other layers. Layers should handle exceptions from those below them, log the error and then throw a more generic exception, so that there is no reference to other layers’ implementation.\nLayers are part of the same process. This is different from tiers, which are processes that are distributed across a network.\n\n\n\nAn interface declares the signature of an implementation. This means that each implementation must implement the methods described. The abstract base class in Python is an example of this.\nClose coupling should be avoided; “new is glue”.\n\n\n\n\nSingle responsibility principle: Each class, module or method should have exactly one responsibility.\nOpen/closed principle: Software should be open for extension but closed for modification. Can be implemented using class inheritance.\nLiskov substitution principle: If S is a subtype of T, then objects of type T can be replaced with objects of type S without altering the program. This looks similar to polymorphism but is stricter; not only should the classes implement the same methods, but the behaviour of those methods should be the same too, there should be no hidden functionality performed by S that is not performed by T or vice versa.\nInterface segregation principle: Many client-specific interfaces are preferable to one general purpose interface.\nDependency inversion principle: High-level modules should depend on abstractions rather than concrete implementations. Relies on dependency injection, where one object supplies the dependencies of another object. A factory method determines which class to load and returns an instance of that class. This also makes testing easier, as you can inject a mock class rather than having to mock out specific functionality.\n\n\n\n\n\nStructure - case, underscores\nContent - class names should be nouns, methods should be imperative verbs\n\n\n\n\nSome best practices:\n\nOnly catch exceptions if you are going to do something with it\nCatch specific exceptions\nUse try-catch on the smallest code fragments possible\nLayers should handle exceptions raised by layers below them, without exposing the specific implementation of the other layer. Catch, log, re-raise a more generic error.\n\nPurposes of logging:\n\nTrack errors\nGather data"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#design-patterns",
    "href": "posts/software/software_architecture/software_architect_notes.html#design-patterns",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "A collection of general, reusable solutions to common software design problems. Popularised in the book “Design patterns: elements of reusable object-oriented software”.\n\nThe factory pattern: Creating objects without specifying the exact class of the object. Avoids strong coupling between classes. Create an interface, then a factory method returns an instance of a class which implements that interface. If the implementation needs to change, we only need to change the factory method.\nThe repository pattern: Modules which don’t work with the data store should be oblivious to the type of data store. This is similar to the concept of the Data Access Layer. Layers are for architects, design patterns are for developers. They both seek to solve the same issue.\nThe facade pattern: Creating a layer of abstraction to mask complex actions. The facade does not create any new functionality, it is just a wrapper integrating existing modular functions together.\nThe command pattern: All the action’s information is encapsulated within an object."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#system-architecture",
    "href": "posts/software/software_architecture/software_architect_notes.html#system-architecture",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "The architecture design is the big picture that should answer the following:\n\nHow will the system work under heavy load?\nWhat will happen if the system crashes at a critical moment?\nHow complicated is it to update?\n\nThe architecture should:\n\nDefine components\nDefine how components communicate\nDefine the system’s quality attributes\n\nMake the correct choice as early as possible\n\n\nEnsuring the services are not strongly tied to other services. Avoid the “spiderweb” - when the graph of connections between services is densely connected.\nPrevents coupling of platforms and URLs.\nTo avoid URL coupling between services, two options are:\n\n“Yellow pages” directory\nGateway\n\nThe “yellow pages” contains a directory of all service URLs. If service A wants to query service D, A queries the yellow pages for D’s URL, then uses that URL to query D. This means that service A does not hardcode any information about service D. If D’s URLs change, then only the yellow pages need to be updated. Services only need to know the yellow pages directory’s URL.\n\n\n\n\n\n\nFigure 2: Yellow Pages\n\n\n\nThe gateway acts as a middleman. It holds a mapping table of all URLs Service A queries the gateway, which in turn queries service E. Service A doesn’t need to know about E or any other services. Services only need to know the gateway’s URL.\n\n\n\n\n\n\nFigure 3: Gateway\n\n\n\n\n\n\nThe application’s state is stored in only 2 places:\n\nThe data store\nThe user interface\n\nStateless architecture is preferred in virtually all circumstances.\nIn a stateful architecture, when a user logs in, the login service retrieves the user details from the database and stored them for future use. Data is stored in code.\nIf the login request was routed to server A, the user’s details are stored there. If the user later tried to add itemsto the cart and their cart service request is routed to server B, then their user details will not exist as those are stored on server A.\nDisadvantages of stateful:\n\nLack of scalability\nLack of redundancy\n\n\n\n\n\n\n\nFigure 4: Stateful\n\n\n\nIn a stateless architecture, no data is stored in the service itself. This means the behaviour will be the same regardless of which server a request was routed to.\n\n\n\n\n\n\nFigure 5: Stateless\n\n\n\n\n\n\nCaches store data locally to avoid retrieving the same data from the database multiple times. It trades reliability of data (it is stored in volatile memory) for improved performance.\nA cache should store data that is frequently accessed and rarely modified.\nTwo types of cache:\n\nIn-memory cache - Cache stored in memory on a single service\n\nPros:\n\nBest performance\nCan store any objects\nEasy to use\n\nCons:\n\nSize is limited by the process’s memory\nCan grow stale/inconsistent if the service is scaled out to multiple servers\n\n\nDistributed cache - Cache is independent of the services and can be accessed by all servers\n\nPros:\n\nSupports scaled out servers\nFailover capabilities\nStorage is unlimited\n\nCons:\n\nSetup is more complex\nOften only support primitive data types\nWorse performance than in-memory cache\n\n\n\n\n\n\nMessaging methods can be evaluated on these criteria:\n\nPerformance\nMessage size\nExecution model\nFeedback (handshaking) and reliability\nComplexity\n\nMessaging methods include:\n\nREST API\nHTTP Push\nQueue\nFile-based and database-based methods\n\n\n\nUniversal standard for HTTP-based systems.\nUseful for traditional web apps.\n\n\n\n\n\n\nFigure 6: REST API\n\n\n\n\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nVery fast\n\n\nMessage size\nSame as HTTP protocol limitations - GET 8KB, POST ~10MB\n\n\nExecution model\nRequest/response - ideal for quick, short actions\n\n\nFeedback\nImmediate feedback via response codes\n\n\nComplexity\nExtremely easy\n\n\n\n\n\n\nA client subscribes to the service waiting for an event. When that event occurs, the server notifies the client.\nFor real-time messaging, these often use web sockets to maintain the subscription connection, rather than a traditional request/response model.\nUseful for chat or monitoring.\n\n\n\n\n\n\nFigure 7: HTTP Push\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nVery fast\n\n\nMessage size\nLimited, few KB\n\n\nExecution model\nWeb socket connection / long polling\n\n\nFeedback\nNone (fire and forget)\n\n\nComplexity\nExtremely easy\n\n\n\n\n\n\nThe queue sits between two (or more) services. If service A wants to send a message to service B, A places the message in the queue and B periodically pulls from the queue.\nThis ensures messages will be handled exactly once and in the order received.\nUseful for complex systems with lots of data, when order and reliability are important.\n\n\n\n\n\n\nFigure 8: Queue\n\n\n\n\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nSlow - push/poll time and database persistence\n\n\nMessage size\nUnlimited but best practice to use small messages\n\n\nExecution model\nPolling\n\n\nFeedback\nVery reliable but feedback depends on the monitoring in place to ensure messages make it to the queue and get executed\n\n\nComplexity\nRequires training and setup to maintain a queue engine\n\n\n\n\n\n\nSimilar to queue, but rather han placing messages in a queue it places them in a file directory or database. There is no guarantee that messages are processed once and only once.\nIf multiple services are polling the same file folder, then when a new file is added we can get two issues:\n\nFile locked\nDuplicate processing\n\nSimilar use case to queues, but queues are generally preferred.\n\n\n\n\n\n\nFigure 9: File-based Messaging\n\n\n\n\n\n\n\n\n\n\nCriteria\nEvaluation\n\n\n\n\nPerformance\nSlow - push/poll time and database persistence\n\n\nMessage size\nUnlimited\n\n\nExecution model\nPolling\n\n\nFeedback\nVery reliable but feedback depends on the monitoring\n\n\nComplexity\nRequires training and setup to maintain the filestore or database\n\n\n\n\n\n\n\n\n\nCreate a central logging service that all other services call to write to a central database. This solves the problem where each microservice has its own log format, data and location. For example, some might be in files, SQL database, NoSQL database, etc.\nImplementation can be via an API or polling folders that each of the services write to.\n\n\n\n\n\n\nFigure 10: Central Logging Service\n\n\n\n\n\n\nCorrelation ID is an identifier attached to the beginning of a user flow and is attached to any action taken by that user, so that if there is an error at any point the user flow can be traced back through the different services’ logs."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#external-considerations",
    "href": "posts/software/software_architecture/software_architect_notes.html#external-considerations",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "External considerations can affect architecture and design decisions.\n\nDeadlines\nDev team skills - New technologies can introduce uncertainty, delays and low quality\nIT support - Assign who will support the product from the outset. This should not be developers.\nCost - Build vs buy. Estimate cost vs value. Spend money to save time, don’t spend time to save money."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#architecture-document",
    "href": "posts/software/software_architecture/software_architect_notes.html#architecture-document",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "This should describe the basic elements of the system:\n\nTechnology stack\nComponents\nServices\nCommunication between components and services\n\nNo development should begin before the document is complete.\nGoals of the document:\n\nDescribe what should be developed and how\nList the functional and non-functional requirements\n\nAudience:\n\nEveryone involved with the system: project manager, CTO, QA leader, developers\nSections for management appear first as they are unlikely to read the whole document\nQA lead can begin preparing test infrastructure ahead of time\n\nFormat: UML (universal modeling language) is often used. It visualises the system’s design with concepts and diagrams. However, this assumes the audience is familiar with UML which is often not the case.\n\nKeep the contents as simple as possible\nUse plain English\nVisualise using whatever software is comfortable and appropriate\n\nStructure:\n\nBackground\nRequirements\nExecutive summary\nArchitecture overview\nComponents drill-down\n\n\n\nThis section validates your point of view and instils confidence that you understand the project.\nOne page for all team and management.\nDescribe the system from a business POV:\n\nThe system’s role\nReasons for replacing the old system\nExpected business impact\n\n\n\nFunctional and non-functional requirements - what should the system do and deal with? This section validates your understadning of the requirements. The requirements dictate the architecture so this section sets the scene for the chosen architecture.\nOne page for all team and management.\nThis should be a brief, bullet point list of requirements with no more than 3 lines on each.\n\n\n\n\nProvide a high-level view of the architecture.\nManagers will not read the whole document; &lt;3 pages for management.\n\nUse charts and diagrams\nWrite this after the rest of the document\nUse technical terms sparsely and only well known ones\nBe concise and don’t repeat yourself\n\n\n\n\nProvide a high-level view of the architecture in technical terms. Do not deep dive into specific components\n&lt;10 pages for developers and QA lead.\n\nGeneral description: type and major non-functional requirements\nHigh-level diagram of logical components, no specifics about hardware or technologies\nDiagram walkthrough: describe the various parts and their roles\nTechnology stack: iff there is a single stack include it here, otherwise include it in the components drill down section\n\n\n\n\nDetailed description of each component.\nUnlimited length, for developers and QA lead.\nFor each component:\n\nComponent’s role\nTechnology stack: Data store, backend, frontend\nComponent’s architecture: Describe the API (URL, logic, response code, comments). Describe the layers. Mention design patterns here.\nDevelopment instructions: Specific development guidelines\n\nBe specific and include rationale behind decisions."
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#advanced-architecture-topics",
    "href": "posts/software/software_architecture/software_architect_notes.html#advanced-architecture-topics",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "These architecture patterns solve specific problems but add complexity to the system, so the costs and benefits should be compared.\n\n\nAn architecture in which functionalities are implemented as separate, loosely coupled services that interact with each other using a standard lightweight protocol.\nProblems with monolithic services:\n\nA single exception can crash the whole process\nUpdates impact all components\nLimited to one development platform/language\nUnoptimised compute resources\n\nWith microservices, each service is independent of others so can be updated separately, use a different platform, and be optimised separately.\nAn example of splitting a monolithic architecture into microservices:\n\n\n\n\n\n\nFigure 11: Monolith Example\n\n\n\nAs a microservice architecture, this becomes:\n\n\n\n\n\n\nFigure 12: Microservice Example\n\n\n\nProblems with microservices:\n\nComplex monitoring of all services and their interactions\nComplex architecture\nComplex testing\n\n\n\n\nStoring the deltas to each entity rather than updating it. The events can then be “rebuilt” from the start to give a view of the state at any given point in time.\nUse when history matters.\n\n\n\n\n\n\nFigure 13: Event Sourcing\n\n\n\nPros:\n\nTracing history\nSimple data model\nPerformance\nReporting\n\nCons:\n\nNo unified view - need to rebuild all events from the start to see the current state\nStorage usage\n\n\n\n\nCommand query responsibility segregation. Data storage and data retrieval are two separate databases, with a sync service between the two.\nThis integrates nicely with event sourcing, where events (deltas) are stored in one database and the current state is periodically built and stored in the retrieval database.\n\n\n\n\n\n\nFigure 14: CQRS\n\n\n\nPros:\n\nUseful with high-frequency updates that require near real-time querying\n\nCons:\n\nComplexity - need 2 databases, a sync service, ETL between the storage and retrieval database"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#soft-skills",
    "href": "posts/software/software_architecture/software_architect_notes.html#soft-skills",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "The architect is often not the direct line manager of the developers implementing the software. You ultimately work with people, not software. Influence without authority is key.\n\nListening - assume you are not the smartest person in the room, collective wisdom is always better.\nDealing with criticism\n\nGenuine questioning: Provide facts and logic, be willing to go back and check again\nMocking: Don’t attack back, provide facts and logic\n\nBe smart not right\nOrganisational politics - be aware of it but don’t engage in it\nPublic speaking - define a goal, know your audience, be confident, don’t read, maintain eye contact\nLearning - blogs (DZone, InfoQ, O’Reilly), articles, conferences"
  },
  {
    "objectID": "posts/software/software_architecture/software_architect_notes.html#references",
    "href": "posts/software/software_architecture/software_architect_notes.html#references",
    "title": "Software Architecture Notes",
    "section": "",
    "text": "“The Complete Guide to Becoming a Software Architect” Udemy course\nDesign patterns\n\n\n\n\nFigure 1: Backend Tech\nFigure 2: Yellow Pages\nFigure 3: Gateway\nFigure 4: Stateful\nFigure 5: Stateless\nFigure 6: REST API\nFigure 7: HTTP Push\nFigure 8: Queue\nFigure 9: File-based Messaging\nFigure 10: Central Logging Service\nFigure 11: Monolith Example\nFigure 12: Microservice Example\nFigure 13: Event Sourcing\nFigure 14: CQRS"
  },
  {
    "objectID": "posts/software/react/7_debugging/post.html",
    "href": "posts/software/react/7_debugging/post.html",
    "title": "React: Debugging",
    "section": "",
    "text": "We’ve done stylin’, now time for profilin’. Woo!\n\n\nDebug react apps using the browser console.\nThis gives the stack trace that raised the error, and the file and line number on which it was raised.\n\n\n\nUse the Sources tab (in chrome) to use the debugger.\nYou’ll see the directory structure of your project. You can click a line number to set a breakpoint and pause execution there, then observe variable values or step through execution.\nYou can also achieve the same by placing a debugger() line in the code.\n\n\n\nStrictMode is a React component which you can wrap any other component in, including the root App component.\nimport { StrictMode } from 'react';\nIt causes React to render every component twice in development mode. This can help surface errors that may not be obvious in normal execution.\n\n\n\nThis is a browser extension. It adds 2 new tabs to the console view in the browser window: profiler and components.\nComponents shows your component tree and highlights these in the browser. It also gives information about that component such as props. You can edit those props values in the browser. You can also see hooks and their state values.\n\n\n\n\nSection 7 of “React: The Complete Guide” Udemy course"
  },
  {
    "objectID": "posts/software/react/7_debugging/post.html#browser-console",
    "href": "posts/software/react/7_debugging/post.html#browser-console",
    "title": "React: Debugging",
    "section": "",
    "text": "Debug react apps using the browser console.\nThis gives the stack trace that raised the error, and the file and line number on which it was raised."
  },
  {
    "objectID": "posts/software/react/7_debugging/post.html#debugger",
    "href": "posts/software/react/7_debugging/post.html#debugger",
    "title": "React: Debugging",
    "section": "",
    "text": "Use the Sources tab (in chrome) to use the debugger.\nYou’ll see the directory structure of your project. You can click a line number to set a breakpoint and pause execution there, then observe variable values or step through execution.\nYou can also achieve the same by placing a debugger() line in the code."
  },
  {
    "objectID": "posts/software/react/7_debugging/post.html#strict-mode",
    "href": "posts/software/react/7_debugging/post.html#strict-mode",
    "title": "React: Debugging",
    "section": "",
    "text": "StrictMode is a React component which you can wrap any other component in, including the root App component.\nimport { StrictMode } from 'react';\nIt causes React to render every component twice in development mode. This can help surface errors that may not be obvious in normal execution."
  },
  {
    "objectID": "posts/software/react/7_debugging/post.html#react-developer-tools",
    "href": "posts/software/react/7_debugging/post.html#react-developer-tools",
    "title": "React: Debugging",
    "section": "",
    "text": "This is a browser extension. It adds 2 new tabs to the console view in the browser window: profiler and components.\nComponents shows your component tree and highlights these in the browser. It also gives information about that component such as props. You can edit those props values in the browser. You can also see hooks and their state values."
  },
  {
    "objectID": "posts/software/react/7_debugging/post.html#references",
    "href": "posts/software/react/7_debugging/post.html#references",
    "title": "React: Debugging",
    "section": "",
    "text": "Section 7 of “React: The Complete Guide” Udemy course"
  },
  {
    "objectID": "posts/software/react/29_testing/post.html",
    "href": "posts/software/react/29_testing/post.html",
    "title": "React: Testing",
    "section": "",
    "text": "We should run tests at different levels of granularity:\n\nUnit tests\nIntegration tests\nEnd-to-end tests\n\nWe need some build tools to help us run tests.\nJest and React Testing Library are useful for unit and integration tests, which are the main focus of this page. End-to-end testing can be done with tools like Selenium or Cypress.\n\n\nJest is a testing framework that allows us to run JavaScript tests.\nThe test function creates a unit test. It takes a name of the test and an anonymous function which runs the test code.\nThe expect function then defines some behaviour to assert. For pure JavaScript util functions, this is all we need.\nFor React components, we need to test the rendered component with the help of React Testing Library.\n\n\n\nReact testing library is a library that lets us simulate rendered components and assert characteristics of them.\nThe render function is used to simulate the dom to render a component in a test. The screen function is used to get properties from the simulated DOM, eg screen.getByText to assert a particular passage of text appears on the screen.\nThe get functions return an error if the object does not exist. The query functions return null instead. The latter is useful if we want to test when something should NOT be rendered.\nThe getByRole function is useful to pick out specific elements. See available roles here.\nThe userEvent object from React Testing Library simulates user actions like click or hover, so we can test interactive behaviour of components.\n\n\n\n\nThe general pattern for testing is: Arrange, Act, Assert.\n\nArrange: Render the component.\nAct: Any user events or interaction (if applicable for the test).\nAssert: Check the desired output is in the DOM.\n\n\n\n\nWe may want to group tests for related features/components together for readability.\nUse the describe function to define a test suite.\nIt takes a test suite description string and anonymous function as argumens. Then each of the tests are defined within it.\n\n\n\nThe get and query functions attempt to retrieve elements from the DOM immediately, as soon as the component is rendered. This is not the behaviour we want for components with asynchronous elements which may take time to fetch data.\nThe findByRole function, and related “find” functions, return a promise which will wait and re-attempt to find the element before failing. This allows us to test async code. You can pass optional args to the find functions to set how long they should wait, when to retry etc.\nWe generally want to avoid sending external requests as part of unit tests, so this is more relevant to integration or end-to-end tests.\nFor unit tests, we can mock the results.\n\n\n\nWe want to mock out external calls like fetch when we run them is our unit tests.\nWe can overwrite the window.fetch method:\nwindow.fetch = jest.fn();\nwindow.fetch.mockResolvedValueOnce({\n    json: async() =&gt; {\n        // The mock values\n        [{id: 1, text: 'example text goes here'}] \n    }\n});\n\n\n\n\nSection 29 of “React: The Complete Guide” Udemy course\nJest\nReact Testing Library\nReact Testing Library with custom hooks"
  },
  {
    "objectID": "posts/software/react/29_testing/post.html#what-to-test-and-how",
    "href": "posts/software/react/29_testing/post.html#what-to-test-and-how",
    "title": "React: Testing",
    "section": "",
    "text": "We should run tests at different levels of granularity:\n\nUnit tests\nIntegration tests\nEnd-to-end tests\n\nWe need some build tools to help us run tests.\nJest and React Testing Library are useful for unit and integration tests, which are the main focus of this page. End-to-end testing can be done with tools like Selenium or Cypress.\n\n\nJest is a testing framework that allows us to run JavaScript tests.\nThe test function creates a unit test. It takes a name of the test and an anonymous function which runs the test code.\nThe expect function then defines some behaviour to assert. For pure JavaScript util functions, this is all we need.\nFor React components, we need to test the rendered component with the help of React Testing Library.\n\n\n\nReact testing library is a library that lets us simulate rendered components and assert characteristics of them.\nThe render function is used to simulate the dom to render a component in a test. The screen function is used to get properties from the simulated DOM, eg screen.getByText to assert a particular passage of text appears on the screen.\nThe get functions return an error if the object does not exist. The query functions return null instead. The latter is useful if we want to test when something should NOT be rendered.\nThe getByRole function is useful to pick out specific elements. See available roles here.\nThe userEvent object from React Testing Library simulates user actions like click or hover, so we can test interactive behaviour of components."
  },
  {
    "objectID": "posts/software/react/29_testing/post.html#the-3-as-of-testing",
    "href": "posts/software/react/29_testing/post.html#the-3-as-of-testing",
    "title": "React: Testing",
    "section": "",
    "text": "The general pattern for testing is: Arrange, Act, Assert.\n\nArrange: Render the component.\nAct: Any user events or interaction (if applicable for the test).\nAssert: Check the desired output is in the DOM."
  },
  {
    "objectID": "posts/software/react/29_testing/post.html#test-suites",
    "href": "posts/software/react/29_testing/post.html#test-suites",
    "title": "React: Testing",
    "section": "",
    "text": "We may want to group tests for related features/components together for readability.\nUse the describe function to define a test suite.\nIt takes a test suite description string and anonymous function as argumens. Then each of the tests are defined within it."
  },
  {
    "objectID": "posts/software/react/29_testing/post.html#testing-asynchronous-code",
    "href": "posts/software/react/29_testing/post.html#testing-asynchronous-code",
    "title": "React: Testing",
    "section": "",
    "text": "The get and query functions attempt to retrieve elements from the DOM immediately, as soon as the component is rendered. This is not the behaviour we want for components with asynchronous elements which may take time to fetch data.\nThe findByRole function, and related “find” functions, return a promise which will wait and re-attempt to find the element before failing. This allows us to test async code. You can pass optional args to the find functions to set how long they should wait, when to retry etc.\nWe generally want to avoid sending external requests as part of unit tests, so this is more relevant to integration or end-to-end tests.\nFor unit tests, we can mock the results."
  },
  {
    "objectID": "posts/software/react/29_testing/post.html#using-mocks",
    "href": "posts/software/react/29_testing/post.html#using-mocks",
    "title": "React: Testing",
    "section": "",
    "text": "We want to mock out external calls like fetch when we run them is our unit tests.\nWe can overwrite the window.fetch method:\nwindow.fetch = jest.fn();\nwindow.fetch.mockResolvedValueOnce({\n    json: async() =&gt; {\n        // The mock values\n        [{id: 1, text: 'example text goes here'}] \n    }\n});"
  },
  {
    "objectID": "posts/software/react/29_testing/post.html#references",
    "href": "posts/software/react/29_testing/post.html#references",
    "title": "React: Testing",
    "section": "",
    "text": "Section 29 of “React: The Complete Guide” Udemy course\nJest\nReact Testing Library\nReact Testing Library with custom hooks"
  },
  {
    "objectID": "posts/software/react/1_getting_started/post.html",
    "href": "posts/software/react/1_getting_started/post.html",
    "title": "React: A Gentle Introduction",
    "section": "",
    "text": "React is a Javascript library for building user interfaces. It is less cumbersome and error-prone than using vanilla JS.\nCode sandbox is an in-browser environment to experiment with UIs. As an example, the same page is implemented in pure Javascript and React. The latter is much easier to follow, modularise and requires less boilerplate.\n\nWith React, you write declarative code: you define the goal, not the steps to get there.\nWith vanilla JS, you write imperative code, defining the steps, not the goal.\n\nA build tool (like Vite or Next.js) is necessary because the Javascript (specifically the JSX) must be transformed. React uses JSX which allows us to “mix” HTML and JS, so that we can define layout and functionality in the same place. This isn’t natively supported by the browser, so a build tool transforms this to pure html and JS.\n\n\n\nKey concepts in React are: components, JSX, props, and state.\n\n\nComponents are a core concept. They bundle html, CSS and JS into reusable blocks.\nIn vanilla JS, the JavaScript and HTML are in different files, so it can be hard to follow what needs to be changed where. Related code lives together, which is a key benefit of React and component style coding.\nJSX is a JavaScript syntax extension that allows us to write HTML in JavaScript files. This is not natively supported by browsers, so requires transformation by the build system, such as Vite.\nThe build process (of some but not all build tools) relies on the jsx file extension to indicate a JSX file that needs transformation. The browser does not care, as it never sees (and cannot read) these jsx files directly. Similarly, some build processes require the file extension in the import statements but others don’t.\nComponents must:\n\nStart with an upper case letter - so they do not clash with built-ins like header\nReturn a renderable object\n\nReact creates a component tree for your app. Your components do not end up in the source code directly. The build process traverses the tree until each component is resolved into built ins, and then these appear in the source code.\n\n\n\nUse curly braces to indicate dynamic values in JSX.\nIdeally declare constants rather than having complicated inline expressions.\nImages should be exported then the dynamic value passed as the src of the image. This prevents the image being lost in the build process if the build ignores files with certain extensions.\nimport myImage from './assets/exampleImage.png'\n\n&lt;img src={myImage}/&gt;\nDynamic values can be passed to components as props. React components take a single argument called props, which is an object of key:value pairs passed to the component.\nIf you have an object of props to pass, you can use the spread operator to avoid writing them out individually. Also use object destructuring inside the component to pick out the variables.\n\n\n\nIt is good practice that each component is in its own file. File name should match the component name and be the default export.\nAlso split out style CSS files and keep these alongside the component. CSS files need to be imported by each component file that uses it.\nThe styles are NOT automatically scoped to the component that uses them. They will apply to all components with that name. For example, if you apply header styling to a custom Header component, it will also apply to the built in header html component.\n\n\n\nThe children prop is passed by all components and it is the value between the component tags. It can be used for HTML tag-style syntax.\nWe can react to events by passing a function to onClick or similar. In vanilla JS, we would need to select the element and add an event listener, but react is declarative. We can define the handleClick function inside the component so that it has access to the component’s props and state. We can pass functions as props. This is useful as we can pass state setter functions down to nested components. This should be a pointer to the function, not the executed function itself, e.g. handleClick NOT handleClick()\nIf we want to modify the args that we pass to the function in onClick, use an anonymous arrow function () =&gt; handleSelect(arg) That doesn’t actually get executed until onClick is called.\nWe can set default values of props by putting the default value in the function signature.\n\n\n\nBy default, React components only execute once, even if an internal variable changes. You have to “tell” React to execute something again. This is where state comes in useful. React checks if UI updates are needed by comparing old output with new and applying the difference. So we use states rather than regular variables to indicate that a re-render is required if the state changes. State is essentially a special registered variable that react handles differently. If the state of a component changes, that component and its children in the component tree re-render.\nThe useState function is a “hook”. Hooks must be declared in the top level of a component function, they can’t be nested in internal functions such as event handlers, and they also can’t be declared outside of functions. It returns an array of two elements, the state value and a setter. A default state value can be passed to use state. The setter “schedules” an update, but that isn’t necessarily immediate. So you can see unexpected things when logging a value after the setter in code, where the logged value is still the “old” state value because the UI update has been scheduled but not completed yet.\nIf setting a state value based on its previous value, pass it as a function. For example, if on a button click we want to invert the value of a Boolean, use\nsetIsEditing((editing) =&gt; {!editing})\nNOT\nsetIsEditing(!isEditing)\nThis is because React schedules when to change state, it doesn’t necessarily do it immediately. So you could get unexpected behaviour. But when passed as a function, this triggers a re-render, similarly to how a hook does.\n\n\n\n\n\n\nOne option is to use JSX with a ternary expression. It is valid for null to be used in place of a component.\n{ selectedState ? ComponentA : ComponentB }\nAn alternative is the and operator which can also be used for this. In JavaScript, if the first term is truthy then it returns the second term, which is what we want.\n{ selectedState && ComponentA }\nA third option is to save the component as a variable and conditionally reassign it.\n// The default component\nlet tabContent = &lt;p&gt;Please select a topic&lt;/p&gt;\n\nif (selectedTopic) {\n    tabContent = (\n        // The component displayed if a topic is selected\n        &lt;div&gt;\n            &lt;h3&gt;selectedTopic.title&lt;/h3&gt;\n             &lt;p&gt;selectedTopic.description&lt;/p&gt;\n        &lt;/div&gt;\n    )\n}\n\n\n\nWe can set className as a JSX expression and use a ternary expression. For example, check if the button is selected and apply a different className depending on whether it’s selected.\n\n\n\nJSX is capable of outputting lists of renderable components. We do this by using map over the array:\nmyArray.map((item) =&gt; (&lt;Component item={item}/&gt;))\nAdd a key prop to the Component which uniquely identifies the item to avoid warnings raised by React.\n\n\n\n\nThe following sections are a collection of less essential topics but provide useful background in React.\n\n\nYou don’t NEED JSX, but it makes life easier.\nThe following is JSX, which is easy to read but requires a build transformation process.\n&lt;div id=\"content\"&gt;\n    &lt;p&gt;Hello World!&lt;/p&gt;\n&lt;/div&gt;\nThe alternative in plain JavaScript is to manipulate the DOM directly. It’s not as clear. But it avoids the need for a build process because it IS valid JavaScript supported by the browser.\nReact.createElement(\n    'div',\n    { id: 'content'},\n    React.createElement(\n        'p',\n        null,\n        'Hello World!'\n    )\n)\n\n\n\nA JavaScript function must return one value, it cannot return multiple values. This is true of React components, since they are really just syntactic sugar around JavaScript functions.\nSo if we have two or more sibling components being returned, we must wrap them in a parent component. Naively, we could just use a div, but this adds an unnecessary extra component to our tree.\nAn alternative is to use Fragment. This can be imported from react and used as a parent component without actually creating any new component when built. In newer versions of react, we can skip the import and just use empty opening and closing tags &lt;&gt; &lt;/&gt; to create a Fragment.\n\n\n\nRemember, React will re-render a component and all its child components when a state changes. So if a state is too high up the component tree, it will cause unnecessary re-rendering of many other components.\nThis is an indication that a component needs to be split out and its state managed lower down the tree.\n\n\n\nReact doesn’t auto-forward props to nested components.\nWe can forward an arbitrary number of props without having to write out each manually. Use the rest operator …extraProps in the function signature. Then use the spread operator (same syntax) to pass them to the inner component that you want …extraProps.\nThis is like **kwargs in Python.\n\n\n\nWe’ve seen how we can pass the special children prop to pass arbitrary JSX to our components.\nWhat if we wanted 2 or more slots in our component with arbitrary content? Components are ultimately just JavaScript code, so we can pass the components for the other slot as a prop (and if there are multiple siblings we can wrap them in a fragment).\n\n\n\nWe can pass a prop (with capital letter since it will be used as a custom component) which we can then vary in the nested component. For example, pass a prop called ButtonContainer which can be:\n\na string for a built in type like “menu”\na function for a custom component like Section (without calling it or using angled brackets)\n\n\n\n\nNot all content needs to go in components. Remember you can modify index.html directly if there is static content that makes more sense there.\n\n\n\nAny files (typically images) stored in the public directory of the root of the project are made publicly available, so anybody can navigate to them.\nIf you want files to be private until used on the website, store them in a folder in src, usually src/assets. Anything in src is not publicly accessible.\n\n\n\n\n\n\nIf we have an input tag, we can manage the user input value as a new state playerName.\nWe use the onChange prop of the input to handle this. We pass a handleChange function to it that takes the event (from the user input) and updates our state based on it.\nfunction handleChange (event) {\n    setPlayerName(event.target.value)\n    }\nWe then pass the value and onChange to the input\n&lt;input value={playerName} onChange={handleChange} /&gt;\nThis technique of passing a value to the input then allowing it to change the value is called a two-way binding.\n\n\n\nWhen your state is an object or array (an array is just a subset of object in JavaScript) then you should create a deep copy of it before altering its value. Objects are passed by reference so you may see unintended side effects otherwise if you try to update the object directly. Use the spread operator to copy to a new variable.\n\n\n\nIf two or more sibling components all need access to the same state, the state should be handled by the closest ancestor component.\n\n\n\nDon’t have two different states in different places which refer to the same state / data. Potential for conflicts and bugs.\nAlso avoid using logic based on state A for the setter logic of state B. For the same reason we use the functional form of the setter when it’s based on previous values; the state may be outdated as it is scheduled to updated but not yet recalculated and rendered.\n\n\n\nManage as few states as possible and pass them as props where needed, then derive any further states from those instead of managing another set of states.\n\n\n\nIf you initialise a state as an object or array, remember that JavaScript passes these by reference.\nSo if you then modify that array, you are modifying the initial array. If you later want to reset the state by setting the state back to that initial array, it won’t work because the original was mutated.\nTo work around this, take a deep copy using the spread operator. If it is a nested array, map for each inner array.\n[…initialArray.map(array =&gt; […array])]\n\n\n\nIf lifting up the state breaks the modularity of the nested components or would trigger the parent function to rerender unnecessarily, avoid lifting.\n\n\n\n\n\nSections 1, 3 and 4 of “React: The Complete Guide” Udemy course"
  },
  {
    "objectID": "posts/software/react/1_getting_started/post.html#what-is-react",
    "href": "posts/software/react/1_getting_started/post.html#what-is-react",
    "title": "React: A Gentle Introduction",
    "section": "",
    "text": "React is a Javascript library for building user interfaces. It is less cumbersome and error-prone than using vanilla JS.\nCode sandbox is an in-browser environment to experiment with UIs. As an example, the same page is implemented in pure Javascript and React. The latter is much easier to follow, modularise and requires less boilerplate.\n\nWith React, you write declarative code: you define the goal, not the steps to get there.\nWith vanilla JS, you write imperative code, defining the steps, not the goal.\n\nA build tool (like Vite or Next.js) is necessary because the Javascript (specifically the JSX) must be transformed. React uses JSX which allows us to “mix” HTML and JS, so that we can define layout and functionality in the same place. This isn’t natively supported by the browser, so a build tool transforms this to pure html and JS."
  },
  {
    "objectID": "posts/software/react/1_getting_started/post.html#key-react-concepts",
    "href": "posts/software/react/1_getting_started/post.html#key-react-concepts",
    "title": "React: A Gentle Introduction",
    "section": "",
    "text": "Key concepts in React are: components, JSX, props, and state.\n\n\nComponents are a core concept. They bundle html, CSS and JS into reusable blocks.\nIn vanilla JS, the JavaScript and HTML are in different files, so it can be hard to follow what needs to be changed where. Related code lives together, which is a key benefit of React and component style coding.\nJSX is a JavaScript syntax extension that allows us to write HTML in JavaScript files. This is not natively supported by browsers, so requires transformation by the build system, such as Vite.\nThe build process (of some but not all build tools) relies on the jsx file extension to indicate a JSX file that needs transformation. The browser does not care, as it never sees (and cannot read) these jsx files directly. Similarly, some build processes require the file extension in the import statements but others don’t.\nComponents must:\n\nStart with an upper case letter - so they do not clash with built-ins like header\nReturn a renderable object\n\nReact creates a component tree for your app. Your components do not end up in the source code directly. The build process traverses the tree until each component is resolved into built ins, and then these appear in the source code.\n\n\n\nUse curly braces to indicate dynamic values in JSX.\nIdeally declare constants rather than having complicated inline expressions.\nImages should be exported then the dynamic value passed as the src of the image. This prevents the image being lost in the build process if the build ignores files with certain extensions.\nimport myImage from './assets/exampleImage.png'\n\n&lt;img src={myImage}/&gt;\nDynamic values can be passed to components as props. React components take a single argument called props, which is an object of key:value pairs passed to the component.\nIf you have an object of props to pass, you can use the spread operator to avoid writing them out individually. Also use object destructuring inside the component to pick out the variables.\n\n\n\nIt is good practice that each component is in its own file. File name should match the component name and be the default export.\nAlso split out style CSS files and keep these alongside the component. CSS files need to be imported by each component file that uses it.\nThe styles are NOT automatically scoped to the component that uses them. They will apply to all components with that name. For example, if you apply header styling to a custom Header component, it will also apply to the built in header html component.\n\n\n\nThe children prop is passed by all components and it is the value between the component tags. It can be used for HTML tag-style syntax.\nWe can react to events by passing a function to onClick or similar. In vanilla JS, we would need to select the element and add an event listener, but react is declarative. We can define the handleClick function inside the component so that it has access to the component’s props and state. We can pass functions as props. This is useful as we can pass state setter functions down to nested components. This should be a pointer to the function, not the executed function itself, e.g. handleClick NOT handleClick()\nIf we want to modify the args that we pass to the function in onClick, use an anonymous arrow function () =&gt; handleSelect(arg) That doesn’t actually get executed until onClick is called.\nWe can set default values of props by putting the default value in the function signature.\n\n\n\nBy default, React components only execute once, even if an internal variable changes. You have to “tell” React to execute something again. This is where state comes in useful. React checks if UI updates are needed by comparing old output with new and applying the difference. So we use states rather than regular variables to indicate that a re-render is required if the state changes. State is essentially a special registered variable that react handles differently. If the state of a component changes, that component and its children in the component tree re-render.\nThe useState function is a “hook”. Hooks must be declared in the top level of a component function, they can’t be nested in internal functions such as event handlers, and they also can’t be declared outside of functions. It returns an array of two elements, the state value and a setter. A default state value can be passed to use state. The setter “schedules” an update, but that isn’t necessarily immediate. So you can see unexpected things when logging a value after the setter in code, where the logged value is still the “old” state value because the UI update has been scheduled but not completed yet.\nIf setting a state value based on its previous value, pass it as a function. For example, if on a button click we want to invert the value of a Boolean, use\nsetIsEditing((editing) =&gt; {!editing})\nNOT\nsetIsEditing(!isEditing)\nThis is because React schedules when to change state, it doesn’t necessarily do it immediately. So you could get unexpected behaviour. But when passed as a function, this triggers a re-render, similarly to how a hook does."
  },
  {
    "objectID": "posts/software/react/1_getting_started/post.html#dynamic-content",
    "href": "posts/software/react/1_getting_started/post.html#dynamic-content",
    "title": "React: A Gentle Introduction",
    "section": "",
    "text": "One option is to use JSX with a ternary expression. It is valid for null to be used in place of a component.\n{ selectedState ? ComponentA : ComponentB }\nAn alternative is the and operator which can also be used for this. In JavaScript, if the first term is truthy then it returns the second term, which is what we want.\n{ selectedState && ComponentA }\nA third option is to save the component as a variable and conditionally reassign it.\n// The default component\nlet tabContent = &lt;p&gt;Please select a topic&lt;/p&gt;\n\nif (selectedTopic) {\n    tabContent = (\n        // The component displayed if a topic is selected\n        &lt;div&gt;\n            &lt;h3&gt;selectedTopic.title&lt;/h3&gt;\n             &lt;p&gt;selectedTopic.description&lt;/p&gt;\n        &lt;/div&gt;\n    )\n}\n\n\n\nWe can set className as a JSX expression and use a ternary expression. For example, check if the button is selected and apply a different className depending on whether it’s selected.\n\n\n\nJSX is capable of outputting lists of renderable components. We do this by using map over the array:\nmyArray.map((item) =&gt; (&lt;Component item={item}/&gt;))\nAdd a key prop to the Component which uniquely identifies the item to avoid warnings raised by React."
  },
  {
    "objectID": "posts/software/react/1_getting_started/post.html#going-deeper",
    "href": "posts/software/react/1_getting_started/post.html#going-deeper",
    "title": "React: A Gentle Introduction",
    "section": "",
    "text": "The following sections are a collection of less essential topics but provide useful background in React.\n\n\nYou don’t NEED JSX, but it makes life easier.\nThe following is JSX, which is easy to read but requires a build transformation process.\n&lt;div id=\"content\"&gt;\n    &lt;p&gt;Hello World!&lt;/p&gt;\n&lt;/div&gt;\nThe alternative in plain JavaScript is to manipulate the DOM directly. It’s not as clear. But it avoids the need for a build process because it IS valid JavaScript supported by the browser.\nReact.createElement(\n    'div',\n    { id: 'content'},\n    React.createElement(\n        'p',\n        null,\n        'Hello World!'\n    )\n)\n\n\n\nA JavaScript function must return one value, it cannot return multiple values. This is true of React components, since they are really just syntactic sugar around JavaScript functions.\nSo if we have two or more sibling components being returned, we must wrap them in a parent component. Naively, we could just use a div, but this adds an unnecessary extra component to our tree.\nAn alternative is to use Fragment. This can be imported from react and used as a parent component without actually creating any new component when built. In newer versions of react, we can skip the import and just use empty opening and closing tags &lt;&gt; &lt;/&gt; to create a Fragment.\n\n\n\nRemember, React will re-render a component and all its child components when a state changes. So if a state is too high up the component tree, it will cause unnecessary re-rendering of many other components.\nThis is an indication that a component needs to be split out and its state managed lower down the tree.\n\n\n\nReact doesn’t auto-forward props to nested components.\nWe can forward an arbitrary number of props without having to write out each manually. Use the rest operator …extraProps in the function signature. Then use the spread operator (same syntax) to pass them to the inner component that you want …extraProps.\nThis is like **kwargs in Python.\n\n\n\nWe’ve seen how we can pass the special children prop to pass arbitrary JSX to our components.\nWhat if we wanted 2 or more slots in our component with arbitrary content? Components are ultimately just JavaScript code, so we can pass the components for the other slot as a prop (and if there are multiple siblings we can wrap them in a fragment).\n\n\n\nWe can pass a prop (with capital letter since it will be used as a custom component) which we can then vary in the nested component. For example, pass a prop called ButtonContainer which can be:\n\na string for a built in type like “menu”\na function for a custom component like Section (without calling it or using angled brackets)\n\n\n\n\nNot all content needs to go in components. Remember you can modify index.html directly if there is static content that makes more sense there.\n\n\n\nAny files (typically images) stored in the public directory of the root of the project are made publicly available, so anybody can navigate to them.\nIf you want files to be private until used on the website, store them in a folder in src, usually src/assets. Anything in src is not publicly accessible."
  },
  {
    "objectID": "posts/software/react/1_getting_started/post.html#more-on-states",
    "href": "posts/software/react/1_getting_started/post.html#more-on-states",
    "title": "React: A Gentle Introduction",
    "section": "",
    "text": "If we have an input tag, we can manage the user input value as a new state playerName.\nWe use the onChange prop of the input to handle this. We pass a handleChange function to it that takes the event (from the user input) and updates our state based on it.\nfunction handleChange (event) {\n    setPlayerName(event.target.value)\n    }\nWe then pass the value and onChange to the input\n&lt;input value={playerName} onChange={handleChange} /&gt;\nThis technique of passing a value to the input then allowing it to change the value is called a two-way binding.\n\n\n\nWhen your state is an object or array (an array is just a subset of object in JavaScript) then you should create a deep copy of it before altering its value. Objects are passed by reference so you may see unintended side effects otherwise if you try to update the object directly. Use the spread operator to copy to a new variable.\n\n\n\nIf two or more sibling components all need access to the same state, the state should be handled by the closest ancestor component.\n\n\n\nDon’t have two different states in different places which refer to the same state / data. Potential for conflicts and bugs.\nAlso avoid using logic based on state A for the setter logic of state B. For the same reason we use the functional form of the setter when it’s based on previous values; the state may be outdated as it is scheduled to updated but not yet recalculated and rendered.\n\n\n\nManage as few states as possible and pass them as props where needed, then derive any further states from those instead of managing another set of states.\n\n\n\nIf you initialise a state as an object or array, remember that JavaScript passes these by reference.\nSo if you then modify that array, you are modifying the initial array. If you later want to reset the state by setting the state back to that initial array, it won’t work because the original was mutated.\nTo work around this, take a deep copy using the spread operator. If it is a nested array, map for each inner array.\n[…initialArray.map(array =&gt; […array])]\n\n\n\nIf lifting up the state breaks the modularity of the nested components or would trigger the parent function to rerender unnecessarily, avoid lifting."
  },
  {
    "objectID": "posts/software/react/1_getting_started/post.html#references",
    "href": "posts/software/react/1_getting_started/post.html#references",
    "title": "React: A Gentle Introduction",
    "section": "",
    "text": "Sections 1, 3 and 4 of “React: The Complete Guide” Udemy course"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson3/chapter3.html",
    "href": "posts/ml/generative_deep_learning/lesson3/chapter3.html",
    "title": "Generative AI: VAEs",
    "section": "",
    "text": "Notes on VAEs.\n\n\n\n\n\n\nStory Time\n\n\n\nImagine an infinite wardrobe organised by “type” of clothing.\nShoes would be close together, but formal shoes might be closer to the suits and trainers closer to the sports gear. Shirts and t-shirts would be close together. Coats might be nearby; the shirt-&gt;coat vector applied to t-shirts might lead you to “invent” gilets.\nThis encapsulates the idea of using a lower dimensional (2D in this case) latent space to encode the representation of more complex objects.\nWe could sample from some of the empty spaces to invent new hybrids of clothing. This generative step is decoding the latent space.\n\n\n\n\nThe idea of autoencoders (read: self-encoders) is that they learn to simplify the input then reconstruct it; the input and target output are the same.\n\nThe encoder learns to compress high-dimensional input data into a lower dimensional representation called the embedding.\nThe decoder takes an embedding and recreates a higher-dimensional image. This should be an accurate reconstruction of the input.\n\nThis can be used as a generative model because we can the sample and decode new points from the latent space to generate novel outputs. The goal of training an autoencoder is to learn a meaningful embedding \\(z\\).\n\n\n\n\n\nflowchart LR\n\n  A(Encoder) --&gt; B(z)\n  B(z) --&gt; c(Decoder)\n\n\n\n\n\n\nThis also makes autoencoders useful as denoising models, because the embedding should retain the salient information but “lose” the noise.\n\n\n\nWe will implement an autoencoder to learn lower-dimensional embeddings for the fashion MNIST data set.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nimport tensorflow as tf\nfrom tensorflow.keras import (\n    layers,\n    models,\n    datasets,\n    callbacks,\n    losses,\n    optimizers,\n    metrics,\n)\n\n\n# Parameters\nIMAGE_SIZE = 32\nCHANNELS = 1\nBATCH_SIZE = 100\nBUFFER_SIZE = 1000\nVALIDATION_SPLIT = 0.2\nEMBEDDING_DIM = 2\nEPOCHS = 3\n\n\n\n\nScale the pixel values and reshape the images.\n\n\nCode\n(x_train, y_train), (x_test, y_test) = datasets.fashion_mnist.load_data()\n\ndef preprocess(images):\n    images = images.astype(\"float32\") / 255.0\n    images = np.pad(images, ((0, 0), (2, 2), (2, 2)), constant_values=0.0)\n    images = np.expand_dims(images, -1)\n    return images\n\nx_train = preprocess(x_train)\nx_test = preprocess(x_test)\n\n\nWe can see an example from our training set:\n\n\nCode\nplt.imshow(x_train[0])\n\n\n\n\n\n\n\n\n\n\n\n\nThe encoder compresses the dimensionality on the input to a smaller embedding dimension.\n\n\nCode\n# Input\nencoder_input = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS),name=\"encoder_input\")\n\n# Conv layers\nx = layers.Conv2D(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(encoder_input)\nx = layers.Conv2D(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2D(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\npre_flatten_shape = tf.keras.backend.int_shape(x)[1:]  # Used by the decoder later\n\n# Output\nx = layers.Flatten()(x)\nencoder_output = layers.Dense(EMBEDDING_DIM, name=\"encoder_output\")(x)\n\n# Model\nencoder = models.Model(encoder_input, encoder_output)\nencoder.summary()\n\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n encoder_input (InputLayer)  [(None, 32, 32, 1)]       0         \n                                                                 \n conv2d (Conv2D)             (None, 16, 16, 32)        320       \n                                                                 \n conv2d_1 (Conv2D)           (None, 8, 8, 64)          18496     \n                                                                 \n conv2d_2 (Conv2D)           (None, 4, 4, 128)         73856     \n                                                                 \n flatten (Flatten)           (None, 2048)              0         \n                                                                 \n encoder_output (Dense)      (None, 2)                 4098      \n                                                                 \n=================================================================\nTotal params: 96770 (378.01 KB)\nTrainable params: 96770 (378.01 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\nThe decoder reconstructs the original image from the embedding.\n\n\nIn a standard convolutional layer, if we have stride=2 it will half the image size.\nIn a convolutional transpose layer, we are increasing the image size. The stride parameter determines the amount of zero padding to add between each pixel. A kernel is then applied to this “internally padded” image to expand the image size.\n\n\nCode\n# Input\ndecoder_input = layers.Input(shape=(EMBEDDING_DIM,),name=\"decoder_input\")\n\n# Reshape the input using the pre-flattening shape from the encoder\nx = layers.Dense(np.prod(pre_flatten_shape))(decoder_input)\nx = layers.Reshape(pre_flatten_shape)(x)\n\n# Scale up the image back to its original size. These are the reverse of the conv layers applied in the encoder.\nx = layers.Conv2DTranspose(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2DTranspose(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n\n# Output\ndecoder_output = layers.Conv2D(\n    CHANNELS,\n    (3, 3),\n    strides=1,\n    activation='sigmoid',\n    padding=\"same\",\n    name=\"decoder_output\",\n)(x)\n\n# Model\ndecoder = models.Model(decoder_input, decoder_output)\ndecoder.summary()\n\n\nModel: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n decoder_input (InputLayer)  [(None, 2)]               0         \n                                                                 \n dense (Dense)               (None, 2048)              6144      \n                                                                 \n reshape (Reshape)           (None, 4, 4, 128)         0         \n                                                                 \n conv2d_transpose (Conv2DTr  (None, 8, 8, 128)         147584    \n anspose)                                                        \n                                                                 \n conv2d_transpose_1 (Conv2D  (None, 16, 16, 64)        73792     \n Transpose)                                                      \n                                                                 \n conv2d_transpose_2 (Conv2D  (None, 32, 32, 32)        18464     \n Transpose)                                                      \n                                                                 \n decoder_output (Conv2D)     (None, 32, 32, 1)         289       \n                                                                 \n=================================================================\nTotal params: 246273 (962.00 KB)\nTrainable params: 246273 (962.00 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\n\nCombine the encoder and decoder into a single model.\n\n\nCode\nautoencoder = models.Model(encoder_input, decoder(encoder_output))\nautoencoder.summary()\n\n\nModel: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n encoder_input (InputLayer)  [(None, 32, 32, 1)]       0         \n                                                                 \n conv2d (Conv2D)             (None, 16, 16, 32)        320       \n                                                                 \n conv2d_1 (Conv2D)           (None, 8, 8, 64)          18496     \n                                                                 \n conv2d_2 (Conv2D)           (None, 4, 4, 128)         73856     \n                                                                 \n flatten (Flatten)           (None, 2048)              0         \n                                                                 \n encoder_output (Dense)      (None, 2)                 4098      \n                                                                 \n model_1 (Functional)        (None, 32, 32, 1)         246273    \n                                                                 \n=================================================================\nTotal params: 343043 (1.31 MB)\nTrainable params: 343043 (1.31 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\nThe autoencoder is trained with the source images as both input and target output.\nThe loss function is usually chosen as either RMSE or binary cross-entropy between pixels of original image vs reconstruction.\n\n\nCode\nautoencoder.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\nautoencoder.fit(\n    x_train,\n    x_train,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    validation_data=(x_test, x_test)\n)\n\n\nEpoch 1/3\n600/600 [==============================] - 35s 58ms/step - loss: 0.2910 - val_loss: 0.2610\nEpoch 2/3\n600/600 [==============================] - 36s 60ms/step - loss: 0.2569 - val_loss: 0.2561\nEpoch 3/3\n600/600 [==============================] - 34s 57ms/step - loss: 0.2536 - val_loss: 0.2540\n\n\n&lt;keras.src.callbacks.History at 0x156e4a310&gt;\n\n\n\n\n\n\nWe can use our trained autoencoder to:\n\nReconstruct images\nAnalyse embeddings\nGenerate new images\n\n\n\nReconstruct a sample of test images using the autoencoder.\nThe reconstruction isn’t perfect; some information is lost when reducing down to just 2 dimensions. But it does a surprisingly good job of compressing 32x32 pixel values into just 2 embedding values.\n\n\nCode\nNUM_IMAGES_TO_RECONSTRUCT = 5000\nexample_images = x_test[:NUM_IMAGES_TO_RECONSTRUCT]\nexample_labels = y_test[:NUM_IMAGES_TO_RECONSTRUCT]\n\npredictions = autoencoder.predict(example_images)\n\n\n  7/157 [&gt;.............................] - ETA: 1s 157/157 [==============================] - 1s 8ms/step\n\n\nOriginal images:\n\n\nCode\ndef plot_sample_images(images, n=10, size=(20, 3), cmap=\"gray_r\"):\n    plt.figure(figsize=size)\n    for i in range(n):\n        _ = plt.subplot(1, n, i + 1)\n        plt.imshow(images[i].astype(\"float32\"), cmap=cmap)\n        plt.axis(\"off\")\n    \n    plt.show()\n\nplot_sample_images(example_images)\n\n\n\n\n\n\n\n\n\nReconstructed images:\n\n\nCode\nplot_sample_images(predictions)\n\n\n\n\n\n\n\n\n\n\n\n\nEach of the images above has been encoded as a 2-dimensional embedding.\nWe can look at these embeddings to gain some insight into how the autoencoder works.\nThe embedding vectors for our sample images above:\n\n\nCode\n# Encode the example images\nembeddings = encoder.predict(example_images)\nprint(embeddings[:10])\n\n\n102/157 [==================&gt;...........] - ETA: 0s157/157 [==============================] - 0s 2ms/step\n[[ 2.2441912  -2.711683  ]\n [ 6.1558456   6.0202003 ]\n [-3.787192    7.3368516 ]\n [-2.5938551   4.2098355 ]\n [ 3.8645594   2.7229536 ]\n [-2.0130231   6.0485506 ]\n [ 1.2749226   2.1347647 ]\n [ 2.8239484   2.898773  ]\n [-0.48542604 -1.0869933 ]\n [ 0.30643728 -2.6099105 ]]\n\n\nWe can plot the 2D latent space, colouring each point by its label. This shows how similar items are clustered together in latent space.\nThis is impressive! Remember, we never showed the model the labels when training, so it has learned to cluster images that look alike.\n\n\nCode\n# Colour the embeddings by their label\nexample_labels = y_test[:NUM_IMAGES_TO_RECONSTRUCT]\n\n# Plot the latent space\nfigsize = 8\nplt.figure(figsize=(figsize, figsize))\nplt.scatter(\n    embeddings[:, 0],\n    embeddings[:, 1],\n    cmap=\"rainbow\",\n    c=example_labels,\n    alpha=0.6,\n    s=3,\n)\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nWe can sample from the latent space and decode these sampled points to generate new images.\nFirst we sample some random points in the latent space:\n\n\nCode\n# Get the range of existing embedding values so we can sample sensible points within the latent space.\nembedding_min = np.min(embeddings, axis=0)\nembedding_max = np.max(embeddings, axis=0)\n\n# Sample some points\ngrid_width = 6\ngrid_height = 3\nsample = np.random.uniform(\n    embedding_min, embedding_max, size=(grid_width * grid_height, EMBEDDING_DIM)\n)\nprint(sample)\n\n\n[[ 1.47862929  9.28394749]\n [-3.19389344 -3.04713146]\n [-0.57161452 -0.35644389]\n [10.97632621 -2.12482484]\n [ 4.05160668  9.04420005]\n [ 9.50105167  5.71270956]\n [ 3.24765456  4.95969011]\n [-3.68217634  4.52120851]\n [-1.7067196   5.87696959]\n [ 5.99883565 -2.11597183]\n [ 1.84553131  6.04266323]\n [ 0.15552252  1.98655625]\n [ 3.55479856  2.35587959]\n [-0.32278762  6.07537408]\n [ 8.98977414 -1.15893539]\n [ 2.1476981   4.97819188]\n [-2.0896675   3.9166368 ]\n [ 6.49229371 -4.75611412]]\n\n\nWe can then decode these sampled points.\n\n\nCode\n# Decode the sampled points\nreconstructions = decoder.predict(sample)\n\n\n1/1 [==============================] - 0s 59ms/step\n\n\n\n\nCode\nfigsize = 8\nplt.figure(figsize=(figsize, figsize))\n\n# Plot the latent space and overlay the positions of the sampled points\nplt.scatter(embeddings[:, 0], embeddings[:, 1], c=\"black\", alpha=0.5, s=2)\nplt.scatter(sample[:, 0], sample[:, 1], c=\"#00B0F0\", alpha=1, s=40)\nplt.show()\n\n# Plot a grid of the reconstructed images which decode those sampled points\nfig = plt.figure(figsize=(figsize, grid_height * 2))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i in range(grid_width * grid_height):\n    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n    ax.axis(\"off\")\n    ax.text(\n        0.5,\n        -0.35,\n        str(np.round(sample[i, :], 1)),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow let’s see what happens when we regularly sample the latent space.\n\n\nCode\n# Colour the embeddings by their label (clothing type - see table)\nfigsize = 12\ngrid_size = 15\nplt.figure(figsize=(figsize, figsize))\nplt.scatter(\n    embeddings[:, 0],\n    embeddings[:, 1],\n    cmap=\"rainbow\",\n    c=example_labels,\n    alpha=0.8,\n    s=300,\n)\nplt.colorbar()\n\nx = np.linspace(min(embeddings[:, 0]), max(embeddings[:, 0]), grid_size)\ny = np.linspace(max(embeddings[:, 1]), min(embeddings[:, 1]), grid_size)\nxv, yv = np.meshgrid(x, y)\nxv = xv.flatten()\nyv = yv.flatten()\ngrid = np.array(list(zip(xv, yv)))\n\nreconstructions = decoder.predict(grid)\n# plt.scatter(grid[:, 0], grid[:, 1], c=\"black\", alpha=1, s=10)\nplt.show()\n\nfig = plt.figure(figsize=(figsize, figsize))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\nfor i in range(grid_size**2):\n    ax = fig.add_subplot(grid_size, grid_size, i + 1)\n    ax.axis(\"off\")\n    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")\n\n\n8/8 [==============================] - 0s 9ms/step\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe latent space exploration above yields some interesting insights into “regular” autoencoders that motivate the use of variational autoencoders to address these shortcomings.\n\nDifferent categories occupy varying amounts of area in latent space.\nThe latent space distribution is not symmetrical or bounded.\nThere are gaps in the latent space.\n\nThis makes it difficult for us to sample from this latent space effectively. We could sample a “gap” and get a nonsensical image. If a category (say, trousers) occupies a larger area in latent space, we are more likely to generate images of trousers than of categories which occupy a small area (say, shoes).\n\n\n\n\n\n\n\n\n\n\nStory Time\n\n\n\nIf we revisit our wardrobe, rather than assigning each item to a specific location, let’s assign it to a general region of the wardrobe.\nAnd let’s also insist that this region should be as close to the centre of the wardrobe as possible, otherwise we are penalised. This should yield a more uniform latent space.\nThis is the idea behind variational autoencoders (VAE).\n\n\n\n\nIn a standard autoencoder, each image is mapped directly to one point in the latent space.\nIn a variational autoencoder, each image is mapped to a multivariate Normal distribution around a point in the latent space. Variational autoencoders assume their is no correlation between latent space dimensions.\nSo we will typically use isotropic Normal distributions, meaning the covariance matrix is diagonal so the distribution is independent in each dimension. The encoder only needs to map each input to a mean vector and a variance vector; it does not need to worry about covariances.\nIn practice we choose to map to log variances because this can be any value in the range \\((-\\infty, \\infty)\\) which gives a smoother value to learn rather than variances whihc are positive.\nIn summary, the encoder maps \\(image \\rightarrow (z_{mean}, z_{log\\_var})\\)\nWe can then sample a point \\(z\\) from this distribution using:\n\\[\nz = z_{mean} + z_{sigma} * epsilon\n\\]\nwhere: \\[\nz_{sigma} = e^{z_{log\\_var} * 0.5}\n\\] \\[\nepsilon \\sim \\mathcal{N}(0, I)\n\\]\n\n\n\nThis is identical to the standard autoencoder.\n\n\n\nPutting these together, we get the overall architecture:\n\n\n\n\n\nflowchart LR\n\n\n  A[Encoder] --&gt; B1(z_mean)\n  A[Encoder] --&gt; B2(z_log_var)\n\n  B1(z_mean) --&gt; C[sample]\n  B2(z_log_var) --&gt; C[sample]\n\n  C[sample] --&gt; D(z)\n  D(z) --&gt; E[Decoder]\n\n\n\n\n\n\nWhy does this change to the encoder help?\nIn the standard autoencoder, there is no requirement for the latent space to be continuous. So we could sample a point, say, \\((1, 2)\\) and decode it to a well-formed image. But there is no guarantee that a point next to it \\((1.1, 2.1)\\) would look similar or even be intelligible.\nThe “variational” part of the VAE addresses this problem. We now sample from an area around z_mean, so the decoder must ensure that all points in that region produce similar images to keep the reconstruction loss small.\n\n\n\nRather than sample directly from a Normal distribution parameterised by z_mean and z_log_var, we can sample epsilon from a standard Normal distribution and manually adjust the sample to correct its mean and variance.\nThis means gradients can backpropagate freely through the layer. The randomness in the layer is all encapsulated in epsilon, so the partial derivative of the layer output w.r.t. the layer input is deterministic, making backpropagation possible.\n\n\n\nThe loss function of the standard autoencoder was the reconstruction loss between original image and its decoded version.\nFor VAEs, we add an additional term which encourages points to have small mean and variance by penalising z_mean and z_log_var variables that differ significantly from 0.\nThis is the Kullback-Leibler (KL) divergence. It measures how much one probability distribution differs from another. We use it to measure how much our Normal distribution, with parameters z_mean and z_log_var, differs from a standard Normal distribution.\nFor this special case of KL divergence between our Normal distribution and a standard Normal, the closed form solution is: \\[\nD_{KL}[\\mathcal{N}(\\mu, \\sigma) || \\mathcal{N}(0, 1)] = -\\frac{1}{2} \\sum (1 + \\log(\\sigma ^2) - \\mu ^2 - \\sigma ^ 2)\n\\]\nSo using our variables, we can describe this in code as:\nkl_loss = -0.5 * sum(1 + z_log_var - z_mean ** 2 - exp(z_log_var))\nThis loss is minimised when z_mean=0 and z_log_var=0, i.e. it encourages our distrubution towards a stand Normal distribution, thus using the space around the origin symmetrically and efficently with few gaps.\nThe original paper simply summed the reconstruction_loss and the kl_loss. A variant of this includes a hyperparameter \\(\\beta\\) to vary the weight of the KL divergence term. This is called a “\\(\\beta-VAE\\)”:\nvae_loss = reconstruction_error + beta * kl_loss\n\n\n\n\n\n\nWe need a sampling layer which allows us to sample \\(z\\) from the distribution defined by \\(z_{mean}\\) and \\(z_{log\\_var}\\).\n\n\nCode\nclass Sampling(layers.Layer):\n    def call(self, z_mean, z_log_var):\n        batch = tf.shape(z_mean)[0]\n        dim = tf.shape(z_mean)[1]\n        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n\n\n\n\n\nThe encoder incorporates the Sampling layer as the final step. This is what is passed to the decoder.\n\n\nCode\n# Encoder\nencoder_input = layers.Input(\n    shape=(IMAGE_SIZE, IMAGE_SIZE, 1), name=\"encoder_input\"\n)\nx = layers.Conv2D(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(encoder_input)\nx = layers.Conv2D(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2D(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nshape_before_flattening = tf.keras.backend.int_shape(x)[1:]  # the decoder will need this!\n\nx = layers.Flatten()(x)\nz_mean = layers.Dense(EMBEDDING_DIM, name=\"z_mean\")(x)\nz_log_var = layers.Dense(EMBEDDING_DIM, name=\"z_log_var\")(x)\nz = Sampling()(z_mean, z_log_var)\n\nencoder = models.Model(encoder_input, [z_mean, z_log_var, z], name=\"encoder\")\nencoder.summary()\n\n\nModel: \"encoder\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n encoder_input (InputLayer)  [(None, 32, 32, 1)]          0         []                            \n                                                                                                  \n conv2d_3 (Conv2D)           (None, 16, 16, 32)           320       ['encoder_input[0][0]']       \n                                                                                                  \n conv2d_4 (Conv2D)           (None, 8, 8, 64)             18496     ['conv2d_3[0][0]']            \n                                                                                                  \n conv2d_5 (Conv2D)           (None, 4, 4, 128)            73856     ['conv2d_4[0][0]']            \n                                                                                                  \n flatten_1 (Flatten)         (None, 2048)                 0         ['conv2d_5[0][0]']            \n                                                                                                  \n z_mean (Dense)              (None, 2)                    4098      ['flatten_1[0][0]']           \n                                                                                                  \n z_log_var (Dense)           (None, 2)                    4098      ['flatten_1[0][0]']           \n                                                                                                  \n sampling (Sampling)         (None, 2)                    0         ['z_mean[0][0]',              \n                                                                     'z_log_var[0][0]']           \n                                                                                                  \n==================================================================================================\nTotal params: 100868 (394.02 KB)\nTrainable params: 100868 (394.02 KB)\nNon-trainable params: 0 (0.00 Byte)\n__________________________________________________________________________________________________\n\n\n\n\n\nThe decoder is the same as a standard autoencoder.\n\n\nCode\n# Decoder\ndecoder_input = layers.Input(shape=(EMBEDDING_DIM,), name=\"decoder_input\")\nx = layers.Dense(np.prod(shape_before_flattening))(decoder_input)\nx = layers.Reshape(shape_before_flattening)(x)\nx = layers.Conv2DTranspose(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2DTranspose(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n\ndecoder_output = layers.Conv2D(1, (3, 3), strides=1, activation=\"sigmoid\", padding=\"same\", name=\"decoder_output\")(x)\n\ndecoder = models.Model(decoder_input, decoder_output)\ndecoder.summary()\n\n\nModel: \"model_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n decoder_input (InputLayer)  [(None, 2)]               0         \n                                                                 \n dense_1 (Dense)             (None, 2048)              6144      \n                                                                 \n reshape_1 (Reshape)         (None, 4, 4, 128)         0         \n                                                                 \n conv2d_transpose_3 (Conv2D  (None, 8, 8, 128)         147584    \n Transpose)                                                      \n                                                                 \n conv2d_transpose_4 (Conv2D  (None, 16, 16, 64)        73792     \n Transpose)                                                      \n                                                                 \n conv2d_transpose_5 (Conv2D  (None, 32, 32, 32)        18464     \n Transpose)                                                      \n                                                                 \n decoder_output (Conv2D)     (None, 32, 32, 1)         289       \n                                                                 \n=================================================================\nTotal params: 246273 (962.00 KB)\nTrainable params: 246273 (962.00 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\nPutting the encoder and decoder together.\n\n\nCode\nEPOCHS = 5\nBETA = 500\n\n\n\n\nCode\nclass VAE(models.Model):\n    def __init__(self, encoder, decoder, **kwargs):\n        super(VAE, self).__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n        self.total_loss_tracker = metrics.Mean(name=\"total_loss\")\n        self.reconstruction_loss_tracker = metrics.Mean(name=\"reconstruction_loss\")\n        self.kl_loss_tracker = metrics.Mean(name=\"kl_loss\")\n\n    @property\n    def metrics(self):\n        return [\n            self.total_loss_tracker,\n            self.reconstruction_loss_tracker,\n            self.kl_loss_tracker,\n        ]\n\n    def call(self, inputs):\n        \"\"\"Call the model on a particular input.\"\"\"\n        z_mean, z_log_var, z = encoder(inputs)\n        reconstruction = decoder(z)\n        return z_mean, z_log_var, reconstruction\n\n    def train_step(self, data):\n        \"\"\"Step run during training.\"\"\"\n        with tf.GradientTape() as tape:\n            z_mean, z_log_var, reconstruction = self(data)\n            reconstruction_loss = tf.reduce_mean(\n                BETA * losses.binary_crossentropy(data, reconstruction, axis=(1, 2, 3))\n            )\n            kl_loss = tf.reduce_mean(\n                tf.reduce_sum(-0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)), axis=1)\n            )\n            total_loss = reconstruction_loss + kl_loss\n\n        grads = tape.gradient(total_loss, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n\n        self.total_loss_tracker.update_state(total_loss)\n        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n        self.kl_loss_tracker.update_state(kl_loss)\n\n        return {m.name: m.result() for m in self.metrics}\n\n    def test_step(self, data):\n        \"\"\"Step run during validation.\"\"\"\n        if isinstance(data, tuple):\n            data = data[0]\n\n        z_mean, z_log_var, reconstruction = self(data)\n        reconstruction_loss = tf.reduce_mean(\n            BETA * losses.binary_crossentropy(data, reconstruction, axis=(1, 2, 3))\n        )\n        kl_loss = tf.reduce_mean(\n            tf.reduce_sum(\n                -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)),\n                axis=1,\n            )\n        )\n        total_loss = reconstruction_loss + kl_loss\n\n        return {\n            \"loss\": total_loss,\n            \"reconstruction_loss\": reconstruction_loss,\n            \"kl_loss\": kl_loss,\n        }\n\n\nInstantiate the VAE model and compile it.\n\n\nCode\nvae = VAE(encoder, decoder)\n\n# optimizer = optimizers.Adam(learning_rate=0.0005)\noptimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0005)\nvae.compile(optimizer=optimizer)\n\n\n\n\n\nTrain the VAE as before.\n\n\nCode\nvae.fit(\n    x_train,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    validation_data=(x_test, x_test),\n)\n\n\nEpoch 1/5\n600/600 [==============================] - 37s 61ms/step - total_loss: 160.4693 - reconstruction_loss: 155.9913 - kl_loss: 4.4779 - val_loss: 141.2442 - val_reconstruction_loss: 136.1877 - val_kl_loss: 5.0565\nEpoch 2/5\n600/600 [==============================] - 34s 57ms/step - total_loss: 135.9397 - reconstruction_loss: 130.9409 - kl_loss: 4.9988 - val_loss: 138.5623 - val_reconstruction_loss: 133.5856 - val_kl_loss: 4.9767\nEpoch 3/5\n600/600 [==============================] - 34s 56ms/step - total_loss: 134.3719 - reconstruction_loss: 129.3381 - kl_loss: 5.0338 - val_loss: 137.1351 - val_reconstruction_loss: 132.1540 - val_kl_loss: 4.9811\nEpoch 4/5\n600/600 [==============================] - 34s 56ms/step - total_loss: 133.4455 - reconstruction_loss: 128.3819 - kl_loss: 5.0637 - val_loss: 136.5461 - val_reconstruction_loss: 131.4780 - val_kl_loss: 5.0681\nEpoch 5/5\n600/600 [==============================] - 34s 57ms/step - total_loss: 132.7808 - reconstruction_loss: 127.6688 - kl_loss: 5.1120 - val_loss: 135.8917 - val_reconstruction_loss: 130.7375 - val_kl_loss: 5.1542\n\n\n&lt;keras.src.callbacks.History at 0x2c59e9f10&gt;\n\n\n\n\n\n\n\n\nAs before, we can eyeball the reconstructions from our model.\n\n\nCode\n# Select a subset of the test set\nn_to_predict = 5000\nexample_images = x_test[:n_to_predict]\nexample_labels = y_test[:n_to_predict]\n\n# Create autoencoder predictions and display\nz_mean, z_log_var, reconstructions = vae.predict(example_images)\nprint(\"Example real clothing items\")\nplot_sample_images(example_images)\nprint(\"Reconstructions\")\nplot_sample_images(reconstructions)\n\n\n 42/157 [=======&gt;......................] - ETA: 1s157/157 [==============================] - 1s 9ms/step\nExample real clothing items\nReconstructions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can sample and decode points in the latent space to observe how the model generates images. We note that:\n\nThe latent space has more even coverage and does not stray to far from a standard Normal distribution. If this is not the case, we can vary the \\(\\beta\\) value used to give more weight to the KL loss term.\nWe do not see as many poorly formed images as we did when sampling a “gap” in a standard autoencoder.\n\n\n\nCode\n# Encode the example images\nz_mean, z_var, z = encoder.predict(example_images)\n\n# Sample some points in the latent space, from the standard normal distribution\ngrid_width, grid_height = (6, 3)\nz_sample = np.random.normal(size=(grid_width * grid_height, 2))\n# Decode the sampled points\nreconstructions = decoder.predict(z_sample)\n# Convert original embeddings and sampled embeddings to p-values\np = norm.cdf(z)\np_sample = norm.cdf(z_sample)\n# Draw a plot of...\nfigsize = 8\nplt.figure(figsize=(figsize, figsize))\n\n# ... the original embeddings ...\nplt.scatter(z[:, 0], z[:, 1], c=\"black\", alpha=0.5, s=2)\n\n# ... and the newly generated points in the latent space\nplt.scatter(z_sample[:, 0], z_sample[:, 1], c=\"#00B0F0\", alpha=1, s=40)\nplt.show()\n\n# Add underneath a grid of the decoded images\nfig = plt.figure(figsize=(figsize, grid_height * 2))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i in range(grid_width * grid_height):\n    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n    ax.axis(\"off\")\n    ax.text(\n        0.5,\n        -0.35,\n        str(np.round(z_sample[i, :], 1)),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")\n\n\n1/1 [==============================] - 0s 14ms/step\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe plots below show the latent space coloured by clothing type. The left plot shows this in terms of z-values and the right in terms of p-values.\nThe latent space is more continuous with fewer gaps, and different categories take similar amounts of space.\n\n\nCode\n# Colour the embeddings by their label (clothing type - see table)\nfigsize = 8\nfig = plt.figure(figsize=(figsize * 2, figsize))\nax = fig.add_subplot(1, 2, 1)\nplot_1 = ax.scatter(\n    z[:, 0], z[:, 1], cmap=\"rainbow\", c=example_labels, alpha=0.8, s=3\n)\nplt.colorbar(plot_1)\nax = fig.add_subplot(1, 2, 2)\nplot_2 = ax.scatter(\n    p[:, 0], p[:, 1], cmap=\"rainbow\", c=example_labels, alpha=0.8, s=3\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nNext we see what happens when we sample from the latent space in a regular grid.\n\n\nCode\n# Colour the embeddings by their label (clothing type - see table)\nfigsize = 12\ngrid_size = 15\nplt.figure(figsize=(figsize, figsize))\nplt.scatter(\n    p[:, 0], p[:, 1], cmap=\"rainbow\", c=example_labels, alpha=0.8, s=300\n)\nplt.colorbar()\n\nx = norm.ppf(np.linspace(0, 1, grid_size))\ny = norm.ppf(np.linspace(1, 0, grid_size))\nxv, yv = np.meshgrid(x, y)\nxv = xv.flatten()\nyv = yv.flatten()\ngrid = np.array(list(zip(xv, yv)))\n\nreconstructions = decoder.predict(grid)\n# plt.scatter(grid[:, 0], grid[:, 1], c=\"black\", alpha=1, s=10)\nplt.show()\n\nfig = plt.figure(figsize=(figsize, figsize))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\nfor i in range(grid_size**2):\n    ax = fig.add_subplot(grid_size, grid_size, i + 1)\n    ax.axis(\"off\")\n    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")\n\n\n8/8 [==============================] - 0s 6ms/step\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 3 of Generative Deep Learning by David Foster."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson3/chapter3.html#autoencoders",
    "href": "posts/ml/generative_deep_learning/lesson3/chapter3.html#autoencoders",
    "title": "Generative AI: VAEs",
    "section": "",
    "text": "The idea of autoencoders (read: self-encoders) is that they learn to simplify the input then reconstruct it; the input and target output are the same.\n\nThe encoder learns to compress high-dimensional input data into a lower dimensional representation called the embedding.\nThe decoder takes an embedding and recreates a higher-dimensional image. This should be an accurate reconstruction of the input.\n\nThis can be used as a generative model because we can the sample and decode new points from the latent space to generate novel outputs. The goal of training an autoencoder is to learn a meaningful embedding \\(z\\).\n\n\n\n\n\nflowchart LR\n\n  A(Encoder) --&gt; B(z)\n  B(z) --&gt; c(Decoder)\n\n\n\n\n\n\nThis also makes autoencoders useful as denoising models, because the embedding should retain the salient information but “lose” the noise."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson3/chapter3.html#building-an-autoencoder",
    "href": "posts/ml/generative_deep_learning/lesson3/chapter3.html#building-an-autoencoder",
    "title": "Generative AI: VAEs",
    "section": "",
    "text": "We will implement an autoencoder to learn lower-dimensional embeddings for the fashion MNIST data set.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nimport tensorflow as tf\nfrom tensorflow.keras import (\n    layers,\n    models,\n    datasets,\n    callbacks,\n    losses,\n    optimizers,\n    metrics,\n)\n\n\n# Parameters\nIMAGE_SIZE = 32\nCHANNELS = 1\nBATCH_SIZE = 100\nBUFFER_SIZE = 1000\nVALIDATION_SPLIT = 0.2\nEMBEDDING_DIM = 2\nEPOCHS = 3\n\n\n\n\nScale the pixel values and reshape the images.\n\n\nCode\n(x_train, y_train), (x_test, y_test) = datasets.fashion_mnist.load_data()\n\ndef preprocess(images):\n    images = images.astype(\"float32\") / 255.0\n    images = np.pad(images, ((0, 0), (2, 2), (2, 2)), constant_values=0.0)\n    images = np.expand_dims(images, -1)\n    return images\n\nx_train = preprocess(x_train)\nx_test = preprocess(x_test)\n\n\nWe can see an example from our training set:\n\n\nCode\nplt.imshow(x_train[0])\n\n\n\n\n\n\n\n\n\n\n\n\nThe encoder compresses the dimensionality on the input to a smaller embedding dimension.\n\n\nCode\n# Input\nencoder_input = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS),name=\"encoder_input\")\n\n# Conv layers\nx = layers.Conv2D(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(encoder_input)\nx = layers.Conv2D(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2D(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\npre_flatten_shape = tf.keras.backend.int_shape(x)[1:]  # Used by the decoder later\n\n# Output\nx = layers.Flatten()(x)\nencoder_output = layers.Dense(EMBEDDING_DIM, name=\"encoder_output\")(x)\n\n# Model\nencoder = models.Model(encoder_input, encoder_output)\nencoder.summary()\n\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n encoder_input (InputLayer)  [(None, 32, 32, 1)]       0         \n                                                                 \n conv2d (Conv2D)             (None, 16, 16, 32)        320       \n                                                                 \n conv2d_1 (Conv2D)           (None, 8, 8, 64)          18496     \n                                                                 \n conv2d_2 (Conv2D)           (None, 4, 4, 128)         73856     \n                                                                 \n flatten (Flatten)           (None, 2048)              0         \n                                                                 \n encoder_output (Dense)      (None, 2)                 4098      \n                                                                 \n=================================================================\nTotal params: 96770 (378.01 KB)\nTrainable params: 96770 (378.01 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\nThe decoder reconstructs the original image from the embedding.\n\n\nIn a standard convolutional layer, if we have stride=2 it will half the image size.\nIn a convolutional transpose layer, we are increasing the image size. The stride parameter determines the amount of zero padding to add between each pixel. A kernel is then applied to this “internally padded” image to expand the image size.\n\n\nCode\n# Input\ndecoder_input = layers.Input(shape=(EMBEDDING_DIM,),name=\"decoder_input\")\n\n# Reshape the input using the pre-flattening shape from the encoder\nx = layers.Dense(np.prod(pre_flatten_shape))(decoder_input)\nx = layers.Reshape(pre_flatten_shape)(x)\n\n# Scale up the image back to its original size. These are the reverse of the conv layers applied in the encoder.\nx = layers.Conv2DTranspose(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2DTranspose(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n\n# Output\ndecoder_output = layers.Conv2D(\n    CHANNELS,\n    (3, 3),\n    strides=1,\n    activation='sigmoid',\n    padding=\"same\",\n    name=\"decoder_output\",\n)(x)\n\n# Model\ndecoder = models.Model(decoder_input, decoder_output)\ndecoder.summary()\n\n\nModel: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n decoder_input (InputLayer)  [(None, 2)]               0         \n                                                                 \n dense (Dense)               (None, 2048)              6144      \n                                                                 \n reshape (Reshape)           (None, 4, 4, 128)         0         \n                                                                 \n conv2d_transpose (Conv2DTr  (None, 8, 8, 128)         147584    \n anspose)                                                        \n                                                                 \n conv2d_transpose_1 (Conv2D  (None, 16, 16, 64)        73792     \n Transpose)                                                      \n                                                                 \n conv2d_transpose_2 (Conv2D  (None, 32, 32, 32)        18464     \n Transpose)                                                      \n                                                                 \n decoder_output (Conv2D)     (None, 32, 32, 1)         289       \n                                                                 \n=================================================================\nTotal params: 246273 (962.00 KB)\nTrainable params: 246273 (962.00 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\n\nCombine the encoder and decoder into a single model.\n\n\nCode\nautoencoder = models.Model(encoder_input, decoder(encoder_output))\nautoencoder.summary()\n\n\nModel: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n encoder_input (InputLayer)  [(None, 32, 32, 1)]       0         \n                                                                 \n conv2d (Conv2D)             (None, 16, 16, 32)        320       \n                                                                 \n conv2d_1 (Conv2D)           (None, 8, 8, 64)          18496     \n                                                                 \n conv2d_2 (Conv2D)           (None, 4, 4, 128)         73856     \n                                                                 \n flatten (Flatten)           (None, 2048)              0         \n                                                                 \n encoder_output (Dense)      (None, 2)                 4098      \n                                                                 \n model_1 (Functional)        (None, 32, 32, 1)         246273    \n                                                                 \n=================================================================\nTotal params: 343043 (1.31 MB)\nTrainable params: 343043 (1.31 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\nThe autoencoder is trained with the source images as both input and target output.\nThe loss function is usually chosen as either RMSE or binary cross-entropy between pixels of original image vs reconstruction.\n\n\nCode\nautoencoder.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\nautoencoder.fit(\n    x_train,\n    x_train,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    validation_data=(x_test, x_test)\n)\n\n\nEpoch 1/3\n600/600 [==============================] - 35s 58ms/step - loss: 0.2910 - val_loss: 0.2610\nEpoch 2/3\n600/600 [==============================] - 36s 60ms/step - loss: 0.2569 - val_loss: 0.2561\nEpoch 3/3\n600/600 [==============================] - 34s 57ms/step - loss: 0.2536 - val_loss: 0.2540\n\n\n&lt;keras.src.callbacks.History at 0x156e4a310&gt;"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson3/chapter3.html#analysing-the-autoencoder",
    "href": "posts/ml/generative_deep_learning/lesson3/chapter3.html#analysing-the-autoencoder",
    "title": "Generative AI: VAEs",
    "section": "",
    "text": "We can use our trained autoencoder to:\n\nReconstruct images\nAnalyse embeddings\nGenerate new images\n\n\n\nReconstruct a sample of test images using the autoencoder.\nThe reconstruction isn’t perfect; some information is lost when reducing down to just 2 dimensions. But it does a surprisingly good job of compressing 32x32 pixel values into just 2 embedding values.\n\n\nCode\nNUM_IMAGES_TO_RECONSTRUCT = 5000\nexample_images = x_test[:NUM_IMAGES_TO_RECONSTRUCT]\nexample_labels = y_test[:NUM_IMAGES_TO_RECONSTRUCT]\n\npredictions = autoencoder.predict(example_images)\n\n\n  7/157 [&gt;.............................] - ETA: 1s 157/157 [==============================] - 1s 8ms/step\n\n\nOriginal images:\n\n\nCode\ndef plot_sample_images(images, n=10, size=(20, 3), cmap=\"gray_r\"):\n    plt.figure(figsize=size)\n    for i in range(n):\n        _ = plt.subplot(1, n, i + 1)\n        plt.imshow(images[i].astype(\"float32\"), cmap=cmap)\n        plt.axis(\"off\")\n    \n    plt.show()\n\nplot_sample_images(example_images)\n\n\n\n\n\n\n\n\n\nReconstructed images:\n\n\nCode\nplot_sample_images(predictions)\n\n\n\n\n\n\n\n\n\n\n\n\nEach of the images above has been encoded as a 2-dimensional embedding.\nWe can look at these embeddings to gain some insight into how the autoencoder works.\nThe embedding vectors for our sample images above:\n\n\nCode\n# Encode the example images\nembeddings = encoder.predict(example_images)\nprint(embeddings[:10])\n\n\n102/157 [==================&gt;...........] - ETA: 0s157/157 [==============================] - 0s 2ms/step\n[[ 2.2441912  -2.711683  ]\n [ 6.1558456   6.0202003 ]\n [-3.787192    7.3368516 ]\n [-2.5938551   4.2098355 ]\n [ 3.8645594   2.7229536 ]\n [-2.0130231   6.0485506 ]\n [ 1.2749226   2.1347647 ]\n [ 2.8239484   2.898773  ]\n [-0.48542604 -1.0869933 ]\n [ 0.30643728 -2.6099105 ]]\n\n\nWe can plot the 2D latent space, colouring each point by its label. This shows how similar items are clustered together in latent space.\nThis is impressive! Remember, we never showed the model the labels when training, so it has learned to cluster images that look alike.\n\n\nCode\n# Colour the embeddings by their label\nexample_labels = y_test[:NUM_IMAGES_TO_RECONSTRUCT]\n\n# Plot the latent space\nfigsize = 8\nplt.figure(figsize=(figsize, figsize))\nplt.scatter(\n    embeddings[:, 0],\n    embeddings[:, 1],\n    cmap=\"rainbow\",\n    c=example_labels,\n    alpha=0.6,\n    s=3,\n)\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nWe can sample from the latent space and decode these sampled points to generate new images.\nFirst we sample some random points in the latent space:\n\n\nCode\n# Get the range of existing embedding values so we can sample sensible points within the latent space.\nembedding_min = np.min(embeddings, axis=0)\nembedding_max = np.max(embeddings, axis=0)\n\n# Sample some points\ngrid_width = 6\ngrid_height = 3\nsample = np.random.uniform(\n    embedding_min, embedding_max, size=(grid_width * grid_height, EMBEDDING_DIM)\n)\nprint(sample)\n\n\n[[ 1.47862929  9.28394749]\n [-3.19389344 -3.04713146]\n [-0.57161452 -0.35644389]\n [10.97632621 -2.12482484]\n [ 4.05160668  9.04420005]\n [ 9.50105167  5.71270956]\n [ 3.24765456  4.95969011]\n [-3.68217634  4.52120851]\n [-1.7067196   5.87696959]\n [ 5.99883565 -2.11597183]\n [ 1.84553131  6.04266323]\n [ 0.15552252  1.98655625]\n [ 3.55479856  2.35587959]\n [-0.32278762  6.07537408]\n [ 8.98977414 -1.15893539]\n [ 2.1476981   4.97819188]\n [-2.0896675   3.9166368 ]\n [ 6.49229371 -4.75611412]]\n\n\nWe can then decode these sampled points.\n\n\nCode\n# Decode the sampled points\nreconstructions = decoder.predict(sample)\n\n\n1/1 [==============================] - 0s 59ms/step\n\n\n\n\nCode\nfigsize = 8\nplt.figure(figsize=(figsize, figsize))\n\n# Plot the latent space and overlay the positions of the sampled points\nplt.scatter(embeddings[:, 0], embeddings[:, 1], c=\"black\", alpha=0.5, s=2)\nplt.scatter(sample[:, 0], sample[:, 1], c=\"#00B0F0\", alpha=1, s=40)\nplt.show()\n\n# Plot a grid of the reconstructed images which decode those sampled points\nfig = plt.figure(figsize=(figsize, grid_height * 2))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i in range(grid_width * grid_height):\n    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n    ax.axis(\"off\")\n    ax.text(\n        0.5,\n        -0.35,\n        str(np.round(sample[i, :], 1)),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow let’s see what happens when we regularly sample the latent space.\n\n\nCode\n# Colour the embeddings by their label (clothing type - see table)\nfigsize = 12\ngrid_size = 15\nplt.figure(figsize=(figsize, figsize))\nplt.scatter(\n    embeddings[:, 0],\n    embeddings[:, 1],\n    cmap=\"rainbow\",\n    c=example_labels,\n    alpha=0.8,\n    s=300,\n)\nplt.colorbar()\n\nx = np.linspace(min(embeddings[:, 0]), max(embeddings[:, 0]), grid_size)\ny = np.linspace(max(embeddings[:, 1]), min(embeddings[:, 1]), grid_size)\nxv, yv = np.meshgrid(x, y)\nxv = xv.flatten()\nyv = yv.flatten()\ngrid = np.array(list(zip(xv, yv)))\n\nreconstructions = decoder.predict(grid)\n# plt.scatter(grid[:, 0], grid[:, 1], c=\"black\", alpha=1, s=10)\nplt.show()\n\nfig = plt.figure(figsize=(figsize, figsize))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\nfor i in range(grid_size**2):\n    ax = fig.add_subplot(grid_size, grid_size, i + 1)\n    ax.axis(\"off\")\n    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")\n\n\n8/8 [==============================] - 0s 9ms/step\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe latent space exploration above yields some interesting insights into “regular” autoencoders that motivate the use of variational autoencoders to address these shortcomings.\n\nDifferent categories occupy varying amounts of area in latent space.\nThe latent space distribution is not symmetrical or bounded.\nThere are gaps in the latent space.\n\nThis makes it difficult for us to sample from this latent space effectively. We could sample a “gap” and get a nonsensical image. If a category (say, trousers) occupies a larger area in latent space, we are more likely to generate images of trousers than of categories which occupy a small area (say, shoes)."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson3/chapter3.html#variational-autoencoders-1",
    "href": "posts/ml/generative_deep_learning/lesson3/chapter3.html#variational-autoencoders-1",
    "title": "Generative AI: VAEs",
    "section": "",
    "text": "Story Time\n\n\n\nIf we revisit our wardrobe, rather than assigning each item to a specific location, let’s assign it to a general region of the wardrobe.\nAnd let’s also insist that this region should be as close to the centre of the wardrobe as possible, otherwise we are penalised. This should yield a more uniform latent space.\nThis is the idea behind variational autoencoders (VAE).\n\n\n\n\nIn a standard autoencoder, each image is mapped directly to one point in the latent space.\nIn a variational autoencoder, each image is mapped to a multivariate Normal distribution around a point in the latent space. Variational autoencoders assume their is no correlation between latent space dimensions.\nSo we will typically use isotropic Normal distributions, meaning the covariance matrix is diagonal so the distribution is independent in each dimension. The encoder only needs to map each input to a mean vector and a variance vector; it does not need to worry about covariances.\nIn practice we choose to map to log variances because this can be any value in the range \\((-\\infty, \\infty)\\) which gives a smoother value to learn rather than variances whihc are positive.\nIn summary, the encoder maps \\(image \\rightarrow (z_{mean}, z_{log\\_var})\\)\nWe can then sample a point \\(z\\) from this distribution using:\n\\[\nz = z_{mean} + z_{sigma} * epsilon\n\\]\nwhere: \\[\nz_{sigma} = e^{z_{log\\_var} * 0.5}\n\\] \\[\nepsilon \\sim \\mathcal{N}(0, I)\n\\]\n\n\n\nThis is identical to the standard autoencoder.\n\n\n\nPutting these together, we get the overall architecture:\n\n\n\n\n\nflowchart LR\n\n\n  A[Encoder] --&gt; B1(z_mean)\n  A[Encoder] --&gt; B2(z_log_var)\n\n  B1(z_mean) --&gt; C[sample]\n  B2(z_log_var) --&gt; C[sample]\n\n  C[sample] --&gt; D(z)\n  D(z) --&gt; E[Decoder]\n\n\n\n\n\n\nWhy does this change to the encoder help?\nIn the standard autoencoder, there is no requirement for the latent space to be continuous. So we could sample a point, say, \\((1, 2)\\) and decode it to a well-formed image. But there is no guarantee that a point next to it \\((1.1, 2.1)\\) would look similar or even be intelligible.\nThe “variational” part of the VAE addresses this problem. We now sample from an area around z_mean, so the decoder must ensure that all points in that region produce similar images to keep the reconstruction loss small.\n\n\n\nRather than sample directly from a Normal distribution parameterised by z_mean and z_log_var, we can sample epsilon from a standard Normal distribution and manually adjust the sample to correct its mean and variance.\nThis means gradients can backpropagate freely through the layer. The randomness in the layer is all encapsulated in epsilon, so the partial derivative of the layer output w.r.t. the layer input is deterministic, making backpropagation possible.\n\n\n\nThe loss function of the standard autoencoder was the reconstruction loss between original image and its decoded version.\nFor VAEs, we add an additional term which encourages points to have small mean and variance by penalising z_mean and z_log_var variables that differ significantly from 0.\nThis is the Kullback-Leibler (KL) divergence. It measures how much one probability distribution differs from another. We use it to measure how much our Normal distribution, with parameters z_mean and z_log_var, differs from a standard Normal distribution.\nFor this special case of KL divergence between our Normal distribution and a standard Normal, the closed form solution is: \\[\nD_{KL}[\\mathcal{N}(\\mu, \\sigma) || \\mathcal{N}(0, 1)] = -\\frac{1}{2} \\sum (1 + \\log(\\sigma ^2) - \\mu ^2 - \\sigma ^ 2)\n\\]\nSo using our variables, we can describe this in code as:\nkl_loss = -0.5 * sum(1 + z_log_var - z_mean ** 2 - exp(z_log_var))\nThis loss is minimised when z_mean=0 and z_log_var=0, i.e. it encourages our distrubution towards a stand Normal distribution, thus using the space around the origin symmetrically and efficently with few gaps.\nThe original paper simply summed the reconstruction_loss and the kl_loss. A variant of this includes a hyperparameter \\(\\beta\\) to vary the weight of the KL divergence term. This is called a “\\(\\beta-VAE\\)”:\nvae_loss = reconstruction_error + beta * kl_loss"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson3/chapter3.html#building-a-variational-autoencoder-vae",
    "href": "posts/ml/generative_deep_learning/lesson3/chapter3.html#building-a-variational-autoencoder-vae",
    "title": "Generative AI: VAEs",
    "section": "",
    "text": "We need a sampling layer which allows us to sample \\(z\\) from the distribution defined by \\(z_{mean}\\) and \\(z_{log\\_var}\\).\n\n\nCode\nclass Sampling(layers.Layer):\n    def call(self, z_mean, z_log_var):\n        batch = tf.shape(z_mean)[0]\n        dim = tf.shape(z_mean)[1]\n        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n\n\n\n\n\nThe encoder incorporates the Sampling layer as the final step. This is what is passed to the decoder.\n\n\nCode\n# Encoder\nencoder_input = layers.Input(\n    shape=(IMAGE_SIZE, IMAGE_SIZE, 1), name=\"encoder_input\"\n)\nx = layers.Conv2D(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(encoder_input)\nx = layers.Conv2D(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2D(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nshape_before_flattening = tf.keras.backend.int_shape(x)[1:]  # the decoder will need this!\n\nx = layers.Flatten()(x)\nz_mean = layers.Dense(EMBEDDING_DIM, name=\"z_mean\")(x)\nz_log_var = layers.Dense(EMBEDDING_DIM, name=\"z_log_var\")(x)\nz = Sampling()(z_mean, z_log_var)\n\nencoder = models.Model(encoder_input, [z_mean, z_log_var, z], name=\"encoder\")\nencoder.summary()\n\n\nModel: \"encoder\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n encoder_input (InputLayer)  [(None, 32, 32, 1)]          0         []                            \n                                                                                                  \n conv2d_3 (Conv2D)           (None, 16, 16, 32)           320       ['encoder_input[0][0]']       \n                                                                                                  \n conv2d_4 (Conv2D)           (None, 8, 8, 64)             18496     ['conv2d_3[0][0]']            \n                                                                                                  \n conv2d_5 (Conv2D)           (None, 4, 4, 128)            73856     ['conv2d_4[0][0]']            \n                                                                                                  \n flatten_1 (Flatten)         (None, 2048)                 0         ['conv2d_5[0][0]']            \n                                                                                                  \n z_mean (Dense)              (None, 2)                    4098      ['flatten_1[0][0]']           \n                                                                                                  \n z_log_var (Dense)           (None, 2)                    4098      ['flatten_1[0][0]']           \n                                                                                                  \n sampling (Sampling)         (None, 2)                    0         ['z_mean[0][0]',              \n                                                                     'z_log_var[0][0]']           \n                                                                                                  \n==================================================================================================\nTotal params: 100868 (394.02 KB)\nTrainable params: 100868 (394.02 KB)\nNon-trainable params: 0 (0.00 Byte)\n__________________________________________________________________________________________________\n\n\n\n\n\nThe decoder is the same as a standard autoencoder.\n\n\nCode\n# Decoder\ndecoder_input = layers.Input(shape=(EMBEDDING_DIM,), name=\"decoder_input\")\nx = layers.Dense(np.prod(shape_before_flattening))(decoder_input)\nx = layers.Reshape(shape_before_flattening)(x)\nx = layers.Conv2DTranspose(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2DTranspose(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n\ndecoder_output = layers.Conv2D(1, (3, 3), strides=1, activation=\"sigmoid\", padding=\"same\", name=\"decoder_output\")(x)\n\ndecoder = models.Model(decoder_input, decoder_output)\ndecoder.summary()\n\n\nModel: \"model_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n decoder_input (InputLayer)  [(None, 2)]               0         \n                                                                 \n dense_1 (Dense)             (None, 2048)              6144      \n                                                                 \n reshape_1 (Reshape)         (None, 4, 4, 128)         0         \n                                                                 \n conv2d_transpose_3 (Conv2D  (None, 8, 8, 128)         147584    \n Transpose)                                                      \n                                                                 \n conv2d_transpose_4 (Conv2D  (None, 16, 16, 64)        73792     \n Transpose)                                                      \n                                                                 \n conv2d_transpose_5 (Conv2D  (None, 32, 32, 32)        18464     \n Transpose)                                                      \n                                                                 \n decoder_output (Conv2D)     (None, 32, 32, 1)         289       \n                                                                 \n=================================================================\nTotal params: 246273 (962.00 KB)\nTrainable params: 246273 (962.00 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\nPutting the encoder and decoder together.\n\n\nCode\nEPOCHS = 5\nBETA = 500\n\n\n\n\nCode\nclass VAE(models.Model):\n    def __init__(self, encoder, decoder, **kwargs):\n        super(VAE, self).__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n        self.total_loss_tracker = metrics.Mean(name=\"total_loss\")\n        self.reconstruction_loss_tracker = metrics.Mean(name=\"reconstruction_loss\")\n        self.kl_loss_tracker = metrics.Mean(name=\"kl_loss\")\n\n    @property\n    def metrics(self):\n        return [\n            self.total_loss_tracker,\n            self.reconstruction_loss_tracker,\n            self.kl_loss_tracker,\n        ]\n\n    def call(self, inputs):\n        \"\"\"Call the model on a particular input.\"\"\"\n        z_mean, z_log_var, z = encoder(inputs)\n        reconstruction = decoder(z)\n        return z_mean, z_log_var, reconstruction\n\n    def train_step(self, data):\n        \"\"\"Step run during training.\"\"\"\n        with tf.GradientTape() as tape:\n            z_mean, z_log_var, reconstruction = self(data)\n            reconstruction_loss = tf.reduce_mean(\n                BETA * losses.binary_crossentropy(data, reconstruction, axis=(1, 2, 3))\n            )\n            kl_loss = tf.reduce_mean(\n                tf.reduce_sum(-0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)), axis=1)\n            )\n            total_loss = reconstruction_loss + kl_loss\n\n        grads = tape.gradient(total_loss, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n\n        self.total_loss_tracker.update_state(total_loss)\n        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n        self.kl_loss_tracker.update_state(kl_loss)\n\n        return {m.name: m.result() for m in self.metrics}\n\n    def test_step(self, data):\n        \"\"\"Step run during validation.\"\"\"\n        if isinstance(data, tuple):\n            data = data[0]\n\n        z_mean, z_log_var, reconstruction = self(data)\n        reconstruction_loss = tf.reduce_mean(\n            BETA * losses.binary_crossentropy(data, reconstruction, axis=(1, 2, 3))\n        )\n        kl_loss = tf.reduce_mean(\n            tf.reduce_sum(\n                -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)),\n                axis=1,\n            )\n        )\n        total_loss = reconstruction_loss + kl_loss\n\n        return {\n            \"loss\": total_loss,\n            \"reconstruction_loss\": reconstruction_loss,\n            \"kl_loss\": kl_loss,\n        }\n\n\nInstantiate the VAE model and compile it.\n\n\nCode\nvae = VAE(encoder, decoder)\n\n# optimizer = optimizers.Adam(learning_rate=0.0005)\noptimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0005)\nvae.compile(optimizer=optimizer)\n\n\n\n\n\nTrain the VAE as before.\n\n\nCode\nvae.fit(\n    x_train,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    validation_data=(x_test, x_test),\n)\n\n\nEpoch 1/5\n600/600 [==============================] - 37s 61ms/step - total_loss: 160.4693 - reconstruction_loss: 155.9913 - kl_loss: 4.4779 - val_loss: 141.2442 - val_reconstruction_loss: 136.1877 - val_kl_loss: 5.0565\nEpoch 2/5\n600/600 [==============================] - 34s 57ms/step - total_loss: 135.9397 - reconstruction_loss: 130.9409 - kl_loss: 4.9988 - val_loss: 138.5623 - val_reconstruction_loss: 133.5856 - val_kl_loss: 4.9767\nEpoch 3/5\n600/600 [==============================] - 34s 56ms/step - total_loss: 134.3719 - reconstruction_loss: 129.3381 - kl_loss: 5.0338 - val_loss: 137.1351 - val_reconstruction_loss: 132.1540 - val_kl_loss: 4.9811\nEpoch 4/5\n600/600 [==============================] - 34s 56ms/step - total_loss: 133.4455 - reconstruction_loss: 128.3819 - kl_loss: 5.0637 - val_loss: 136.5461 - val_reconstruction_loss: 131.4780 - val_kl_loss: 5.0681\nEpoch 5/5\n600/600 [==============================] - 34s 57ms/step - total_loss: 132.7808 - reconstruction_loss: 127.6688 - kl_loss: 5.1120 - val_loss: 135.8917 - val_reconstruction_loss: 130.7375 - val_kl_loss: 5.1542\n\n\n&lt;keras.src.callbacks.History at 0x2c59e9f10&gt;"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson3/chapter3.html#analysing-the-vae",
    "href": "posts/ml/generative_deep_learning/lesson3/chapter3.html#analysing-the-vae",
    "title": "Generative AI: VAEs",
    "section": "",
    "text": "As before, we can eyeball the reconstructions from our model.\n\n\nCode\n# Select a subset of the test set\nn_to_predict = 5000\nexample_images = x_test[:n_to_predict]\nexample_labels = y_test[:n_to_predict]\n\n# Create autoencoder predictions and display\nz_mean, z_log_var, reconstructions = vae.predict(example_images)\nprint(\"Example real clothing items\")\nplot_sample_images(example_images)\nprint(\"Reconstructions\")\nplot_sample_images(reconstructions)\n\n\n 42/157 [=======&gt;......................] - ETA: 1s157/157 [==============================] - 1s 9ms/step\nExample real clothing items\nReconstructions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can sample and decode points in the latent space to observe how the model generates images. We note that:\n\nThe latent space has more even coverage and does not stray to far from a standard Normal distribution. If this is not the case, we can vary the \\(\\beta\\) value used to give more weight to the KL loss term.\nWe do not see as many poorly formed images as we did when sampling a “gap” in a standard autoencoder.\n\n\n\nCode\n# Encode the example images\nz_mean, z_var, z = encoder.predict(example_images)\n\n# Sample some points in the latent space, from the standard normal distribution\ngrid_width, grid_height = (6, 3)\nz_sample = np.random.normal(size=(grid_width * grid_height, 2))\n# Decode the sampled points\nreconstructions = decoder.predict(z_sample)\n# Convert original embeddings and sampled embeddings to p-values\np = norm.cdf(z)\np_sample = norm.cdf(z_sample)\n# Draw a plot of...\nfigsize = 8\nplt.figure(figsize=(figsize, figsize))\n\n# ... the original embeddings ...\nplt.scatter(z[:, 0], z[:, 1], c=\"black\", alpha=0.5, s=2)\n\n# ... and the newly generated points in the latent space\nplt.scatter(z_sample[:, 0], z_sample[:, 1], c=\"#00B0F0\", alpha=1, s=40)\nplt.show()\n\n# Add underneath a grid of the decoded images\nfig = plt.figure(figsize=(figsize, grid_height * 2))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i in range(grid_width * grid_height):\n    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n    ax.axis(\"off\")\n    ax.text(\n        0.5,\n        -0.35,\n        str(np.round(z_sample[i, :], 1)),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")\n\n\n1/1 [==============================] - 0s 14ms/step\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe plots below show the latent space coloured by clothing type. The left plot shows this in terms of z-values and the right in terms of p-values.\nThe latent space is more continuous with fewer gaps, and different categories take similar amounts of space.\n\n\nCode\n# Colour the embeddings by their label (clothing type - see table)\nfigsize = 8\nfig = plt.figure(figsize=(figsize * 2, figsize))\nax = fig.add_subplot(1, 2, 1)\nplot_1 = ax.scatter(\n    z[:, 0], z[:, 1], cmap=\"rainbow\", c=example_labels, alpha=0.8, s=3\n)\nplt.colorbar(plot_1)\nax = fig.add_subplot(1, 2, 2)\nplot_2 = ax.scatter(\n    p[:, 0], p[:, 1], cmap=\"rainbow\", c=example_labels, alpha=0.8, s=3\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nNext we see what happens when we sample from the latent space in a regular grid.\n\n\nCode\n# Colour the embeddings by their label (clothing type - see table)\nfigsize = 12\ngrid_size = 15\nplt.figure(figsize=(figsize, figsize))\nplt.scatter(\n    p[:, 0], p[:, 1], cmap=\"rainbow\", c=example_labels, alpha=0.8, s=300\n)\nplt.colorbar()\n\nx = norm.ppf(np.linspace(0, 1, grid_size))\ny = norm.ppf(np.linspace(1, 0, grid_size))\nxv, yv = np.meshgrid(x, y)\nxv = xv.flatten()\nyv = yv.flatten()\ngrid = np.array(list(zip(xv, yv)))\n\nreconstructions = decoder.predict(grid)\n# plt.scatter(grid[:, 0], grid[:, 1], c=\"black\", alpha=1, s=10)\nplt.show()\n\nfig = plt.figure(figsize=(figsize, figsize))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\nfor i in range(grid_size**2):\n    ax = fig.add_subplot(grid_size, grid_size, i + 1)\n    ax.axis(\"off\")\n    ax.imshow(reconstructions[i, :, :], cmap=\"Greys\")\n\n\n8/8 [==============================] - 0s 6ms/step"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson3/chapter3.html#references",
    "href": "posts/ml/generative_deep_learning/lesson3/chapter3.html#references",
    "title": "Generative AI: VAEs",
    "section": "",
    "text": "Chapter 3 of Generative Deep Learning by David Foster."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html",
    "href": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html",
    "title": "Generative AI: Deep Learning Foundations",
    "section": "",
    "text": "Notes on deep learning concepts in the context of generative AI.\n\n\n\nDeep learning is a class of ML algorithms that use multiple stacked layers of processing units to learn high-level representations from unstructured data.\n\nStructured data is naturally arranged into columns of features. Think of relational database tables.\nBy contrast, unstructured data refers to any data that is not structured in tabular format. E.g. images, audio, text. Individual pixels – or frequencies or words – are not informative alone, but higher-level representations are, so we aim to learn these.\n\n\n\nMost deep learning systems in practice are ANNs,so deep learning has become synonymous with deep deural networks, vut there are other forms such as deep belief networks.\nA neural network where all adjacent layers are fully connected is called a Multilayer Perceptron (MLP) for historical reasons.\nBatches of inputs (e.g. images) are passed through and the predicted outputs are compared to the ground truth. This gives a loss function.\nThe prediction error is then back propagated through the network to adjust the weights incrementally.\nThis allows the model to learn features without manual feature engineering. Earlier layers learn low-level features (like edges) and these are combined in intermediate leayers to learn features like teeth, through to later layers which learning high-level representations like smiling.\n\n\n\nWe will implement an MLP for a supervised (read: not generative) learning problem to classify images.\nThe following uses the CIFAR-10 dataset.\n\n\nLoad the dataset, then scale pixel values to be in the 0 to 1 range and one-hot encode the labels.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras import layers, models, optimizers, utils, datasets\n\n\n# Load the data set\n(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n\n# Scale pixel values to be between 0 and 1.\nx_train = x_train.astype(\"float32\") / 255.\nx_test = x_test.astype(\"float32\") / 255.\n\n# One-hot encode the labels. The CIFAR-10 dataset contains 10 categories.\nNUM_CLASSES = 10\ny_train = utils.to_categorical(y_train, NUM_CLASSES)\ny_test = utils.to_categorical(y_test, NUM_CLASSES)\n\n\nDownloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n170498071/170498071 [==============================] - 64s 0us/step\n\n\n\nplt.imshow(x_train[1])\n\n\n\n\n\n\n\n\n\nprint(y_train[:5])\n\n[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n\n\n\n\n\nThe model is a series of fully connected dense layers.\n\n\nCode\nINPUT_SHAPE = (32, 32, 3,)\n\ninput_layer = layers.Input(INPUT_SHAPE)\nx = layers.Flatten()(input_layer)\nx = layers.Dense(units=200, activation='relu')(x)\nx = layers.Dense(units=150, activation='relu')(x)\noutput_layer = layers.Dense(units=10, activation='softmax')(x)\n\nmodel = models.Model(input_layer, output_layer)\n\n\nThe nonlinear activation function is critical, as this ensures the model is able to learn complex functions of the input rather than simply a linear combination of inputs.\nEach unit within a given layer has a bias term that alwways output 1, which ensures that the output of a unit can be non-zero even if the inputs were all 0. This helps prevent vanishing gradients.\nThe number of parameters in the 200-unit Dense layer is thus: \\(200 * (3072 + 1) = 614600\\)\n\nmodel.summary()\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n                                                                 \n flatten (Flatten)           (None, 3072)              0         \n                                                                 \n dense (Dense)               (None, 200)               614600    \n                                                                 \n dense_1 (Dense)             (None, 150)               30150     \n                                                                 \n dense_2 (Dense)             (None, 10)                1510      \n                                                                 \n=================================================================\nTotal params: 646260 (2.47 MB)\nTrainable params: 646260 (2.47 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\nTo train the model, we require an optimizer and a loss function.\n\n\nThis is used by the network to compare predicted outputs \\(p_i\\) with ground truth labels \\(y_i\\).\nFor regression tasks use mean-squared error: \\[\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - p_i)^2\n\\]\nFor classification tasks with exclusive categories uses categorical cross-entropy: \\[\nCCE = - \\sum_{i=1}^{n} y_i \\log(p_i)\n\\]\nFor classification tasks with binary one output unit or if an observation can belong to multiple classes simultaneously use binary cross-entropy: \\[\nBCE = - \\frac{1}{n} \\sum_{i=1}^{n} (y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) )\n\\]\n\n\n\nThis is the algorithm that updates the weights of the model based on the loss function.\nAdaptive Moment Estimation (ADAM) or Root Mean Squared Propagation (RMSProp) are commonly used.\nThe learning rate is the hyperparameter that is most likely to need tweaking.\n\n\n\nThis determines gow many images are shown to the model in each training step.\nThe larger the batch size, the more stable the gradient calculation but at the expense of a slower training step.\n\nIt is recommend to increase the batch size as training progresses\n\n\n\nCode\n# WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n# opt = optimizers.Adam(learning_rate=0.0005)\nopt = optimizers.legacy.Adam(learning_rate=0.0005)\n\nmodel.compile(\n    loss=\"categorical_crossentropy\",\n    optimizer=opt,\n    metrics=[\"accuracy\"]\n)\n\n\n\n\nCode\nmodel.fit(x_train, y_train, batch_size=32, epochs=10, shuffle=True)\n\n\nEpoch 1/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.8519 - accuracy: 0.3320\nEpoch 2/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.6692 - accuracy: 0.4044\nEpoch 3/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.5840 - accuracy: 0.4360\nEpoch 4/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.5353 - accuracy: 0.4532\nEpoch 5/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4925 - accuracy: 0.4711\nEpoch 6/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4675 - accuracy: 0.4764\nEpoch 7/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4386 - accuracy: 0.4878\nEpoch 8/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4146 - accuracy: 0.4971\nEpoch 9/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.3917 - accuracy: 0.5055\nEpoch 10/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.3691 - accuracy: 0.5124\n\n\n&lt;keras.src.callbacks.History at 0x149e41610&gt;\n\n\n\n\n\n\nUse the model to predict the classification of unseen images from the test set.\nEvaluate the model loss and accuracy over the test set:\n\n\nCode\nmodel.evaluate(x_test, y_test)\n\n\n313/313 [==============================] - 0s 560us/step - loss: 1.4571 - accuracy: 0.4813\n\n\n[1.4570728540420532, 0.4812999963760376]\n\n\n\n\nCode\n# Look at the predictions and ground truth labels from the test set\nCLASSES = np.array(\n    [\n        \"airplane\",\n        \"automobile\",\n        \"bird\",\n        \"cat\",\n        \"deer\",\n        \"dog\",\n        \"frog\",\n        \"horse\",\n        \"ship\",\n        \"truck\",\n    ]\n)\n\ny_pred = model.predict(x_test)\ny_pred_single = CLASSES[np.argmax(y_pred, axis=1)]\ny_test_single = CLASSES[np.argmax(y_test, axis=1)]\n\n\n313/313 [==============================] - 0s 493us/step\n\n\nPlot a comparison of a random selection of test images.\n\n\nCode\nN_IMAGES = 10\nindices = np.random.choice(range(len(x_test)), N_IMAGES)\n\nfig = plt.figure(figsize=(15, 3))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i, idx in enumerate(indices):\n    img = x_test[idx]\n    ax = fig.add_subplot(1, N_IMAGES, i + 1)\n    ax.axis(\"off\")\n    ax.text(\n        0.5,\n        -0.35,\n        \"pred = \" + str(y_pred_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.text(\n        0.5,\n        -0.7,\n        \"act = \" + str(y_test_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.imshow(img)\n\n\n\n\n\n\n\n\n\n\n\n\n\nA simple MLP does not take into account the spatial structure of images, so we can improve performance by using a convolutional layer.\nThis slides a kernel or filter (say, a 3x3 window) over the image and multiply the values elementwise.\nA convolutional layer is a collection of kernels where the (initially random) weights are learned through training.\n\n\nThis is the step size used when moving the kernel window across the image.\nThis determines the output size of the layer. For example, a stride=2 layer will half the height and width of the input image.\n\n\n\nWhen stride=1, we may still end up with a smaller images as the kernel does not overhang the edges of the image.\nUsing padding='same' adds 0s around the border to ensure that the output size is the same as the input size in this case (i.e. it lets the kernel overhang the edges of the original image). This helps to keep track of the size of the tensor.\n\n\n\nThe output of a convolutional layer is another 4-dimensional tensor with shape (batch_size, height, width, num_kernels).\nSo we can stack convolutional layers in series to learn higher-level representations.\n\n\n\nThe exploding gradient problem is common in deep learning: when errors are back-propagated, the gradient values in earlier layers can grow exponentially large.\nIn practice, this can cause overflow errors resulting in the loss function returning NaN.\nThe input layer is scaled, so we may naively expect the activations of all future layers to be well-scaled too. This may initially be true, but as training moves the weights further from their initial values the assumption of outputs being well-scaled breaks down. This is called covariate shift.\nBatch normalization z-scores each input channel, i.e. it subtract the mean and divides by the standard deviation of each mini-batch \\(B\\): \\[\n\\hat{x_i} = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_{B}^2 + \\epsilon}}\n\\]\nWe then learn two parameters, scale \\(\\gamma\\) and shift \\(\\beta\\). Then the output of the BatchNormalization layer is: \\[\ny_i = \\gamma \\hat{x_i} + \\beta\n\\]\nAt test time we may predict only a single image, so the batch normalization value would not be meaningful. Instead, the BatchNormalization layer also stores the moving average of \\(\\gamma\\) and \\(\\beta\\) as tow additional (derived therefore non-trainable) parameters. These moving average values are used at test time.\nBatch normalization has the added benefit of reducing overfitting in practice.\nUsually, batch normalization layers are placed before activation layers, although this is a matter of preference. The acronym BAD can be helpful to remember the general order: Batchnorm, Activation, Dropout.\n\n\n\nWe want to ensure that the model generalises well to unseen data, so it cannot rely on any single layer or neuron too heavily. Dropout acts as a regularization technique to prevent overfitting.\nEach Dropout layer chooses a random set of units from the previous layer and sets their value to 0\nAt test time the Dropout layer does not drop any connections, utilising the full network.\n\n\n\n\n\n\nThis is the same data set as the MLP case so we can re-use the same steps.\n\n\n\nThis is a sequence of stacks containing Conv2D layers with nonlinear acitvation functions.\n\n\nCode\ninput_layer = layers.Input((32, 32, 3))\n\n# Stack 1\nx = layers.Conv2D(filters=32, kernel_size=3, strides=1, padding=\"same\")(\n    input_layer\n)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Stack 2\nx = layers.Conv2D(filters=32, kernel_size=3, strides=2, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Stack 3\nx = layers.Conv2D(filters=64, kernel_size=3, strides=1, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Stack 4\nx = layers.Conv2D(filters=64, kernel_size=3, strides=2, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Dense final layers\nx = layers.Flatten()(x)\nx = layers.Dense(128)(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\nx = layers.Dropout(rate=0.5)(x)\nx = layers.Dense(NUM_CLASSES)(x)\n\n# Output\noutput_layer = layers.Activation(\"softmax\")(x)\nmodel = models.Model(input_layer, output_layer)\n\n\n\nmodel.summary()\n\nModel: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_4 (InputLayer)        [(None, 32, 32, 3)]       0         \n                                                                 \n conv2d_4 (Conv2D)           (None, 32, 32, 32)        896       \n                                                                 \n batch_normalization_5 (Bat  (None, 32, 32, 32)        128       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_5 (LeakyReLU)   (None, 32, 32, 32)        0         \n                                                                 \n conv2d_5 (Conv2D)           (None, 16, 16, 32)        9248      \n                                                                 \n batch_normalization_6 (Bat  (None, 16, 16, 32)        128       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_6 (LeakyReLU)   (None, 16, 16, 32)        0         \n                                                                 \n conv2d_6 (Conv2D)           (None, 16, 16, 64)        18496     \n                                                                 \n batch_normalization_7 (Bat  (None, 16, 16, 64)        256       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_7 (LeakyReLU)   (None, 16, 16, 64)        0         \n                                                                 \n conv2d_7 (Conv2D)           (None, 8, 8, 64)          36928     \n                                                                 \n batch_normalization_8 (Bat  (None, 8, 8, 64)          256       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_8 (LeakyReLU)   (None, 8, 8, 64)          0         \n                                                                 \n flatten_2 (Flatten)         (None, 4096)              0         \n                                                                 \n dense_5 (Dense)             (None, 128)               524416    \n                                                                 \n batch_normalization_9 (Bat  (None, 128)               512       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_9 (LeakyReLU)   (None, 128)               0         \n                                                                 \n dropout_1 (Dropout)         (None, 128)               0         \n                                                                 \n dense_6 (Dense)             (None, 10)                1290      \n                                                                 \n activation_1 (Activation)   (None, 10)                0         \n                                                                 \n=================================================================\nTotal params: 592554 (2.26 MB)\nTrainable params: 591914 (2.26 MB)\nNon-trainable params: 640 (2.50 KB)\n_________________________________________________________________\n\n\n\n\n\nThis is identical to the previous MLP model\n\n\nCode\n# WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n# opt = optimizers.Adam(learning_rate=0.0005)\nopt = optimizers.legacy.Adam(learning_rate=0.0005)\n\nmodel.compile(\n    loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"]\n)\n\n\n\n\nCode\nmodel.fit(\n    x_train,\n    y_train,\n    batch_size=32,\n    epochs=10,\n    shuffle=True,\n    validation_data=(x_test, y_test),\n)\n\n\nEpoch 1/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 1.5393 - accuracy: 0.4599 - val_loss: 1.5656 - val_accuracy: 0.4825\nEpoch 2/10\n1563/1563 [==============================] - 31s 20ms/step - loss: 1.1390 - accuracy: 0.5980 - val_loss: 1.2575 - val_accuracy: 0.5584\nEpoch 3/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 0.9980 - accuracy: 0.6515 - val_loss: 0.9970 - val_accuracy: 0.6513\nEpoch 4/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 0.9146 - accuracy: 0.6798 - val_loss: 0.9783 - val_accuracy: 0.6613\nEpoch 5/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.8515 - accuracy: 0.7044 - val_loss: 0.8269 - val_accuracy: 0.7094\nEpoch 6/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 0.7940 - accuracy: 0.7230 - val_loss: 0.8417 - val_accuracy: 0.7102\nEpoch 7/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.7511 - accuracy: 0.7362 - val_loss: 0.8542 - val_accuracy: 0.7044\nEpoch 8/10\n1563/1563 [==============================] - 29s 18ms/step - loss: 0.7167 - accuracy: 0.7495 - val_loss: 0.8554 - val_accuracy: 0.7083\nEpoch 9/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.6833 - accuracy: 0.7622 - val_loss: 1.0474 - val_accuracy: 0.6651\nEpoch 10/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.6510 - accuracy: 0.7726 - val_loss: 0.8449 - val_accuracy: 0.7020\n\n\n&lt;keras.src.callbacks.History at 0x14aa81d50&gt;\n\n\n\n\n\nAs before, we use the model to predict the classification of unseen images from the test set.\nEvaluate the model loss and accuracy over the test set:\n\n\nCode\nmodel.evaluate(x_test, y_test)\n\n\n313/313 [==============================] - 2s 5ms/step - loss: 0.8449 - accuracy: 0.7020\n\n\n[0.8448793888092041, 0.7020000219345093]\n\n\n\n\nCode\ny_pred = model.predict(x_test)\ny_pred_single = CLASSES[np.argmax(y_pred, axis=1)]\ny_test_single = CLASSES[np.argmax(y_test, axis=1)]\n\n\n313/313 [==============================] - 2s 5ms/step\n\n\nPlot a comparison of a random selection of test images.\n\n\nCode\nN_IMAGES = 10\nindices = np.random.choice(range(len(x_test)), N_IMAGES)\n\nfig = plt.figure(figsize=(15, 3))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i, idx in enumerate(indices):\n    img = x_test[idx]\n    ax = fig.add_subplot(1, N_IMAGES, i + 1)\n    ax.axis(\"off\")\n    ax.text(\n        0.5,\n        -0.35,\n        \"pred = \" + str(y_pred_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.text(\n        0.5,\n        -0.7,\n        \"act = \" + str(y_test_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.imshow(img)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 2 of Generative Deep Learning by David Foster."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#what-is-deep-learning",
    "href": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#what-is-deep-learning",
    "title": "Generative AI: Deep Learning Foundations",
    "section": "",
    "text": "Deep learning is a class of ML algorithms that use multiple stacked layers of processing units to learn high-level representations from unstructured data.\n\nStructured data is naturally arranged into columns of features. Think of relational database tables.\nBy contrast, unstructured data refers to any data that is not structured in tabular format. E.g. images, audio, text. Individual pixels – or frequencies or words – are not informative alone, but higher-level representations are, so we aim to learn these."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#deep-neural-networks",
    "href": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#deep-neural-networks",
    "title": "Generative AI: Deep Learning Foundations",
    "section": "",
    "text": "Most deep learning systems in practice are ANNs,so deep learning has become synonymous with deep deural networks, vut there are other forms such as deep belief networks.\nA neural network where all adjacent layers are fully connected is called a Multilayer Perceptron (MLP) for historical reasons.\nBatches of inputs (e.g. images) are passed through and the predicted outputs are compared to the ground truth. This gives a loss function.\nThe prediction error is then back propagated through the network to adjust the weights incrementally.\nThis allows the model to learn features without manual feature engineering. Earlier layers learn low-level features (like edges) and these are combined in intermediate leayers to learn features like teeth, through to later layers which learning high-level representations like smiling."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#building-a-multilayer-perceptron",
    "href": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#building-a-multilayer-perceptron",
    "title": "Generative AI: Deep Learning Foundations",
    "section": "",
    "text": "We will implement an MLP for a supervised (read: not generative) learning problem to classify images.\nThe following uses the CIFAR-10 dataset.\n\n\nLoad the dataset, then scale pixel values to be in the 0 to 1 range and one-hot encode the labels.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras import layers, models, optimizers, utils, datasets\n\n\n# Load the data set\n(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n\n# Scale pixel values to be between 0 and 1.\nx_train = x_train.astype(\"float32\") / 255.\nx_test = x_test.astype(\"float32\") / 255.\n\n# One-hot encode the labels. The CIFAR-10 dataset contains 10 categories.\nNUM_CLASSES = 10\ny_train = utils.to_categorical(y_train, NUM_CLASSES)\ny_test = utils.to_categorical(y_test, NUM_CLASSES)\n\n\nDownloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n170498071/170498071 [==============================] - 64s 0us/step\n\n\n\nplt.imshow(x_train[1])\n\n\n\n\n\n\n\n\n\nprint(y_train[:5])\n\n[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n\n\n\n\n\nThe model is a series of fully connected dense layers.\n\n\nCode\nINPUT_SHAPE = (32, 32, 3,)\n\ninput_layer = layers.Input(INPUT_SHAPE)\nx = layers.Flatten()(input_layer)\nx = layers.Dense(units=200, activation='relu')(x)\nx = layers.Dense(units=150, activation='relu')(x)\noutput_layer = layers.Dense(units=10, activation='softmax')(x)\n\nmodel = models.Model(input_layer, output_layer)\n\n\nThe nonlinear activation function is critical, as this ensures the model is able to learn complex functions of the input rather than simply a linear combination of inputs.\nEach unit within a given layer has a bias term that alwways output 1, which ensures that the output of a unit can be non-zero even if the inputs were all 0. This helps prevent vanishing gradients.\nThe number of parameters in the 200-unit Dense layer is thus: \\(200 * (3072 + 1) = 614600\\)\n\nmodel.summary()\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n                                                                 \n flatten (Flatten)           (None, 3072)              0         \n                                                                 \n dense (Dense)               (None, 200)               614600    \n                                                                 \n dense_1 (Dense)             (None, 150)               30150     \n                                                                 \n dense_2 (Dense)             (None, 10)                1510      \n                                                                 \n=================================================================\nTotal params: 646260 (2.47 MB)\nTrainable params: 646260 (2.47 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\nTo train the model, we require an optimizer and a loss function.\n\n\nThis is used by the network to compare predicted outputs \\(p_i\\) with ground truth labels \\(y_i\\).\nFor regression tasks use mean-squared error: \\[\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - p_i)^2\n\\]\nFor classification tasks with exclusive categories uses categorical cross-entropy: \\[\nCCE = - \\sum_{i=1}^{n} y_i \\log(p_i)\n\\]\nFor classification tasks with binary one output unit or if an observation can belong to multiple classes simultaneously use binary cross-entropy: \\[\nBCE = - \\frac{1}{n} \\sum_{i=1}^{n} (y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) )\n\\]\n\n\n\nThis is the algorithm that updates the weights of the model based on the loss function.\nAdaptive Moment Estimation (ADAM) or Root Mean Squared Propagation (RMSProp) are commonly used.\nThe learning rate is the hyperparameter that is most likely to need tweaking.\n\n\n\nThis determines gow many images are shown to the model in each training step.\nThe larger the batch size, the more stable the gradient calculation but at the expense of a slower training step.\n\nIt is recommend to increase the batch size as training progresses\n\n\n\nCode\n# WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n# opt = optimizers.Adam(learning_rate=0.0005)\nopt = optimizers.legacy.Adam(learning_rate=0.0005)\n\nmodel.compile(\n    loss=\"categorical_crossentropy\",\n    optimizer=opt,\n    metrics=[\"accuracy\"]\n)\n\n\n\n\nCode\nmodel.fit(x_train, y_train, batch_size=32, epochs=10, shuffle=True)\n\n\nEpoch 1/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.8519 - accuracy: 0.3320\nEpoch 2/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.6692 - accuracy: 0.4044\nEpoch 3/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.5840 - accuracy: 0.4360\nEpoch 4/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.5353 - accuracy: 0.4532\nEpoch 5/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4925 - accuracy: 0.4711\nEpoch 6/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4675 - accuracy: 0.4764\nEpoch 7/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4386 - accuracy: 0.4878\nEpoch 8/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.4146 - accuracy: 0.4971\nEpoch 9/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.3917 - accuracy: 0.5055\nEpoch 10/10\n1563/1563 [==============================] - 2s 1ms/step - loss: 1.3691 - accuracy: 0.5124\n\n\n&lt;keras.src.callbacks.History at 0x149e41610&gt;\n\n\n\n\n\n\nUse the model to predict the classification of unseen images from the test set.\nEvaluate the model loss and accuracy over the test set:\n\n\nCode\nmodel.evaluate(x_test, y_test)\n\n\n313/313 [==============================] - 0s 560us/step - loss: 1.4571 - accuracy: 0.4813\n\n\n[1.4570728540420532, 0.4812999963760376]\n\n\n\n\nCode\n# Look at the predictions and ground truth labels from the test set\nCLASSES = np.array(\n    [\n        \"airplane\",\n        \"automobile\",\n        \"bird\",\n        \"cat\",\n        \"deer\",\n        \"dog\",\n        \"frog\",\n        \"horse\",\n        \"ship\",\n        \"truck\",\n    ]\n)\n\ny_pred = model.predict(x_test)\ny_pred_single = CLASSES[np.argmax(y_pred, axis=1)]\ny_test_single = CLASSES[np.argmax(y_test, axis=1)]\n\n\n313/313 [==============================] - 0s 493us/step\n\n\nPlot a comparison of a random selection of test images.\n\n\nCode\nN_IMAGES = 10\nindices = np.random.choice(range(len(x_test)), N_IMAGES)\n\nfig = plt.figure(figsize=(15, 3))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i, idx in enumerate(indices):\n    img = x_test[idx]\n    ax = fig.add_subplot(1, N_IMAGES, i + 1)\n    ax.axis(\"off\")\n    ax.text(\n        0.5,\n        -0.35,\n        \"pred = \" + str(y_pred_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.text(\n        0.5,\n        -0.7,\n        \"act = \" + str(y_test_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.imshow(img)"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#cnn-concepts-and-layers",
    "href": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#cnn-concepts-and-layers",
    "title": "Generative AI: Deep Learning Foundations",
    "section": "",
    "text": "A simple MLP does not take into account the spatial structure of images, so we can improve performance by using a convolutional layer.\nThis slides a kernel or filter (say, a 3x3 window) over the image and multiply the values elementwise.\nA convolutional layer is a collection of kernels where the (initially random) weights are learned through training.\n\n\nThis is the step size used when moving the kernel window across the image.\nThis determines the output size of the layer. For example, a stride=2 layer will half the height and width of the input image.\n\n\n\nWhen stride=1, we may still end up with a smaller images as the kernel does not overhang the edges of the image.\nUsing padding='same' adds 0s around the border to ensure that the output size is the same as the input size in this case (i.e. it lets the kernel overhang the edges of the original image). This helps to keep track of the size of the tensor.\n\n\n\nThe output of a convolutional layer is another 4-dimensional tensor with shape (batch_size, height, width, num_kernels).\nSo we can stack convolutional layers in series to learn higher-level representations.\n\n\n\nThe exploding gradient problem is common in deep learning: when errors are back-propagated, the gradient values in earlier layers can grow exponentially large.\nIn practice, this can cause overflow errors resulting in the loss function returning NaN.\nThe input layer is scaled, so we may naively expect the activations of all future layers to be well-scaled too. This may initially be true, but as training moves the weights further from their initial values the assumption of outputs being well-scaled breaks down. This is called covariate shift.\nBatch normalization z-scores each input channel, i.e. it subtract the mean and divides by the standard deviation of each mini-batch \\(B\\): \\[\n\\hat{x_i} = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_{B}^2 + \\epsilon}}\n\\]\nWe then learn two parameters, scale \\(\\gamma\\) and shift \\(\\beta\\). Then the output of the BatchNormalization layer is: \\[\ny_i = \\gamma \\hat{x_i} + \\beta\n\\]\nAt test time we may predict only a single image, so the batch normalization value would not be meaningful. Instead, the BatchNormalization layer also stores the moving average of \\(\\gamma\\) and \\(\\beta\\) as tow additional (derived therefore non-trainable) parameters. These moving average values are used at test time.\nBatch normalization has the added benefit of reducing overfitting in practice.\nUsually, batch normalization layers are placed before activation layers, although this is a matter of preference. The acronym BAD can be helpful to remember the general order: Batchnorm, Activation, Dropout.\n\n\n\nWe want to ensure that the model generalises well to unseen data, so it cannot rely on any single layer or neuron too heavily. Dropout acts as a regularization technique to prevent overfitting.\nEach Dropout layer chooses a random set of units from the previous layer and sets their value to 0\nAt test time the Dropout layer does not drop any connections, utilising the full network."
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#building-a-convolutional-neural-network",
    "href": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#building-a-convolutional-neural-network",
    "title": "Generative AI: Deep Learning Foundations",
    "section": "",
    "text": "This is the same data set as the MLP case so we can re-use the same steps.\n\n\n\nThis is a sequence of stacks containing Conv2D layers with nonlinear acitvation functions.\n\n\nCode\ninput_layer = layers.Input((32, 32, 3))\n\n# Stack 1\nx = layers.Conv2D(filters=32, kernel_size=3, strides=1, padding=\"same\")(\n    input_layer\n)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Stack 2\nx = layers.Conv2D(filters=32, kernel_size=3, strides=2, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Stack 3\nx = layers.Conv2D(filters=64, kernel_size=3, strides=1, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Stack 4\nx = layers.Conv2D(filters=64, kernel_size=3, strides=2, padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\n# Dense final layers\nx = layers.Flatten()(x)\nx = layers.Dense(128)(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\nx = layers.Dropout(rate=0.5)(x)\nx = layers.Dense(NUM_CLASSES)(x)\n\n# Output\noutput_layer = layers.Activation(\"softmax\")(x)\nmodel = models.Model(input_layer, output_layer)\n\n\n\nmodel.summary()\n\nModel: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_4 (InputLayer)        [(None, 32, 32, 3)]       0         \n                                                                 \n conv2d_4 (Conv2D)           (None, 32, 32, 32)        896       \n                                                                 \n batch_normalization_5 (Bat  (None, 32, 32, 32)        128       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_5 (LeakyReLU)   (None, 32, 32, 32)        0         \n                                                                 \n conv2d_5 (Conv2D)           (None, 16, 16, 32)        9248      \n                                                                 \n batch_normalization_6 (Bat  (None, 16, 16, 32)        128       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_6 (LeakyReLU)   (None, 16, 16, 32)        0         \n                                                                 \n conv2d_6 (Conv2D)           (None, 16, 16, 64)        18496     \n                                                                 \n batch_normalization_7 (Bat  (None, 16, 16, 64)        256       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_7 (LeakyReLU)   (None, 16, 16, 64)        0         \n                                                                 \n conv2d_7 (Conv2D)           (None, 8, 8, 64)          36928     \n                                                                 \n batch_normalization_8 (Bat  (None, 8, 8, 64)          256       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_8 (LeakyReLU)   (None, 8, 8, 64)          0         \n                                                                 \n flatten_2 (Flatten)         (None, 4096)              0         \n                                                                 \n dense_5 (Dense)             (None, 128)               524416    \n                                                                 \n batch_normalization_9 (Bat  (None, 128)               512       \n chNormalization)                                                \n                                                                 \n leaky_re_lu_9 (LeakyReLU)   (None, 128)               0         \n                                                                 \n dropout_1 (Dropout)         (None, 128)               0         \n                                                                 \n dense_6 (Dense)             (None, 10)                1290      \n                                                                 \n activation_1 (Activation)   (None, 10)                0         \n                                                                 \n=================================================================\nTotal params: 592554 (2.26 MB)\nTrainable params: 591914 (2.26 MB)\nNon-trainable params: 640 (2.50 KB)\n_________________________________________________________________\n\n\n\n\n\nThis is identical to the previous MLP model\n\n\nCode\n# WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n# opt = optimizers.Adam(learning_rate=0.0005)\nopt = optimizers.legacy.Adam(learning_rate=0.0005)\n\nmodel.compile(\n    loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"]\n)\n\n\n\n\nCode\nmodel.fit(\n    x_train,\n    y_train,\n    batch_size=32,\n    epochs=10,\n    shuffle=True,\n    validation_data=(x_test, y_test),\n)\n\n\nEpoch 1/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 1.5393 - accuracy: 0.4599 - val_loss: 1.5656 - val_accuracy: 0.4825\nEpoch 2/10\n1563/1563 [==============================] - 31s 20ms/step - loss: 1.1390 - accuracy: 0.5980 - val_loss: 1.2575 - val_accuracy: 0.5584\nEpoch 3/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 0.9980 - accuracy: 0.6515 - val_loss: 0.9970 - val_accuracy: 0.6513\nEpoch 4/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 0.9146 - accuracy: 0.6798 - val_loss: 0.9783 - val_accuracy: 0.6613\nEpoch 5/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.8515 - accuracy: 0.7044 - val_loss: 0.8269 - val_accuracy: 0.7094\nEpoch 6/10\n1563/1563 [==============================] - 30s 19ms/step - loss: 0.7940 - accuracy: 0.7230 - val_loss: 0.8417 - val_accuracy: 0.7102\nEpoch 7/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.7511 - accuracy: 0.7362 - val_loss: 0.8542 - val_accuracy: 0.7044\nEpoch 8/10\n1563/1563 [==============================] - 29s 18ms/step - loss: 0.7167 - accuracy: 0.7495 - val_loss: 0.8554 - val_accuracy: 0.7083\nEpoch 9/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.6833 - accuracy: 0.7622 - val_loss: 1.0474 - val_accuracy: 0.6651\nEpoch 10/10\n1563/1563 [==============================] - 29s 19ms/step - loss: 0.6510 - accuracy: 0.7726 - val_loss: 0.8449 - val_accuracy: 0.7020\n\n\n&lt;keras.src.callbacks.History at 0x14aa81d50&gt;\n\n\n\n\n\nAs before, we use the model to predict the classification of unseen images from the test set.\nEvaluate the model loss and accuracy over the test set:\n\n\nCode\nmodel.evaluate(x_test, y_test)\n\n\n313/313 [==============================] - 2s 5ms/step - loss: 0.8449 - accuracy: 0.7020\n\n\n[0.8448793888092041, 0.7020000219345093]\n\n\n\n\nCode\ny_pred = model.predict(x_test)\ny_pred_single = CLASSES[np.argmax(y_pred, axis=1)]\ny_test_single = CLASSES[np.argmax(y_test, axis=1)]\n\n\n313/313 [==============================] - 2s 5ms/step\n\n\nPlot a comparison of a random selection of test images.\n\n\nCode\nN_IMAGES = 10\nindices = np.random.choice(range(len(x_test)), N_IMAGES)\n\nfig = plt.figure(figsize=(15, 3))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i, idx in enumerate(indices):\n    img = x_test[idx]\n    ax = fig.add_subplot(1, N_IMAGES, i + 1)\n    ax.axis(\"off\")\n    ax.text(\n        0.5,\n        -0.35,\n        \"pred = \" + str(y_pred_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.text(\n        0.5,\n        -0.7,\n        \"act = \" + str(y_test_single[idx]),\n        fontsize=10,\n        ha=\"center\",\n        transform=ax.transAxes,\n    )\n    ax.imshow(img)"
  },
  {
    "objectID": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#references",
    "href": "posts/ml/generative_deep_learning/lesson2/chapter2_mlp.html#references",
    "title": "Generative AI: Deep Learning Foundations",
    "section": "",
    "text": "Chapter 2 of Generative Deep Learning by David Foster."
  },
  {
    "objectID": "posts/ml/fastai/lesson3/lesson.html",
    "href": "posts/ml/fastai/lesson3/lesson.html",
    "title": "FastAI Lesson 3: How Does a Neural Network Learn?",
    "section": "",
    "text": "These are notes from lesson 3 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\nRecreate the spreadsheet to train a linear model and a neural network from scratch: see spreadsheet\n\n\n\n\nSome options for cloud environments: Kaggle, Colab, Paperspace.\nA comparison of performance vs training time for different image models useful for model selection is provided here. Resnet and convnext are generally good to start with.\nThe best practice is to start with a “good enough” model with a quick training time, so you can iterate quickly. Then as you get further on with your research and need a better model, you can move to a slower, more accurate model.\nThe criteria we generally care about are:\n\nHow fast are they\nHow much memory do they use\nHow accurate are they\n\nA learner object contains the pre-processing steps and the model itself.\n\n\n\nHow do we fit a function to data? An ML model is just a function fitted to data. Deep learning is just fitting an infinitely flexible model to data.\nIn this notebook there is an interactive cell to fit a quadratic function to some noisy data: y = ax^2 +bx + c\nWe can vary a, b and c to get a better fit by eye. We can make this more rigorous by defining a loss function to quantify how good the fit is.\nIn this case, use the mean absolute error: mae = mean(abs(actual - preds)) We can then use stochastic gradient descent to autome the tweaking of a, b and c to minimise the loss.\nWe can store the parameters as a tensor, then pytorch will calculate gradients of the loss function based on that tensor.\nabc = torch.tensor([1.1,1.1,1.1]) \nabc.requires_grad_()  # Modifies abc in place so that it will include gradient calculations on anything which uses abc downstream\nloss = quad_mae(abc)  # `loss` uses abc so pytorch will give the gradient of loss too\nloss.backward()  # Back-propagate the gradients\nabc.grad  # Returns a tensor of the loss gradients\nWe can then take a “small step” in the direction that will decrease the gradient. The size of the step should be proportional to the size of the gradient. We define a learning rate hyperparameter to determine how much to scale the gradients by.\nlearning_rate = 0.01\nabc -= abc.grad * learning_rate\nloss = quad_mae(abc)  # The loss should now be smaller\nWe can repeat this process to take multiple steps to minimise the gradient.\nfor step in range(10):\n    loss = quad_mae(abc)\n    loss.backward()\n    with torch.no_grad():\n        abc -= abc.grad * learning_rate\n    print(f'step={step}; loss={loss:.2f}')\nThe learning rate should decrease as we get closer to the minimum to ensure we don’t overshoot the minimum and increase the loss. A learning rate schedule can be specified to do this.\n\n\n\nFor deep learning, the premise is the same but instead of quadratic functions, we fit ReLUs and other non-linear functions.\nUniversal approximation theorem states that this is infinitely expressive if enough ReLUs (or other non-linear units) are combined. This means we can learn any computable function.\nA ReLU is essentially a linear function with the negative values clipped to 0.\ndef relu(m,c,x):\n    y = m * x + c\n    return np.clip(y, 0.)\nThis is all that deep learning is! All we need is:\n\nA model - a bunch of ReLUs combined will be flexible\nA loss function - mean absolute error between the actual data values and the values predicted by the model\nAn optimiser - stochastic gradient descent can start from random weights to incrementally improve the loss until we get a “good enough” fit\n\nWe just need enough time and data. There are a few hacks to decrease the time and data required:\n\nData augmentation\nRunning on GPUs to parallelise matrix multiplications\nConvolutions to skip over values to reduce the number of matrix multiplications required\nTransfer learning - initialise with parameters from another pre-trained model instead of random weights.\n\nThis spreadsheet from the homework task is a worked example of manually training a multivariate linear model, then extending that to a neural network summing two ReLUs.\n\n\n\n\nCourse lesson page\nComparison of computer vision models\n“How does a neural network really work?” notebook"
  },
  {
    "objectID": "posts/ml/fastai/lesson3/lesson.html#model-selection",
    "href": "posts/ml/fastai/lesson3/lesson.html#model-selection",
    "title": "FastAI Lesson 3: How Does a Neural Network Learn?",
    "section": "",
    "text": "Some options for cloud environments: Kaggle, Colab, Paperspace.\nA comparison of performance vs training time for different image models useful for model selection is provided here. Resnet and convnext are generally good to start with.\nThe best practice is to start with a “good enough” model with a quick training time, so you can iterate quickly. Then as you get further on with your research and need a better model, you can move to a slower, more accurate model.\nThe criteria we generally care about are:\n\nHow fast are they\nHow much memory do they use\nHow accurate are they\n\nA learner object contains the pre-processing steps and the model itself."
  },
  {
    "objectID": "posts/ml/fastai/lesson3/lesson.html#fitting-a-quadratic-function",
    "href": "posts/ml/fastai/lesson3/lesson.html#fitting-a-quadratic-function",
    "title": "FastAI Lesson 3: How Does a Neural Network Learn?",
    "section": "",
    "text": "How do we fit a function to data? An ML model is just a function fitted to data. Deep learning is just fitting an infinitely flexible model to data.\nIn this notebook there is an interactive cell to fit a quadratic function to some noisy data: y = ax^2 +bx + c\nWe can vary a, b and c to get a better fit by eye. We can make this more rigorous by defining a loss function to quantify how good the fit is.\nIn this case, use the mean absolute error: mae = mean(abs(actual - preds)) We can then use stochastic gradient descent to autome the tweaking of a, b and c to minimise the loss.\nWe can store the parameters as a tensor, then pytorch will calculate gradients of the loss function based on that tensor.\nabc = torch.tensor([1.1,1.1,1.1]) \nabc.requires_grad_()  # Modifies abc in place so that it will include gradient calculations on anything which uses abc downstream\nloss = quad_mae(abc)  # `loss` uses abc so pytorch will give the gradient of loss too\nloss.backward()  # Back-propagate the gradients\nabc.grad  # Returns a tensor of the loss gradients\nWe can then take a “small step” in the direction that will decrease the gradient. The size of the step should be proportional to the size of the gradient. We define a learning rate hyperparameter to determine how much to scale the gradients by.\nlearning_rate = 0.01\nabc -= abc.grad * learning_rate\nloss = quad_mae(abc)  # The loss should now be smaller\nWe can repeat this process to take multiple steps to minimise the gradient.\nfor step in range(10):\n    loss = quad_mae(abc)\n    loss.backward()\n    with torch.no_grad():\n        abc -= abc.grad * learning_rate\n    print(f'step={step}; loss={loss:.2f}')\nThe learning rate should decrease as we get closer to the minimum to ensure we don’t overshoot the minimum and increase the loss. A learning rate schedule can be specified to do this."
  },
  {
    "objectID": "posts/ml/fastai/lesson3/lesson.html#fit-a-deep-learning-model",
    "href": "posts/ml/fastai/lesson3/lesson.html#fit-a-deep-learning-model",
    "title": "FastAI Lesson 3: How Does a Neural Network Learn?",
    "section": "",
    "text": "For deep learning, the premise is the same but instead of quadratic functions, we fit ReLUs and other non-linear functions.\nUniversal approximation theorem states that this is infinitely expressive if enough ReLUs (or other non-linear units) are combined. This means we can learn any computable function.\nA ReLU is essentially a linear function with the negative values clipped to 0.\ndef relu(m,c,x):\n    y = m * x + c\n    return np.clip(y, 0.)\nThis is all that deep learning is! All we need is:\n\nA model - a bunch of ReLUs combined will be flexible\nA loss function - mean absolute error between the actual data values and the values predicted by the model\nAn optimiser - stochastic gradient descent can start from random weights to incrementally improve the loss until we get a “good enough” fit\n\nWe just need enough time and data. There are a few hacks to decrease the time and data required:\n\nData augmentation\nRunning on GPUs to parallelise matrix multiplications\nConvolutions to skip over values to reduce the number of matrix multiplications required\nTransfer learning - initialise with parameters from another pre-trained model instead of random weights.\n\nThis spreadsheet from the homework task is a worked example of manually training a multivariate linear model, then extending that to a neural network summing two ReLUs."
  },
  {
    "objectID": "posts/ml/fastai/lesson3/lesson.html#references",
    "href": "posts/ml/fastai/lesson3/lesson.html#references",
    "title": "FastAI Lesson 3: How Does a Neural Network Learn?",
    "section": "",
    "text": "Course lesson page\nComparison of computer vision models\n“How does a neural network really work?” notebook"
  },
  {
    "objectID": "posts/ml/fastai/lesson5/lesson.html",
    "href": "posts/ml/fastai/lesson5/lesson.html",
    "title": "FastAI Lesson 5: Natural Language Processing",
    "section": "",
    "text": "These are notes from lesson 5 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\n\nRecreate the Jupyter notebook to train a linear model and a neural network from scratch - see from scratch notebook\nThen repeat the exercise using the fastai framework (it’s much easier!) - see framework notebook\nRead numpy broadcasting rules\n\n\n\n\n\nTrain a linear model from scratch in Python using on the Titanic dataset. Then train a neural network from scratch in a similar way. This is an extension of the spreadsheet approach to make a neural network from scratch in lesson 3.\n\n\nNever throw away data.\nAn easy way of imputing missing values is to fill them with the mode. This is good enough for a first pass at creating a model.\n\n\n\nNumeric values that can grow exponentially like prices or population sizes often have long-tailed distributions.\nAn easy way to scale is to take log(x+1). The +1 term is just to avoid taking log of 0.\n\n\n\nOne-hot encode any categorical variables.\nWe should include an “other” category for each in case the validation or test set contains a category we didn’t encounter in the training set.\nIf there are categories with ony a small number of observations we can group them into an “other” category too.\n\n\n\nBroadcasting arrays together avoids boilerplate code to make dimensions match.\nFor reference, see numpy’s broadcasting rules and try APL\n\n\n\nFor a binary classification model, the outputs should be between 0 and 1. If you train a linear model, it might result in negative values or values &gt;1.\nThis means we can improve the loss by just clipping to ensure they stay between 0 and 1.\nThis is the idea behind the sigmoid function for output layers: smaller values will tend to 0 and larger values will tend to 1. In general, for any binary classification model, we should always have a sigmoid as the final layer. If the model isn’t training well, it’s worth checking that the final activation function is a sigmoid.\nThe sigmoid function is defined as: \\[ y = \\frac{1}{1+e^{-x}} \\]\n\n\n\nIn general, the middle layers of a neural network are similar between different problems.\nThe input layer will depend on the data for our specific problem. The output will depend on the target for our specific problem.\nSo we spend most of our time thinking about the correct input and output layers, and the hidden layers are less important.\n\n\n\n\nWhen creating the models from scratch, there was a lot of boilerplate code to:\n\nImpute missing values using the “obvious” method (fill with mode)\nNormalise continuous variables to be between 0 and 1\nOne-hot encode categorical variables\nRepeat all of these steps in the same order for the test set\n\nThe benefits of using a framework like fastai:\n\nLess boilerplate, so the obvious things are done automatically unless you say otherwise\nRepeating all of the feature engineering steps on the output is trivial with DataLoaders\n\n\n\n\nCreating independent models and taking the mean of their predictions improves the accuracy. There are a few different approaches to ensembling a categorical variable:\n\nTake the mean of the predictions (binary 0 or 1)\nTake the mean of the probabilities (continuous between 0 and 1) then threshold the result\nTake the mode of the predictions (binary 0 or 1)\n\nIn general the mean approaches work better but there’s no rule as to why, so try all of them.\n\n\n\n\nCourse lesson page\nNumpy broadcasting rules\nAPL"
  },
  {
    "objectID": "posts/ml/fastai/lesson5/lesson.html#training-a-model-from-scratch",
    "href": "posts/ml/fastai/lesson5/lesson.html#training-a-model-from-scratch",
    "title": "FastAI Lesson 5: Natural Language Processing",
    "section": "",
    "text": "Train a linear model from scratch in Python using on the Titanic dataset. Then train a neural network from scratch in a similar way. This is an extension of the spreadsheet approach to make a neural network from scratch in lesson 3.\n\n\nNever throw away data.\nAn easy way of imputing missing values is to fill them with the mode. This is good enough for a first pass at creating a model.\n\n\n\nNumeric values that can grow exponentially like prices or population sizes often have long-tailed distributions.\nAn easy way to scale is to take log(x+1). The +1 term is just to avoid taking log of 0.\n\n\n\nOne-hot encode any categorical variables.\nWe should include an “other” category for each in case the validation or test set contains a category we didn’t encounter in the training set.\nIf there are categories with ony a small number of observations we can group them into an “other” category too.\n\n\n\nBroadcasting arrays together avoids boilerplate code to make dimensions match.\nFor reference, see numpy’s broadcasting rules and try APL\n\n\n\nFor a binary classification model, the outputs should be between 0 and 1. If you train a linear model, it might result in negative values or values &gt;1.\nThis means we can improve the loss by just clipping to ensure they stay between 0 and 1.\nThis is the idea behind the sigmoid function for output layers: smaller values will tend to 0 and larger values will tend to 1. In general, for any binary classification model, we should always have a sigmoid as the final layer. If the model isn’t training well, it’s worth checking that the final activation function is a sigmoid.\nThe sigmoid function is defined as: \\[ y = \\frac{1}{1+e^{-x}} \\]\n\n\n\nIn general, the middle layers of a neural network are similar between different problems.\nThe input layer will depend on the data for our specific problem. The output will depend on the target for our specific problem.\nSo we spend most of our time thinking about the correct input and output layers, and the hidden layers are less important."
  },
  {
    "objectID": "posts/ml/fastai/lesson5/lesson.html#using-a-framework",
    "href": "posts/ml/fastai/lesson5/lesson.html#using-a-framework",
    "title": "FastAI Lesson 5: Natural Language Processing",
    "section": "",
    "text": "When creating the models from scratch, there was a lot of boilerplate code to:\n\nImpute missing values using the “obvious” method (fill with mode)\nNormalise continuous variables to be between 0 and 1\nOne-hot encode categorical variables\nRepeat all of these steps in the same order for the test set\n\nThe benefits of using a framework like fastai:\n\nLess boilerplate, so the obvious things are done automatically unless you say otherwise\nRepeating all of the feature engineering steps on the output is trivial with DataLoaders"
  },
  {
    "objectID": "posts/ml/fastai/lesson5/lesson.html#ensembles",
    "href": "posts/ml/fastai/lesson5/lesson.html#ensembles",
    "title": "FastAI Lesson 5: Natural Language Processing",
    "section": "",
    "text": "Creating independent models and taking the mean of their predictions improves the accuracy. There are a few different approaches to ensembling a categorical variable:\n\nTake the mean of the predictions (binary 0 or 1)\nTake the mean of the probabilities (continuous between 0 and 1) then threshold the result\nTake the mode of the predictions (binary 0 or 1)\n\nIn general the mean approaches work better but there’s no rule as to why, so try all of them."
  },
  {
    "objectID": "posts/ml/fastai/lesson5/lesson.html#references",
    "href": "posts/ml/fastai/lesson5/lesson.html#references",
    "title": "FastAI Lesson 5: Natural Language Processing",
    "section": "",
    "text": "Course lesson page\nNumpy broadcasting rules\nAPL"
  },
  {
    "objectID": "posts/ml/fastai/lesson6_1/lesson.html",
    "href": "posts/ml/fastai/lesson6_1/lesson.html",
    "title": "FastAI Lesson 6.5: Road to the Top",
    "section": "",
    "text": "These are notes on the “Road to the Top” notebooks that span lessons 6 and 7 of Fast AI Practical Deep Learning for Coders. I’ve separated these from the main topics of those lectires to keep the posts focused.\n\n\n\n\n\n\nHomework Task\n\n\n\n\nRecreate the softmax and cross-entropy loss spreadsheet example\nRead the “Road to the Top” notebook series - parts 1, 2 and 3\nRead “Things that confused me about cross entropy” by Chris Said.\n\n\n\n\n\nThe focus should be:\n\nCreate an effective validation set\nIterate quickly to find changes which improve the validation set\n\nTrain a simple model straight away, get a result and submit it. You can iterate and improve later.\nData augmentation, in particular test-time augmentation (TTA), can improve results.\n\n\n\nRules of thumb on model selection by problem type:\n\nFor computer vision use CNNs, fine-tune a pre-trained model. See comparison of pre-trained models here.\n\nIf unsure which model to choose, start with (small) convnext.\nDifferent models can have big differences in accuracy so try a few small models from different families.\nOnce you are happy with the model, try a size up in that family.\nResizing images to a square is a good, easy compromise to accomodate many different image sizes, orientations and aspect ratios. A more involved approach that may improve results is to batch images together and resize them to the median rectangle size.\n\nFor tabular data, random forests will give a reasonably good result.\n\nGBMs will give a better result eventually, but with more effort required to get a good model.\nWorth running a hyperparameter grid search for GBM because it’s fiddly.\n\n\nRules of thumb on hardware:\n\nYou generally want ~8 physical CPUs per GPU\nIf model training is CPU bound, it can help to resize images to be smaller. The file I/O on the CPU may be taking too long.\nIf model training is CPU bound and GPU usage is low, you might as well go up to a “better” (i.e. bigger) model with no increase of overall execution time.\n\n\n\n\nThis is a technique that allows you to run larger batch sizes without running out of GPU memory.\nRather than needing to fit a whole batch (say, 64 images) in memory, we can split this up into sub-batches. Let’s say we use an accumulation factor of 4; this means each sub-batch would have 16 images.\nWe calculate the gradient for each sub-batch but don’t immediately subtract it from the weights. We also keep a running count of the number of images we have seen. Once we have seen 64 images, i.e. 4 batches, then apply the total accumulated gradient and reset the count. Remember that in Pytorch, if we calculate another gradient without zeroing in between, it will add the new gradient to the old one, so this is in effect the “default behaviour”.\nGradient accumulation gives an identical result unless using batch normalisation, since the moving averages will be more volatile. Most big models use layer normalisation rather than batch normalisation so often this is not an issue.\nHow do we pick a batch size? Just pick the largest you can without running out of GPU memory.\nWhy not just reduce the batch size? If using a pretrained model, the other parameters such as learning rate work for that batch size and might not work for others. So we’d have to perform a hyperparameter search again to find a good combination for the new batch size. Gradient accumulation sidesteps this issue.\n\n\n\nConsider the case where we want to train a model with two different predicted labels, say plant species and disease type. If we have 10 plant species and 10 diseases, we create a model with 20 output neurons.\nHow does the model know what each should do? Through the loss function. We make one loss function for disease which takes the first 10 columns of the input to the final layer, and a second loss function for variety which takes the last 10 columns of the input to the final layer. The overall loss function is the sum of the two.\nIf you train it for longer, the multi target model can get better at predicting one target, say disease, than a single target model trained on just disease alone.\n\nThe early layers that are useful for variety may also be useful for disease\nKnowing the variety may be useful in predicting disease if there are confounding factors between disease and variety. For example, certain diseases affect certain varieties.\n\n\n\n\n\nCourse lesson page\n“Road to the Top” notebook\n“Things that confused me about cross entropy” by Chris Said."
  },
  {
    "objectID": "posts/ml/fastai/lesson6_1/lesson.html#general-points",
    "href": "posts/ml/fastai/lesson6_1/lesson.html#general-points",
    "title": "FastAI Lesson 6.5: Road to the Top",
    "section": "",
    "text": "The focus should be:\n\nCreate an effective validation set\nIterate quickly to find changes which improve the validation set\n\nTrain a simple model straight away, get a result and submit it. You can iterate and improve later.\nData augmentation, in particular test-time augmentation (TTA), can improve results."
  },
  {
    "objectID": "posts/ml/fastai/lesson6_1/lesson.html#some-rules-of-thumb",
    "href": "posts/ml/fastai/lesson6_1/lesson.html#some-rules-of-thumb",
    "title": "FastAI Lesson 6.5: Road to the Top",
    "section": "",
    "text": "Rules of thumb on model selection by problem type:\n\nFor computer vision use CNNs, fine-tune a pre-trained model. See comparison of pre-trained models here.\n\nIf unsure which model to choose, start with (small) convnext.\nDifferent models can have big differences in accuracy so try a few small models from different families.\nOnce you are happy with the model, try a size up in that family.\nResizing images to a square is a good, easy compromise to accomodate many different image sizes, orientations and aspect ratios. A more involved approach that may improve results is to batch images together and resize them to the median rectangle size.\n\nFor tabular data, random forests will give a reasonably good result.\n\nGBMs will give a better result eventually, but with more effort required to get a good model.\nWorth running a hyperparameter grid search for GBM because it’s fiddly.\n\n\nRules of thumb on hardware:\n\nYou generally want ~8 physical CPUs per GPU\nIf model training is CPU bound, it can help to resize images to be smaller. The file I/O on the CPU may be taking too long.\nIf model training is CPU bound and GPU usage is low, you might as well go up to a “better” (i.e. bigger) model with no increase of overall execution time."
  },
  {
    "objectID": "posts/ml/fastai/lesson6_1/lesson.html#gradient-accumulation",
    "href": "posts/ml/fastai/lesson6_1/lesson.html#gradient-accumulation",
    "title": "FastAI Lesson 6.5: Road to the Top",
    "section": "",
    "text": "This is a technique that allows you to run larger batch sizes without running out of GPU memory.\nRather than needing to fit a whole batch (say, 64 images) in memory, we can split this up into sub-batches. Let’s say we use an accumulation factor of 4; this means each sub-batch would have 16 images.\nWe calculate the gradient for each sub-batch but don’t immediately subtract it from the weights. We also keep a running count of the number of images we have seen. Once we have seen 64 images, i.e. 4 batches, then apply the total accumulated gradient and reset the count. Remember that in Pytorch, if we calculate another gradient without zeroing in between, it will add the new gradient to the old one, so this is in effect the “default behaviour”.\nGradient accumulation gives an identical result unless using batch normalisation, since the moving averages will be more volatile. Most big models use layer normalisation rather than batch normalisation so often this is not an issue.\nHow do we pick a batch size? Just pick the largest you can without running out of GPU memory.\nWhy not just reduce the batch size? If using a pretrained model, the other parameters such as learning rate work for that batch size and might not work for others. So we’d have to perform a hyperparameter search again to find a good combination for the new batch size. Gradient accumulation sidesteps this issue."
  },
  {
    "objectID": "posts/ml/fastai/lesson6_1/lesson.html#multi-target-classification",
    "href": "posts/ml/fastai/lesson6_1/lesson.html#multi-target-classification",
    "title": "FastAI Lesson 6.5: Road to the Top",
    "section": "",
    "text": "Consider the case where we want to train a model with two different predicted labels, say plant species and disease type. If we have 10 plant species and 10 diseases, we create a model with 20 output neurons.\nHow does the model know what each should do? Through the loss function. We make one loss function for disease which takes the first 10 columns of the input to the final layer, and a second loss function for variety which takes the last 10 columns of the input to the final layer. The overall loss function is the sum of the two.\nIf you train it for longer, the multi target model can get better at predicting one target, say disease, than a single target model trained on just disease alone.\n\nThe early layers that are useful for variety may also be useful for disease\nKnowing the variety may be useful in predicting disease if there are confounding factors between disease and variety. For example, certain diseases affect certain varieties."
  },
  {
    "objectID": "posts/ml/fastai/lesson6_1/lesson.html#references",
    "href": "posts/ml/fastai/lesson6_1/lesson.html#references",
    "title": "FastAI Lesson 6.5: Road to the Top",
    "section": "",
    "text": "Course lesson page\n“Road to the Top” notebook\n“Things that confused me about cross entropy” by Chris Said."
  },
  {
    "objectID": "posts/ml/fastai/lesson8/lesson.html",
    "href": "posts/ml/fastai/lesson8/lesson.html",
    "title": "FastAI Lesson 8: Convolutions",
    "section": "",
    "text": "These are notes from lesson 8 of Fast AI Practical Deep Learning for Coders.\n\n\n\n\n\n\nHomework Task\n\n\n\nRecreate the convolutions in a spreadsheet which underpins the discussion below.\n\n\n\n\n\n\nConvolutions slide a window of numbers, say 3x3, across our original image.\nDepending on the values in the filter, it will be able to pick out different features like horizontal or vertical edges. Subsequent layers can combine these into more sophisticated layers, like corners. These can eventually be combined to detect complex features of the image.\nAn interactive example of this to see the intuition behind the sliding window is here.\n\n\n\nThis is a technique to reduce the size of the input tensor.\nA 2x2 max pool layer slides a 2x2 filter over the input and replaces each value with the max of the 4 values in the image.\nNowadays, using the stride length is generally preferred over max pool layers.\n\n\n\nAn alternative technique to reduce the size of the input is to skip pixels when we slide our filter over the image.\nFor example, a stride=2 convolution would apply to every second pixel and therefore halves the image size in each axis, having the same effect as a 2x2 max pool.\n\n\n\nAs a regularisation technique to make sure the model is not overly reliant on any single pixel or region, we can add a dropout layer.\nConceptually, this is the same as initialisng a random tensor the same size as the input and masking the input based on whether the random weight is above a threshold.\n\n\n\nWe eventually want to reduce our input image size to output a tensor with one value per class.\nOne approach is to apply a dense layer when the image has been reduced “enough”, taking the dot product between the reduced image tensor and the dense layer. This is again deprecated in favour of the next approach…\n\n\n\nNowadays, we use stride=2 convolutions until we get a small (7x7) tensor, then apply a single average pool layer to it.\nThis 7x7 tensor (for a bear detector, say) effectively gives a value quantifying “is there a bear in this part of the image?”. It then takes the average of all of these to determine if there is a bear in the overall photo.\nThis works fine if the bear occupies most of the photo, but less well if the bear occupies a small region in the corner of the photo. So the details of the model depend on the use case. If we want ot be able to detect small bears in the corner, max pooling would work better here.\nConcat pool is a hybrid approach which does the max pool AND the average pool and concatentates the two results together.\n\n\n\n\nConvolutions can be reframed in different ways, as matrix multiplications or as systems of linear equations.\nThis article is a a helpful exploration of the topic.\n\n\n\nA summary of questions to end the course.\n\nRead Meta Learning.\nDon’t try to know everything. Pick a bit that you’re interested and dig in. You’ll start to recognise the same ideas cropping up with slight tweaks.\nDoes success in deep learning boil down to more compute power? No, we can be smarter about our approach. Also pick your problems to be ones that you can actually manage with smaller compute resources.\nDragonbox Algebra 5+ can teach little kids algebra.\nTurning a model into a startup. The key to a legitimate business venture is to solve a legitimate problem, and one that people will pay you to solve. Start with the problem, not the prototype. The LEan Startup by Eric Ries: create the MVP as quick as possible (and fake the solution) then gradually make it “less fake” as more people use it (and pay you).\nMake the things you want to do easier, then you’ll want to do them more.\n\n\n\n\n\nCourse lesson page\nMeta Learning"
  },
  {
    "objectID": "posts/ml/fastai/lesson8/lesson.html#the-intuition-behind-cnns",
    "href": "posts/ml/fastai/lesson8/lesson.html#the-intuition-behind-cnns",
    "title": "FastAI Lesson 8: Convolutions",
    "section": "",
    "text": "Convolutions slide a window of numbers, say 3x3, across our original image.\nDepending on the values in the filter, it will be able to pick out different features like horizontal or vertical edges. Subsequent layers can combine these into more sophisticated layers, like corners. These can eventually be combined to detect complex features of the image.\nAn interactive example of this to see the intuition behind the sliding window is here.\n\n\n\nThis is a technique to reduce the size of the input tensor.\nA 2x2 max pool layer slides a 2x2 filter over the input and replaces each value with the max of the 4 values in the image.\nNowadays, using the stride length is generally preferred over max pool layers.\n\n\n\nAn alternative technique to reduce the size of the input is to skip pixels when we slide our filter over the image.\nFor example, a stride=2 convolution would apply to every second pixel and therefore halves the image size in each axis, having the same effect as a 2x2 max pool.\n\n\n\nAs a regularisation technique to make sure the model is not overly reliant on any single pixel or region, we can add a dropout layer.\nConceptually, this is the same as initialisng a random tensor the same size as the input and masking the input based on whether the random weight is above a threshold.\n\n\n\nWe eventually want to reduce our input image size to output a tensor with one value per class.\nOne approach is to apply a dense layer when the image has been reduced “enough”, taking the dot product between the reduced image tensor and the dense layer. This is again deprecated in favour of the next approach…\n\n\n\nNowadays, we use stride=2 convolutions until we get a small (7x7) tensor, then apply a single average pool layer to it.\nThis 7x7 tensor (for a bear detector, say) effectively gives a value quantifying “is there a bear in this part of the image?”. It then takes the average of all of these to determine if there is a bear in the overall photo.\nThis works fine if the bear occupies most of the photo, but less well if the bear occupies a small region in the corner of the photo. So the details of the model depend on the use case. If we want ot be able to detect small bears in the corner, max pooling would work better here.\nConcat pool is a hybrid approach which does the max pool AND the average pool and concatentates the two results together."
  },
  {
    "objectID": "posts/ml/fastai/lesson8/lesson.html#convolutions-from-different-viewpoints",
    "href": "posts/ml/fastai/lesson8/lesson.html#convolutions-from-different-viewpoints",
    "title": "FastAI Lesson 8: Convolutions",
    "section": "",
    "text": "Convolutions can be reframed in different ways, as matrix multiplications or as systems of linear equations.\nThis article is a a helpful exploration of the topic."
  },
  {
    "objectID": "posts/ml/fastai/lesson8/lesson.html#assorted-thoughts-from-jeremy",
    "href": "posts/ml/fastai/lesson8/lesson.html#assorted-thoughts-from-jeremy",
    "title": "FastAI Lesson 8: Convolutions",
    "section": "",
    "text": "A summary of questions to end the course.\n\nRead Meta Learning.\nDon’t try to know everything. Pick a bit that you’re interested and dig in. You’ll start to recognise the same ideas cropping up with slight tweaks.\nDoes success in deep learning boil down to more compute power? No, we can be smarter about our approach. Also pick your problems to be ones that you can actually manage with smaller compute resources.\nDragonbox Algebra 5+ can teach little kids algebra.\nTurning a model into a startup. The key to a legitimate business venture is to solve a legitimate problem, and one that people will pay you to solve. Start with the problem, not the prototype. The LEan Startup by Eric Ries: create the MVP as quick as possible (and fake the solution) then gradually make it “less fake” as more people use it (and pay you).\nMake the things you want to do easier, then you’ll want to do them more."
  },
  {
    "objectID": "posts/ml/fastai/lesson8/lesson.html#references",
    "href": "posts/ml/fastai/lesson8/lesson.html#references",
    "title": "FastAI Lesson 8: Convolutions",
    "section": "",
    "text": "Course lesson page\nMeta Learning"
  },
  {
    "objectID": "posts/ml/fastai/lesson6/lesson.html",
    "href": "posts/ml/fastai/lesson6/lesson.html",
    "title": "FastAI Lesson 6: Random Forests",
    "section": "",
    "text": "These are notes from lesson 6 of Fast AI Practical Deep Learning for Coders.\n\n\nIt’s worth starting with a random forest as a baseline model because “it’s very hard to screw up”. Decision trees only consider the ordering of data, so are not sensitive to outliers, skewed distributions, dummy variables etc.\nA random forest is an ensemble of trees. A tree is an ensemble of binary splits.\nbinary split -&gt; tree -&gt; forest\n\n\nPick a column of the data set and split the rows into two groups. Predict the outcome based just on that.\nFor example, in the titanic dataset, pick the Sex column. This splits into male vs female. If we predict that all female passengers survived and all male passengers died, that’s a reasonably accurate prediction.\nA model which iterates through the variables and finds the best binary split is called a “OneR” or one rule model.\nThis was actually the state-of-the-art in the 90s and performs reasonably well across a lot of problems.\n\n\n\nThis extends the “OneR” idea to two or more splits.\nFor example, if we first split by sex, then find the next best variable to split on to create a two layer tree.\nLeaf nodes are the number of terminating nodes in the tree. OneR creates 2 leaf nodes. If we split one side again, we’ll have 3 leaf nodes. If we split the other side, to create a balanced two layer tree, we’ll have 4 leaf nodes.\n\n\n\nThe idea behind bagging is that if we have many unbiased, uncorrelated models, we can average their predictions to get an aggregate model that is better than any of them.\nEach individual model will overfit, so either be too high or too low for a given point, a positive or negative error term respectively. By combining multiple uncorrelated models, the error will average to 0.\nAn easy way to get many uncorrelated models is to only use a random subset of the data each time. Then build a decision tree for each subset.\nThis is the idea behind random forests.\nThe error decreases with the number of trees, with diminishing returns.\n\nJeremy’s rule of thumb: improvements level off after about 30 trees and he doesn’t often use &gt;100.\n\n\n\n\n\n\n\nA nice side effect of using decision trees is that we get feature importance plots for free.\nGini is a measure of inequality, similar to the score used in the notebook to quantify how good a split was.\nIntuitively, this measures the likelihood that if you picked two samples from a group, how likely is it they’d be the same every time. If the group is all the same, the probability is 1. If every item is different, the probability is 0.\nThe idea is for each split of a decision tree, we can track the column used to split and the amount that the gini decreased by. If we loop through the tree and accumulate the “gini reduction” for each column, we have a metric of how important that column was in splitting the dataset.\nThis makes random forests a useful first model when tackling a new big dataset, as it can give an insight into how useful each column is.\n\n\n\nA comment on explainability, regarding feature importance compared to SHAP, LIME etc. These explain the model, so to be useful the model needs to be accurate.\nIf you use feature importance from a bad model then the columns it claims are important might not actually be. So the usefulness of explainability techniques boils down to what models can they explain and how accurate are those models.\n\n\n\nFor each tree, we trained on a subset of the rows. We can then see how each one performed on the held out data; this is the out-of-bag (OOB) error per tree.\nWe can average this over all of the trees to get an overall OOB error for the forest.\n\n\n\nThese are not specific to random forests and can be applied to any ML model including deep neural networks.\nIf we want to know how an independent variable affects the dependent variable, a naive approach would be to just plot them.\nBut this could include a confounding variable. For example, the price of bulldozers increases over time, but driven by the presence of air conditioning which also increased over time.\nA partial dependence plot takes each row in turn and sets the column(s) of interest to the first value, say, year=1950. Then we predict the target variable using our model. Then repeat this for year=1951, 1952, etc.\nWe can then average the target variable per year to get a view of how it depends on the independent variable, all else being equal.\n\n\n\n\nWe make multiple trees, but instead of fitting all to different data subsets, we fit to residuals.\nSo we fit a very small tree (OneR even) to the data to get a first prediction. Then we calculate the error term. Then we fit another tree to predict the error term. Then calculate the second order error term. Then fit a tree to this, etc.\nThen our prediction is the sum of these trees, rather than the average like with a random forest.\nThis is “boosting”; calculating an error term then fitting another model to it.\nContrast this with “bagging” which was when we calculate multiple models to different subsets of data and average them.\n\n\n\n\nCourse lesson page\nComparison of computer vision models for fine-tuning\nBagging predictors"
  },
  {
    "objectID": "posts/ml/fastai/lesson6/lesson.html#background-on-random-forests",
    "href": "posts/ml/fastai/lesson6/lesson.html#background-on-random-forests",
    "title": "FastAI Lesson 6: Random Forests",
    "section": "",
    "text": "It’s worth starting with a random forest as a baseline model because “it’s very hard to screw up”. Decision trees only consider the ordering of data, so are not sensitive to outliers, skewed distributions, dummy variables etc.\nA random forest is an ensemble of trees. A tree is an ensemble of binary splits.\nbinary split -&gt; tree -&gt; forest\n\n\nPick a column of the data set and split the rows into two groups. Predict the outcome based just on that.\nFor example, in the titanic dataset, pick the Sex column. This splits into male vs female. If we predict that all female passengers survived and all male passengers died, that’s a reasonably accurate prediction.\nA model which iterates through the variables and finds the best binary split is called a “OneR” or one rule model.\nThis was actually the state-of-the-art in the 90s and performs reasonably well across a lot of problems.\n\n\n\nThis extends the “OneR” idea to two or more splits.\nFor example, if we first split by sex, then find the next best variable to split on to create a two layer tree.\nLeaf nodes are the number of terminating nodes in the tree. OneR creates 2 leaf nodes. If we split one side again, we’ll have 3 leaf nodes. If we split the other side, to create a balanced two layer tree, we’ll have 4 leaf nodes.\n\n\n\nThe idea behind bagging is that if we have many unbiased, uncorrelated models, we can average their predictions to get an aggregate model that is better than any of them.\nEach individual model will overfit, so either be too high or too low for a given point, a positive or negative error term respectively. By combining multiple uncorrelated models, the error will average to 0.\nAn easy way to get many uncorrelated models is to only use a random subset of the data each time. Then build a decision tree for each subset.\nThis is the idea behind random forests.\nThe error decreases with the number of trees, with diminishing returns.\n\nJeremy’s rule of thumb: improvements level off after about 30 trees and he doesn’t often use &gt;100."
  },
  {
    "objectID": "posts/ml/fastai/lesson6/lesson.html#attributes-of-random-forests",
    "href": "posts/ml/fastai/lesson6/lesson.html#attributes-of-random-forests",
    "title": "FastAI Lesson 6: Random Forests",
    "section": "",
    "text": "A nice side effect of using decision trees is that we get feature importance plots for free.\nGini is a measure of inequality, similar to the score used in the notebook to quantify how good a split was.\nIntuitively, this measures the likelihood that if you picked two samples from a group, how likely is it they’d be the same every time. If the group is all the same, the probability is 1. If every item is different, the probability is 0.\nThe idea is for each split of a decision tree, we can track the column used to split and the amount that the gini decreased by. If we loop through the tree and accumulate the “gini reduction” for each column, we have a metric of how important that column was in splitting the dataset.\nThis makes random forests a useful first model when tackling a new big dataset, as it can give an insight into how useful each column is.\n\n\n\nA comment on explainability, regarding feature importance compared to SHAP, LIME etc. These explain the model, so to be useful the model needs to be accurate.\nIf you use feature importance from a bad model then the columns it claims are important might not actually be. So the usefulness of explainability techniques boils down to what models can they explain and how accurate are those models.\n\n\n\nFor each tree, we trained on a subset of the rows. We can then see how each one performed on the held out data; this is the out-of-bag (OOB) error per tree.\nWe can average this over all of the trees to get an overall OOB error for the forest.\n\n\n\nThese are not specific to random forests and can be applied to any ML model including deep neural networks.\nIf we want to know how an independent variable affects the dependent variable, a naive approach would be to just plot them.\nBut this could include a confounding variable. For example, the price of bulldozers increases over time, but driven by the presence of air conditioning which also increased over time.\nA partial dependence plot takes each row in turn and sets the column(s) of interest to the first value, say, year=1950. Then we predict the target variable using our model. Then repeat this for year=1951, 1952, etc.\nWe can then average the target variable per year to get a view of how it depends on the independent variable, all else being equal."
  },
  {
    "objectID": "posts/ml/fastai/lesson6/lesson.html#gradient-boosting-forests",
    "href": "posts/ml/fastai/lesson6/lesson.html#gradient-boosting-forests",
    "title": "FastAI Lesson 6: Random Forests",
    "section": "",
    "text": "We make multiple trees, but instead of fitting all to different data subsets, we fit to residuals.\nSo we fit a very small tree (OneR even) to the data to get a first prediction. Then we calculate the error term. Then we fit another tree to predict the error term. Then calculate the second order error term. Then fit a tree to this, etc.\nThen our prediction is the sum of these trees, rather than the average like with a random forest.\nThis is “boosting”; calculating an error term then fitting another model to it.\nContrast this with “bagging” which was when we calculate multiple models to different subsets of data and average them."
  },
  {
    "objectID": "posts/ml/fastai/lesson6/lesson.html#references",
    "href": "posts/ml/fastai/lesson6/lesson.html#references",
    "title": "FastAI Lesson 6: Random Forests",
    "section": "",
    "text": "Course lesson page\nComparison of computer vision models for fine-tuning\nBagging predictors"
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html",
    "href": "posts/ml/meta_learning/meta_learning.html",
    "title": "Meta Learning",
    "section": "",
    "text": "These are notes on the book Meta Learning by Radek Osmulski."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#learning-to-program",
    "href": "posts/ml/meta_learning/meta_learning.html#learning-to-program",
    "title": "Meta Learning",
    "section": "1. Learning to Program",
    "text": "1. Learning to Program\nYou must become a developer before you can be a deep learning expert. Start by tinkering with things you enjoy. Don’t exert yourself too much so that you can stay consistent above all else.\n\n1.1. CS Foundations\nStart with a programming MOOC like Harvard CS50 if coming into this fresh. Don’t get bogged down in tutorial hell, just get familiar enough with high-level concepts to be able to google the rest.\n“It doesn’t make sense to invest all your time into learnng calligraphy if you have nothing to write about”.\n\n\n1.2. IDE\nLearn to use an IDE effectively. If stuck, just start with VSCode.\n\n\n1.3. Version control\nLearn to use git and GitHub.\n\n\n1.4. DevOps\nLearn how to use your computer in the context of writing code: spin up a cloud VM, ssh into it, move data around, etc. A good resource is The Missing Semester.\n\n\n1.5. Start Learning Deep Learning the Right Way\nThe above 4 steps are to get to a stage where you can take the Practical Deep Learning for Coders which is the best starting foundation to get a high-level grounding in ML.\nUse the top-down approach to learning championed by fastai:\n\nWatch a lecture.\nLook through the associated notebook - run the code and understand the inputs and outputs of each cell.\nStart with a new notebook and try to work through the same steps from scratch as an open-book exercise. Also read the documentation as you go along to fill any gaps.\nFind a similar dataset (or make one) and try the same techniques you’ve just learned. Creating a dataset is a great way to think about feature engineering and choices of labels.\n\nLearn the whole game then play out of town."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#theory-vs-practice",
    "href": "posts/ml/meta_learning/meta_learning.html#theory-vs-practice",
    "title": "Meta Learning",
    "section": "2. Theory vs Practice",
    "text": "2. Theory vs Practice\nStarting from an “elements-first” approach can feel like a never ending struggle. You want to learn ML, but then need to study calculus, but then you need to study real analysis before that, but then you need to study set theory before that…\nThe problem with theory: theory follows practice!\nBecome a great practitioner first and the theory will make more sense afterwards, once you have some intuition. Practical problems give you an incredibly useful feedback loop for your learning that you don’t get from following a linear theoretical curriculum.\nFor best results, combine practice and theory, in that order."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#programming-is-about-what-you-have-to-say",
    "href": "posts/ml/meta_learning/meta_learning.html#programming-is-about-what-you-have-to-say",
    "title": "Meta Learning",
    "section": "3. Programming is About What You Have to Say",
    "text": "3. Programming is About What You Have to Say\nYour ability as a developer is measured by the utility of things you can do in your language of choice. What you have to say is the most important thing!\nThe key to getting started is to read and write a lot of code. Start with 100-200 line projects, anything under 1000 lines should be possible to follow. Then graduate to larger problems.\nDomain knowledge comes first. Once you know what you are trying to achieve and broadly how you can achieve it, you can worry about best practices to write clean, maintainable code later.\nThe fastest way to learn to program is to learn to say something useful."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#the-secret-of-good-developers",
    "href": "posts/ml/meta_learning/meta_learning.html#the-secret-of-good-developers",
    "title": "Meta Learning",
    "section": "4. The Secret of Good Developers",
    "text": "4. The Secret of Good Developers\nBecoming familiar with a codebase or problem requires holding multiple things in your head: the layout of the code, how it is tested, how you intend to change it, other places that might be affected by your change, etc. This requires uninterrupted focus. When you are interrupted, you drop those things you were holding in your head, and you might not always pick them up again when you switch back.\nThe price of context switching is extremely high!\nLong, uninterrupted sessions - “deep work” - are the key to progress."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#the-best-way-to-improve-as-a-developer",
    "href": "posts/ml/meta_learning/meta_learning.html#the-best-way-to-improve-as-a-developer",
    "title": "Meta Learning",
    "section": "5. The Best Way to Improve as a Developer",
    "text": "5. The Best Way to Improve as a Developer\nYou sharpen your skills with practice. To get better at a thing, do the thing!\nThe key is to read and write code. Everything else, like MOOCs, books, articles etc are nice as a side dish but they are not the main course."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#achieving-flow",
    "href": "posts/ml/meta_learning/meta_learning.html#achieving-flow",
    "title": "Meta Learning",
    "section": "6. Achieving Flow",
    "text": "6. Achieving Flow\nFlow is difficult to achieve, but we can help ourselves by removing any obvious obstacles.\nLearn your IDE inside out, and know all of the keyboard shortcuts so that you don’t interrupt your flow by switching to your mouse or searching through settings.\nLikewise, address easy things like making sure you’ve charged your laptop, keyboard, mouse etc and you don’t spend time battling connectivity issues or just plugging things in.\nFlow is a spectrum. Don’t think of it as “in flow” or “not in flow”. Rather, “how much are you flowing?”. Take small actions to increase it.\nJust work on what you want to work on. Don’t overthink it and talk yourself out of doing something because you’re not a “real” developer/author/startup founder etc."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#genuine-work-compounds",
    "href": "posts/ml/meta_learning/meta_learning.html#genuine-work-compounds",
    "title": "Meta Learning",
    "section": "7. Genuine Work Compounds",
    "text": "7. Genuine Work Compounds\n\n“Reading a book without taking notes is like discovering a new territory without making a map.”\n\nDoing &gt; thinking\nThinking about something without writing notes or doing more work on it is like running on a treadmill when your goal is to get from A to B. Just do a little bit: write some notes one day, then outline the project then next, then start the first component, etc. Before long, you will have made more progress than you expected.\nThe more “atoms you move” the more feeback you can get, so the more you can reflect and learn."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#how-to-structure-an-ml-project",
    "href": "posts/ml/meta_learning/meta_learning.html#how-to-structure-an-ml-project",
    "title": "Meta Learning",
    "section": "8. How to Structure an ML Project",
    "text": "8. How to Structure an ML Project\n\n“The only way to maintain your sanity in the long run is to be paranoid in the short run.”\n\nThe key is a good train-validation-test set split.\nThen construct a baseline. The smaller and simpler, the better.\nThis helps us know we are moving in the right direction when we iterate. It also gives an idea of what shape our reeal results should be.\nStart broad. Explore different models and architectures first, rather than diving straight in to tuning hyperparameters.\nMake changes incrementally, then run the model and check that your output is the correct shape and not all zeros. You can’t write a complex model all in one sweep! This requires running the entire pipelines regularly. This could be a big time sink to run on the whole dataset, so just train on 5% or less to keep iterations fast.\nTime is a key component of success. You might get a decent solution quite quickly. But going from good to great is a creative endeavour, which requires time to think and mull over in your subconscious."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#how-to-win-at-kaggle",
    "href": "posts/ml/meta_learning/meta_learning.html#how-to-win-at-kaggle",
    "title": "Meta Learning",
    "section": "9. How to Win at Kaggle",
    "text": "9. How to Win at Kaggle\n\nJoin a competition early\n\nDownload the dataset, understand the schema of inputs and outputs\nStart writing code immediately. A good starting point is to just download the data and make a submission (of random numbers or all zeros) to make sure you have all of the mechanics in place before working on your model.\nMore time for more iteration loops\nMore time to mull the problem and think creatively\n\nRead Kaggle forums (daily)\nMake small improvements (daily)\n\nSmall tweaks compound into big results\nInitial exploration should cover as much ground as possible, so try multiple architectures rather than focusing on tuning one specific model in the early stages.\n\nFind an appropriate validation split that mirrors the leaderboard\n\nIs random sampling appropriate or does the split require more thought?\nTrain two models and submit them. The leaderboard results should reflect what you saw locally (i.e. which was the better model). If not, you might have problems with your validation split.\n\nPosts by top Kagglers will take you 80% of the way\nPapers, blogs and creativity will take you the remaining 20%\n\nWhen reading papers, you don’t need to understand every paragraph, and trying to do so would be overly time-consuming and counter-productive. Scan the paper to understand the general idea and whether it is relevant to your problem.\nBlog posts are often more accessible and quicker to implement.\n\nEnsemble your results\n\nCross-validation is a related concept and also essential."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#hardware-for-deep-learning",
    "href": "posts/ml/meta_learning/meta_learning.html#hardware-for-deep-learning",
    "title": "Meta Learning",
    "section": "10. Hardware for Deep Learning",
    "text": "10. Hardware for Deep Learning\nExplore hardware only to the extent that you find it interesting. Otherwise it’s a false economy: you are paying with your time learning about concepts that might save a few $ here and there.\nStart with a cloud VM, colab or Kaggle kernels.\nOnce you know you are serious, a home rig is the most cost effective option. Get a GPU with the most RAM you can afford. This first rig should just get you to a stage where you know what you like to work on and what the bottlenecks worth improving are: more RAM, better CPU, more storage (and how fast does the storage need to be).\nDebugging and profiling your code (use %%timeit in Jupyter) is essential."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#sharing-your-work",
    "href": "posts/ml/meta_learning/meta_learning.html#sharing-your-work",
    "title": "Meta Learning",
    "section": "11. Sharing Your Work",
    "text": "11. Sharing Your Work\nA resume is of limited use nowadays. Meet people who influence hiring decisions where they hang out: conferences, social media, meetups, blogs etc.\nCredibility is key. Helping others and showing your work builds credibility.\nIf you are looking for work, say so! On Twitter, Linkedin etc, and reach out to people you know.\nThe deep learning community is active on Twitter. But keep your time on Twitter limited and focused. Your goal isn’t to become a content creator.\nShare your work. This builds a personal brand. It also gives you a milestone to work towards and defines when it is “done”. The sooner you start sharing your work, the better. There are fewer, if any, of the negative consequences you might anticipate when sharing work online. Failure is just “cheap feedback”; embrace it!\nMost people’s biggest regret when learning ML is not enough time spent doing and too much time spent learning theory.\nThere’s one guaranteed way to fail: stop trying. Learning compounds; you need to give it time before you see exponential results.\nA general technique that works for all aspects of life: oberseve whether you are getting the desired results. If not, change your approach.\nFind a mentor. This might not necessarily be someone you know or interact with directly, or even someone who’s alive. If you follow them (on Twitter) and learn from what they have to say, they are a mentor.\nThe “permissionless apprenticeship” approach to finding a mentor means to give value first before you receive value."
  },
  {
    "objectID": "posts/ml/meta_learning/meta_learning.html#references",
    "href": "posts/ml/meta_learning/meta_learning.html#references",
    "title": "Meta Learning",
    "section": "References",
    "text": "References\n\nThe Missing Semester\nValidation sets\nPersonal brand"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Projects\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBrazilian Jiu Jitsu Taxonomy\n\n\n\nBJJ\n\n\nWebDev\n\n\n\nGrappling with Web Development\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplainable AI in Healthcare\n\n\n\nResearch\n\n\nAI\n\n\nHealthcare\n\n\n\nOpening the black box in medical AI\n\n\n\n\n\n\n\n\n\n\n\n\n\nTradeIntel\n\n\n\nTrading\n\n\nApp\n\n\n\nNext-generation robo advisor\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "plan.html",
    "href": "plan.html",
    "title": "Gurpreet Johl",
    "section": "",
    "text": "Fast AI\nSales\nAWS\nKalman filter"
  },
  {
    "objectID": "plan.html#current",
    "href": "plan.html#current",
    "title": "Gurpreet Johl",
    "section": "",
    "text": "Fast AI\nSales\nAWS\nKalman filter"
  },
  {
    "objectID": "plan.html#done",
    "href": "plan.html#done",
    "title": "Gurpreet Johl",
    "section": "Done",
    "text": "Done\n\nSQL\nDeep learning (Tensorflow)\nSoftware architecture\nMarketing\nPublic speaking\nPitching\nSaaS\nConsulting\nSystem design"
  },
  {
    "objectID": "plan.html#skills",
    "href": "plan.html#skills",
    "title": "Gurpreet Johl",
    "section": "Skills",
    "text": "Skills\n\nSoft skills\nDone: - Marketing - 1 page marketing plan; 22 immutable laws of marketing? - Public speaking - TED talk book. Steal the show by Michael Port? - Pitching - Pitch Anything by Oren Klaff - SaaS: Start small stay small; SaaS playbook - Consulting business book\nIn Progress: - Sales - Spin selling by Neil Rackham\nTo Do: - Strategy - startup bible book - Negotiating - never split the difference book and masterclass; influence the psychology of persuasion by Robert Cialdini - Sales - Founder Sales\nBacklog: - Operations - the goal by Eliyahu Goldratt - Leadership - teams of teams by Stanley mchrystal; Start with Why: How Great Leaders Inspire Everyone to Take Action by Simon Sinek - Networking - how to be a power connector by Judy Robinett - Fundraising - crack the funding code by Judy Robinett; venture deals by Brad Feld; the art of startup fundraising by Alejandro Cremades; the startup checklist by David Rose\n\n\nSoftware\nDone: - SQL - Udemy course - Software architecture - Udemy course - System design - Udemy course\nIn progress: - AWS solutions architect Udemy course\nTo Do: - Docker - Udemy course\nBacklog: - Game development - Unity udemy course - Cracking the coding interview - Data structures and algorithms https://allendowney.github.io/DSIRP/ - Devops - Continuous deployment book\n\n\nEngineering/maths\nDone: - Deep learning (Tensorflow) - Udemy course\nIn progress: - Fast AI course - Kalman filter book\nBacklog: - Computational linear algebra https://www.fast.ai/posts/2017-07-17-num-lin-alg.html - Timeseries forecasting: https://www.kaggle.com/learn/time-series?rvi=1 - RNNs? - https://greenteapress.com/wp/think-complexity-2e/\n\n\nStats\nBacklog: - Intro to probability book - Classical stats: t-stat, ANOVA https://www.udacity.com/course/intro-to-inferential-statistics–ud201 - Filters https://www.udacity.com/course/artificial-intelligence-for-robotics–cs373\n\n\nBig data\nBacklog: - https://www.tutorialspoint.com/pyspark/index.htm - https://www.udemy.com/course/best-hands-on-big-data-practices-and-use-cases-using-pyspark/ - https://www.udacity.com/course/learn-spark-at-udacity–ud2002\n\n\nInterview prep\nBacklog: - https://www.udacity.com/course/refresh-your-resume–ud243 - https://www.udacity.com/course/data-science-interview-prep–ud944 - https://www.udacity.com/course/machine-learning-interview-prep–ud1001"
  },
  {
    "objectID": "gen-deep-learning-series.html",
    "href": "gen-deep-learning-series.html",
    "title": "Series: Generative AI",
    "section": "",
    "text": "Generative AI: GANs\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\nGAN\n\n\n\nPart 4: Generative Adversarial Networks\n\n\n\n\n\nApr 10, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI: VAEs\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\nVAE\n\n\n\nPart 3: Variational Autoencoders\n\n\n\n\n\nMar 6, 2024\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI: Deep Learning Foundations\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\n\nPart 2: The Building Blocks for Generative AI\n\n\n\n\n\nFeb 26, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI: Intro\n\n\n\n\n\n\nAI\n\n\nEngineering\n\n\nGenerativeAI\n\n\n\nPart 1: Introduction to Generative AI\n\n\n\n\n\nFeb 19, 2024\n\n\n5 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  }
]