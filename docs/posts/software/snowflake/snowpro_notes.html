<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Gurpreet Johl">
<meta name="dcterms.date" content="2025-02-02">
<meta name="description" content="Snowflake? Snow Problem.">

<title>Gurpreet Johl - Snowflake: SnowPro Core</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="../../../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../../../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../../../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Gurpreet Johl</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../projects.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../blog.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/gjohl"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/gurpreetjohl"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/gurpreetjohl"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Snowflake: SnowPro Core</h1>
                  <div>
        <div class="description">
          Snowflake? Snow Problem.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Software</div>
                <div class="quarto-category">DataEngineering</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Gurpreet Johl </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 2, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">1. Overview</a></li>
  <li><a href="#snowflake-architecture" id="toc-snowflake-architecture" class="nav-link" data-scroll-target="#snowflake-architecture">2. Snowflake Architecture</a>
  <ul class="collapse">
  <li><a href="#multi-cluster-shared-disk" id="toc-multi-cluster-shared-disk" class="nav-link" data-scroll-target="#multi-cluster-shared-disk">2.1. Multi-Cluster Shared Disk</a></li>
  <li><a href="#layers-of-snowflake" id="toc-layers-of-snowflake" class="nav-link" data-scroll-target="#layers-of-snowflake">2.2. Layers of Snowflake</a></li>
  <li><a href="#loading-data-into-snowflake" id="toc-loading-data-into-snowflake" class="nav-link" data-scroll-target="#loading-data-into-snowflake">2.3. Loading Data into Snowflake</a></li>
  <li><a href="#snowflake-editions" id="toc-snowflake-editions" class="nav-link" data-scroll-target="#snowflake-editions">2.4. Snowflake Editions</a></li>
  <li><a href="#compute-costs" id="toc-compute-costs" class="nav-link" data-scroll-target="#compute-costs">2.5. Compute Costs</a>
  <ul class="collapse">
  <li><a href="#overview-of-cost-categories" id="toc-overview-of-cost-categories" class="nav-link" data-scroll-target="#overview-of-cost-categories">2.5.1. Overview of Cost Categories</a></li>
  <li><a href="#calculating-number-of-credits-consumed-" id="toc-calculating-number-of-credits-consumed-" class="nav-link" data-scroll-target="#calculating-number-of-credits-consumed-">2.5.2. Calculating Number of Credits Consumed-</a></li>
  </ul></li>
  <li><a href="#storage-and-data-costs" id="toc-storage-and-data-costs" class="nav-link" data-scroll-target="#storage-and-data-costs">2.6 Storage and Data Costs</a>
  <ul class="collapse">
  <li><a href="#storage-types-and-costs" id="toc-storage-types-and-costs" class="nav-link" data-scroll-target="#storage-types-and-costs">2.6.1. Storage Types and Costs</a></li>
  <li><a href="#transfer-costs" id="toc-transfer-costs" class="nav-link" data-scroll-target="#transfer-costs">2.6.2. Transfer Costs</a></li>
  </ul></li>
  <li><a href="#storage-monitoring" id="toc-storage-monitoring" class="nav-link" data-scroll-target="#storage-monitoring">2.7. Storage Monitoring</a></li>
  <li><a href="#resource-monitors" id="toc-resource-monitors" class="nav-link" data-scroll-target="#resource-monitors">2.8. Resource Monitors</a></li>
  <li><a href="#warehouses-and-multi-clustering" id="toc-warehouses-and-multi-clustering" class="nav-link" data-scroll-target="#warehouses-and-multi-clustering">2.9. Warehouses and Multi Clustering</a>
  <ul class="collapse">
  <li><a href="#warehouse-properties" id="toc-warehouse-properties" class="nav-link" data-scroll-target="#warehouse-properties">2.9.1. Warehouse Properties</a></li>
  <li><a href="#creating-a-warehouse" id="toc-creating-a-warehouse" class="nav-link" data-scroll-target="#creating-a-warehouse">2.9.2. Creating a Warehouse</a></li>
  </ul></li>
  <li><a href="#snowflake-objects" id="toc-snowflake-objects" class="nav-link" data-scroll-target="#snowflake-objects">2.10. Snowflake Objects</a></li>
  <li><a href="#snowsql" id="toc-snowsql" class="nav-link" data-scroll-target="#snowsql">2.11. SnowSQL</a></li>
  </ul></li>
  <li><a href="#loading-and-unloading-data" id="toc-loading-and-unloading-data" class="nav-link" data-scroll-target="#loading-and-unloading-data">3. Loading and Unloading Data</a>
  <ul class="collapse">
  <li><a href="#stages" id="toc-stages" class="nav-link" data-scroll-target="#stages">3.1. Stages</a>
  <ul class="collapse">
  <li><a href="#internal-stage" id="toc-internal-stage" class="nav-link" data-scroll-target="#internal-stage">3.1.1. Internal Stage</a></li>
  <li><a href="#external-stage" id="toc-external-stage" class="nav-link" data-scroll-target="#external-stage">3.1.2. External Stage</a></li>
  <li><a href="#commands-for-stages" id="toc-commands-for-stages" class="nav-link" data-scroll-target="#commands-for-stages">3.1.3. Commands For Stages</a></li>
  </ul></li>
  <li><a href="#copy-into" id="toc-copy-into" class="nav-link" data-scroll-target="#copy-into">3.2. COPY INTO</a>
  <ul class="collapse">
  <li><a href="#loading-data" id="toc-loading-data" class="nav-link" data-scroll-target="#loading-data">3.2.1. Loading Data</a></li>
  <li><a href="#unloading-data" id="toc-unloading-data" class="nav-link" data-scroll-target="#unloading-data">3.2.2. Unloading Data</a></li>
  </ul></li>
  <li><a href="#file-format" id="toc-file-format" class="nav-link" data-scroll-target="#file-format">3.3. File Format</a></li>
  <li><a href="#insert-and-update" id="toc-insert-and-update" class="nav-link" data-scroll-target="#insert-and-update">3.4. Insert and Update</a></li>
  <li><a href="#storage-integration-object" id="toc-storage-integration-object" class="nav-link" data-scroll-target="#storage-integration-object">3.5. Storage Integration Object</a></li>
  <li><a href="#snowpipe" id="toc-snowpipe" class="nav-link" data-scroll-target="#snowpipe">3.6. Snowpipe</a></li>
  <li><a href="#copy-options" id="toc-copy-options" class="nav-link" data-scroll-target="#copy-options">3.7. Copy Options</a>
  <ul class="collapse">
  <li><a href="#on_error" id="toc-on_error" class="nav-link" data-scroll-target="#on_error">3.7.1. ON_ERROR</a></li>
  <li><a href="#size_limit" id="toc-size_limit" class="nav-link" data-scroll-target="#size_limit">3.7.2. SIZE_LIMIT</a></li>
  <li><a href="#purge" id="toc-purge" class="nav-link" data-scroll-target="#purge">3.7.3. PURGE</a></li>
  <li><a href="#match_by_column_name" id="toc-match_by_column_name" class="nav-link" data-scroll-target="#match_by_column_name">3.7.4. MATCH_BY_COLUMN_NAME</a></li>
  <li><a href="#enforce_length" id="toc-enforce_length" class="nav-link" data-scroll-target="#enforce_length">3.7.5. ENFORCE_LENGTH</a></li>
  <li><a href="#force" id="toc-force" class="nav-link" data-scroll-target="#force">3.7.6. FORCE</a></li>
  <li><a href="#load_uncertain_files" id="toc-load_uncertain_files" class="nav-link" data-scroll-target="#load_uncertain_files">3.7.7. LOAD_UNCERTAIN_FILES</a></li>
  <li><a href="#validation_mode" id="toc-validation_mode" class="nav-link" data-scroll-target="#validation_mode">3.7.8. VALIDATION_MODE</a></li>
  </ul></li>
  <li><a href="#validate" id="toc-validate" class="nav-link" data-scroll-target="#validate">3.8. VALIDATE</a></li>
  <li><a href="#unloading" id="toc-unloading" class="nav-link" data-scroll-target="#unloading">3.9. Unloading</a></li>
  </ul></li>
  <li><a href="#data-transformation" id="toc-data-transformation" class="nav-link" data-scroll-target="#data-transformation">4. Data Transformation</a>
  <ul class="collapse">
  <li><a href="#transformations-and-functions" id="toc-transformations-and-functions" class="nav-link" data-scroll-target="#transformations-and-functions">4.1. Transformations and Functions</a></li>
  <li><a href="#estimation-functions" id="toc-estimation-functions" class="nav-link" data-scroll-target="#estimation-functions">4.2. Estimation Functions</a>
  <ul class="collapse">
  <li><a href="#number-of-distinct-values---hll" id="toc-number-of-distinct-values---hll" class="nav-link" data-scroll-target="#number-of-distinct-values---hll">4.2.1. Number of Distinct Values - <code>HLL()</code></a></li>
  <li><a href="#frequent-values---approx_top_k" id="toc-frequent-values---approx_top_k" class="nav-link" data-scroll-target="#frequent-values---approx_top_k">4.2.2. Frequent Values - <code>APPROX_TOP_K()</code></a></li>
  <li><a href="#percentile-values---approx_percentile" id="toc-percentile-values---approx_percentile" class="nav-link" data-scroll-target="#percentile-values---approx_percentile">4.2.3. Percentile Values - <code>APPROX_PERCENTILE()</code></a></li>
  <li><a href="#similarity-of-two-or-more-data-sets---minhash-approximate_similarity" id="toc-similarity-of-two-or-more-data-sets---minhash-approximate_similarity" class="nav-link" data-scroll-target="#similarity-of-two-or-more-data-sets---minhash-approximate_similarity">4.2.4. Similarity of Two or More Data Sets - <code>MINHASH &amp; APPROXIMATE_SIMILARITY()</code></a></li>
  </ul></li>
  <li><a href="#user-defined-functions-udf" id="toc-user-defined-functions-udf" class="nav-link" data-scroll-target="#user-defined-functions-udf">4.3. User-Defined Functions (UDF)</a>
  <ul class="collapse">
  <li><a href="#defining-a-udf" id="toc-defining-a-udf" class="nav-link" data-scroll-target="#defining-a-udf">4.3.1. Defining a UDF</a></li>
  <li><a href="#using-a-udf" id="toc-using-a-udf" class="nav-link" data-scroll-target="#using-a-udf">4.3.2. Using a UDF</a></li>
  <li><a href="#function-properties" id="toc-function-properties" class="nav-link" data-scroll-target="#function-properties">4.3.3. Function Properties</a></li>
  </ul></li>
  <li><a href="#stored-procedures" id="toc-stored-procedures" class="nav-link" data-scroll-target="#stored-procedures">4.4. Stored Procedures</a>
  <ul class="collapse">
  <li><a href="#creating-a-stored-procedure" id="toc-creating-a-stored-procedure" class="nav-link" data-scroll-target="#creating-a-stored-procedure">4.4.1. Creating a Stored Procedure</a></li>
  <li><a href="#calling-a-stored-procedure" id="toc-calling-a-stored-procedure" class="nav-link" data-scroll-target="#calling-a-stored-procedure">4.4.2. Calling a Stored Procedure</a></li>
  </ul></li>
  <li><a href="#external-functions" id="toc-external-functions" class="nav-link" data-scroll-target="#external-functions">4.5. External Functions</a></li>
  <li><a href="#secure-udfs-and-procedures" id="toc-secure-udfs-and-procedures" class="nav-link" data-scroll-target="#secure-udfs-and-procedures">4.6. Secure UDFs and Procedures</a></li>
  <li><a href="#sequences" id="toc-sequences" class="nav-link" data-scroll-target="#sequences">4.7. Sequences</a></li>
  <li><a href="#semi-structured-data" id="toc-semi-structured-data" class="nav-link" data-scroll-target="#semi-structured-data">4.8. Semi-structured Data</a>
  <ul class="collapse">
  <li><a href="#what-is-semi-structured-data" id="toc-what-is-semi-structured-data" class="nav-link" data-scroll-target="#what-is-semi-structured-data">4.8.1. What is Semi-structured Data?</a></li>
  <li><a href="#querying-semi-structured-data" id="toc-querying-semi-structured-data" class="nav-link" data-scroll-target="#querying-semi-structured-data">4.8.2. Querying Semi-structured Data</a></li>
  <li><a href="#flatten-hierarchical-data" id="toc-flatten-hierarchical-data" class="nav-link" data-scroll-target="#flatten-hierarchical-data">4.8.3. Flatten Hierarchical Data</a></li>
  <li><a href="#insert-json-data" id="toc-insert-json-data" class="nav-link" data-scroll-target="#insert-json-data">4.8.4. Insert JSON Data</a></li>
  </ul></li>
  <li><a href="#unstructured-data" id="toc-unstructured-data" class="nav-link" data-scroll-target="#unstructured-data">4.9. Unstructured Data</a></li>
  <li><a href="#data-sampling" id="toc-data-sampling" class="nav-link" data-scroll-target="#data-sampling">4.10. Data Sampling</a></li>
  <li><a href="#tasks" id="toc-tasks" class="nav-link" data-scroll-target="#tasks">4.11. Tasks</a></li>
  <li><a href="#streams" id="toc-streams" class="nav-link" data-scroll-target="#streams">4.12. Streams</a></li>
  </ul></li>
  <li><a href="#snowflake-tools-and-connectors" id="toc-snowflake-tools-and-connectors" class="nav-link" data-scroll-target="#snowflake-tools-and-connectors">5. Snowflake Tools and Connectors</a>
  <ul class="collapse">
  <li><a href="#connectors-and-drivers" id="toc-connectors-and-drivers" class="nav-link" data-scroll-target="#connectors-and-drivers">5.1. Connectors and Drivers</a></li>
  <li><a href="#snowflake-scripting" id="toc-snowflake-scripting" class="nav-link" data-scroll-target="#snowflake-scripting">5.2. Snowflake Scripting</a></li>
  <li><a href="#snowpark" id="toc-snowpark" class="nav-link" data-scroll-target="#snowpark">5.3. Snowpark</a></li>
  </ul></li>
  <li><a href="#continuous-data-protection" id="toc-continuous-data-protection" class="nav-link" data-scroll-target="#continuous-data-protection">6. Continuous Data Protection</a>
  <ul class="collapse">
  <li><a href="#time-travel" id="toc-time-travel" class="nav-link" data-scroll-target="#time-travel">6.1. Time Travel</a></li>
  <li><a href="#undrop" id="toc-undrop" class="nav-link" data-scroll-target="#undrop">6.2. UNDROP</a></li>
  <li><a href="#retention-period" id="toc-retention-period" class="nav-link" data-scroll-target="#retention-period">6.3. Retention Period</a></li>
  <li><a href="#fail-safe" id="toc-fail-safe" class="nav-link" data-scroll-target="#fail-safe">6.4. Fail Safe</a></li>
  <li><a href="#storage-costs" id="toc-storage-costs" class="nav-link" data-scroll-target="#storage-costs">6.5. Storage Costs</a></li>
  <li><a href="#table-types" id="toc-table-types" class="nav-link" data-scroll-target="#table-types">6.6. Table Types</a></li>
  </ul></li>
  <li><a href="#zero-copy-cloning-and-sharing" id="toc-zero-copy-cloning-and-sharing" class="nav-link" data-scroll-target="#zero-copy-cloning-and-sharing">7. Zero-Copy Cloning and Sharing</a>
  <ul class="collapse">
  <li><a href="#zero-copy-cloning" id="toc-zero-copy-cloning" class="nav-link" data-scroll-target="#zero-copy-cloning">7.1. Zero Copy Cloning</a></li>
  <li><a href="#privileges" id="toc-privileges" class="nav-link" data-scroll-target="#privileges">7.2. Privileges</a></li>
  </ul></li>
  <li><a href="#account-and-security" id="toc-account-and-security" class="nav-link" data-scroll-target="#account-and-security">8. Account and Security</a></li>
  <li><a href="#performance-concepts" id="toc-performance-concepts" class="nav-link" data-scroll-target="#performance-concepts">9. Performance Concepts</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="overview" class="level1">
<h1>1. Overview</h1>
<p>The <strong>Snowsight</strong> interface is the GUI through which we interact with Snowflake.</p>
<p>When querying a Snowflake table, a <strong>fully qualified table name</strong> means <code>database_name + schema_name + table_name</code>. For example, “DERIVED_DB.PUBLIC.TRADES_DATA”</p>
<p><strong>Worksheets</strong> are associated with a <strong>role</strong>.</p>
<p>A <strong>warehouse</strong> is needed for <strong>compute</strong> to execute a query.</p>
<p>Snowflake is a “self-managed cloud data platform”. It is cloud only. No on premise option.</p>
<p>“Self-managed” service means:</p>
<ul>
<li>No hardware</li>
<li>No software</li>
<li>No maintenance</li>
</ul>
<p>“Data platform” means it can function as:</p>
<ul>
<li>Data warehouse</li>
<li>Data lake - mix of structured and semi structured data</li>
<li>Data science - use your preferred language via <em>Snowpark</em></li>
</ul>
</section>
<section id="snowflake-architecture" class="level1">
<h1>2. Snowflake Architecture</h1>
<section id="multi-cluster-shared-disk" class="level2">
<h2 class="anchored" data-anchor-id="multi-cluster-shared-disk">2.1. Multi-Cluster Shared Disk</h2>
<p>In general, there are two approaches to designing a dsitributed data / compute platform: shared-disk and shared-nothing.</p>
<p><strong>Shared-disk</strong> uses <em>central data storage</em> connected to <em>multiple compute nodes</em>.</p>
<ul>
<li>Pros: simple, easy data management since their is only one database/disk</li>
<li>Cons: limited scalability (bottleneck of the central disk), single point of failure</li>
</ul>
<p><strong>Shared-nothing</strong> keeps <em>each node independent</em>. Each node is a <em>separate processor, memory and disk</em>.</p>
<ul>
<li>Pros: scalability, availability</li>
<li>Cons: complicated, expensive</li>
</ul>
<p>Snowflake uses a <strong>hybrid approach</strong>: “multi-cluster shared-data”.</p>
<ul>
<li>There is a <em>single data repository like shared-disk</em>.</li>
<li>There are <em>multiple clusters or nodes</em> that store a <strong>portion</strong> of the data <strong>locally</strong>, like shared-nothing.</li>
</ul>
<p>This combines the pros of both: simplicity and scalability.</p>
</section>
<section id="layers-of-snowflake" class="level2">
<h2 class="anchored" data-anchor-id="layers-of-snowflake">2.2. Layers of Snowflake</h2>
<p>There are three distinct layers of Snowflake:</p>
<ol type="1">
<li>Database storage
<ol type="a">
<li>Compressed columnar storage.</li>
<li>This is stored as blobs in AWS, Azure, GCP etc.</li>
<li><strong>Snowflake abstracts this away</strong> so we just interact with it like a table.</li>
<li>This is optimised for OLAP (analytical purposes) which is <strong>read-heavy</strong>, rather than OLTP which is write-heavy.</li>
</ol></li>
<li>Compute
<ol type="a">
<li>“The <strong>muscle</strong> of the system”.</li>
<li>Query processing.</li>
<li>Queries are processed using <strong>“virtual warehouses”</strong>. These are massive parallel processing compute clusters, e.g.&nbsp;EC2 on AWS.</li>
</ol></li>
<li>Cloud services
<ol type="a">
<li>“The <strong>brain</strong> of the system”.</li>
<li>Collection of services to manage and coordinate components, e.g.&nbsp;the S3 and EC2 instances used in the other two layers.</li>
<li>The cloud services layer also runs on a compute instance of the cloud provider and is <em>completely handled by Snowflake</em>.</li>
<li>This layer handles: <strong>authentication, access control, metadata management, infrastructure management, query parsing and optimisation</strong>. The query <strong>execution</strong> happens in the compute layer.</li>
</ol></li>
</ol>
</section>
<section id="loading-data-into-snowflake" class="level2">
<h2 class="anchored" data-anchor-id="loading-data-into-snowflake">2.3. Loading Data into Snowflake</h2>
<p>This is covered more extensivelyt in its own section, but this sub-section serves as a brief introduction.</p>
<p>The usual SQL commands can be used to create databases and tables.</p>
<pre><code>CREATE DATABASE myfirstdb
ALTER DATABASE myfirstdb RENAME firstdb
CREATE TABLE loan_payments (
    col1 string,
    col2 string,
);</code></pre>
<p>We can specify a database to use with the <code>USE DATABASE</code> command to switch the active database. This avoids having to use the <em>fully qualified table name</em> everywhere.</p>
<pre><code>USE DATABASE firstdb

COPY INTO loan_payments
FROM s3/… -- The URL to copy from
file_format = (delimiter = “,”,
               skip rows=1,
               type=csv);</code></pre>
</section>
<section id="snowflake-editions" class="level2">
<h2 class="anchored" data-anchor-id="snowflake-editions">2.4. Snowflake Editions</h2>
<p>The different Snowflake editions vary by <em>features and pricing</em>. The feature matrix is available on the <a href="https://docs.snowflake.com/en/user-guide/intro-editions">Snowflake docs</a>.</p>
<ul>
<li>Standard
<ul>
<li>Complete DWH, automatic data encryption, support for standard and special data types, time travel 1 day, disaster recovery for 7 days beyond time travel, network policies, federated auth and SSO, 24/7 support</li>
</ul></li>
<li>Enterprise
<ul>
<li>Multi cluster warehouse, time travel 90 days, materialised views, search optimisation, column-level security, 24 hour early access to new releases</li>
</ul></li>
<li>Business critical
<ul>
<li>Additional security features such as customer managed encryption, support for data specific regulation, database failover and fallback</li>
</ul></li>
<li>Virtual private
<ul>
<li>Dedicated virtual servers and warehouse, dedicated metadata store. Isolated from all other snowflake accounts.</li>
</ul></li>
</ul>
</section>
<section id="compute-costs" class="level2">
<h2 class="anchored" data-anchor-id="compute-costs">2.5. Compute Costs</h2>
<section id="overview-of-cost-categories" class="level3">
<h3 class="anchored" data-anchor-id="overview-of-cost-categories">2.5.1. Overview of Cost Categories</h3>
<p>Compute costs and storage costs are decoupled and can be scaled separately. “Pay for what you need”.</p>
<ul>
<li><strong>Active warehouses</strong>
<ul>
<li>Used for standard query processing.</li>
<li>Billed per second (minimum 1 minute).</li>
<li>Depends on <em>size of warehouse, time and number of warehouses</em>.</li>
</ul></li>
<li><strong>Cloud services</strong>
<ul>
<li>Behind-the-scenes cloud service tasks.</li>
<li><em>Only charged if &gt;10% of warehouse consumption</em>, which is not the case for most customers.</li>
</ul></li>
<li><strong>Serverless</strong>
<ul>
<li>Used for <em>search optimisation</em> and <em>Snowpipe</em>.</li>
<li>This is compute that is managed by snowflake, e.g.&nbsp;event-based processing.</li>
</ul></li>
</ul>
<p>These are charged in <strong>Snowflake credits</strong>.</p>
</section>
<section id="calculating-number-of-credits-consumed-" class="level3">
<h3 class="anchored" data-anchor-id="calculating-number-of-credits-consumed-">2.5.2. Calculating Number of Credits Consumed-</h3>
<p>The warehouses consume the following number of credits per hour:</p>
<table class="table">
<thead>
<tr class="header">
<th>Warehouse Size</th>
<th>Number of Credits</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>XS</td>
<td>1</td>
</tr>
<tr class="even">
<td>S</td>
<td>2</td>
</tr>
<tr class="odd">
<td>M</td>
<td>4</td>
</tr>
<tr class="even">
<td>L</td>
<td>8</td>
</tr>
<tr class="odd">
<td>XL</td>
<td>16</td>
</tr>
<tr class="even">
<td>4XL</td>
<td>128</td>
</tr>
</tbody>
</table>
<p><strong>Credits cost different amounts per edition.</strong> It also depends on the <em>cloud provider</em> (AWS) and <em>region</em> (US-East-1). Indicative costs for AWS US-East-1 are:</p>
<table class="table">
<thead>
<tr class="header">
<th>Edition</th>
<th>$ / Credit</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Standard</td>
<td>2</td>
</tr>
<tr class="even">
<td>Enterprise</td>
<td>3</td>
</tr>
<tr class="odd">
<td>Business Critical</td>
<td>4</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="storage-and-data-costs" class="level2">
<h2 class="anchored" data-anchor-id="storage-and-data-costs">2.6 Storage and Data Costs</h2>
<section id="storage-types-and-costs" class="level3">
<h3 class="anchored" data-anchor-id="storage-types-and-costs">2.6.1. Storage Types and Costs</h3>
<p>Monthly storage costs are based on <strong>average storage used per month</strong>. Also depends on <strong>cloud provider and region</strong>. Cost is calculated <strong>AFTER Snowflake’s data compression</strong>.</p>
<p>There are two options for storage pricing:</p>
<ul>
<li><strong>On demand storage</strong>: Pay for what you use.</li>
<li><strong>Capacity storage</strong>: Pay upfront for defined capacity.</li>
</ul>
<p>Typically start with on demand until we understand our actual usage, then shift to capacity storage once this is stable.</p>
</section>
<section id="transfer-costs" class="level3">
<h3 class="anchored" data-anchor-id="transfer-costs">2.6.2. Transfer Costs</h3>
<p>This depends on data <strong>ingress vs egress</strong>.</p>
<ul>
<li><strong>Data IN is free</strong>
<ul>
<li>Snowflake wants to remove friction to getting your data in.</li>
</ul></li>
<li><strong>Data OUT is charged</strong>
<ul>
<li>Snowflake wants to add friction to leaving.</li>
<li>Depends on <em>cloud provider and region</em>. <strong>In-region transfers are free</strong>. Cross-region or cross-providers are charged.</li>
</ul></li>
</ul>
</section>
</section>
<section id="storage-monitoring" class="level2">
<h2 class="anchored" data-anchor-id="storage-monitoring">2.7. Storage Monitoring</h2>
<p>We can monitor storage for <em>individual tables</em>.</p>
<p><code>SHOW TABLES</code> gives general table storage stats and properties.</p>
<p>We get more detailed views with <code>TABLE_STORAGE_METRICS</code>. We can run this against the information schema or the account storage. These split the sizes into active bytes, time travel bytes and failsafe bytes.</p>
<p>For the information schema metrics:</p>
<pre><code>SELECT * FROM DB_NAME.INFORMATION_SCHEMA.TABLE_STORAGE_METRICS;</code></pre>
<p>For the account admin metrics, this needs to use the correct account admin role <code>USE ROLE ACCOUNTADMIN</code>.</p>
<pre><code>SELECT * FROM SNOWFLAKE.ACCOUNT_USAGE.TABLE_STORAGE_METRICS;</code></pre>
<p>We can also look at the <code>Admin -&gt; Usage</code> screen in the Snowflake GUI.</p>
</section>
<section id="resource-monitors" class="level2">
<h2 class="anchored" data-anchor-id="resource-monitors">2.8. Resource Monitors</h2>
<p>Resource monitors help us <strong>control and monitor credit usage</strong> of individual warehouses and the entire account.</p>
<p>We can set a <strong>credit quota</strong> which limit the credits used per period. For example, the maximim number of credits that can be spent per month.</p>
<p>We can set <strong>actions</strong> based on when a percentage of the credit limit is reached. These percentages can be &gt;100%. There are three options for the choice of action:</p>
<ul>
<li>Notify</li>
<li>Suspend and notify (but continue running tasks that have already started)</li>
<li>Suspend immediately (aborting any running queries) and notify.</li>
</ul>
<p>We set this using the Usage tab in the ACCOUNTADMIN role in the snowsight UI under <code>Admin -&gt; Usage</code>. Other roles can be granted MONITOR and MODIFY privileges.</p>
<p>We can select a warehouse then filter on different dimensions, for example, distinguishing storage vs compute vs data transfer costs.</p>
<p>To set up a new resource monitor, we give it:</p>
<ul>
<li>Name</li>
<li>Credit quota: how many credits to limit to</li>
<li>Monitor type: specific warehouse, group of warehouses, or overall account</li>
<li>Schedule</li>
<li>Actions</li>
</ul>
</section>
<section id="warehouses-and-multi-clustering" class="level2">
<h2 class="anchored" data-anchor-id="warehouses-and-multi-clustering">2.9. Warehouses and Multi Clustering</h2>
<section id="warehouse-properties" class="level3">
<h3 class="anchored" data-anchor-id="warehouse-properties">2.9.1. Warehouse Properties</h3>
<p>There are different <em>types</em> and <em>sizes</em> of warehouse and they can be <em>multi-clustered</em>.</p>
<p><strong>Types</strong>: standard and snowpark-optimised (for memeory-intensive tasks like ML)</p>
<p><strong>Size</strong>: XS to XXL. Snowpark type is only M or bigger and consumes 50% more credits</p>
<p><strong>Multi-clustering</strong> is good for more queries, i.e.&nbsp;more concurrent users. We scale horizontally so there are multiple small warehouses rather than one big one. They can be in <strong>maximised mode</strong> (set size) or <strong>autoscaled mode</strong> (number of nodes scales between predefined min and max)</p>
<p>The <em>autoscaler</em> decides to add warehouses based on the queue, according to the <strong>scaling policy</strong>.</p>
<ul>
<li>Standard
<ul>
<li>Favours starting extra clusters.</li>
<li>Starts a new cluster <strong>as soon as there is a query queued</strong>.</li>
<li>Cluster shuts down after 2 to 3 successful checks. A “check” is when the load on the least used node could be redistributed to other nodes.</li>
</ul></li>
<li>Economy
<ul>
<li>Favours <strong>conserving credits</strong>.</li>
<li>Starts a new cluster once the workload for the cluster would keep it <strong>running for &gt; 6 mins</strong>.</li>
<li>Cluster shuts down after 5-6 successful checks.</li>
</ul></li>
</ul>
</section>
<section id="creating-a-warehouse" class="level3">
<h3 class="anchored" data-anchor-id="creating-a-warehouse">2.9.2. Creating a Warehouse</h3>
<p>To create a warehouse, we need to use the ACCOUNTADMIN, SECURTIYADMIN or SYSADMIN role.</p>
<p>Warehouses can either be created through UI or SQL.</p>
<pre><code>CREATE WAREHOUSE my_wh
WITH
WAREHOUSE_SIZE = XSMALL
MIN_CLUSTER_COUNT = 1
MAX_CLUSTER_COUNT = 3
AUTO_RESUME = TRUE
AUTO_SUSPEND = 300
COMMENT = 'This is the first warehouse'</code></pre>
<p>We can also <code>ALTER</code> or <code>DROP</code> a warehouse in SQL, just like we normally would with <code>DROP TABLE</code>.</p>
<pre><code>DROP WAREHOUSE my_wh;</code></pre>
</section>
</section>
<section id="snowflake-objects" class="level2">
<h2 class="anchored" data-anchor-id="snowflake-objects">2.10. Snowflake Objects</h2>
<p>There is a hierarchy of objects in Snowflake.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD


  A(Organisation) --&gt; B1(Account 1)
  A(Organisation) --&gt; B2(Account 2)


  B1 --&gt; C1(Users)
  B1 --&gt; C2(Roles)
  B1 --&gt; C3(Databases)
  B1 --&gt; C4(Warehouses)
  B1 --&gt; C5(Other account objects)
  
  C3 --&gt; D1(Schemas)

  D1 --&gt; E1(UDFs)
  D1 --&gt; E2(Views)
  D1 --&gt; E3(Tables)
  D1 --&gt; E4(Stages)
  D1 --&gt; E5(Other database objects)
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>An <strong>organisation</strong> (managed by ORGADMIN) can have multiple <strong>accounts</strong> (each managed by am ACCOUNTADMIN). These accounts might be by cloud region or department.</p>
<p>Within each <strong>account</strong> we have multiple account objects: <strong>users, roles, databases, warehouses, other objects</strong>.</p>
<p><strong>Databases</strong> can have multiple <strong>schemas</strong>.</p>
<p><strong>Schemas</strong> can have multiples <strong>UDFs, views, tables, stages, other objects</strong>.</p>
</section>
<section id="snowsql" class="level2">
<h2 class="anchored" data-anchor-id="snowsql">2.11. SnowSQL</h2>
<p>SnowSQL is used to connect to Snowflake via the <strong>command line</strong>. It needs to be installed on your local machine.</p>
<p>We can execute queries, load and unload data, etc.</p>
</section>
</section>
<section id="loading-and-unloading-data" class="level1">
<h1>3. Loading and Unloading Data</h1>
<section id="stages" class="level2">
<h2 class="anchored" data-anchor-id="stages">3.1. Stages</h2>
<p>Stages are <strong>locations used to store data</strong>.</p>
<p>From the stage, say an S3 bucket, we can <strong>load</strong> data from stage -&gt; database. Likewise, we can <strong>unload</strong> data from database -&gt; stage (S3 bucket).</p>
<p>Stages can be <strong>internal</strong> (managed by Snowflake) or <strong>external</strong> (managed by your cloud provider, eg AWS S3).</p>
<section id="internal-stage" class="level3">
<h3 class="anchored" data-anchor-id="internal-stage">3.1.1. Internal Stage</h3>
<p>An internal stage is managed by Snowflake.</p>
<p>We <strong>upload data into an internal stage</strong> using the <code>PUT</code> command. By default, files are compressed with gzip and encrypted.</p>
<p>We <strong>load</strong> it into the database using the <code>COPY INTO</code> command. We can also <strong>unload</strong> using the <code>COPY INTO</code> command by varying the destination.</p>
<p>There are three types of stage:</p>
<ul>
<li>User stage
<ul>
<li>Can only be accessed by <strong>one user</strong></li>
<li>Every user has one by default</li>
<li>Cannot be altered or dropped</li>
<li>Accessed with <code>@~</code></li>
</ul></li>
<li>Table stage
<ul>
<li>Can only be accessed by <strong>one table</strong></li>
<li>Cannot be altered or dropped</li>
<li>Use this to load to a specific table</li>
<li>Accessed with <code>@%</code></li>
</ul></li>
<li>Named stage
<ul>
<li><code>CREATE STAGE</code> to create your own</li>
<li>This is then just like any other database object, so you can modify it or grant privileges</li>
<li>Most commonly used stage</li>
<li>Accessed with <code>@</code></li>
</ul></li>
</ul>
<p>A typical use case for an internal stage is when we have a file on our local system that we want to load into Snowflake, but we don’t have an external cloud provider set up.</p>
</section>
<section id="external-stage" class="level3">
<h3 class="anchored" data-anchor-id="external-stage">3.1.2. External Stage</h3>
<p>An external stage connects to an external cloud provider, such as an S3 bucket.</p>
<p>We create it with the <code>CREATE STAGE</code> command as with an internal stage. This creates a Snowflake object that we can modify and grant privileges to.</p>
<pre><code>CREATE STAGE stage_name 
  URL='s3://bucket/path/'</code></pre>
<p>We <em>can</em> add <code>CREDENTIALS</code> argument but this would store them in plain text. A better practice is to pass a <code>STORAGE_INTEGRATION</code> argument that points to credentials.</p>
<p>We can also specify the <code>FILE_FORMAT</code>.</p>
</section>
<section id="commands-for-stages" class="level3">
<h3 class="anchored" data-anchor-id="commands-for-stages">3.1.3. Commands For Stages</h3>
<p>Some of the most common commands for stages:</p>
<ul>
<li><code>LIST</code>
<ul>
<li>List all files (and additional properties) in the stage.</li>
</ul></li>
<li><code>COPY INTO</code>
<ul>
<li>Load data into the stage, or unload data from the stage.</li>
</ul></li>
<li><code>SELECT</code>
<ul>
<li>Query from stage</li>
</ul></li>
<li><code>DESC</code>
<ul>
<li>Describe the stage. Shows the default values or arguments.</li>
</ul></li>
</ul>
</section>
</section>
<section id="copy-into" class="level2">
<h2 class="anchored" data-anchor-id="copy-into">3.2. COPY INTO</h2>
<p>This can <strong>bulk load or unload data</strong>.</p>
<p>A <em>warehouse is needed</em>. Data transfer costs may apply if moving across regions or cloud providers.</p>
<section id="loading-data" class="level3">
<h3 class="anchored" data-anchor-id="loading-data">3.2.1. Loading Data</h3>
<p>Load data from a stage to a table with:</p>
<pre><code>COPY INTO table_name 
FROM stage_name</code></pre>
<p>We can specify a file or list of files with the <code>FILES</code> argument.</p>
<p>Supported file formats are:</p>
<ul>
<li>csv (default)</li>
<li>json</li>
<li>avro</li>
<li>orc</li>
<li>parquet</li>
<li>xml</li>
</ul>
<p>We can also use the <code>PATTERN</code> argument to match a file pattern with wildcards, e.g.&nbsp;<code>order*.csv</code></p>
</section>
<section id="unloading-data" class="level3">
<h3 class="anchored" data-anchor-id="unloading-data">3.2.2. Unloading Data</h3>
<p>Unloading data from the table to a stage uses the same syntax:</p>
<pre><code>COPY INTO stage_name 
FROM table_name</code></pre>
<p>As with loading, we can specify a file format with the <code>FILE_FORMAT</code> arg, or pass a reusable <code>FILE_FORMAT</code> object.</p>
<pre><code>COPY INTO stage_name 
FROM table_name
FILE_FORMAT = ( FORMAT_NAME = 'file_format_name' |
                TYPE = CSV )</code></pre>
</section>
</section>
<section id="file-format" class="level2">
<h2 class="anchored" data-anchor-id="file-format">3.3. File Format</h2>
<p>If the file format is not specified, it defaults to csv. You can see this and other default values by describing the stage with:</p>
<pre><code>DESC STAGE stage_name</code></pre>
<p>We can overrule the defaults by specifying <code>FILE_FORMAT</code> argument in the <code>COPY INTO</code> command.</p>
<p>A better practice is to use the file_format arg to pass a file_format object such as</p>
<pre><code>FILE_FORMAT = (TYPE = CSV)</code></pre>
<p>We create this object with</p>
<pre><code>CREATE FILE FORMAT file_format_name
TYPE = CSV
FIELD_DELIMITER = ‘,’
SKIP_HEADER = 1</code></pre>
<p>We write this file format to a table like <code>manage_db</code>. Then we can reuse it in multiple places when creating the stage or table, loading or unloading data, etc.</p>
</section>
<section id="insert-and-update" class="level2">
<h2 class="anchored" data-anchor-id="insert-and-update">3.4. Insert and Update</h2>
<p><strong>Insert</strong> is the same as standard SQL:</p>
<pre><code>INSERT INTO table_name
VALUES (1, 0.5, 'string')</code></pre>
<p>To only insert <em>specific columns</em>:</p>
<pre><code>INSERT INTO table_name (col1, col2)
VALUES (1, 0.5)</code></pre>
<p><code>INSERT OVERWRITE</code> will <strong>truncate any existing data</strong> and insert <strong>only</strong> the given values. <strong>Use with caution!</strong> Any previous data is dropped, the table with only have the rows in this command.</p>
<p><strong>Update</strong> also works like standard SQL:</p>
<pre><code>UPDATE table_name
SET col1=10
WHERE col1=1</code></pre>
<p><code>TRUNCATE</code> removes all of the values in the table.</p>
<p><code>DROP</code> removes the entire table object and its contents.</p>
</section>
<section id="storage-integration-object" class="level2">
<h2 class="anchored" data-anchor-id="storage-integration-object">3.5. Storage Integration Object</h2>
<p>This object stores a <strong>generated identity for external cloud storage</strong>.</p>
<p>We create it as a Snowflake object which constrains the allowed location and grant permissions to it in AWS, Azure etc.</p>
</section>
<section id="snowpipe" class="level2">
<h2 class="anchored" data-anchor-id="snowpipe">3.6. Snowpipe</h2>
<p>The discussion so far has focused on <strong>bulk loading</strong>, i.e.&nbsp;manual loading of a batch of data.</p>
<p>Snowpipe is used for <strong>continuous data loading</strong>.</p>
<p>A <strong>pipe</strong> is a Snowflake object. It <em>loads data immediately when a file appears in blob storage</em>. It triggers a predefined <code>COPY</code> command. This is useful <strong>when data needs to be available immediately</strong>.</p>
<p>Snowpipe uses <strong>serverless</strong> features rather than warehouses.</p>
<p>When files are uploaded to an S3 bucket, it sends an event notification to a serverless process which executes the copy command into the Snowflake database.</p>
<pre><code>CREATE PIPE pipe_name
AUTO_INGEST = TRUE
INGESTION = notification integration from cloud storage 
COMMENT = string
AS COPY INTO table_name
FROM stage_name</code></pre>
<p>Snowpipe can be triggered by <strong>cloud messages</strong> or <strong>REST API</strong>. Cloud messages are for external stages only with that cloud provider. REST API can be internal or external stage.</p>
<ul>
<li><strong>Cost</strong> is based on “per second per core” of the serverless process.</li>
<li><strong>Time</strong> depends on size and number of files.</li>
<li><strong>Ideal file size</strong> is between 100-250 MB.</li>
</ul>
<p>Snowflake stores metadata about the file loading. Old history is retained for 14 days. The location of the pipe is stored in a schema in the database.</p>
<p>The <strong>schedule can be paused or resumed</strong> by altering the pipe.</p>
<pre><code>ALTER PIPE pipe_name
SET PIPE_EXECUTION_PAUSED = True</code></pre>
</section>
<section id="copy-options" class="level2">
<h2 class="anchored" data-anchor-id="copy-options">3.7. Copy Options</h2>
<p>These are arguments we can pass to <code>COPY INTO</code> for loading and unloading. Some options only apply to loading and do not apply to unloading.</p>
<p>They are <em>properties of the stage object</em>, so if the arguments are not passed Snowflake will fall back to these default values.</p>
<section id="on_error" class="level3">
<h3 class="anchored" data-anchor-id="on_error">3.7.1. ON_ERROR</h3>
<ul>
<li><strong>Data Type</strong>: String</li>
<li><strong>Description</strong>: Only for data loading. How to handle errors in files.</li>
<li><strong>Possible Values</strong>:
<ul>
<li><code>CONTINUE</code> - Continue loading file if errors are found.</li>
<li><code>SKIP_FILE</code> - Skip loading this file if errors are found. This is the <strong>default for Snowpipe</strong>.</li>
<li><code>SKIP_FILE_&lt;num&gt;</code> - Skip if <code>&gt;= num</code> errors are found (absolute).</li>
<li><code>SKIP_FILE_&lt;pct&gt;%</code> - Skip if <code>&gt;= pct</code> errors are found (percentage).</li>
<li><code>ABORT_STATEMENT</code> - Abort loading if an error is found. This is the <strong>default for bulk load</strong>.</li>
</ul></li>
</ul>
</section>
<section id="size_limit" class="level3">
<h3 class="anchored" data-anchor-id="size_limit">3.7.2. SIZE_LIMIT</h3>
<ul>
<li><strong>Data Type</strong>: Int</li>
<li><strong>Description</strong>: Maximum <em>cumulative size</em>, in bytes, to load. Once this amount of data has been loaded, skip any remaining files.</li>
<li><strong>Possible Values</strong>: Int bytes.</li>
</ul>
</section>
<section id="purge" class="level3">
<h3 class="anchored" data-anchor-id="purge">3.7.3. PURGE</h3>
<ul>
<li><strong>Data Type</strong>: Bool</li>
<li><strong>Description</strong>: Remove files from the stage after they have been loaded.</li>
<li><strong>Possible Values</strong>: <code>FALSE</code> (default) | <code>TRUE</code></li>
</ul>
</section>
<section id="match_by_column_name" class="level3">
<h3 class="anchored" data-anchor-id="match_by_column_name">3.7.4. MATCH_BY_COLUMN_NAME</h3>
<ul>
<li><strong>Data Type</strong>: String</li>
<li><strong>Description</strong>: Load semi structured data by matching field names.</li>
<li><strong>Possible Values</strong>: <code>NONE</code> (default) | <code>CASE_SENSITIVE</code> | <code>CASE_INSENSITIVE</code></li>
</ul>
</section>
<section id="enforce_length" class="level3">
<h3 class="anchored" data-anchor-id="enforce_length">3.7.5. ENFORCE_LENGTH</h3>
<ul>
<li><strong>Data Type</strong>: Bool</li>
<li><strong>Description</strong>: If we have a varchar(10) field, how should we handle data that is too long?</li>
<li><strong>Possible Values</strong>:
<ul>
<li><code>TRUE</code> (default) - Raise an error</li>
<li><code>FALSE</code> - Automatically truncate strings</li>
</ul></li>
</ul>
<p><code>TRUNCATECOLUMNS</code> is an alternative arg that does the <strong>opposite</strong>.</p>
</section>
<section id="force" class="level3">
<h3 class="anchored" data-anchor-id="force">3.7.6. FORCE</h3>
<ul>
<li><strong>Data Type</strong>: Bool</li>
<li><strong>Description</strong>: If we have loaded this file before, should we load it again?</li>
<li><strong>Possible Values</strong>: <code>False</code> (default) | <code>TRUE</code></li>
</ul>
</section>
<section id="load_uncertain_files" class="level3">
<h3 class="anchored" data-anchor-id="load_uncertain_files">3.7.7. LOAD_UNCERTAIN_FILES</h3>
<ul>
<li><strong>Data Type</strong>: Bool</li>
<li><strong>Description</strong>: Should we load files if the load status is unknown?</li>
<li><strong>Possible Values</strong>: <code>False</code> (default) | <code>TRUE</code></li>
</ul>
</section>
<section id="validation_mode" class="level3">
<h3 class="anchored" data-anchor-id="validation_mode">3.7.8. VALIDATION_MODE</h3>
<ul>
<li><strong>Data Type</strong>: String</li>
<li><strong>Description</strong>: Validate the data instead of actually loading it.</li>
<li><strong>Possible Values</strong>:
<ul>
<li><code>RETURN_N_ROWS</code> - Validate the first N rows and returns them (like a SELECT statement would). If there is one or more errors in those rows, raise the first.</li>
<li><code>RETURN_ERRORS</code> - Return all errors in the file.</li>
</ul></li>
</ul>
</section>
</section>
<section id="validate" class="level2">
<h2 class="anchored" data-anchor-id="validate">3.8. VALIDATE</h2>
<p>The <code>VALIDATE</code> function validates the files loaded in a <strong>previous COPY INTO</strong>.</p>
<p>Returns a list of errors from that bulk load. This is a <em>table function</em>, which means it returns multiple rows.</p>
<pre><code>SELECT * 
FROM TABLE(VALIDATE(table_name, JOB_ID =&gt; ‘_last’))</code></pre>
<p>We can pass a query ID instead of _last to use a specific job run rather than the last run.</p>
</section>
<section id="unloading" class="level2">
<h2 class="anchored" data-anchor-id="unloading">3.9. Unloading</h2>
<p>The syntax for unloading data from a table into a stage is the same as loading, we just swap the source and target.</p>
<pre><code>COPY INTO stage_name FROM table_name</code></pre>
<p>We can unload specific rows or columns by using a <code>SELECT</code> statement:</p>
<pre><code>COPY INTO stage_name 
FROM (SELECT col1, col2 FROM table_name)</code></pre>
<p>We can pass a <code>FILE_FORMAT</code> object and <code>HEADER</code> args.</p>
<p>We can also specify the <strong>prefix</strong> or <strong>suffix</strong> for each file. By default the prefix is data_ and the suffix is _0, _1, etc.</p>
<pre><code>COPY INTO stage_name/myprefix</code></pre>
<p>This is the default behaviour to <strong>split the output into multiple files</strong> once <code>MAX_FILE_SIZE</code> is reached, setting an upper limit on the output. The <code>SINGLE</code> parameter can be passed to override this, to force the unloading task to keep the output to a single file without splitting.</p>
<p>If unloading to an <em>internal stage</em>, to get the data on your local machine use SnowSQL to run a GET command on the internal stage after unloading.</p>
<p>You can then use the <code>REMOVE</code> command to delete from the internal stage.</p>
</section>
</section>
<section id="data-transformation" class="level1">
<h1>4. Data Transformation</h1>
<section id="transformations-and-functions" class="level2">
<h2 class="anchored" data-anchor-id="transformations-and-functions">4.1. Transformations and Functions</h2>
<p>We can specify transformations in the <code>SELECT</code> statement of the <code>COPY</code> command.</p>
<p>This can simplify ETL pipelines when performing simple transformations such as: column reordering, casting data types, removing columns, truncating strings to a certain length. We can also use a subset of SQL functions inside the COPY command. Supports most standard SQL functions defined in SQL:1999.</p>
<p>Snowflake does <strong>not</strong> support more complex SQL inside the COPY command, such as FLATTEN, aggregations, GROUP BY, filtering with WHERE, JOINs.</p>
<p>Supported functions:</p>
<ul>
<li>Scalar functions. Return one value per row. E.g. <code>DAYNAME</code></li>
<li>Aggregate functions. Return one value per group / table. E.g. <code>MAX</code>.</li>
<li>Window functions. Aggregate functions that return one value per row. E.g. <code>SELECT ORDER_ID, SUBCATEGORY, MAX(amount) OVER (PARTITION BY SUBCATEGORY) FROM ORDERS;</code></li>
<li>Table functions. Return multiple rows per input row. E.g. <code>SELECT * FROM TABLE(VALIDATE(table_name, JOB_ID =&gt; ‘_last’))</code></li>
<li>System functions. Control and information functions. E.g. <code>SYSTEM$CANCEL_ALL_QUERIES</code></li>
<li>UDFs. User-defined functions.</li>
<li>External functions. Stored and executed outside of Snowflake.</li>
</ul>
</section>
<section id="estimation-functions" class="level2">
<h2 class="anchored" data-anchor-id="estimation-functions">4.2. Estimation Functions</h2>
<p>Exact calculations on large tables can be very compute-intensive or memory-intensive. Sometimes an estimate is good enough.</p>
<p>Snowflake has some algorithms implemented out of the box that can give useful estimates with fewer resources.</p>
<section id="number-of-distinct-values---hll" class="level3">
<h3 class="anchored" data-anchor-id="number-of-distinct-values---hll">4.2.1. Number of Distinct Values - <code>HLL()</code></h3>
<p>HyperLogLog algorithm.</p>
<p>Average error is ~1.6%.</p>
<p>Replace</p>
<pre><code>COUNT(DISTINCT (col1, col2, ...))</code></pre>
<p>with</p>
<pre><code>HLL(col1, col2, ...)</code></pre>
<p>or</p>
<pre><code>APPROX_COUNT_DISTINCT (col1, col2, ...)</code></pre>
</section>
<section id="frequent-values---approx_top_k" class="level3">
<h3 class="anchored" data-anchor-id="frequent-values---approx_top_k">4.2.2. Frequent Values - <code>APPROX_TOP_K()</code></h3>
<p>Estimate the most frequent values and their frequencies. Space-saving algorithm.</p>
<p>Use the following command. The <code>k</code> argument is optional and defaults to 1. The <code>counters</code> argument is optional and specifies the maximum number of distinct values to track. If using this, we should use <code>counters &gt;&gt; k</code>.</p>
<pre><code>APPROX_TOP_K (col1, k[optional], counters[optional])</code></pre>
</section>
<section id="percentile-values---approx_percentile" class="level3">
<h3 class="anchored" data-anchor-id="percentile-values---approx_percentile">4.2.3. Percentile Values - <code>APPROX_PERCENTILE()</code></h3>
<p>t-Digest algorithm.</p>
<pre><code>APPROX_PERCENTILE (col1, percentile)</code></pre>
</section>
<section id="similarity-of-two-or-more-data-sets---minhash-approximate_similarity" class="level3">
<h3 class="anchored" data-anchor-id="similarity-of-two-or-more-data-sets---minhash-approximate_similarity">4.2.4. Similarity of Two or More Data Sets - <code>MINHASH &amp; APPROXIMATE_SIMILARITY()</code></h3>
<p>The full calculation uses the Jaccard similarity cofficient. This returns a value between 0 and 1 indicating similarity. <span class="math display">\[
J(A, B) = |(A \cap B)| / |A \cup B|
\]</span></p>
<p>The approximation is a two-step process that uses the MinHash algorithm to hash each table, then the <code>APPROXIMATE_SIMILARITY</code> function to estimate <span class="math inline">\(J(A, B)\)</span>.</p>
<p>The argument <code>k</code> in MINHASH is the number of hash functions to use. Higher <code>k</code> is more accurate but slower. We can pass individual column names instead oif <code>*</code>.</p>
<pre><code>SELECT MINHASH(k, *) AS mh FROM table_name;</code></pre>
<p>The full approximation command is:</p>
<pre><code>SELECT APPROXIMATE_SIMILARITY(mh)
FROM (
    SELECT MINHASH(100, *) AS mh FROM mhtab1
    UNION ALL
    SELECT MINHASH(100, *) AS mh FROM mhtab2
);</code></pre>
</section>
</section>
<section id="user-defined-functions-udf" class="level2">
<h2 class="anchored" data-anchor-id="user-defined-functions-udf">4.3. User-Defined Functions (UDF)</h2>
<p>These are one of several ways of extending functionality with additional functions.</p>
<p>(The other approaches are stored procedures and external functions, which are covered in the next sections.)</p>
<p>UDFs support the following languages: <strong>SQL, Python, Java, JavaScript</strong></p>
<section id="defining-a-udf" class="level3">
<h3 class="anchored" data-anchor-id="defining-a-udf">4.3.1. Defining a UDF</h3>
<p>We can define a SQL UDF using <code>create function</code>.</p>
<pre><code>CREATE FUNCTION add_two(n int)
returns int
    AS
    $$
    n+2
    $$;</code></pre>
<p>Defining a UDF in Python is similar, we just need to specify the <code>language</code> and some other options.</p>
<pre><code>CREATE FUNCTION add_two(n int)
returns int
language Python
runtime_version =‘3.8’
handler = ‘addtwo’ 
    AS
    $$
    def add_two(n):
        return n+2
    $$;</code></pre>
</section>
<section id="using-a-udf" class="level3">
<h3 class="anchored" data-anchor-id="using-a-udf">4.3.2. Using a UDF</h3>
<p>We just call the UDF from SnowSQL, for example</p>
<pre><code>SELECT add_two(3);</code></pre>
</section>
<section id="function-properties" class="level3">
<h3 class="anchored" data-anchor-id="function-properties">4.3.3. Function Properties</h3>
<p>Functions can be:</p>
<ul>
<li><strong>Scalar functions</strong>: Return <em>one output row</em> per input row.</li>
<li><strong>Tabular functions</strong>: Return a <em>table</em> per input row.</li>
</ul>
<p>Functions are <em>schema-level</em> objects in Snowflake. We can see them under <code>Schema.Public.Functions</code>.</p>
<p>We can manage access and grant privileges to functions, just like any other Snowflake object.</p>
</section>
</section>
<section id="stored-procedures" class="level2">
<h2 class="anchored" data-anchor-id="stored-procedures">4.4. Stored Procedures</h2>
<p>Another way of extending functionality, like UDFs.</p>
<p><strong>UDF vs stored procedures</strong>: - UDF is typically used to <strong>calculate</strong> a value. It <em>needs to return a value</em>. No need to have access to the objects referenced in the function. - A stored procedure is typically used for <strong>database operations</strong> like INSERT, UPDATE, etc. It <em>doesn’t need to return a value</em>.</p>
<p>Can rely on the <em>caller’s or the owner’s access rights</em>.</p>
<p>Procedures are <em>securable objects</em> like functions, so we can grant access to them.</p>
<p>Supported languages:</p>
<ul>
<li>Snowflake scripting - Snowflake SQL + procedural logic</li>
<li>JavaScript</li>
<li>Snowpark API - Python, Scala, Java</li>
</ul>
<section id="creating-a-stored-procedure" class="level3">
<h3 class="anchored" data-anchor-id="creating-a-stored-procedure">4.4.1. Creating a Stored Procedure</h3>
<pre><code>CREATE PROCEDURE find_min(n1 int, n2 int)
returns int
language sql
    AS
    BEGIN
    IF (n1 &lt; n2)
        THEN RETURN n1;
        ELSE RETURN n2;
    END IF;
    END;</code></pre>
<p>Stored procedures can be run with the caller’s rights or the user’s rights. This is defined with the stored procedure. By default, they run as <em>owner</em> but we can override this with:</p>
<pre><code>execute as caller</code></pre>
</section>
<section id="calling-a-stored-procedure" class="level3">
<h3 class="anchored" data-anchor-id="calling-a-stored-procedure">4.4.2. Calling a Stored Procedure</h3>
<pre><code>CALL find_min(5,7)</code></pre>
<p>We can reference <strong>dynamic values</strong>, such as variables in the user’s sessions, in stored procedures. - If <em>argument</em> is referenced in SQL, use <code>:argname</code> - If an object such as a table is referenced, use <code>IDENTIFIER(:table_name)</code></p>
</section>
</section>
<section id="external-functions" class="level2">
<h2 class="anchored" data-anchor-id="external-functions">4.5. External Functions</h2>
<p>These are user-defined functions that are <strong>stored and executed outside of Snowflake</strong>. Remotely executed code is referred to as a “remote service”.</p>
<p>This means it can reference third-party libraries, services and data.</p>
<p>Examples of external API integrations are: AWS lambda function, Microsoft Azure function, HTTPS server.</p>
<pre><code>CREATE EXTERNAL FUNCTION my_func(string_col VAR_CHAR)
returns variant
api_integration = azure_external_api_integration
AS 'https://url/goes/here'</code></pre>
<p>Security-related information is stored in an API integration. This is a schema-level object, so it is securable and can be granted access to .</p>
<p>Advantages:</p>
<ul>
<li>Can use other languages</li>
<li>Access 3rd-party libraries</li>
<li>Can be called from elsewhere, not just Snowflake, so we can have one central repository.</li>
</ul>
<p>Limitations:</p>
<ul>
<li>Only scalar functions</li>
<li>Slower performance - overhead of external functions</li>
<li>Not shareable</li>
</ul>
</section>
<section id="secure-udfs-and-procedures" class="level2">
<h2 class="anchored" data-anchor-id="secure-udfs-and-procedures">4.6. Secure UDFs and Procedures</h2>
<p>We may want to hide certain information such as the function definition, or prevent users from seeing underlying data.</p>
<p>We just use the <code>SECURE</code> keyword.</p>
<pre><code>CREATE SECURE FUNCTION ...</code></pre>
<p>Disadvantages: - Lower query performance. The optimiser exposes some security risks, so this is locked down which restricts the optimisation options available therefore impacting performance lock this down the optimisations are restricted.</p>
<p>We should use it for sensitive data, otherwise the performance trade-off isn’t worthwhile.</p>
</section>
<section id="sequences" class="level2">
<h2 class="anchored" data-anchor-id="sequences">4.7. Sequences</h2>
<p>Sequences are typically used for <code>DEFAULT</code> values in <code>CREATE TABLE</code> statements. Sequences are <strong>not</strong> guaranteed to be <em>gap-free</em>.</p>
<p>Sequences are <em>securable objects</em> that we can grant privileges to.</p>
<pre><code>CREATE SEQUENCE my_seq
START = 1
INCREMENT = 1</code></pre>
<p>Both <code>START</code> and <code>INCREMENT</code> default to 1.</p>
<p>We invoke a sequence with <code>my_seq.nextval</code>. For example:</p>
<pre><code>CREATE TABLE my_table(
    id int DEFAULT my_seq.nextval,
    first_name varchar
    last_name varchar
);

INSERT INTO my_table(first_name, last_name)
VALUES ('John', 'Cena'), ('Dwayne', 'Johnson'),('Steve','Austin');</code></pre>
</section>
<section id="semi-structured-data" class="level2">
<h2 class="anchored" data-anchor-id="semi-structured-data">4.8. Semi-structured Data</h2>
<section id="what-is-semi-structured-data" class="level3">
<h3 class="anchored" data-anchor-id="what-is-semi-structured-data">4.8.1. What is Semi-structured Data?</h3>
<p>Semi-structured data has <strong>no fixed schema</strong>, but contains <em>tags/labels</em> and has a <em>nested structure</em>.</p>
<p>This is in contrast to structured data like a table. Or unstructured data which is a free-for-all.</p>
<p><strong>Supported formats: json, xml, parquet, orc, avro</strong></p>
<p>Snowflake deals with unstructured data using 3 different data types:</p>
<ul>
<li>Object - think of this in the JavaScript sense, i.e.&nbsp;key:value pairs</li>
<li>Array</li>
<li>Variant - this can store data of any other type, including arrays and objects. Native support for semi-structured data.</li>
</ul>
<p>We typically let Snowflake convert semi-structured data into a <em>hierarchy of arrays and objects</em> within a <strong>variant object</strong>.</p>
<p>Nulls and non-native strings like dates are <em>cast to strings</em> within a variant.</p>
<p>Variant can store up to <strong>16 MB per row</strong>. If the data exceeds this, we will need to restructure or split the input.</p>
<p>The standard “ELT” approach (Extract, Load, Transform) for semi-structured data is to load the data as is, then transform it later once we’ve eyeballed it. This is a tweak of the classic ETL approach.</p>
<p>We often need to <code>FLATTEN</code> the data.</p>
</section>
<section id="querying-semi-structured-data" class="level3">
<h3 class="anchored" data-anchor-id="querying-semi-structured-data">4.8.2. Querying Semi-structured Data</h3>
<p>To access elements of a <code>VARIANT</code> column, we use a <code>:</code>.</p>
<p>For example, if the <code>raw_column</code> column has a top-level key of <code>heading1</code>, we can query:</p>
<pre><code>SELECT raw_column:heading1
FROM table_with_variant</code></pre>
<p>We can also refer to columns by their position. So to access the first column:</p>
<pre><code>SELECT $1:heading1
FROM table_with_variant</code></pre>
<p>To access <strong>nested</strong> elements, use <code>.</code>:</p>
<pre><code>SELECT raw_column:heading1.subheading2
FROM table_with_variant</code></pre>
<p>If there is an array, we can <strong>access elements of the array</strong> with <code>[index]</code>. Arrays are 0-indexed, so to access the first element:</p>
<pre><code>SELECT raw_column:heading1.subheading2.array_field[0]
FROM table_with_variant</code></pre>
<p>We may also need to <strong>cast the types</strong> of the element with <code>::</code></p>
<pre><code>SELECT raw_column:heading1.subheading2.array_field[0]::VARCHAR
FROM table_with_variant</code></pre>
</section>
<section id="flatten-hierarchical-data" class="level3">
<h3 class="anchored" data-anchor-id="flatten-hierarchical-data">4.8.3. Flatten Hierarchical Data</h3>
</section>
<section id="insert-json-data" class="level3">
<h3 class="anchored" data-anchor-id="insert-json-data">4.8.4. Insert JSON Data</h3>
</section>
</section>
<section id="unstructured-data" class="level2">
<h2 class="anchored" data-anchor-id="unstructured-data">4.9. Unstructured Data</h2>
</section>
<section id="data-sampling" class="level2">
<h2 class="anchored" data-anchor-id="data-sampling">4.10. Data Sampling</h2>
</section>
<section id="tasks" class="level2">
<h2 class="anchored" data-anchor-id="tasks">4.11. Tasks</h2>
</section>
<section id="streams" class="level2">
<h2 class="anchored" data-anchor-id="streams">4.12. Streams</h2>
</section>
</section>
<section id="snowflake-tools-and-connectors" class="level1">
<h1>5. Snowflake Tools and Connectors</h1>
<section id="connectors-and-drivers" class="level2">
<h2 class="anchored" data-anchor-id="connectors-and-drivers">5.1. Connectors and Drivers</h2>
<p>Snowflake provides two interfaces:</p>
<ul>
<li>Snowsight web UI</li>
<li>SnowSQL command line tool</li>
</ul>
<p>Drivers:</p>
<ul>
<li>Go</li>
<li>JDBC</li>
<li>.NET</li>
<li>node.js</li>
<li>ODBC</li>
<li>PHP</li>
<li>Python</li>
</ul>
<p>Connectors:</p>
<ul>
<li>Python</li>
<li>Kafka</li>
<li>Spark</li>
</ul>
<p><strong>Partner Connect</strong> allows us to create a trial account for third-party add-ons to Snowflake and integrate them with Snowflake.</p>
<p>They span many different categories and tools - BI, CI/CD, etc.</p>
</section>
<section id="snowflake-scripting" class="level2">
<h2 class="anchored" data-anchor-id="snowflake-scripting">5.2. Snowflake Scripting</h2>
<p>Mostly used in stored procedures but can also be used for writing procedural code outside of this.</p>
<p>We can use@ if, case, for, repeat, while, loop</p>
<p>This is written in a <strong>“block”</strong>:</p>
<pre><code>DECLARE
BEGIN 
EXCEPTION
END</code></pre>
<p><code>DECLARE</code> and <code>EXCEPTION</code> are optional.</p>
<p>This is available in Snowsight. The classic UI or SnowSQL requires <code>$$</code> around the block.</p>
<p><strong>Objects</strong> created in the block are <em>available outside</em> of it too. <strong>Variables</strong> created in the block can only be used <em>inside</em> it.</p>
<p>This is a similar syntax, but different from <strong>transactions</strong> which use <code>BEGIN</code> and <code>END</code>.</p>
</section>
<section id="snowpark" class="level2">
<h2 class="anchored" data-anchor-id="snowpark">5.3. Snowpark</h2>
<p>Snowpark API provides support for three programming languages: <em>Python, Java, Scala</em>.</p>
<p>Python code <strong>converts to SQL</strong> which then queries Snowflake with serverless Snowflake engine.</p>
<p>This means there is <strong>no need to move data outside of Snowflake</strong>.</p>
<p>Benefits:</p>
<ul>
<li>Lazy evaluation</li>
<li>Pushdown - query is executed in Snowflake rather than unloading all data outside of Snowflake and then transforming</li>
<li>UDFs can be defined inline</li>
</ul>
</section>
</section>
<section id="continuous-data-protection" class="level1">
<h1>6. Continuous Data Protection</h1>
<p>This section covers the data protection lifecycle. From most accessible to least accessible:</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR

  A(Current data storage) --&gt; B(Time travel) --&gt; C(Fail safe)
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<section id="time-travel" class="level2">
<h2 class="anchored" data-anchor-id="time-travel">6.1. Time Travel</h2>
<p>What if someone drops a database or table accidentally? We need a <strong>backup to recover data</strong>. <em>Time travel</em> enables accessing historical data.</p>
<p>We can:</p>
<ul>
<li>Query data that has been deleted or updated</li>
<li>Restore dropped tables, schemas and databases</li>
<li>Create clones of tables, schemas and databases from a previous date</li>
</ul>
<p>We can use a SQL query to access time travel data within a retention period. The <code>AT</code> keyword allows us to time travel.</p>
<pre><code>SELECT * 
FROM table
AT (TIMESTAMP &gt;= timestamp)</code></pre>
<p>Or use <code>OFFSET</code> in seconds. So for 10 minutes:</p>
<pre><code>SELECT * 
FROM table 
AT (OFFSET &gt;= 10*60)</code></pre>
<p>Alternatively we can use <code>BEFORE</code>, where <em>query_ID</em> is the ID where we messed things up:</p>
<pre><code>SELECT * 
FROM table 
BEFORE (STATEMENT&gt;= query_id)</code></pre>
<p>Go to <code>Activity -&gt; Query History</code> in the Snowsight UI to get the correct query ID.</p>
<p>It is best practice to <strong>recover to an intermediate backup table first</strong> to confirm the recovered version is correct. E.g.</p>
<pre><code>CREATE OR REPLACE TABLE table_name_backup
FROM table
AT (TIMESTAMP &gt;= timestamp)</code></pre>
</section>
<section id="undrop" class="level2">
<h2 class="anchored" data-anchor-id="undrop">6.2. UNDROP</h2>
<p>We can use the <code>UNDROP</code> keyword to recover objects.</p>
<pre><code>UNDROP TABLE table_name</code></pre>
<p>Same for <code>SCHEMA</code> or <code>DATABASE</code>.</p>
<p>Considerations.</p>
<ul>
<li>UNDROP fails if an object with the <strong>same name already exists</strong>.</li>
<li>We need <strong>ownership privileges</strong> to UNDROP an object.</li>
</ul>
<p>When working with time zones, it can be helpful to change the time zone to match your local time. This only affects the current session, not the whole account.</p>
<pre><code>ALTER SESSION SET TIMEZONE = 'UTC'</code></pre>
</section>
<section id="retention-period" class="level2">
<h2 class="anchored" data-anchor-id="retention-period">6.3. Retention Period</h2>
<p>Retention period is the number of days for which historical data is preserved. This determines how far back we can time travel.</p>
<p>The retention period is configurable for table, schema, database or account.</p>
<p>The default <code>DATA_RETENTION_TIME_IN_DAYS=1</code>. If we want to disable time travel, we can set this to 0.</p>
<p>We can set this when we create a table, or alter it for an existing table:</p>
<pre><code>ALTER TABLE table_name (
    SET DATA_RETENTION_TIME_IN_DAYS=2
)</code></pre>
<p>We can set a <em>minimum value</em> at the account level:</p>
<pre><code>ALTER ACCOUNT SET 
MIN_ DATA_RETENTION_TIME_IN_DAYS=3</code></pre>
<p>The <strong>account type determines the maximum retention period</strong>:</p>
<ul>
<li>Standard: up to 1 day</li>
<li>Enterprise or higher: up to 90 days</li>
</ul>
</section>
<section id="fail-safe" class="level2">
<h2 class="anchored" data-anchor-id="fail-safe">6.4. Fail Safe</h2>
<p>This is for <strong>disaster recovery</strong> beyond time travel. This is not configurable and set to 7 days beyond time travel period for permanent tables.</p>
<p>We as <strong>users cannot access this data</strong>. We have to <strong>contact Snowflake support</strong> to restore it for us.</p>
</section>
<section id="storage-costs" class="level2">
<h2 class="anchored" data-anchor-id="storage-costs">6.5. Storage Costs</h2>
<p>Time travel and fail safe contribute to storage costs. You only pay for what is modified.</p>
<p>You can see the breakdown in the <code>Admin -&gt; Usage</code> tab of the UI.</p>
<p>You can see how much storage is being used with:</p>
<pre><code>SELECT * 
FROM SNOWFLAKE_ACCOUNT_USAGE.TABLE_STORAGE_METRICS</code></pre>
</section>
<section id="table-types" class="level2">
<h2 class="anchored" data-anchor-id="table-types">6.6. Table Types</h2>
<p>The following table summarises the differences between the 3 types of table.</p>
<table class="table">
<colgroup>
<col style="width: 6%">
<col style="width: 12%">
<col style="width: 16%">
<col style="width: 5%">
<col style="width: 58%">
</colgroup>
<thead>
<tr class="header">
<th>Table Type</th>
<th>Command</th>
<th>Time Travel Retention Period</th>
<th>Fail Safe</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Permanent</td>
<td>CREATE TABLE</td>
<td>0-90</td>
<td>Y</td>
<td>Standard tables. Persist data until dropped.</td>
</tr>
<tr class="even">
<td>Transient</td>
<td>CREATE TRANSIENT TABLE</td>
<td>0-1</td>
<td>N</td>
<td>For large tables that do not need to be protected. Persist data until dropped.</td>
</tr>
<tr class="odd">
<td>Temporary</td>
<td>CREATE TEMPORARY TABLE</td>
<td>0-1</td>
<td>N</td>
<td>Non-permanent data just for this session.Only in session - data is deleted after the worksheet is closed.</td>
</tr>
</tbody>
</table>
<p>These types are <strong>also available for other Snowflake objects</strong>: tables, stages, schema, database.</p>
<p>If a <strong>database</strong> is transient, <strong>so are all of its objects</strong>.</p>
<p><strong>Temporary table names</strong> do not clash with permanent or transient tables. The temporary table name takes precedence in the session and hides the others with the same name.</p>
<p>It is <strong>not possible to change the type</strong> of an existing object.</p>
</section>
</section>
<section id="zero-copy-cloning-and-sharing" class="level1">
<h1>7. Zero-Copy Cloning and Sharing</h1>
<section id="zero-copy-cloning" class="level2">
<h2 class="anchored" data-anchor-id="zero-copy-cloning">7.1. Zero Copy Cloning</h2>
<p>Zero copy cloning makes it easy to <strong>copy an existing object</strong> in a <strong>storage-efficient</strong> way.</p>
<p>For example:</p>
<pre><code>CREATE TABLE table_name
CLONE table_source</code></pre>
<p>We can clone almost* any object: database, schema, table, stream, file format, sequence, task, stage, pipe.</p>
<blockquote class="blockquote">
<p>*the exceptions are:</p>
<ul>
<li><strong>pipes</strong> which can only be cloned if referencing an <strong>external stage</strong></li>
<li><strong>stages</strong> which cannot be cloned for <strong>named internal stages</strong></li>
</ul>
</blockquote>
<p>When we clone a database or schema, <strong>all of its child objects are also cloned</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="zero_copy_clone.png" class="img-fluid figure-img"></p>
<figcaption>Zero Copy Clone</figcaption>
</figure>
</div>
<p>It’s called a “zero copy clone” because it <em>does not actually copy the data at clone time</em>. It is a <strong>metadata operation</strong> occurring in the cloud service layer.</p>
<p>The “copy” is a snapshot of the “original”. <strong>Both reference the same underlying micro-partitions</strong> in block storage. Think of it like pass-by-reference rather than pass-by-value.</p>
<p>When modifying the “copy” <strong>Snowflake only stores the deltas</strong>, not the entire database again. The “original” and “copy” can be <strong>modified independently</strong> without affecting the other.</p>
<p>A typical use case is to create backups for dev work.</p>
<p>We can clone from a time travel version of a table:</p>
<pre><code>CREATE TABLE table_new
CLONE table_source
BEFORE (TIMESTAMP &gt;= ‘2025-01-31 08:00:00’)</code></pre>
</section>
<section id="privileges" class="level2">
<h2 class="anchored" data-anchor-id="privileges">7.2. Privileges</h2>
<p>The <strong>privileges of the CHILD objects are inherited from the clone source</strong>, but the <strong>privileges of the cloned object itself are NOT inherited</strong> (i.e.&nbsp;the cloned database or schema). They need to be specified separately by the administrator.</p>
<p>The privileges required to clone an object depends on the object:</p>
<ul>
<li>Table: SELECT privileges</li>
<li>Pipe, stream, task: OWNER privileges</li>
<li>All other objects: USAGE privileges</li>
</ul>
<p>When we clone a table, its <strong>load history metadata is NOT copied</strong>. This means loaded data can be loaded again without causing a metadata clash.</p>
</section>
</section>
<section id="account-and-security" class="level1">
<h1>8. Account and Security</h1>
</section>
<section id="performance-concepts" class="level1">
<h1>9. Performance Concepts</h1>
</section>
<section id="references" class="level1">
<h1>References</h1>
<ul>
<li><a href="https://www.udemy.com/course/snowflake-certification-snowpro-core-exam-prep">“Snowflake Certification: SnowPro Core COF-C02 Exam Prep” Udemy course</a></li>
<li><a href="https://docs.snowflake.com/en/user-guide/intro-editions">Snowflake feature matrix</a></li>
</ul>


</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>