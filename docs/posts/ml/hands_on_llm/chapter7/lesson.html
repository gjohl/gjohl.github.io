<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Gurpreet Johl">
<meta name="dcterms.date" content="2025-03-05">
<meta name="description" content="Part 7: Chains and Agents">

<title>Gurpreet Johl - Hands-On LLMs: Advanced Text Generation Techniques and Tools</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="../../../../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../../../../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../../../../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../../index.html">
    <span class="navbar-title">Gurpreet Johl</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../projects.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../blog.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/gjohl"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/gurpreetjohl"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/gurpreetjohl"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Hands-On LLMs: Advanced Text Generation Techniques and Tools</h1>
                  <div>
        <div class="description">
          Part 7: Chains and Agents
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">AI</div>
                <div class="quarto-category">Engineering</div>
                <div class="quarto-category">GenerativeAI</div>
                <div class="quarto-category">LLM</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Gurpreet Johl </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 5, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#advanced-text-generation-techniques-and-tools" id="toc-advanced-text-generation-techniques-and-tools" class="nav-link active" data-scroll-target="#advanced-text-generation-techniques-and-tools">Advanced Text Generation Techniques and Tools</a></li>
  <li><a href="#model-io" id="toc-model-io" class="nav-link" data-scroll-target="#model-io">1. Model I/O</a>
  <ul class="collapse">
  <li><a href="#quantization" id="toc-quantization" class="nav-link" data-scroll-target="#quantization">1.1. Quantization</a></li>
  </ul></li>
  <li><a href="#chains" id="toc-chains" class="nav-link" data-scroll-target="#chains">2. Chains</a>
  <ul class="collapse">
  <li><a href="#basic-prompt-chain" id="toc-basic-prompt-chain" class="nav-link" data-scroll-target="#basic-prompt-chain">2.1. Basic Prompt Chain</a></li>
  <li><a href="#multiple-prompt-chain" id="toc-multiple-prompt-chain" class="nav-link" data-scroll-target="#multiple-prompt-chain">2.2. Multiple Prompt Chain</a></li>
  </ul></li>
  <li><a href="#memory" id="toc-memory" class="nav-link" data-scroll-target="#memory">3. Memory</a>
  <ul class="collapse">
  <li><a href="#conversation-buffer" id="toc-conversation-buffer" class="nav-link" data-scroll-target="#conversation-buffer">3.1. Conversation Buffer</a>
  <ul class="collapse">
  <li><a href="#simple-conversation-buffer" id="toc-simple-conversation-buffer" class="nav-link" data-scroll-target="#simple-conversation-buffer">3.1.1. Simple Conversation Buffer</a></li>
  <li><a href="#windowed-conversation-buffer" id="toc-windowed-conversation-buffer" class="nav-link" data-scroll-target="#windowed-conversation-buffer">3.1.2 Windowed Conversation Buffer</a></li>
  </ul></li>
  <li><a href="#conversation-summary" id="toc-conversation-summary" class="nav-link" data-scroll-target="#conversation-summary">3.2. Conversation Summary</a></li>
  <li><a href="#comparison-of-memory-approaches" id="toc-comparison-of-memory-approaches" class="nav-link" data-scroll-target="#comparison-of-memory-approaches">3.3. Comparison of Memory Approaches</a></li>
  </ul></li>
  <li><a href="#agents" id="toc-agents" class="nav-link" data-scroll-target="#agents">4. Agents</a>
  <ul class="collapse">
  <li><a href="#react" id="toc-react" class="nav-link" data-scroll-target="#react">4.1. ReAct</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="advanced-text-generation-techniques-and-tools" class="level1">
<h1>Advanced Text Generation Techniques and Tools</h1>
<p>Going beyond prompt engineering, there are several areas where we can improve the quality of generated text:</p>
<ul>
<li>Model I/O</li>
<li>Memory</li>
<li>Agents</li>
<li>Chains</li>
</ul>
<p><a href="https://www.langchain.com/">Langchain</a> is a framework that provides useful abstractions for these kinds of things and helps connect them together. We will use LangChain here, but alternatives include LlamaIndex, DSPy and Haystack.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="lesson_files/figure-html/cell-1-1-image.png" class="img-fluid figure-img"></p>
<figcaption>LangChain example</figcaption>
</figure>
</div>
</section>
<section id="model-io" class="level1">
<h1>1. Model I/O</h1>
<section id="quantization" class="level2">
<h2 class="anchored" data-anchor-id="quantization">1.1. Quantization</h2>
<p>We can load <strong>quantized models</strong> using the <a href="https://github.com/ggml-org/ggml/blob/master/docs/gguf.md">GGUF file format</a> which is a binary format optimised for fast loading of pytorch models.</p>
<p>The benefit of a quantized model is a smaller size in memory while retaining <em>most</em> of the original information. For example, if the model was trained using 32-bit floats for parameters, we can use 16-bit floats instead. This reduces the memory requirement but also reduces the precision. Often this trade-off is worthwhile.</p>
<p><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization">This page</a> goes into detail on the mechanics of quantisation.</p>
<p>The “best” model is constantly changing, so we can refer to the <a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/">Open LLM leaderboard</a>.</p>
<p>We can <a href="https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/blob/main/Phi-3-mini-4k-instruct-fp16.gguf">download</a> a 16-bit quantized version of the Phi-3 mini model from HuggingFace.</p>
<div id="cell-3" class="cell" data-vscode="{&quot;languageId&quot;:&quot;shellscript&quot;}">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>wget https:<span class="op">//</span>huggingface.co<span class="op">/</span>microsoft<span class="op">/</span>Phi<span class="op">-</span><span class="dv">3</span><span class="op">-</span>mini<span class="op">-</span><span class="dv">4</span><span class="er">k</span><span class="op">-</span>instructgguf<span class="op">/</span>resolve<span class="op">/</span>main<span class="op">/</span>Phi<span class="op">-</span><span class="dv">3</span><span class="op">-</span>mini<span class="op">-</span><span class="dv">4</span><span class="er">k</span><span class="op">-</span>instruct<span class="op">-</span>fp16.gguf</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Then we can use LangChain to load the GGUF file.</p>
<p>Note: an alternative is to use the <a href="https://huggingface.co/blog/langchain">langchain_huggingface library</a>.</p>
<div id="cell-5" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># This cell *should* work, but due to some funkiness with incompatible langchain vs llama versions (I think)</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co">#  it's easier to just create a custom LangChain wrapper in the following cell.</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain <span class="im">import</span> LlamaCpp</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Make sure model_path points at the file location of the GGUF file</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>MODEL_DIR <span class="op">=</span> <span class="st">"/Users/gurpreetjohl/workspace/python/ml-practice/ml-practice/models/"</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>MODEL_NAME <span class="op">=</span> <span class="st">"Phi-3-mini-4k-instruct-fp16.gguf"</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>model_path <span class="op">=</span> MODEL_DIR<span class="op">+</span>MODEL_NAME</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>llm <span class="op">=</span> LlamaCpp(</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    model_path<span class="op">=</span>MODEL_DIR<span class="op">+</span>MODEL_NAME,</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    n_gpu_layers<span class="op">=-</span><span class="dv">1</span>,</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    max_tokens<span class="op">=</span><span class="dv">500</span>,</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    n_ctx<span class="op">=</span><span class="dv">2048</span>,</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    seed<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="va">False</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/langchain_community/llms/__init__.py:312: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.

For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`
with: `from pydantic import BaseModel`
or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet.     from pydantic.v1 import BaseModel

  from langchain_community.llms.llamacpp import LlamaCpp</code></pre>
</div>
<div class="cell-output cell-output-error">
<pre><code>ValidationError: 1 validation error for LlamaCpp
client
  Field required [type=missing, input_value={'model_path': '/Users/gu...': 42, 'verbose': False}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing</code></pre>
</div>
</div>
<div id="cell-6" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Any, Dict, List, Optional</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain_core.language_models <span class="im">import</span> LLM</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> llama_cpp <span class="im">import</span> Llama</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CustomLlamaLLM(LLM):</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    model_path: <span class="bu">str</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    n_gpu_layers: <span class="bu">int</span> <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    max_tokens: <span class="bu">int</span> <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    n_ctx: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2048</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    seed: Optional[<span class="bu">int</span>] <span class="op">=</span> <span class="dv">42</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    verbose: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    client: Any <span class="op">=</span> <span class="va">None</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, <span class="op">**</span>kwargs):</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.client <span class="op">=</span> Llama(</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>            model_path<span class="op">=</span><span class="va">self</span>.model_path,</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>            n_gpu_layers<span class="op">=</span><span class="va">self</span>.n_gpu_layers,</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>            max_tokens<span class="op">=</span><span class="va">self</span>.max_tokens,</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>            n_ctx<span class="op">=</span><span class="va">self</span>.n_ctx,</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>            seed<span class="op">=</span><span class="va">self</span>.seed,</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>            verbose<span class="op">=</span><span class="va">self</span>.verbose</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _llm_type(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="st">"CustomLlama"</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _call(<span class="va">self</span>, prompt: <span class="bu">str</span>, stop: Optional[List[<span class="bu">str</span>]] <span class="op">=</span> <span class="va">None</span>) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>        response <span class="op">=</span> <span class="va">self</span>.client(prompt, stop<span class="op">=</span>stop)</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> response[<span class="st">"choices"</span>][<span class="dv">0</span>][<span class="st">"text"</span>]</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Instantiate the custom LLM class</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>llm <span class="op">=</span> CustomLlamaLLM(</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>    model_path<span class="op">=</span>model_path,</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>    n_gpu_layers<span class="op">=-</span><span class="dv">1</span>,</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>    max_tokens<span class="op">=</span><span class="dv">500</span>,</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>    n_ctx<span class="op">=</span><span class="dv">2048</span>,</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>    seed<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="va">True</span></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>llama_model_loader: loaded meta data with 23 key-value pairs and 195 tensors from /Users/gurpreetjohl/workspace/python/ml-practice/ml-practice/models/Phi-3-mini-4k-instruct-fp16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.name str              = Phi3
llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096
llama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv   5:                           phi3.block_count u32              = 32
llama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = ["&lt;unk&gt;", "&lt;s&gt;", "&lt;/s&gt;", "&lt;0x00&gt;", "&lt;...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  130 tensors
llm_load_vocab: special tokens cache size = 323
llm_load_vocab: token to piece cache size = 0.1687 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = phi3
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32064
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 3072
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 96
llm_load_print_meta: n_embd_head_k    = 96
llm_load_print_meta: n_embd_head_v    = 96
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3072
llm_load_print_meta: n_embd_v_gqa     = 3072
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 3.82 B
llm_load_print_meta: model size       = 7.12 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = Phi3
llm_load_print_meta: BOS token        = 1 '&lt;s&gt;'
llm_load_print_meta: EOS token        = 32000 '&lt;|endoftext|&gt;'
llm_load_print_meta: UNK token        = 0 '&lt;unk&gt;'
llm_load_print_meta: PAD token        = 32000 '&lt;|endoftext|&gt;'
llm_load_print_meta: LF token         = 13 '&lt;0x0A&gt;'
llm_load_print_meta: EOT token        = 32007 '&lt;|end|&gt;'
llm_load_tensors: ggml ctx size =    0.22 MiB
ggml_backend_metal_log_allocated_size: allocated buffer, size =  7100.64 MiB, (23176.83 / 10922.67)ggml_backend_metal_log_allocated_size: warning: current allocated size is greater than the recommended max working set size
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   187.88 MiB
llm_load_tensors:      Metal buffer size =  7100.64 MiB
....................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M2
ggml_metal_init: picking default device: Apple M2
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M2
ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction support   = true
ggml_metal_init: simdgroup matrix mul. support = true
ggml_metal_init: hasUnifiedMemory              = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
llama_kv_cache_init:      Metal KV buffer size =   768.00 MiB
llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
llama_new_context_with_model:      Metal compute buffer size =   168.00 MiB
llama_new_context_with_model:        CPU compute buffer size =    10.01 MiB
llama_new_context_with_model: graph nodes  = 1286
llama_new_context_with_model: graph splits = 2
AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
Model metadata: {'tokenizer.chat_template': "{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'&lt;|user|&gt;' + '\n' + message['content'] + '&lt;|end|&gt;' + '\n' + '&lt;|assistant|&gt;' + '\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '&lt;|end|&gt;' + '\n'}}{% endif %}{% endfor %}", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.pre': 'default', 'general.file_type': '1', 'phi3.rope.dimension_count': '96', 'tokenizer.ggml.bos_token_id': '1', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.attention.head_count_kv': '32', 'phi3.attention.head_count': '32', 'tokenizer.ggml.model': 'llama', 'phi3.block_count': '32', 'general.architecture': 'phi3', 'phi3.feed_forward_length': '8192', 'phi3.embedding_length': '3072', 'general.name': 'Phi3', 'phi3.context_length': '4096'}
Available chat formats from metadata: chat_template.default
Using gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'&lt;|user|&gt;' + '
' + message['content'] + '&lt;|end|&gt;' + '
' + '&lt;|assistant|&gt;' + '
'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '&lt;|end|&gt;' + '
'}}{% endif %}{% endfor %}
Using chat eos_token: &lt;|endoftext|&gt;
Using chat bos_token: &lt;s&gt;
ggml_metal_free: deallocating</code></pre>
</div>
</div>
</section>
</section>
<section id="chains" class="level1">
<h1>2. Chains</h1>
<section id="basic-prompt-chain" class="level2">
<h2 class="anchored" data-anchor-id="basic-prompt-chain">2.1. Basic Prompt Chain</h2>
<p>In LangChain, we use the <code>invoke</code> function to generate an output.</p>
<p>However, each model requires a specific prompt template. If we blindly called <code>invoke</code> on our model, we get no response:</p>
<div id="cell-9" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>llm.invoke(<span class="st">"Hi! My name is Gurp. What is 1 + 1?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>
llama_print_timings:        load time =     840.73 ms
llama_print_timings:      sample time =       0.89 ms /     1 runs   (    0.89 ms per token,  1123.60 tokens per second)
llama_print_timings: prompt eval time =     840.28 ms /    18 tokens (   46.68 ms per token,    21.42 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =     841.04 ms /    19 tokens</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>''</code></pre>
</div>
</div>
<p>This is where the LangChain abstractions come in useful.</p>
<p>We will create a simple chain with a single link:</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR

  subgraph PromptChain
    B(Prompt template) --&gt; C[LLM]
  end

  A(User prompt) --&gt; B
  C --&gt; D(Output)
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>For our particular case, Phi-3 prompts require start, end , user and assistant tokens.</p>
<div id="cell-12" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain <span class="im">import</span> PromptTemplate </span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a prompt template with the "input_prompt" variable </span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>template <span class="op">=</span> <span class="st">"""&lt;s&gt;&lt;|user|&gt; </span><span class="sc">{input_prompt}</span><span class="st">&lt;|end|&gt; &lt;|assistant|&gt;"""</span> </span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> PromptTemplate(template<span class="op">=</span>template, input_variables<span class="op">=</span>[<span class="st">"input_prompt"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can then create a chain by chaining the prompt and LLM together. Then we can call <code>invoke</code> and get the intended text generation.</p>
<div id="cell-14" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>basic_chain <span class="op">=</span> prompt <span class="op">|</span> llm</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>basic_chain.invoke({<span class="st">"input_prompt"</span>: <span class="st">"Hi! My name is Gurp. What is 1 + 1?"</span>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?
/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading "&lt;s&gt;" in prompt, this will likely reduce response quality, consider removing it...
  inputs = [input]
Llama.generate: prefix-match hit

llama_print_timings:        load time =     840.73 ms
llama_print_timings:      sample time =       5.89 ms /    16 runs   (    0.37 ms per token,  2716.93 tokens per second)
llama_print_timings: prompt eval time =     295.30 ms /    21 tokens (   14.06 ms per token,    71.11 tokens per second)
llama_print_timings:        eval time =    1349.54 ms /    15 runs   (   89.97 ms per token,    11.11 tokens per second)
llama_print_timings:       total time =    1668.23 ms /    36 tokens</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>' Hello Gurp! The answer to 1 + 1 is 2'</code></pre>
</div>
</div>
<p>Note that we just passed the entire <code>input_prompt</code> as a variable to the chain, but we can define other variables. For example, if we wanted a more specialised use case where we don’t give te user as much flexibility, we can pre-define some of the prompt.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>template <span class="op">=</span> <span class="st">"Create a funny name for a business that sells </span><span class="sc">{product}</span><span class="st">."</span> </span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>name_prompt <span class="op">=</span> PromptTemplate(template<span class="op">=</span>template, input_variables<span class="op">=</span>[<span class="st">"product"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="multiple-prompt-chain" class="level2">
<h2 class="anchored" data-anchor-id="multiple-prompt-chain">2.2. Multiple Prompt Chain</h2>
<p>If we have more complex prompts or use cases, we can split the task into smaller subtasks that run sequentially. Each link in the chain deals with a specific subtask.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR

  subgraph MultiPromptChain
    B1(Prompt 1) --&gt; B2(Prompt 2)
    B1 &lt;--&gt; C[LLM]
    B2 &lt;--&gt; C
  end

  A(User input) --&gt; B1

  B2 --&gt; D(Output)
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>As an example, we can prompt the LLM to create a story. First we ask it for a title based on the user prompt, then characters based on the title, then a story based on the characters and title. The first link is the only one that requires user input.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="lesson_files/figure-html/cell-16-1-image.png" class="img-fluid figure-img"></p>
<figcaption>Story prompt chain</figcaption>
</figure>
</div>
<div id="cell-17" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain <span class="im">import</span> LLMChain</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a chain for the title of our story </span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>title_template <span class="op">=</span> <span class="st">"""&lt;s&gt;&lt;|user|&gt; Create a title for a story about </span><span class="sc">{summary}</span><span class="st">. Only return the title. &lt;|end|&gt; &lt;|assistant|&gt;"""</span> </span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>title_prompt <span class="op">=</span> PromptTemplate(template<span class="op">=</span>title_template, input_variables<span class="op">=</span> [<span class="st">"summary"</span>]) </span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>title <span class="op">=</span> LLMChain(llm<span class="op">=</span>llm, prompt<span class="op">=</span>title_prompt, output_key<span class="op">=</span><span class="st">"title"</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a chain for the character description using the summary and title </span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>character_template <span class="op">=</span> <span class="st">"""&lt;s&gt;&lt;|user|&gt; Describe the main character of a story about </span><span class="sc">{summary}</span><span class="st"> with the title </span><span class="sc">{title}</span><span class="st">. Use only two sentences.&lt;|end|&gt; &lt;|assistant|&gt;"""</span> </span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>character_prompt <span class="op">=</span> PromptTemplate(template<span class="op">=</span>character_template, input_variables<span class="op">=</span>[<span class="st">"summary"</span>, <span class="st">"title"</span>]) </span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>character <span class="op">=</span> LLMChain(llm<span class="op">=</span>llm, prompt<span class="op">=</span>character_prompt, output_key<span class="op">=</span><span class="st">"character"</span>)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a chain for the story using the summary, title, and character description </span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>story_template <span class="op">=</span> <span class="st">"""&lt;s&gt;&lt;|user|&gt; Create a story about </span><span class="sc">{summary}</span><span class="st"> with the title </span><span class="sc">{title}</span><span class="st">. The main character is: </span><span class="sc">{character}</span><span class="st">. Only return the story and it cannot be longer than one paragraph. &lt;|end|&gt; &lt;|assistant|&gt;"""</span> </span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>story_prompt <span class="op">=</span> PromptTemplate(template<span class="op">=</span>story_template, input_variables<span class="op">=</span>[<span class="st">"summary"</span>, <span class="st">"title"</span>, <span class="st">"character"</span>]) </span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>story <span class="op">=</span> LLMChain(llm<span class="op">=</span>llm, prompt<span class="op">=</span>story_prompt, output_key<span class="op">=</span><span class="st">"story"</span>)</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine all three components to create the full chain </span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>llm_chain <span class="op">=</span> title <span class="op">|</span> character <span class="op">|</span> story</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>ggml_metal_free: deallocating</code></pre>
</div>
</div>
<p>Now we can invoke the chain just like before:</p>
<div id="cell-19" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>llm_chain.invoke({<span class="st">"summary"</span>: <span class="st">"a dog that can smell danger"</span>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?
/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading "&lt;s&gt;" in prompt, this will likely reduce response quality, consider removing it...
  inputs = [input]
Llama.generate: prefix-match hit

llama_print_timings:        load time =     840.73 ms
llama_print_timings:      sample time =       1.34 ms /    16 runs   (    0.08 ms per token, 11958.15 tokens per second)
llama_print_timings: prompt eval time =     295.27 ms /    23 tokens (   12.84 ms per token,    77.89 tokens per second)
llama_print_timings:        eval time =    1424.98 ms /    15 runs   (   95.00 ms per token,    10.53 tokens per second)
llama_print_timings:       total time =    1725.28 ms /    38 tokens
llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?
/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading "&lt;s&gt;" in prompt, this will likely reduce response quality, consider removing it...
  inputs = [input]
Llama.generate: prefix-match hit

llama_print_timings:        load time =     840.73 ms
llama_print_timings:      sample time =       1.97 ms /    16 runs   (    0.12 ms per token,  8109.48 tokens per second)
llama_print_timings: prompt eval time =     190.41 ms /    44 tokens (    4.33 ms per token,   231.08 tokens per second)
llama_print_timings:        eval time =    1315.12 ms /    15 runs   (   87.67 ms per token,    11.41 tokens per second)
llama_print_timings:       total time =    1511.43 ms /    59 tokens
llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?
/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading "&lt;s&gt;" in prompt, this will likely reduce response quality, consider removing it...
  inputs = [input]
Llama.generate: prefix-match hit

llama_print_timings:        load time =     840.73 ms
llama_print_timings:      sample time =       1.48 ms /    16 runs   (    0.09 ms per token, 10803.51 tokens per second)
llama_print_timings: prompt eval time =     290.88 ms /    70 tokens (    4.16 ms per token,   240.65 tokens per second)
llama_print_timings:        eval time =    1480.88 ms /    15 runs   (   98.73 ms per token,    10.13 tokens per second)
llama_print_timings:       total time =    1777.98 ms /    85 tokens</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>{'summary': 'a dog that can smell danger',
 'title': ' "Scent of Peril: The Canine Detective\'s N',
 'character': ' In the heartwarming tale, Scent of Peril follows Max,',
 'story': ' In a quaint suburban neighborhood, there lived an extraordinary German Shepherd'}</code></pre>
</div>
</div>
</section>
</section>
<section id="memory" class="level1">
<h1>3. Memory</h1>
<p>By default, most LLMs will not remember what was said previously in the conversation. For example, if you give your name in one prompt, it will not be able to recall it later.</p>
<div id="cell-21" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's give the LLM our name </span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>basic_chain.invoke({<span class="st">"input_prompt"</span>: <span class="st">"Hi! My name is Gurp. What is 1 + 1?"</span>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?
/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading "&lt;s&gt;" in prompt, this will likely reduce response quality, consider removing it...
  inputs = [input]
Llama.generate: prefix-match hit

llama_print_timings:        load time =     840.73 ms
llama_print_timings:      sample time =       2.43 ms /    16 runs   (    0.15 ms per token,  6573.54 tokens per second)
llama_print_timings: prompt eval time =     286.44 ms /    19 tokens (   15.08 ms per token,    66.33 tokens per second)
llama_print_timings:        eval time =    1401.11 ms /    15 runs   (   93.41 ms per token,    10.71 tokens per second)
llama_print_timings:       total time =    1694.51 ms /    34 tokens</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>' Hello Gurp! The answer to 1 + 1 is 2'</code></pre>
</div>
</div>
<div id="cell-22" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Next, we ask the LLM to reproduce the name</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>basic_chain.invoke({<span class="st">"input_prompt"</span>: <span class="st">"What is my name?"</span>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?
/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading "&lt;s&gt;" in prompt, this will likely reduce response quality, consider removing it...
  inputs = [input]
Llama.generate: prefix-match hit

llama_print_timings:        load time =     840.73 ms
llama_print_timings:      sample time =       3.35 ms /    16 runs   (    0.21 ms per token,  4771.85 tokens per second)
llama_print_timings: prompt eval time =     255.48 ms /     7 tokens (   36.50 ms per token,    27.40 tokens per second)
llama_print_timings:        eval time =    1341.37 ms /    15 runs   (   89.42 ms per token,    11.18 tokens per second)
llama_print_timings:       total time =    1606.10 ms /    22 tokens</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>" I'm unable to determine your name as I don't have access to"</code></pre>
</div>
</div>
<p>Models are <strong>stateless</strong>; they do not store memory. We can add specific types of memory to the chain to help it “remember” conversations:</p>
<ul>
<li>Conversation buffer</li>
<li>Conversation summary</li>
</ul>
<section id="conversation-buffer" class="level2">
<h2 class="anchored" data-anchor-id="conversation-buffer">3.1. Conversation Buffer</h2>
<p>The simplest way we can force the LLM to remember previous conversation is by <strong>passing the full conversation history in our prompt</strong>.</p>
<p>This approach is called conversation buffer memory. We update our prompt with the history of the chat.</p>
<section id="simple-conversation-buffer" class="level3">
<h3 class="anchored" data-anchor-id="simple-conversation-buffer">3.1.1. Simple Conversation Buffer</h3>
<div id="cell-25" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an updated prompt template to include a chat history </span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>template <span class="op">=</span> <span class="st">"""&lt;s&gt;&lt;|user|&gt;Current conversation:</span><span class="sc">{chat_history}</span><span class="st"> </span><span class="sc">{input_prompt}</span><span class="st">&lt;|end|&gt; &lt;|assistant|&gt;"""</span> </span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> PromptTemplate(template<span class="op">=</span>template, input_variables<span class="op">=</span>[<span class="st">"input_prompt"</span>, <span class="st">"chat_history"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, we create a <code>ConversationBufferMemory</code> link in the chain that will store the conversations we have previously had with the LLM.</p>
<div id="cell-27" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.memory <span class="im">import</span> ConversationBufferMemory</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the type of memory we will use </span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>memory <span class="op">=</span> ConversationBufferMemory(memory_key<span class="op">=</span><span class="st">"chat_history"</span>) </span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Chain the LLM, prompt, and memory together </span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>llm_chain <span class="op">=</span> LLMChain(prompt<span class="op">=</span>prompt, llm<span class="op">=</span>llm, memory<span class="op">=</span>memory)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can verify if this has worked by seeing if it now remembers our name in later prompts:</p>
<div id="cell-29" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a conversation and ask a basic question </span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>llm_chain.invoke({<span class="st">"input_prompt"</span>: <span class="st">"Hi! My name is Gurp. What is 1 + 1?"</span>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?
/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading "&lt;s&gt;" in prompt, this will likely reduce response quality, consider removing it...
  inputs = [input]
Llama.generate: prefix-match hit

llama_print_timings:        load time =     840.73 ms
llama_print_timings:      sample time =       5.11 ms /    16 runs   (    0.32 ms per token,  3129.89 tokens per second)
llama_print_timings: prompt eval time =     262.37 ms /    19 tokens (   13.81 ms per token,    72.42 tokens per second)
llama_print_timings:        eval time =    1334.30 ms /    15 runs   (   88.95 ms per token,    11.24 tokens per second)
llama_print_timings:       total time =    1608.21 ms /    34 tokens</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>{'input_prompt': 'Hi! My name is Gurp. What is 1 + 1?',
 'chat_history': '',
 'text': ' Hello, Gurp! The answer to 1 + 1 is '}</code></pre>
</div>
</div>
<div id="cell-30" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Does the LLM remember my name? </span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> llm_chain.invoke({<span class="st">"input_prompt"</span>: <span class="st">"What is my name?"</span>})</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(response)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?
/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading "&lt;s&gt;" in prompt, this will likely reduce response quality, consider removing it...
  inputs = [input]
Llama.generate: prefix-match hit

llama_print_timings:        load time =     840.73 ms
llama_print_timings:      sample time =       1.28 ms /    16 runs   (    0.08 ms per token, 12539.18 tokens per second)
llama_print_timings: prompt eval time =     344.80 ms /    47 tokens (    7.34 ms per token,   136.31 tokens per second)
llama_print_timings:        eval time =    1425.89 ms /    15 runs   (   95.06 ms per token,    10.52 tokens per second)
llama_print_timings:       total time =    1775.85 ms /    62 tokens</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>{'input_prompt': 'What is my name?', 'chat_history': 'Human: Hi! My name is Gurp. What is 1 + 1?\nAI:  Hello, Gurp! The answer to 1 + 1 is ', 'text': ' Hi Gurp! The answer to 1 + 1 is 2'}</code></pre>
</div>
</div>
<div id="cell-31" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>llm_chain.invoke({<span class="st">"input_prompt"</span>: <span class="st">"What is my name?"</span>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?
/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading "&lt;s&gt;" in prompt, this will likely reduce response quality, consider removing it...
  inputs = [input]
Llama.generate: prefix-match hit

llama_print_timings:        load time =     840.73 ms
llama_print_timings:      sample time =       6.22 ms /    16 runs   (    0.39 ms per token,  2571.93 tokens per second)
llama_print_timings: prompt eval time =     346.93 ms /    36 tokens (    9.64 ms per token,   103.77 tokens per second)
llama_print_timings:        eval time =    1359.76 ms /    15 runs   (   90.65 ms per token,    11.03 tokens per second)
llama_print_timings:       total time =    1721.41 ms /    51 tokens</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="26">
<pre><code>{'input_prompt': 'What is my name?',
 'chat_history': 'Human: Hi! My name is Gurp. What is 1 + 1?\nAI:  Hello, Gurp! The answer to 1 + 1 is \nHuman: What is my name?\nAI:  Hi Gurp! The answer to 1 + 1 is 2',
 'text': " Hi Gurp! You're asking your own name; I am an"}</code></pre>
</div>
</div>
</section>
<section id="windowed-conversation-buffer" class="level3">
<h3 class="anchored" data-anchor-id="windowed-conversation-buffer">3.1.2 Windowed Conversation Buffer</h3>
<p>As the conversation goes on, the size of the chat history grows until eventually it may exceed the token limit.</p>
<p>One approach to work around this is to only hold the last <span class="math inline">\(k\)</span> conversations in memory rather than the entire history.</p>
<div id="cell-33" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.memory <span class="im">import</span> ConversationBufferWindowMemory </span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Retain only the last 2 conversations in memory </span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>memory <span class="op">=</span> ConversationBufferWindowMemory(k<span class="op">=</span><span class="dv">2</span>, memory_key<span class="op">=</span><span class="st">"chat_history"</span>) </span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Chain the LLM, prompt, and memory together </span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>llm_chain <span class="op">=</span> LLMChain(prompt<span class="op">=</span>prompt, llm<span class="op">=</span>llm, memory<span class="op">=</span>memory)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/var/folders/8k/8jqhnfbd1t99blb07r1hs5440000gn/T/ipykernel_47761/3936398832.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/
  memory = ConversationBufferWindowMemory(k=2, memory_key="chat_history")</code></pre>
</div>
</div>
<p>This approach is not ideal for longer conversations. An alternative is to <strong>summarise</strong> the chat history to fit in the token limit, rather than truncating it.</p>
</section>
</section>
<section id="conversation-summary" class="level2">
<h2 class="anchored" data-anchor-id="conversation-summary">3.2. Conversation Summary</h2>
<p>This approach uses an LLM to summarise the main points of the history so far to reduce the number of tokens required to pass to the main LLM.</p>
<p>The “summary LLM” can be a different model to our “main LLM”. We may want to use a smaller LLM for the “easier” task of summarisation to speed up computation.</p>
<p>There will now be two LLM calls per invocation: the user prompt and the summarisation prompt.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="lesson_files/figure-html/cell-34-1-image.png" class="img-fluid figure-img"></p>
<figcaption>Conversation summary diagram</figcaption>
</figure>
</div>
<div id="cell-35" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a summary prompt template </span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>summary_prompt_template <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="st">    &lt;s&gt;&lt;|user|&gt;Summarize the conversations and update with the new lines. </span></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="st">    Current summary: </span><span class="sc">{summary}</span><span class="st"> new lines of conversation: </span><span class="sc">{new_lines}</span><span class="st"> </span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="st">    New summary:&lt;|end|&gt; &lt;|assistant|&gt;</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>summary_prompt <span class="op">=</span> PromptTemplate(</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>    input_variables<span class="op">=</span>[<span class="st">"new_lines"</span>, <span class="st">"summary"</span>],</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>    template<span class="op">=</span>summary_prompt_template</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In this example, we’ll pass both calls to the same LLM, but in general we don’t have to.</p>
<div id="cell-37" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.memory <span class="im">import</span> ConversationSummaryMemory </span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the type of memory we will use </span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>memory <span class="op">=</span> ConversationSummaryMemory(</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>    llm<span class="op">=</span>llm,</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>    memory_key<span class="op">=</span><span class="st">"chat_history"</span>,</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>    prompt<span class="op">=</span>summary_prompt</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>) </span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Chain the LLM, prompt, and memory together </span></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>llm_chain <span class="op">=</span> LLMChain(prompt<span class="op">=</span>prompt, llm<span class="op">=</span>llm, memory<span class="op">=</span>memory)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/var/folders/8k/8jqhnfbd1t99blb07r1hs5440000gn/T/ipykernel_47761/178162952.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/
  memory = ConversationSummaryMemory(</code></pre>
</div>
</div>
<p>We can try this out by having a short conversation with the LLM and checking it has retained previous information.</p>
<div id="cell-39" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a conversation and ask for the name </span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>llm_chain.invoke({<span class="st">"input_prompt"</span>: <span class="st">"Hi! My name is Gurp. What is 1 + 1?"</span>}) </span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>llm_chain.invoke({<span class="st">"input_prompt"</span>: <span class="st">"What is my name?"</span>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?
/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading "&lt;s&gt;" in prompt, this will likely reduce response quality, consider removing it...
  inputs = [input]
Llama.generate: prefix-match hit

llama_print_timings:        load time =     840.73 ms
llama_print_timings:      sample time =       1.33 ms /    16 runs   (    0.08 ms per token, 12066.37 tokens per second)
llama_print_timings: prompt eval time =     273.75 ms /    19 tokens (   14.41 ms per token,    69.41 tokens per second)
llama_print_timings:        eval time =    1438.78 ms /    15 runs   (   95.92 ms per token,    10.43 tokens per second)
llama_print_timings:       total time =    1717.89 ms /    34 tokens
Llama.generate: prefix-match hit

llama_print_timings:        load time =     840.73 ms
llama_print_timings:      sample time =       5.37 ms /    16 runs   (    0.34 ms per token,  2977.85 tokens per second)
llama_print_timings: prompt eval time =     291.94 ms /    77 tokens (    3.79 ms per token,   263.75 tokens per second)
llama_print_timings:        eval time =    1356.44 ms /    15 runs   (   90.43 ms per token,    11.06 tokens per second)
llama_print_timings:       total time =    1661.81 ms /    92 tokens
llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?
/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading "&lt;s&gt;" in prompt, this will likely reduce response quality, consider removing it...
  inputs = [input]
Llama.generate: prefix-match hit

llama_print_timings:        load time =     840.73 ms
llama_print_timings:      sample time =       1.40 ms /    16 runs   (    0.09 ms per token, 11428.57 tokens per second)
llama_print_timings: prompt eval time =     133.14 ms /    28 tokens (    4.76 ms per token,   210.30 tokens per second)
llama_print_timings:        eval time =    1437.07 ms /    15 runs   (   95.80 ms per token,    10.44 tokens per second)
llama_print_timings:       total time =    1575.77 ms /    43 tokens
Llama.generate: prefix-match hit

llama_print_timings:        load time =     840.73 ms
llama_print_timings:      sample time =       4.67 ms /    16 runs   (    0.29 ms per token,  3427.59 tokens per second)
llama_print_timings: prompt eval time =     294.58 ms /    81 tokens (    3.64 ms per token,   274.96 tokens per second)
llama_print_timings:        eval time =    1346.13 ms /    15 runs   (   89.74 ms per token,    11.14 tokens per second)
llama_print_timings:       total time =    1654.38 ms /    96 tokens</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>{'input_prompt': 'What is my name?',
 'chat_history': ' Gurp initiated a conversation by introducing himself and asking for the sum',
 'text': ' It seems there may have been a misunderstanding. In our current conversation, you'}</code></pre>
</div>
</div>
<div id="cell-40" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check whether it has summarized everything thus far </span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>llm_chain.invoke({<span class="st">"input_prompt"</span>: <span class="st">"What was the first question I asked?"</span>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>llama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?
/Users/gurpreetjohl/miniconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading "&lt;s&gt;" in prompt, this will likely reduce response quality, consider removing it...
  inputs = [input]
Llama.generate: prefix-match hit

llama_print_timings:        load time =     840.73 ms
llama_print_timings:      sample time =       3.82 ms /    16 runs   (    0.24 ms per token,  4187.39 tokens per second)
llama_print_timings: prompt eval time =     274.15 ms /    31 tokens (    8.84 ms per token,   113.08 tokens per second)
llama_print_timings:        eval time =    1342.18 ms /    15 runs   (   89.48 ms per token,    11.18 tokens per second)
llama_print_timings:       total time =    1625.96 ms /    46 tokens
Llama.generate: prefix-match hit

llama_print_timings:        load time =     840.73 ms
llama_print_timings:      sample time =       1.95 ms /    16 runs   (    0.12 ms per token,  8200.92 tokens per second)
llama_print_timings: prompt eval time =     300.16 ms /    84 tokens (    3.57 ms per token,   279.85 tokens per second)
llama_print_timings:        eval time =    1455.62 ms /    15 runs   (   97.04 ms per token,    10.30 tokens per second)
llama_print_timings:       total time =    1761.25 ms /    99 tokens</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>{'input_prompt': 'What was the first question I asked?',
 'chat_history': ' Gurp introduced himself to the human and inquired about their name. The',
 'text': ' The first question you asked could be, "Nice to meet you, G'}</code></pre>
</div>
</div>
<div id="cell-41" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check what the summary is thus far </span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>memory.load_memory_variables({})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>{'chat_history': ' Gurp introduced himself to the human and inquired about their name, while'}</code></pre>
</div>
</div>
<p>The conversation summary approach reduces the tokens required, but it does risk losing information depending on the quality of the summary.</p>
</section>
<section id="comparison-of-memory-approaches" class="level2">
<h2 class="anchored" data-anchor-id="comparison-of-memory-approaches">3.3. Comparison of Memory Approaches</h2>
<p><strong>Conversation buffer</strong></p>
<p>Pros:</p>
<ul>
<li>Easiest to implement</li>
<li>Ensures <em>no loss of info</em> (as long as conversation fits in congtext window)</li>
</ul>
<p>Cons:</p>
<ul>
<li><em>Slower generation</em> (more tokens needed)</li>
<li>Only suitable for LLMs with large context windows</li>
<li>Handles larger chat histories poorly</li>
</ul>
<p><strong>Windowed conversation buffer</strong></p>
<p>Pros:</p>
<ul>
<li>Can use LLMs with smaller context windows</li>
<li>Good for shorter chats; no information loss over the last k interactions</li>
</ul>
<p>Cons:</p>
<ul>
<li><em>Only captures k interactions</em></li>
<li>No compression, so can still require a large context window if k is large</li>
</ul>
<p><strong>Conversation summary</strong></p>
<p>Pros:</p>
<ul>
<li>Captures <em>full history</em></li>
<li>Enables long chats</li>
<li>Reduces required tokens</li>
</ul>
<p>Cons:</p>
<ul>
<li>Requires an <em>additional LLM call</em> per interaction</li>
<li>Quality of response depends on LLM’s summarisation quality</li>
</ul>
</section>
</section>
<section id="agents" class="level1">
<h1>4. Agents</h1>
<p>We can think of agents as an extension of multiple prompt chains. The difference is rather than requiring a user to input multiple prompts, <em>the LLM decides</em> on what actions it should take and in which order. Understanding both the query and <strong>deciding which tool to use and when</strong> is the crux of what makes agents useful.</p>
<p>Agents can make use of all the tools so far, including chains and memory. Two more that are vital for agents are:</p>
<ul>
<li><strong>Tools</strong> that the agent uses to do things it cannot do by itself</li>
<li>The <strong>agent type</strong> wich plans the actions to take (and the tools to use)</li>
</ul>
<p>Agents show more advanced behaviour like creating and self-correcting a roadmap to achieve a goal, and they can interact with the real world through tools.</p>
<section id="react" class="level2">
<h2 class="anchored" data-anchor-id="react">4.1. ReAct</h2>
<p>Many agent-based systems rely on the <strong>ReAct</strong> framework, which standard for <strong>Re</strong>asoning and <strong>Act</strong>ing.</p>
<p>We can give the LLM the ability to use tools, but it can only generate text, so it needs to generate the right text to interact with tools. For example, if we let it use a weather forecasting API, it needs to provide a request in the correct format.</p>
<p>ReAct merges the concepts of reasoning and acting as they are essentially two sides of the same coin: we want reasonong to afect actions and actions to affect reasoning. It does this by iteratively following these three steps:</p>
<ol type="1">
<li>Thought</li>
<li>Action</li>
<li>Observation</li>
</ol>
<p>We incorporate this into a prompt template like so:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="lesson_files/figure-html/cell-44-1-image.png" class="img-fluid figure-img"></p>
<figcaption>ReAct prompt template</figcaption>
</figure>
</div>
<p>We ask it to create a thought about the prompt, then trigger an action based on the thought, then observe the output, i.e.&nbsp;whatever it retrieved from an external tool.</p>
<p>An example of this is using an LLM to use a calculator.</p>
<div id="cell-46" class="cell">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os </span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain_openai <span class="im">import</span> ChatOpenAI </span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Load OpenAI's LLMs with LangChain </span></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">"OPENAI_API_KEY"</span>] <span class="op">=</span> <span class="st">"MY_KEY"</span> </span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>openai_llm <span class="op">=</span> ChatOpenAI(model_name<span class="op">=</span><span class="st">"gpt-3.5-turbo"</span>, temperature<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-47" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the ReAct template </span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>react_template <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="st">    Answer the following questions as best you can. </span></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a><span class="st">    You have access to the following tools: </span><span class="sc">{tools}</span><span class="st"> </span></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a><span class="st">    </span></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a><span class="st">    Use the following format: </span></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a><span class="st">    </span></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a><span class="st">    Question: the input question you must answer </span></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a><span class="st">    Thought: you should always think about what to do </span></span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a><span class="st">    Action: the action to take, should be one of [</span><span class="sc">{tool_names}</span><span class="st">] </span></span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a><span class="st">    Action Input: the input to the action </span></span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a><span class="st">    Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) </span></span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a><span class="st">    Thought: I now know the final answer </span></span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a><span class="st">    Final Answer: the final answer to the original input question</span></span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a><span class="st">    </span></span>
<span id="cb51-17"><a href="#cb51-17" aria-hidden="true" tabindex="-1"></a><span class="st">    Begin! </span></span>
<span id="cb51-18"><a href="#cb51-18" aria-hidden="true" tabindex="-1"></a><span class="st">    </span></span>
<span id="cb51-19"><a href="#cb51-19" aria-hidden="true" tabindex="-1"></a><span class="st">    Question: </span><span class="sc">{input}</span><span class="st"> </span></span>
<span id="cb51-20"><a href="#cb51-20" aria-hidden="true" tabindex="-1"></a><span class="st">    Thought:</span><span class="sc">{agent_scratchpad}</span></span>
<span id="cb51-21"><a href="#cb51-21" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span> </span>
<span id="cb51-22"><a href="#cb51-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb51-23"><a href="#cb51-23" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> PromptTemplate(</span>
<span id="cb51-24"><a href="#cb51-24" aria-hidden="true" tabindex="-1"></a>    template<span class="op">=</span>react_template,</span>
<span id="cb51-25"><a href="#cb51-25" aria-hidden="true" tabindex="-1"></a>    input_variables<span class="op">=</span>[<span class="st">"tools"</span>, <span class="st">"tool_names"</span>, <span class="st">"input"</span>, <span class="st">"agent_scratchpad"</span>]</span>
<span id="cb51-26"><a href="#cb51-26" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next we need to define the tools it can use to interact with the outside world.</p>
<div id="cell-49" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.agents <span class="im">import</span> load_tools, Tool </span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.tools <span class="im">import</span> DuckDuckGoSearchResults </span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="co"># You can create the tool to pass to an agent </span></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>search <span class="op">=</span> DuckDuckGoSearchResults() </span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>search_tool <span class="op">=</span> Tool(</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"duckduck"</span>,</span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>    description<span class="op">=</span><span class="st">"A web search engine. Use this to as a search engine for general queries."</span>,</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>    func<span class="op">=</span>search.run</span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare tools </span></span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a>tools <span class="op">=</span> load_tools([<span class="st">"llm-math"</span>], llm<span class="op">=</span>llm)   <span class="co"># Calculator tool is included by default </span></span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a>tools.append(search_tool)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally we can create the ReAct agent and pass it to the <code>AgentExecutor</code> which handles the execution steps.</p>
<div id="cell-51" class="cell">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain.agents <span class="im">import</span> AgentExecutor, create_react_agen</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Construct the ReAct agent </span></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> create_react_agent(openai_llm, tools, prompt) </span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>agent_executor <span class="op">=</span> AgentExecutor(</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>    agent<span class="op">=</span>agent,</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>    tools<span class="op">=</span>tools,</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>    handle_parsing_errors<span class="op">=</span><span class="va">True</span></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can invoke the LLM to find the price of an item and convert the currency. It will choose the appropriate tools to use for this.</p>
<div id="cell-53" class="cell">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># What is the price of a MacBook Pro? </span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>agent_executor.invoke({</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"input"</span>: <span class="st">"What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD."</span></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The agent generates intermediate steps during execution which we can use to follow its train of thought.</p>
<p>The important thing to consider when using agents is that there is <strong>no human in the loop</strong>; it will generate <em>an answer</em> but there is no guarantee that it is the <em>correct answer</em>.</p>
<p>We can make some tweaks to help ourselves debug this. For exampe, asking the agent to return the website’s URL that it retrieved prices from to make manual verification easier.</p>
</section>
</section>
<section id="references" class="level1">
<h1>References</h1>
<ul>
<li>Chapter 7 of Hands-On Large Language Models by Jay Alammar &amp; Marten Grootendoorst</li>
<li><a href="https://www.langchain.com/">Langchain docs</a></li>
<li><a href="https://github.com/ggml-org/ggml/blob/master/docs/gguf.md">GGUF file format</a></li>
<li><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization">Visual guide to quantization</a></li>
<li><a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/">Open LLM leaderboard</a></li>
<li><a href="https://huggingface.co/blog/langchain">The langchain_huggingface library</a></li>
</ul>


</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>