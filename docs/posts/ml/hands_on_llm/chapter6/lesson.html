<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Gurpreet Johl">
<meta name="dcterms.date" content="2024-12-13">
<meta name="description" content="Professional prompts">

<title>Gurpreet Johl - Hands-On LLMs: Prompt Engineering</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../../index.html">
    <span class="navbar-title">Gurpreet Johl</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../projects.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../blog.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/gjohl"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/gurpreetjohl"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/gurpreetjohl"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Hands-On LLMs: Prompt Engineering</h1>
                  <div>
        <div class="description">
          Professional prompts
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">AI</div>
                <div class="quarto-category">Engineering</div>
                <div class="quarto-category">GenerativeAI</div>
                <div class="quarto-category">LLM</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Gurpreet Johl </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 13, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#prompt-engineering" id="toc-prompt-engineering" class="nav-link active" data-scroll-target="#prompt-engineering">Prompt Engineering</a></li>
  <li><a href="#how-the-model-interprets-a-basic-prompt" id="toc-how-the-model-interprets-a-basic-prompt" class="nav-link" data-scroll-target="#how-the-model-interprets-a-basic-prompt">1. How the model interprets a basic prompt</a>
  <ul class="collapse">
  <li><a href="#special-tokens" id="toc-special-tokens" class="nav-link" data-scroll-target="#special-tokens">1.1. Special Tokens</a></li>
  <li><a href="#temperature" id="toc-temperature" class="nav-link" data-scroll-target="#temperature">1.2. Temperature</a></li>
  <li><a href="#constraining-the-sample-space-top-p-and-top-k" id="toc-constraining-the-sample-space-top-p-and-top-k" class="nav-link" data-scroll-target="#constraining-the-sample-space-top-p-and-top-k">1.3. Constraining the sample space: Top p and Top k</a></li>
  <li><a href="#example-use-cases" id="toc-example-use-cases" class="nav-link" data-scroll-target="#example-use-cases">1.4. Example use cases</a></li>
  </ul></li>
  <li><a href="#basic-prompt-engineering" id="toc-basic-prompt-engineering" class="nav-link" data-scroll-target="#basic-prompt-engineering">2. Basic prompt engineering</a>
  <ul class="collapse">
  <li><a href="#the-ingredients-of-a-prompt" id="toc-the-ingredients-of-a-prompt" class="nav-link" data-scroll-target="#the-ingredients-of-a-prompt">2.1. The ingredients of a prompt</a></li>
  <li><a href="#instruction-based-tasks" id="toc-instruction-based-tasks" class="nav-link" data-scroll-target="#instruction-based-tasks">2.2. Instruction-based tasks</a></li>
  </ul></li>
  <li><a href="#modular-components-of-prompts" id="toc-modular-components-of-prompts" class="nav-link" data-scroll-target="#modular-components-of-prompts">3. Modular components of prompts</a></li>
  <li><a href="#in-context-learning" id="toc-in-context-learning" class="nav-link" data-scroll-target="#in-context-learning">4. In-context learning</a>
  <ul class="collapse">
  <li><a href="#zero-shot" id="toc-zero-shot" class="nav-link" data-scroll-target="#zero-shot">4.1. Zero-shot</a></li>
  <li><a href="#one-shot-and-few-shot" id="toc-one-shot-and-few-shot" class="nav-link" data-scroll-target="#one-shot-and-few-shot">4.2. One-shot and Few-shot</a></li>
  </ul></li>
  <li><a href="#chain-prompting" id="toc-chain-prompting" class="nav-link" data-scroll-target="#chain-prompting">5. Chain Prompting</a></li>
  <li><a href="#reasoning" id="toc-reasoning" class="nav-link" data-scroll-target="#reasoning">6. Reasoning</a>
  <ul class="collapse">
  <li><a href="#chain-of-thought" id="toc-chain-of-thought" class="nav-link" data-scroll-target="#chain-of-thought">6.1. Chain of thought</a></li>
  <li><a href="#self-consistency" id="toc-self-consistency" class="nav-link" data-scroll-target="#self-consistency">6.2. Self-consistency</a></li>
  <li><a href="#tree-of-thought" id="toc-tree-of-thought" class="nav-link" data-scroll-target="#tree-of-thought">6.3. Tree of thought</a></li>
  </ul></li>
  <li><a href="#output-verification" id="toc-output-verification" class="nav-link" data-scroll-target="#output-verification">7. Output verification</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="prompt-engineering" class="level1">
<h1>Prompt Engineering</h1>
<p>Prompt engineering is an umbrella term for techniques that can improve the quality of a response from an LLM.</p>
<p>It extends beyond this as a tool to evaluate the model’s outputs and implement guardrails on the response.</p>
</section>
<section id="how-the-model-interprets-a-basic-prompt" class="level1">
<h1>1. How the model interprets a basic prompt</h1>
<p>It’s helpful to start by studying how a model sees a basic prompt.</p>
<section id="special-tokens" class="level2">
<h2 class="anchored" data-anchor-id="special-tokens">1.1. Special Tokens</h2>
<p>We can explore the tokens used by a model in a transformers pipeline by inspecting the chat template.</p>
<pre><code>prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False)</code></pre>
<p>This gives a prompt in the form:</p>
<pre><code>&lt;s&gt;&lt;|user|&gt; Create a funny joke about chickens.&lt;|end|&gt; &lt;|assistant|&gt; </code></pre>
<p>There are special tokens to indicate:</p>
<ul>
<li><code>&lt;s&gt;</code> - the start of the prompt</li>
<li><code>&lt;|user|&gt;</code> - when a user (i.e.&nbsp;you) begins their message</li>
<li><code>&lt;|end|&gt;</code> - when that message ends</li>
<li><code>&lt;|assistant|&gt;</code> - when the assistant (the LLM) begins their message.</li>
</ul>
<p>This gets passed to the model to complete the sequence, one token and a time until it generates an <code>&lt;|end|&gt;</code> token.</p>
<p>These special tokens help the model keep track of the context.</p>
</section>
<section id="temperature" class="level2">
<h2 class="anchored" data-anchor-id="temperature">1.2. Temperature</h2>
<p>LLMs are fundamentally trained neural networks, so their output should be (and is) deterministic.</p>
<p>But we don’t want to always get the same response to a given prompt. That would be boring. We want a bit of pizazz.</p>
<p>When the model is predicting the next token, what it actually does is create a probability distribution over all possible tokens in the vocabulary. It then <em>samples</em> from this distribution to choose the next word.</p>
<p><strong>Temperature</strong> defines how likely it is to choose less probable tokens.</p>
<p>A <code>temperature=0</code> will always choose the most likely token; this is greedy sampling.</p>
<p>Higher temperatures lead to more creative outputs.</p>
</section>
<section id="constraining-the-sample-space-top-p-and-top-k" class="level2">
<h2 class="anchored" data-anchor-id="constraining-the-sample-space-top-p-and-top-k">1.3. Constraining the sample space: Top p and Top k</h2>
<p>A related concept in sampling is to restrict the sample space, so rather than having the possibility (however small) of selecting <em>any</em> token in the vocab list, we constrain the possibilities.</p>
<p>The <code>top_p</code> parameter considers tokens from most to least probable until the cumulative probability reaches the given value. So <code>top_p=1</code> considers all tokens, and a lower value filters out less probable tokens. This is known as <em>nucleus sampling</em>.</p>
<p>The <code>top_k</code> parameter restrict the sample space to the <code>k</code> most probable tokens.</p>
</section>
<section id="example-use-cases" class="level2">
<h2 class="anchored" data-anchor-id="example-use-cases">1.4. Example use cases</h2>
<table class="table">
<colgroup>
<col style="width: 23%">
<col style="width: 30%">
<col style="width: 16%">
<col style="width: 30%">
</colgroup>
<thead>
<tr class="header">
<th>Use case</th>
<th><code>temperature</code></th>
<th><code>top_p</code></th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Brainstorming</td>
<td>High</td>
<td>High</td>
<td>Creative and unexpected responses</td>
</tr>
<tr class="even">
<td>Email generation</td>
<td>Low</td>
<td>Low</td>
<td>Predictable and focused responses that aren’t too out there</td>
</tr>
<tr class="odd">
<td>Creative writing</td>
<td>High</td>
<td>Low</td>
<td>Creative but still coherent</td>
</tr>
<tr class="even">
<td>Translation</td>
<td>Low</td>
<td>High</td>
<td>Deterministic output but with linguistic variety</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="basic-prompt-engineering" class="level1">
<h1>2. Basic prompt engineering</h1>
<section id="the-ingredients-of-a-prompt" class="level2">
<h2 class="anchored" data-anchor-id="the-ingredients-of-a-prompt">2.1. The ingredients of a prompt</h2>
<p>The most basic prompt is simply an <code>input</code>, without even an instruction. The LLM will simply complete the sequence. E.g.</p>
<blockquote class="blockquote">
<p>The sky is</p>
</blockquote>
<blockquote class="blockquote">
<p>blue</p>
</blockquote>
<p>We can extend this to an instruction prompt where we now have two components:</p>
<ol type="1">
<li>Instruction - Classify the text into negative or positive</li>
<li>Data - “This is a great movie!”</li>
</ol>
<p>The model may have seen similar instructions in its training data, or at least seen similar enough example to allow it to generalise.</p>
<p>This is called <strong>instruction-based prompting</strong>.</p>
</section>
<section id="instruction-based-tasks" class="level2">
<h2 class="anchored" data-anchor-id="instruction-based-tasks">2.2. Instruction-based tasks</h2>
<p>It’s helpful to understand common tasks LLMs are used to perform, as they may have been trained on such examples, so if you use the phrasing they are familiar with, they are more likely to give you the desired output.</p>
<ul>
<li>Classification - Classify the text into positive, neutral or negative.</li>
<li>Search - Find the ___ in the following text</li>
<li>Summarization - Summarise the following text</li>
<li>Code generation - Generate python code for…</li>
<li>Named entity recognition - An entity is… Extract the named entities from the following text</li>
</ul>
<p>The common features of these prompts for these different tasks are:</p>
<ul>
<li>Specificity - accurately describe what you want to achieve</li>
<li>Hallucination - LLMs are confident, not correct. We can ask the LLM to generate an answer if it knows the answer, otherwise respond with I don’t know</li>
<li>Order - The instruction should come either at the <strong>beginning or end</strong>. LLMs tend to focus at either extreme, known as the <em>primacy effect</em> and the <em>recency effect</em> respectively.</li>
</ul>
</section>
</section>
<section id="modular-components-of-prompts" class="level1">
<h1>3. Modular components of prompts</h1>
<p>We can build a complex prompt out of reusable components.</p>
<table class="table">
<colgroup>
<col style="width: 33%">
<col style="width: 39%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Persona</td>
<td>Describe what role the LLM should take on</td>
<td>“You are an expert in astrophysics”</td>
</tr>
<tr class="even">
<td>Instruction</td>
<td>The <strong>specific</strong> task (see last section)</td>
<td>“Classify the following text…”</td>
</tr>
<tr class="odd">
<td>Context</td>
<td>Additional info on the problem or task</td>
<td>“Your summary should extract the crucial points for executives to understand the vital information”</td>
</tr>
<tr class="even">
<td>Format</td>
<td>Useful if you need a particular structured output</td>
<td>“Create a bullet point summary”</td>
</tr>
<tr class="odd">
<td>Audience</td>
<td>Target of the response</td>
<td>“Explain like I’m 5 years old”</td>
</tr>
<tr class="even">
<td>Tone</td>
<td>The tone of voice of the response</td>
<td>“The tone should be professional and clear.”</td>
</tr>
<tr class="odd">
<td>Emotional stimuli!</td>
<td>Get creative, make the model feel</td>
<td>“This is very important for my career” changes the response!</td>
</tr>
<tr class="even">
<td>Data</td>
<td>The data related to the task itself</td>
<td>…</td>
</tr>
</tbody>
</table>
<p>Due to the modular nature, we can add and remove components and judge the effect on the output.</p>
<p>The <strong>order</strong> of these components matters — the primacy and recency effects again — so experimentation is key.</p>
<p>This is also <strong>model-dependent</strong>; some models respond better to certain prompting.</p>
</section>
<section id="in-context-learning" class="level1">
<h1>4. In-context learning</h1>
<p>This is when we provide the model with examples of the correct behaviour.</p>
<section id="zero-shot" class="level2">
<h2 class="anchored" data-anchor-id="zero-shot">4.1. Zero-shot</h2>
<p><strong>Zero-shot</strong> prompts provide no examples but outline the shape of the desired response. E.g.</p>
<blockquote class="blockquote">
<p>Classify the text into positive, neutral or negative Text: The food was great! Sentiment:</p>
</blockquote>
</section>
<section id="one-shot-and-few-shot" class="level2">
<h2 class="anchored" data-anchor-id="one-shot-and-few-shot">4.2. One-shot and Few-shot</h2>
<p><strong>Few-shot</strong> prompts provide two or more examples, <strong>one-shot</strong> prompts provide one example.</p>
<p>We can use the <code>user</code> and <code>assistant</code> roles in the prompt to distinguish the user prompt from the exemplar assistant output.</p>
<pre><code>one_shot_prompt = [  
    {"role": "user",
    "content": "A 'Gigamuru' is a type of Japanese musical instrument. An example of a sentence that uses the word Gigamuru is:"  },
    {"role": "assistant",
     "content": "I have a Gigamuru that my uncle gave me as a gift. I love to play it at home."  },
    {"role": "user",
     "content": "To 'screeg' something is to swing a sword at it. An example of a sentence that uses the word screeg is:"}
]</code></pre>
</section>
</section>
<section id="chain-prompting" class="level1">
<h1>5. Chain Prompting</h1>
<p>Rather than dumping everything in a single prompt, we take the output of one prompt and use it as the input to the next.</p>
<p>This gives the LLM more time (and context tokens) to focus on each part.</p>
<p>E.g. if we want to generate a product name, slogan and sales pitch for a new product, we <em>could</em> ask for all of those in a single prompt.</p>
<p><em>Or</em> we could chain prompt: ask for a name, then ask for a slogan given the name, then ask for a sales pitch given the name and slogan.</p>
<p>This idea is at the core or LangChain and agents.</p>
<p>Two other use cases for chain prompting are:</p>
<ol type="1">
<li>Response validation - send a follow-up prompt asking the model to check its answer</li>
<li>Parallel prompts - send multiple prompts and ask the LLM to merge the answers</li>
</ol>
</section>
<section id="reasoning" class="level1">
<h1>6. Reasoning</h1>
<p>Emergent behaviour of LLMs “resembles” reasoning, though is generally considered to do this just by memorisation and pattern-matching. (Although who says that isn’t what humans are doing too?)</p>
<section id="chain-of-thought" class="level2">
<h2 class="anchored" data-anchor-id="chain-of-thought">6.1. Chain of thought</h2>
<p>We can coax this behaviour which mimics reasoning out of the LLM to improve the output.</p>
<p>This is analogous to the System 1 and System 2 thinking of Kahneman and Tversky; by default the LLM will give a knee-jerk System 1 response, but if we ask it to reason it will give an more considered System 2 response.</p>
<p>We can encourage this with a one-shot <strong>chain-of-thought</strong> prompt demonstrating reasoning. E.g.</p>
<pre><code>cot_prompt = [
    {"role": "user",
     "content": "Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?"},
    {"role": "assistant", 
     "content": "Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11."},
     {"role": "user",
      "content": "The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?"}
]</code></pre>
<p>This guides the model towards providing an explanation as well as the answer. “Show your working!”</p>
<p>There is a <strong>zero-shot chain-of-thought</strong> approach that doesn’t require us to give an example of reasoning. A common and effective method is use this phrase to prime reasoning:</p>
<blockquote class="blockquote">
<p>Let’s think set-by-step</p>
</blockquote>
<p>(AKA the Bobby Valentino approach: slow down!)</p>
</section>
<section id="self-consistency" class="level2">
<h2 class="anchored" data-anchor-id="self-consistency">6.2. Self-consistency</h2>
<p>We can use the same prompt multiple times and get different responses (with differing quality) due to the sampling nature of LLMs.</p>
<p>We can sample multiple responses and ask the LLM to give the majority vote as the response.</p>
<p>This does make it slower and more expensive though, since we’re prompting <code>n</code> times for each prompt.</p>
</section>
<section id="tree-of-thought" class="level2">
<h2 class="anchored" data-anchor-id="tree-of-thought">6.3. Tree of thought</h2>
<p>This is a combination of the previous ideas.</p>
<pre><code>Tree of thought = Chain of thought + Self consistency </code></pre>
<p>We break the problem down into multiple steps. At each step, we ask the model to explore different solutions, then vote for the best solution(s) and move on to the next step. The thoughts are rates, with the most promising kept and the least promising pruned.</p>
<p>The disadvantage is that it requires even more calls to the model, slowing things down.</p>
<p>A <strong>zero-shot</strong> tree-of-thought approach is to ask the model to emulate a “discussion between multiple experts”. E.g.</p>
<pre><code>zeroshot_tot_prompt = [
    {"role": "user",
     "content": "Imagine three different experts are answering this question. All experts will write down 1 step of their thinking, then share it with the group. Then all experts will go on to the next step, etc. If any expert realizes they're wrong at any point then they leave. The question is 'The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?' Make sure to discuss the results."}
]</code></pre>
<p>An extention of Three-of-thought is <strong>Graph-of-thought</strong>, where each prompt is treated like a node in a graph. Rather than a tree following a linear train of thought, the prompts can be reused in different orders and combinations in a graph.</p>
</section>
</section>
<section id="output-verification" class="level1">
<h1>7. Output verification</h1>
<p>There are several reasons we may wish to verify the output of the model:</p>
<ul>
<li><strong>Structured output</strong> - the use case may require a certain format like JSON</li>
<li><strong>Valid output</strong> - if a model is asked to pick one of two choices, it should not create a third</li>
<li><strong>Ethics</strong> - biases, personal info leakage, etc</li>
<li><strong>Accuracy</strong> - fact-check</li>
</ul>
<p>There are generally three ways to control the output of a generative model:</p>
<ol type="1">
<li><strong>Examples</strong> - of the expected output</li>
<li><strong>Grammar</strong> - control the token selection process</li>
<li><strong>Fine-tuning</strong> - tune a model on data which contains the desired output</li>
</ol>
<p><strong>Example-based approaches</strong> are essentially the in-context learning approach discussed previously. The model may still disregard your examples though.</p>
<p><strong>Grammar-based approaches</strong> pass the output through another LLM to validate it meets the requirements. Packages like Guidance, Guardrails and LMQL constrain and validate the output of LLMs.</p>
<p>We can also constrain the tokens which the model is allowed to sample from.</p>
<p>Some models, like Llama, allow a response_format to be specified if it is a standard format like JSON.</p>
</section>
<section id="references" class="level1">
<h1>References</h1>
<ul>
<li>Chapter 6 of Hands-On Large Language Models by Jay Alammar &amp; Marten Grootendoorst</li>
<li><a href="https://cameronrwolfe.substack.com/p/modern-advances-in-prompt-engineering">https://cameronrwolfe.substack.com/p/modern-advances-in-prompt-engineering</a></li>
</ul>


</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>