<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Gurpreet Johl">
<meta name="dcterms.date" content="2024-11-26">
<meta name="description" content="Part 1: Introduction to LLMs">

<title>Gurpreet Johl - Hands-On LLMs</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="../../../../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../../../../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../../../../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../../index.html">
    <span class="navbar-title">Gurpreet Johl</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../projects.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../blog.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/gjohl"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/gurpreetjohl"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/gurpreetjohl"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Hands-On LLMs</h1>
                  <div>
        <div class="description">
          Part 1: Introduction to LLMs
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">AI</div>
                <div class="quarto-category">Engineering</div>
                <div class="quarto-category">GenerativeAI</div>
                <div class="quarto-category">LLM</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Gurpreet Johl </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 26, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-to-large-language-models" id="toc-introduction-to-large-language-models" class="nav-link active" data-scroll-target="#introduction-to-large-language-models">Introduction to Large Language Models</a>
  <ul class="collapse">
  <li><a href="#a-brief-history-of-language-ai" id="toc-a-brief-history-of-language-ai" class="nav-link" data-scroll-target="#a-brief-history-of-language-ai">1. A Brief History of Language AI</a>
  <ul class="collapse">
  <li><a href="#bag-of-words" id="toc-bag-of-words" class="nav-link" data-scroll-target="#bag-of-words">1.1. Bag of Words</a></li>
  <li><a href="#word2vec" id="toc-word2vec" class="nav-link" data-scroll-target="#word2vec">1.2. Word2Vec</a></li>
  <li><a href="#rnns" id="toc-rnns" class="nav-link" data-scroll-target="#rnns">1.3. RNNs</a></li>
  <li><a href="#transformers" id="toc-transformers" class="nav-link" data-scroll-target="#transformers">1.4. Transformers</a></li>
  <li><a href="#representation-models" id="toc-representation-models" class="nav-link" data-scroll-target="#representation-models">1.5. Representation Models</a></li>
  <li><a href="#generative-models" id="toc-generative-models" class="nav-link" data-scroll-target="#generative-models">1.6. Generative Models</a></li>
  <li><a href="#other-modern-architectures" id="toc-other-modern-architectures" class="nav-link" data-scroll-target="#other-modern-architectures">1.7. Other Modern Architectures</a></li>
  </ul></li>
  <li><a href="#how-large-is-a-large-language-model" id="toc-how-large-is-a-large-language-model" class="nav-link" data-scroll-target="#how-large-is-a-large-language-model">2. How Large is a Large Language Model?</a></li>
  <li><a href="#ethical-considerations-of-llms" id="toc-ethical-considerations-of-llms" class="nav-link" data-scroll-target="#ethical-considerations-of-llms">3. Ethical Considerations of LLMS</a></li>
  <li><a href="#using-an-llm-locally" id="toc-using-an-llm-locally" class="nav-link" data-scroll-target="#using-an-llm-locally">4. Using an LLM Locally</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="introduction-to-large-language-models" class="level1">
<h1>Introduction to Large Language Models</h1>
<section id="a-brief-history-of-language-ai" class="level2">
<h2 class="anchored" data-anchor-id="a-brief-history-of-language-ai">1. A Brief History of Language AI</h2>
<section id="bag-of-words" class="level3">
<h3 class="anchored" data-anchor-id="bag-of-words">1.1. Bag of Words</h3>
<p>This approach originated in the 1950s but gained popularity in the 2000s.</p>
<p>It treats unstructured text as a bag or words, throwing away any information from the position / ordering of the words and any semantic meaning of text.</p>
<ol type="1">
<li>Tokenise the text. A straightforward way to do this is to split on the spaces so we have a list of words.</li>
<li>Create a vocabulary of length N, containing every word in our training data.</li>
<li>We can then represent any sentence or document as a one-hot encoded N-dimensional vector.</li>
<li>Use those vectors for downstream tasks, e.g.&nbsp;cosine similarity between vectors to measure the similarity of documents for recommender systems.</li>
</ol>
<p>For example, if our vocabulary contains the words: <code>that is a cute dog my cat</code></p>
<p>Then we can encode the sentence “that is a cute dog” as:</p>
<table class="table">
<thead>
<tr class="header">
<th>that</th>
<th>is</th>
<th>a</th>
<th>cute</th>
<th>dog</th>
<th>my</th>
<th>cat</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>And another sentence “my cat is cute” as:</p>
<table class="table">
<thead>
<tr class="header">
<th>that</th>
<th>is</th>
<th>a</th>
<th>cute</th>
<th>dog</th>
<th>my</th>
<th>cat</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>Then to compare how similar the two sentences are, we can compare those vectors, for example using cosine similarity.</p>
</section>
<section id="word2vec" class="level3">
<h3 class="anchored" data-anchor-id="word2vec">1.2. Word2Vec</h3>
<p>A limitation of bag-of-words is that it makes no attempt to capture meaning from the text, treating each word as an unrelated token. By encoding text as one-hot encoded vectors, it does not capture that the word “cute” might be similar to “adorable” or “scrum-diddly-umptious”; every word is simply an arbitrary element of the vocabulary.</p>
<p>Dense vector embeddings attempt to capture these differences; rather than treating words as discrete elements, we can introduce a continuous scale for each embedding dimension, and learn where each word falls on the scale. Word2Vec was an early, and successful, approach to generating these embeddings.</p>
<p>The approach is to:</p>
<ol type="1">
<li>Assign every word in the vocabulary an (initial random) vector of the embedding dimension, say 50.</li>
<li>Take pairs of words from the training data, and train a model to predict whether they are likely to be neighbors in a sentence.</li>
<li>If two words typically share the same neighbouring words, they are likely to share similar embedding vectors, and vice versa.</li>
</ol>
<p><a href="https://jalammar.github.io/illustrated-word2vec/">Illustrated word2vec</a> provides a deeper dive.</p>
<p>These embeddings then have interesting properties. The classic example is that adding/subtracting the vectors for the corresponding words gives: <span class="math display">\[
king - man + woman \approx queen
\]</span></p>
<blockquote class="blockquote">
<p>The numbers don’t lie and they spell disaster</p>
<p>- “Big Poppa Pump” Scott Steiner</p>
</blockquote>
<ul>
<li><strong>N-grams</strong> are sliding windows of N words sampled from text. These can be used to train a model where the input is N words and the output is the predicted next word.</li>
<li><strong>Continuous bag or words</strong> (CBOW) tries to predict a missing word given the N preceding and following words.</li>
<li><strong>Skip-grams</strong> take a single word and try to predict the surrounding words. The are the “opposite” of CBOW.</li>
</ul>
<table class="table">
<colgroup>
<col style="width: 15%">
<col style="width: 27%">
<col style="width: 28%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th>Architecture</th>
<th>Task</th>
<th>Inputs</th>
<th>Output</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>N-gram</td>
<td>The numbers ___</td>
<td>[The, numbers]</td>
<td>don’t</td>
</tr>
<tr class="even">
<td>CBOW</td>
<td>The numbers ___ lie and</td>
<td>[The, numbers, lie, and]</td>
<td>don’t</td>
</tr>
<tr class="odd">
<td>Skip-gram</td>
<td>___ ___ don’t ___ ___</td>
<td>don’t</td>
<td>[The, numbers, lie, and]</td>
</tr>
</tbody>
</table>
<p><strong>Negative sampling</strong> is used to speed up the next-word prediction process. Instead of predicting the next token (a computationally expensive neural network), we reframe the task as “given two words, what is the probability that they are neighbours?” (a much faster logistic regression problem.)</p>
<p>But the issue is, our training dataset only has <em>positive</em> examples of neighbours. So the model could just always output 1 to get 100% accuracy. To avoid this, we introduce negative exmaples by taking random combinations of words in the vocabulary that aren’t neighbours. This idea is called <strong>noise-contrastive estimation</strong>.</p>
<p><strong>Word2vec is then just “skip gram with negative sampling”</strong> to generate <em>word</em> embeddings.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Types of embeddings">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Types of embeddings
</div>
</div>
<div class="callout-body-container callout-body">
<p>There are different types of embeddings that indicate different levels of abstraction.</p>
<p>We can create an embedding for a <strong>sub-word, a word, a sentence or a whole document</strong>. In each case, the result is an N-dimensional vector where N is the embedding size.</p>
</div>
</div>
</section>
<section id="rnns" class="level3">
<h3 class="anchored" data-anchor-id="rnns">1.3. RNNs</h3>
<p>The embeddings so far have been <strong>static</strong>: the embedding for “bank” will be the same regardless of whether it’s referring to the bank of a river or a branch of Santander.</p>
<p>The next development notes that the embeddings should vary depending on their context, i.e.&nbsp;the surrounding words.</p>
<p>Recurrent Neural Networks (RNNs) were initially used with attention mechanisms. These would:</p>
<ol type="1">
<li>Take pre-generated embeddings (say, from word2vec) as inputs</li>
<li>Pass this to an <strong>encoder RNN</strong> to generate a <strong>context embedding</strong></li>
<li>Pass this to a <strong>decoder RNN</strong> to generate an output, such as the input text translated to another language.</li>
</ol>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR

  A(Pre-generated embeddings) --&gt; B[[Encoder RNN]] --&gt; C(Context embedding) --&gt; D[[Decoder RNN]] --&gt; E(Output)
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="transformers" class="level3">
<h3 class="anchored" data-anchor-id="transformers">1.4. Transformers</h3>
<p>Transformers were introduced in the 2017 paper “Attention is All You Need”, which solely used the attention mechanism and removed the RNNs.</p>
<p>The original transformer was an encoder-decoder model for machine translation.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR

  A(Pre-generated embeddings) --&gt; B[[Transformer Encoder]] --&gt; C[[Transformer Decoder]] --&gt; E(Output)
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="representation-models" class="level3">
<h3 class="anchored" data-anchor-id="representation-models">1.5. Representation Models</h3>
<p>By splitting the encoder-decoder architecture and focusing only on the <strong>encoder</strong>, we can create models the excel at <em>creating meaningful representations of language</em>.</p>
<p>This is the premise behind models like BERT. The classification token is appended to the input, and the encoder alone is trained.</p>
</section>
<section id="generative-models" class="level3">
<h3 class="anchored" data-anchor-id="generative-models">1.6. Generative Models</h3>
<p>Similarly, we can split the encoder-decoder architecture and focusing only on the <strong>decoder</strong>. These excel at <em>text generation</em>.</p>
<p>This is the premise behind models like GPT.</p>
<p>Generative LLMs are essentially sequence-to-sequence machines: given some input text, predict the next tokens. The primary use case these days is being fine-tuned for “instruct” or “chat” models that are trained to provide an answer when given a question.</p>
<p><strong>Foundation models</strong> are open-source base models that can be fine-tuned for specific tasks.</p>
</section>
<section id="other-modern-architectures" class="level3">
<h3 class="anchored" data-anchor-id="other-modern-architectures">1.7. Other Modern Architectures</h3>
<p>Aside from Transformers, Mamba and RWKV perform well.</p>
</section>
</section>
<section id="how-large-is-a-large-language-model" class="level2">
<h2 class="anchored" data-anchor-id="how-large-is-a-large-language-model">2. How Large is a Large Language Model?</h2>
<p>This size of a large larnguage model is a moving target as the field develops and model sizes scale.</p>
<p>Considerations:</p>
<ul>
<li>What if a new model has the same capabilities as an existing LLM but with a fraction of the parameters. Is this new model still “large”?</li>
<li>What if we train a model the same size as GPT-4 but for text classification instead of generation? Is it still an LLM?</li>
</ul>
<p>The creation of LLMs is typically done in two stages:</p>
<ol type="1">
<li><strong>Language modeling</strong>: Create a foundation model by (unsuperivsed) training on a vast corpus of text. This step allows the model to learn the grammar, structure and patterns of the language. It is not yet directed at a specific task. This takes the majority of the training time.</li>
<li><strong>Fine-tuning</strong>: Using the foundation model for (supervised) training on a specific task.</li>
</ol>
</section>
<section id="ethical-considerations-of-llms" class="level2">
<h2 class="anchored" data-anchor-id="ethical-considerations-of-llms">3. Ethical Considerations of LLMS</h2>
<ul>
<li>Bias and fariness: Training data is seldom shared, so may contain implicit biases</li>
<li>Transparency and accountability: Unintended consequences when there is no “human in the loop”. Who is accountable for the outcomes of the LLM? The company that trained it? Or the one that used it? Or the patient?</li>
<li>Generating harmful content</li>
<li>Intellectual property: Who owns the output of an LLM? The user? The company that trained it? Or the original creators of the training data?</li>
<li>Regulation</li>
</ul>
</section>
<section id="using-an-llm-locally" class="level2">
<h2 class="anchored" data-anchor-id="using-an-llm-locally">4. Using an LLM Locally</h2>
<p>The following can be run in Google Colab on a free GPU.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForCausalLM, AutoTokenizer, pipeline</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load model and tokenizer </span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"microsoft/Phi-3-mini-4k-instruct"</span>, device_map<span class="op">=</span><span class="st">"cuda"</span>, torch_dtype<span class="op">=</span><span class="st">"auto"</span>, trust_remote_code<span class="op">=</span><span class="va">True</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>) </span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"microsoft/Phi-3-mini4k-instruct"</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a pipeline </span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>generator <span class="op">=</span> pipeline(</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"text-generation"</span>, model<span class="op">=</span>model, tokenizer<span class="op">=</span>tokenizer, return_full_text<span class="op">=</span><span class="va">False</span>, max_new_tokens<span class="op">=</span><span class="dv">500</span>, do_sample<span class="op">=</span><span class="va">False</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># The prompt (user input / query) </span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>messages <span class="op">=</span> [{<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: <span class="st">"Create a funny joke about chickens."</span>}]</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate output </span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> generator(messages)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(output[<span class="dv">0</span>][<span class="st">"generated_text"</span>])</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> Why don<span class="st">'t chickens like to go to the gym? Because they can'</span>t crack the egg<span class="op">-</span>sistence of it<span class="op">!</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li>Chapter 1 of Hands-On Large Language Models by Jay Alammar &amp; Marten Grootendoorst</li>
<li><a href="https://jalammar.github.io/illustrated-word2vec/">https://jalammar.github.io/illustrated-word2vec/</a></li>
</ul>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>