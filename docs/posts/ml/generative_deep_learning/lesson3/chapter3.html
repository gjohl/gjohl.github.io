<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Gurpreet Johl">
<meta name="dcterms.date" content="2024-02-28">
<meta name="description" content="Variational Autoencoders">

<title>Gurpreet Johl - Generative AI: Chapter 3</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="../../../../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../../../../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../../../../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../../index.html">
    <span class="navbar-title">Gurpreet Johl</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../projects.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../blog.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/gjohl"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/gurpreetjohl"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/gurpreetjohl"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Generative AI: Chapter 3</h1>
                  <div>
        <div class="description">
          Variational Autoencoders
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">AI</div>
                <div class="quarto-category">Engineering</div>
                <div class="quarto-category">GenerativeAI</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Gurpreet Johl </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 28, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#variational-autoencoders" id="toc-variational-autoencoders" class="nav-link active" data-scroll-target="#variational-autoencoders">Variational Autoencoders</a>
  <ul class="collapse">
  <li><a href="#autoencoders" id="toc-autoencoders" class="nav-link" data-scroll-target="#autoencoders">1. Autoencoders</a></li>
  <li><a href="#building-an-autoencoder" id="toc-building-an-autoencoder" class="nav-link" data-scroll-target="#building-an-autoencoder">2. Building an Autoencoder</a>
  <ul class="collapse">
  <li><a href="#load-and-pre-process-the-data" id="toc-load-and-pre-process-the-data" class="nav-link" data-scroll-target="#load-and-pre-process-the-data">2.1. Load and pre-process the data</a></li>
  <li><a href="#build-the-encoder" id="toc-build-the-encoder" class="nav-link" data-scroll-target="#build-the-encoder">2.2. Build the Encoder</a></li>
  <li><a href="#build-the-decoder" id="toc-build-the-decoder" class="nav-link" data-scroll-target="#build-the-decoder">2.3. Build the Decoder</a></li>
  <li><a href="#build-the-autoencoder" id="toc-build-the-autoencoder" class="nav-link" data-scroll-target="#build-the-autoencoder">2.4. Build the Autoencoder</a></li>
  <li><a href="#train-the-autoencoder" id="toc-train-the-autoencoder" class="nav-link" data-scroll-target="#train-the-autoencoder">2.5. Train the Autoencoder</a></li>
  </ul></li>
  <li><a href="#analysing-the-autoencoder" id="toc-analysing-the-autoencoder" class="nav-link" data-scroll-target="#analysing-the-autoencoder">3. Analysing the Autoencoder</a>
  <ul class="collapse">
  <li><a href="#reconstruct-images-using-the-autoencoder" id="toc-reconstruct-images-using-the-autoencoder" class="nav-link" data-scroll-target="#reconstruct-images-using-the-autoencoder">3.1. Reconstruct Images Using the Autoencoder</a></li>
  <li><a href="#analyse-the-embeddings" id="toc-analyse-the-embeddings" class="nav-link" data-scroll-target="#analyse-the-embeddings">3.2. Analyse the embeddings</a></li>
  <li><a href="#generating-new-images" id="toc-generating-new-images" class="nav-link" data-scroll-target="#generating-new-images">3.3. Generating New Images</a></li>
  <li><a href="#the-limitations-of-autoencoders" id="toc-the-limitations-of-autoencoders" class="nav-link" data-scroll-target="#the-limitations-of-autoencoders">3.4. The Limitations of Autoencoders</a></li>
  </ul></li>
  <li><a href="#variational-autoencoders-1" id="toc-variational-autoencoders-1" class="nav-link" data-scroll-target="#variational-autoencoders-1">4. Variational Autoencoders</a>
  <ul class="collapse">
  <li><a href="#the-encoder" id="toc-the-encoder" class="nav-link" data-scroll-target="#the-encoder">4.1. The Encoder</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="variational-autoencoders" class="level1">
<h1>Variational Autoencoders</h1>
<p>These are notes from chapter 3 of Generative Deep Learning by David Foster.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Story Time">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Story Time
</div>
</div>
<div class="callout-body-container callout-body">
<p>Imagine an infinite wardrobe organised by “type” of clothing.</p>
<p>Shoes would be close together, but formal shoes might be closer to the suits and trainers closer to the sports gear. Shirts and t-shirts would be close together. Coats might be nearby; the shirt-&gt;coat vector applied to t-shirts might lead you to “invent” gilets.</p>
<p>This encapsulates the idea of using a lower dimensional (2D in this case) latent space to <strong>encode</strong> the representation of more complex objects.</p>
<p>We could <em>sample</em> from some of the empty spaces to invent new hybrids of clothing. This generative step is <strong>decoding</strong> the latent space.</p>
</div>
</div>
<section id="autoencoders" class="level2">
<h2 class="anchored" data-anchor-id="autoencoders">1. Autoencoders</h2>
<p>The idea of autoencoders (read: self-encoders) is that they learn to simplify the input then reconstruct it; the input and target output are the same.</p>
<ul>
<li>The <strong>encoder</strong> learns to compress high-dimensional input data into a lower dimensional representation called the <em>embedding</em>.</li>
<li>The <strong>decoder</strong> takes an embedding and recreates a higher-dimensional image. This should be an accurate reconstruction of the input.</li>
</ul>
<p>This can be used as a generative model because we can the sample and decode <em>new</em> points from the latent space to generate novel outputs. The goal of training an autoencoder is to learn a meaningful embedding <span class="math inline">\(z\)</span>.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR

  A(Encoder) --&gt; B(z)
  B(z) --&gt; c(Decoder)
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>This also makes autoencoders useful as <em>denoising</em> models, because the embedding should retain the salient information but “lose” the noise.</p>
</section>
<section id="building-an-autoencoder" class="level2">
<h2 class="anchored" data-anchor-id="building-an-autoencoder">2. Building an Autoencoder</h2>
<p>We will implement an autoencoder to learn lower-dimensional embeddings for the <a href="https://github.com/zalandoresearch/fashion-mnist">fashion MNIST data set</a>.</p>
<div id="cell-2" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> layers, models, datasets, callbacks</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>IMAGE_SIZE <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>CHANNELS <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>BUFFER_SIZE <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>VALIDATION_SPLIT <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>EMBEDDING_DIM <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>EPOCHS <span class="op">=</span> <span class="dv">3</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="load-and-pre-process-the-data" class="level3">
<h3 class="anchored" data-anchor-id="load-and-pre-process-the-data">2.1. Load and pre-process the data</h3>
<p>Scale the pixel values and reshape the images.</p>
<div id="cell-4" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>(x_train, y_train), (x_test, y_test) <span class="op">=</span> datasets.fashion_mnist.load_data()</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> preprocess(images):</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    images <span class="op">=</span> images.astype(<span class="st">"float32"</span>) <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    images <span class="op">=</span> np.pad(images, ((<span class="dv">0</span>, <span class="dv">0</span>), (<span class="dv">2</span>, <span class="dv">2</span>), (<span class="dv">2</span>, <span class="dv">2</span>)), constant_values<span class="op">=</span><span class="fl">0.0</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    images <span class="op">=</span> np.expand_dims(images, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> images</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>x_train <span class="op">=</span> preprocess(x_train)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>x_test <span class="op">=</span> preprocess(x_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz
29515/29515 [==============================] - 0s 0us/step
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz
26421880/26421880 [==============================] - 12s 0us/step
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz
5148/5148 [==============================] - 0s 0us/step
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz
4422102/4422102 [==============================] - 2s 0us/step</code></pre>
</div>
</div>
<p>We can see an example from our training set:</p>
<div id="cell-6" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>plt.imshow(x_train[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter3_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="build-the-encoder" class="level3">
<h3 class="anchored" data-anchor-id="build-the-encoder">2.2. Build the Encoder</h3>
<p>The encoder compresses the dimensionality on the input to a smaller embedding dimension.</p>
<div id="cell-8" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Input</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>encoder_input <span class="op">=</span> layers.Input(shape<span class="op">=</span>(IMAGE_SIZE, IMAGE_SIZE, CHANNELS),name<span class="op">=</span><span class="st">"encoder_input"</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Conv layers</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(<span class="dv">32</span>, (<span class="dv">3</span>, <span class="dv">3</span>), strides<span class="op">=</span><span class="dv">2</span>, activation<span class="op">=</span><span class="st">"relu"</span>, padding<span class="op">=</span><span class="st">"same"</span>)(encoder_input)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(<span class="dv">64</span>, (<span class="dv">3</span>, <span class="dv">3</span>), strides<span class="op">=</span><span class="dv">2</span>, activation<span class="op">=</span><span class="st">"relu"</span>, padding<span class="op">=</span><span class="st">"same"</span>)(x)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(<span class="dv">128</span>, (<span class="dv">3</span>, <span class="dv">3</span>), strides<span class="op">=</span><span class="dv">2</span>, activation<span class="op">=</span><span class="st">"relu"</span>, padding<span class="op">=</span><span class="st">"same"</span>)(x)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>pre_flatten_shape <span class="op">=</span> tf.keras.backend.int_shape(x)[<span class="dv">1</span>:]  <span class="co"># Used by the decoder later</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Output</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Flatten()(x)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>encoder_output <span class="op">=</span> layers.Dense(EMBEDDING_DIM, name<span class="op">=</span><span class="st">"encoder_output"</span>)(x)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Model</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>encoder <span class="op">=</span> models.Model(encoder_input, encoder_output)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>encoder.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 encoder_input (InputLayer)  [(None, 32, 32, 1)]       0         
                                                                 
 conv2d (Conv2D)             (None, 16, 16, 32)        320       
                                                                 
 conv2d_1 (Conv2D)           (None, 8, 8, 64)          18496     
                                                                 
 conv2d_2 (Conv2D)           (None, 4, 4, 128)         73856     
                                                                 
 flatten (Flatten)           (None, 2048)              0         
                                                                 
 encoder_output (Dense)      (None, 2)                 4098      
                                                                 
=================================================================
Total params: 96770 (378.01 KB)
Trainable params: 96770 (378.01 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________</code></pre>
</div>
</div>
</section>
<section id="build-the-decoder" class="level3">
<h3 class="anchored" data-anchor-id="build-the-decoder">2.3. Build the Decoder</h3>
<p>The decoder reconstructs the original image from the embedding.</p>
<section id="convolutional-transpose-layers" class="level4">
<h4 class="anchored" data-anchor-id="convolutional-transpose-layers">Convolutional Transpose Layers</h4>
<p>In a standard convolutional layer, if we have <code>stride=2</code> it will half the image size.</p>
<p>In a convolutional transpose layer, we are <em>increasing</em> the image size. The <code>stride</code> parameter determines the amount of zero padding to add between each pixel. A kernel is then applied to this “internally padded” image to expand the image size.</p>
<div id="cell-10" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Input</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>decoder_input <span class="op">=</span> layers.Input(shape<span class="op">=</span>(EMBEDDING_DIM,),name<span class="op">=</span><span class="st">"decoder_input"</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Reshape the input using the pre-flattening shape from the encoder</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Dense(np.prod(pre_flatten_shape))(decoder_input)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Reshape(pre_flatten_shape)(x)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Scale up the image back to its original size. These are the reverse of the conv layers applied in the encoder.</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2DTranspose(<span class="dv">128</span>, (<span class="dv">3</span>, <span class="dv">3</span>), strides<span class="op">=</span><span class="dv">2</span>, activation<span class="op">=</span><span class="st">"relu"</span>, padding<span class="op">=</span><span class="st">"same"</span>)(x)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2DTranspose(<span class="dv">64</span>, (<span class="dv">3</span>, <span class="dv">3</span>), strides<span class="op">=</span><span class="dv">2</span>, activation<span class="op">=</span><span class="st">"relu"</span>, padding<span class="op">=</span><span class="st">"same"</span>)(x)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2DTranspose(<span class="dv">32</span>, (<span class="dv">3</span>, <span class="dv">3</span>), strides<span class="op">=</span><span class="dv">2</span>, activation<span class="op">=</span><span class="st">"relu"</span>, padding<span class="op">=</span><span class="st">"same"</span>)(x)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Output</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>decoder_output <span class="op">=</span> layers.Conv2D(</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    CHANNELS,</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">3</span>, <span class="dv">3</span>),</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    strides<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    activation<span class="op">=</span><span class="st">'sigmoid'</span>,</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    padding<span class="op">=</span><span class="st">"same"</span>,</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"decoder_output"</span>,</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>)(x)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Model</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>decoder <span class="op">=</span> models.Model(decoder_input, decoder_output)</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>decoder.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 decoder_input (InputLayer)  [(None, 2)]               0         
                                                                 
 dense_2 (Dense)             (None, 2048)              6144      
                                                                 
 reshape_2 (Reshape)         (None, 4, 4, 128)         0         
                                                                 
 conv2d_transpose_6 (Conv2D  (None, 8, 8, 128)         147584    
 Transpose)                                                      
                                                                 
 conv2d_transpose_7 (Conv2D  (None, 16, 16, 64)        73792     
 Transpose)                                                      
                                                                 
 conv2d_transpose_8 (Conv2D  (None, 32, 32, 32)        18464     
 Transpose)                                                      
                                                                 
 decoder_output (Conv2D)     (None, 32, 32, 1)         289       
                                                                 
=================================================================
Total params: 246273 (962.00 KB)
Trainable params: 246273 (962.00 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________</code></pre>
</div>
</div>
</section>
</section>
<section id="build-the-autoencoder" class="level3">
<h3 class="anchored" data-anchor-id="build-the-autoencoder">2.4. Build the Autoencoder</h3>
<p>Combine the encoder and decoder into a single model.</p>
<div id="cell-12" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>autoencoder <span class="op">=</span> models.Model(encoder_input, decoder(encoder_output))</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>autoencoder.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 encoder_input (InputLayer)  [(None, 32, 32, 1)]       0         
                                                                 
 conv2d (Conv2D)             (None, 16, 16, 32)        320       
                                                                 
 conv2d_1 (Conv2D)           (None, 8, 8, 64)          18496     
                                                                 
 conv2d_2 (Conv2D)           (None, 4, 4, 128)         73856     
                                                                 
 flatten (Flatten)           (None, 2048)              0         
                                                                 
 encoder_output (Dense)      (None, 2)                 4098      
                                                                 
 model_2 (Functional)        (None, 32, 32, 1)         246273    
                                                                 
=================================================================
Total params: 343043 (1.31 MB)
Trainable params: 343043 (1.31 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________</code></pre>
</div>
</div>
</section>
<section id="train-the-autoencoder" class="level3">
<h3 class="anchored" data-anchor-id="train-the-autoencoder">2.5. Train the Autoencoder</h3>
<p>The autoencoder is trained with the source images as both input and target output.</p>
<p>The loss function is usually chosen as either RMSE or binary cross-entropy between pixels of original image vs reconstruction.</p>
<div id="cell-14" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>autoencoder.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">"adam"</span>, loss<span class="op">=</span><span class="st">"binary_crossentropy"</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>autoencoder.fit(</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    x_train,</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    x_train,</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span>EPOCHS,</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>(x_test, x_test)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/3
600/600 [==============================] - 36s 59ms/step - loss: 0.2981 - val_loss: 0.2656
Epoch 2/3
600/600 [==============================] - 37s 61ms/step - loss: 0.2600 - val_loss: 0.2583
Epoch 3/3
600/600 [==============================] - 38s 63ms/step - loss: 0.2558 - val_loss: 0.2562</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>&lt;keras.src.callbacks.History at 0x297028990&gt;</code></pre>
</div>
</div>
</section>
</section>
<section id="analysing-the-autoencoder" class="level2">
<h2 class="anchored" data-anchor-id="analysing-the-autoencoder">3. Analysing the Autoencoder</h2>
<p>We can use our trained autoencoder to:</p>
<ol type="1">
<li>Reconstruct images</li>
<li>Analyse embeddings</li>
<li>Generate new images</li>
</ol>
<section id="reconstruct-images-using-the-autoencoder" class="level3">
<h3 class="anchored" data-anchor-id="reconstruct-images-using-the-autoencoder">3.1. Reconstruct Images Using the Autoencoder</h3>
<p>Reconstruct a sample of test images using the autoencoder.</p>
<p>The reconstruction isn’t perfect; some information is lost when reducing down to just 2 dimensions. But it does a surprisingly good job of compressing 32x32 pixel values into just 2 embedding values.</p>
<div id="cell-17" class="cell" data-execution_count="21">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>NUM_IMAGES_TO_RECONSTRUCT <span class="op">=</span> <span class="dv">5000</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>example_images <span class="op">=</span> x_test[:NUM_IMAGES_TO_RECONSTRUCT]</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>example_labels <span class="op">=</span> y_test[:NUM_IMAGES_TO_RECONSTRUCT]</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> autoencoder.predict(example_images)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>157/157 [==============================] - 1s 9ms/step</code></pre>
</div>
</div>
<p>Original images:</p>
<div id="cell-19" class="cell" data-execution_count="23">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_sample_images(images, n<span class="op">=</span><span class="dv">10</span>, size<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">3</span>), cmap<span class="op">=</span><span class="st">"gray_r"</span>):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>size)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>        _ <span class="op">=</span> plt.subplot(<span class="dv">1</span>, n, i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>        plt.imshow(images[i].astype(<span class="st">"float32"</span>), cmap<span class="op">=</span>cmap)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>        plt.axis(<span class="st">"off"</span>)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>plot_sample_images(example_images)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter3_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Reconstructed images:</p>
<div id="cell-21" class="cell" data-execution_count="24">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>plot_sample_images(predictions)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter3_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="analyse-the-embeddings" class="level3">
<h3 class="anchored" data-anchor-id="analyse-the-embeddings">3.2. Analyse the embeddings</h3>
<p>Each of the images above has been encoded as a 2-dimensional embedding.</p>
<p>We can look at these embeddings to gain some insight into how the autoencoder works.</p>
<p>The embedding vectors for our sample images above:</p>
<div id="cell-24" class="cell" data-execution_count="29">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Encode the example images</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>embeddings <span class="op">=</span> encoder.predict(example_images)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(embeddings[:<span class="dv">10</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>157/157 [==============================] - 0s 2ms/step
[[  2.329792     5.081217  ]
 [ -3.5425975   -3.196322  ]
 [-16.850313    10.458025  ]
 [-13.082482    10.525161  ]
 [ -2.4375374   -0.2749687 ]
 [-10.373022     5.53389   ]
 [ -3.6757205    2.3856945 ]
 [ -2.994627     0.74553806]
 [ -1.4679942    9.046445  ]
 [  1.2849879    9.3865385 ]]</code></pre>
</div>
</div>
<p>We can plot the 2D latent space, colouring each point by its label. This shows how similar items are clustered together in latent space.</p>
<p>This is impressive! Remember, we never showed the model the labels when training, so it has learned to cluster images that look alike.</p>
<div id="cell-26" class="cell" data-execution_count="31">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Colour the embeddings by their label</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>example_labels <span class="op">=</span> y_test[:NUM_IMAGES_TO_RECONSTRUCT]</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the latent space</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>figsize <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(figsize, figsize))</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>plt.scatter(</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    embeddings[:, <span class="dv">0</span>],</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    embeddings[:, <span class="dv">1</span>],</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    cmap<span class="op">=</span><span class="st">"rainbow"</span>,</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    c<span class="op">=</span>example_labels,</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    alpha<span class="op">=</span><span class="fl">0.6</span>,</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    s<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter3_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="generating-new-images" class="level3">
<h3 class="anchored" data-anchor-id="generating-new-images">3.3. Generating New Images</h3>
<p>We can sample from the latent space and decode these sampled points to generate new images.</p>
<p>First we sample some random points in the latent space:</p>
<div id="cell-29" class="cell" data-execution_count="40">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the range of existing embedding values so we can sample sensible points within the latent space.</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>embedding_min <span class="op">=</span> np.<span class="bu">min</span>(embeddings, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>embedding_max <span class="op">=</span> np.<span class="bu">max</span>(embeddings, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample some points</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>grid_width <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>grid_height <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>sample <span class="op">=</span> np.random.uniform(</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    embedding_min, embedding_max, size<span class="op">=</span>(grid_width <span class="op">*</span> grid_height, EMBEDDING_DIM)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sample)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[[ -0.49943115   1.02098943]
 [-16.65572441   6.96547782]
 [-11.72685548  14.24565505]
 [ -5.44522844   7.63668958]
 [  0.53313438  -3.95172388]
 [ -4.18291681   0.13892611]
 [ -8.97108471  -4.5148637 ]
 [ -2.34931462   9.41724957]
 [ -5.31146502   2.74073718]
 [ -9.83316333  13.77455175]
 [-14.54977666   9.52349768]
 [-16.76102961  13.72802771]
 [-12.3427831    5.1277622 ]
 [-18.38723042  14.19956358]
 [ -7.12388153  -1.41235603]
 [  2.59077578  -1.60023945]
 [  4.30528696  12.95034234]
 [-14.32301235  15.41919473]]</code></pre>
</div>
</div>
<p>We can then decode these sampled points.</p>
<div id="cell-31" class="cell" data-execution_count="41">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Decode the sampled points</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>reconstructions <span class="op">=</span> decoder.predict(sample)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>1/1 [==============================] - 0s 51ms/step</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>array([[[[1.63236909e-05],
         [2.90390130e-06],
         [5.43164379e-06],
         ...,
         [4.57848756e-07],
         [4.09775168e-07],
         [5.94741177e-05]],

        [[1.18803644e-06],
         [2.32933303e-07],
         [1.06183370e-06],
         ...,
         [1.37428330e-07],
         [3.67753969e-08],
         [1.21626326e-05]],

        [[4.16460671e-06],
         [4.63096967e-06],
         [9.47717344e-05],
         ...,
         [1.00781072e-04],
         [2.14847796e-06],
         [6.22331354e-05]],

        ...,

        [[7.97207861e-07],
         [3.45792728e-06],
         [2.01895903e-03],
         ...,
         [2.52830400e-03],
         [9.69846496e-06],
         [9.02931351e-05]],

        [[2.08561858e-07],
         [2.72046833e-07],
         [1.48678164e-05],
         ...,
         [2.22502672e-06],
         [1.87276441e-07],
         [1.36805993e-05]],

        [[3.03991710e-05],
         [1.71808369e-05],
         [7.44444769e-05],
         ...,
         [1.39519507e-05],
         [3.72187651e-06],
         [3.91025096e-04]]],


       [[[3.48859552e-17],
         [4.75370151e-19],
         [8.98937538e-18],
         ...,
         [1.15914002e-15],
         [2.97793855e-17],
         [3.39554795e-12]],

        [[3.04463555e-21],
         [3.65699785e-22],
         [6.24295232e-19],
         ...,
         [2.14110381e-16],
         [1.83756557e-20],
         [5.82498256e-14]],

        [[1.07039259e-19],
         [2.40445224e-17],
         [9.81418558e-11],
         ...,
         [2.69895661e-10],
         [2.12531697e-16],
         [2.63894310e-12]],

        ...,

        [[7.09873347e-35],
         [6.28466074e-30],
         [1.94322148e-14],
         ...,
         [2.22350402e-10],
         [1.56591171e-13],
         [8.46497761e-10]],

        [[8.62318962e-34],
         [3.58614099e-30],
         [1.91546037e-18],
         ...,
         [4.20077223e-15],
         [4.56952199e-16],
         [4.19501586e-11]],

        [[2.54806445e-21],
         [4.40923456e-20],
         [1.51082997e-14],
         ...,
         [6.25977482e-12],
         [1.41048111e-12],
         [4.78561439e-08]]],


       [[[1.06899241e-20],
         [4.85096477e-23],
         [8.34942499e-22],
         ...,
         [2.43039147e-18],
         [7.76541649e-20],
         [7.57926233e-14]],

        [[1.44880863e-25],
         [5.92251114e-27],
         [1.22336652e-23],
         ...,
         [7.31508438e-19],
         [2.85722328e-23],
         [8.73637352e-16]],

        [[2.40200636e-23],
         [7.45366553e-21],
         [8.64334984e-14],
         ...,
         [7.57713545e-11],
         [8.31263990e-18],
         [2.02787101e-13]],

        ...,

        [[1.26213070e-34],
         [8.05792189e-28],
         [6.77919665e-11],
         ...,
         [1.14610090e-08],
         [1.90689532e-13],
         [3.78936993e-10]],

        [[1.80603586e-35],
         [4.33894713e-31],
         [2.08190317e-18],
         ...,
         [5.14569593e-15],
         [9.62266698e-16],
         [6.38490857e-11]],

        [[3.38649341e-23],
         [1.08819790e-21],
         [7.53881293e-16],
         ...,
         [1.45968296e-12],
         [1.25957870e-12],
         [5.73432040e-08]]],


       ...,


       [[[1.14409141e-02],
         [4.04429389e-03],
         [5.94330579e-03],
         ...,
         [1.52589564e-04],
         [1.68523518e-04],
         [3.81657132e-03]],

        [[3.00783711e-03],
         [8.85234564e-04],
         [2.20768712e-03],
         ...,
         [9.57584634e-05],
         [6.08509872e-05],
         [1.76054344e-03]],

        [[2.57944246e-03],
         [1.18813571e-03],
         [4.87227459e-03],
         ...,
         [1.08896801e-02],
         [1.38390542e-03],
         [6.62241923e-03]],

        ...,

        [[6.98678335e-03],
         [4.42432240e-03],
         [1.81216877e-02],
         ...,
         [3.73369977e-02],
         [2.79519911e-04],
         [1.03694689e-03]],

        [[4.55282954e-03],
         [1.45832077e-03],
         [1.65569806e-03],
         ...,
         [5.34098654e-04],
         [1.93739543e-05],
         [2.77274376e-04]],

        [[2.18557995e-02],
         [7.83375837e-03],
         [5.33270603e-03],
         ...,
         [1.18058175e-03],
         [1.57052127e-04],
         [3.08558973e-03]]],


       [[[1.91661289e-13],
         [2.80343553e-15],
         [5.79173304e-15],
         ...,
         [1.55523110e-14],
         [6.16173237e-14],
         [2.57673460e-09]],

        [[1.40150521e-16],
         [3.20992255e-18],
         [4.92417674e-17],
         ...,
         [1.90860706e-15],
         [3.47597758e-16],
         [1.18012392e-10]],

        [[3.35055317e-15],
         [9.15232821e-15],
         [4.00927512e-12],
         ...,
         [3.22521065e-10],
         [1.29657873e-12],
         [2.96333780e-09]],

        ...,

        [[4.19838872e-21],
         [9.50531789e-20],
         [2.81639360e-12],
         ...,
         [3.14195024e-12],
         [1.28090605e-12],
         [3.50674441e-08]],

        [[6.09088870e-21],
         [6.69063294e-20],
         [2.05994178e-14],
         ...,
         [4.18583007e-12],
         [2.67161033e-11],
         [2.59599801e-07]],

        [[1.06346057e-13],
         [1.23503665e-13],
         [4.08999119e-11],
         ...,
         [3.91034582e-09],
         [1.87179445e-08],
         [2.47102271e-05]]],


       [[[1.03007236e-22],
         [2.97955130e-25],
         [7.55196582e-24],
         ...,
         [9.11860643e-20],
         [1.52619246e-21],
         [4.51792509e-15]],

        [[4.45488204e-28],
         [1.67742856e-29],
         [9.51116405e-26],
         ...,
         [2.56194476e-20],
         [2.57298595e-25],
         [3.48043390e-17]],

        [[1.19272157e-25],
         [9.46365946e-23],
         [9.00637936e-15],
         ...,
         [9.71548883e-12],
         [2.05151041e-19],
         [1.31166076e-14]],

        ...,

        [[0.00000000e+00],
         [1.13202022e-31],
         [1.14732334e-12],
         ...,
         [7.91147481e-10],
         [7.72887918e-15],
         [4.14370760e-11]],

        [[0.00000000e+00],
         [6.81408878e-35],
         [1.46787691e-20],
         ...,
         [1.39860505e-16],
         [2.20253895e-17],
         [4.99228913e-12]],

        [[6.64629981e-26],
         [3.77431762e-24],
         [1.68866788e-17],
         ...,
         [9.72009270e-14],
         [6.94714062e-14],
         [9.39310763e-09]]]], dtype=float32)</code></pre>
</div>
</div>
<div id="cell-32" class="cell" data-execution_count="45">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>figsize <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(figsize, figsize))</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the latent space and overlay the positions of the sampled points</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>plt.scatter(embeddings[:, <span class="dv">0</span>], embeddings[:, <span class="dv">1</span>], c<span class="op">=</span><span class="st">"black"</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, s<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>plt.scatter(sample[:, <span class="dv">0</span>], sample[:, <span class="dv">1</span>], c<span class="op">=</span><span class="st">"#00B0F0"</span>, alpha<span class="op">=</span><span class="dv">1</span>, s<span class="op">=</span><span class="dv">40</span>)</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot a grid of the reconstructed images which decode those sampled points</span></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(figsize, grid_height <span class="op">*</span> <span class="dv">2</span>))</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>fig.subplots_adjust(hspace<span class="op">=</span><span class="fl">0.4</span>, wspace<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(grid_width <span class="op">*</span> grid_height):</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> fig.add_subplot(grid_height, grid_width, i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>    ax.axis(<span class="st">"off"</span>)</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>    ax.text(</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>        <span class="fl">0.5</span>,</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>        <span class="op">-</span><span class="fl">0.35</span>,</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>        <span class="bu">str</span>(np.<span class="bu">round</span>(sample[i, :], <span class="dv">1</span>)),</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>        fontsize<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>        ha<span class="op">=</span><span class="st">"center"</span>,</span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>        transform<span class="op">=</span>ax.transAxes,</span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a>    ax.imshow(reconstructions[i, :, :], cmap<span class="op">=</span><span class="st">"Greys"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter3_files/figure-html/cell-16-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter3_files/figure-html/cell-16-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Now let’s see what happens when we regularly sample the latent space.</p>
<div id="cell-34" class="cell" data-execution_count="43">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Colour the embeddings by their label (clothing type - see table)</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>figsize <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>grid_size <span class="op">=</span> <span class="dv">15</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(figsize, figsize))</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>plt.scatter(</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    embeddings[:, <span class="dv">0</span>],</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>    embeddings[:, <span class="dv">1</span>],</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>    cmap<span class="op">=</span><span class="st">"rainbow"</span>,</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>    c<span class="op">=</span>example_labels,</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>    alpha<span class="op">=</span><span class="fl">0.8</span>,</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>    s<span class="op">=</span><span class="dv">300</span>,</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="bu">min</span>(embeddings[:, <span class="dv">0</span>]), <span class="bu">max</span>(embeddings[:, <span class="dv">0</span>]), grid_size)</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.linspace(<span class="bu">max</span>(embeddings[:, <span class="dv">1</span>]), <span class="bu">min</span>(embeddings[:, <span class="dv">1</span>]), grid_size)</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>xv, yv <span class="op">=</span> np.meshgrid(x, y)</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>xv <span class="op">=</span> xv.flatten()</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>yv <span class="op">=</span> yv.flatten()</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> np.array(<span class="bu">list</span>(<span class="bu">zip</span>(xv, yv)))</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>reconstructions <span class="op">=</span> decoder.predict(grid)</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.scatter(grid[:, 0], grid[:, 1], c="black", alpha=1, s=10)</span></span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(figsize, figsize))</span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a>fig.subplots_adjust(hspace<span class="op">=</span><span class="fl">0.4</span>, wspace<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(grid_size<span class="op">**</span><span class="dv">2</span>):</span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> fig.add_subplot(grid_size, grid_size, i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a>    ax.axis(<span class="st">"off"</span>)</span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a>    ax.imshow(reconstructions[i, :, :], cmap<span class="op">=</span><span class="st">"Greys"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>8/8 [==============================] - 0s 18ms/step</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter3_files/figure-html/cell-17-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter3_files/figure-html/cell-17-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="the-limitations-of-autoencoders" class="level3">
<h3 class="anchored" data-anchor-id="the-limitations-of-autoencoders">3.4. The Limitations of Autoencoders</h3>
<p>The latent space exploration above yields some interesting insights into “regular” autoencoders that motivate the use of variational autoencoders to address these shortcomings.</p>
<ul>
<li>Different categories occupy varying amounts of area in latent space.</li>
<li>The latent space distribution is not symmetrical or bounded.</li>
<li>There are gaps in the latent space.</li>
</ul>
<p>This makes it difficult for us to sample from this latent space effectively. We could sample a “gap” and get a nonsensical image. If a category (say, trousers) occupies a larger area in latent space, we are more likely to generate images of trousers than of categories which occupy a small area (say, shoes).</p>
</section>
</section>
<section id="variational-autoencoders-1" class="level2">
<h2 class="anchored" data-anchor-id="variational-autoencoders-1">4. Variational Autoencoders</h2>
<div class="callout callout-style-default callout-tip callout-titled" title="Story Time">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Story Time
</div>
</div>
<div class="callout-body-container callout-body">
<p>If we revisit our wardrobe, rather than assigning each item to a specific location, let’s assign it to a general <em>region</em> of the wardrobe.</p>
<p>And let’s also insist that this region should be as close to the centre of the wardrobe as possible, otherwise we are penalised. This should yield a more uniform latent space.</p>
<p>This is the idea behind <strong>variational autoencoders</strong> (VAE).</p>
</div>
</div>
<section id="the-encoder" class="level3">
<h3 class="anchored" data-anchor-id="the-encoder">4.1. The Encoder</h3>


</section>
</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>