{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Algorithms\"\n",
    "description: \"Big G Explains Big O\"\n",
    "date: \"2024-04-17\"\n",
    "categories: [Engineering, ComputerScience, InterviewPrep]\n",
    "# format:\n",
    "#   html:\n",
    "#     code-fold: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms\n",
    "\n",
    "## 1. Big O\n",
    "\n",
    "The mathematical explanation is given in terms of the upper bound of the growth rate of a function. Very briefly: if a function $g(x)$ grows no faster than $f(x)$ then $g$ is a member of $O(f)$. This isn't particularly helpful for intuitive understanding though.\n",
    "\n",
    "The fundamental question is:\n",
    "\n",
    ">If there are $N$ data elements, how many steps will the algorithm take?\n",
    "\n",
    "This helps us understand how the performance of the algorithm changes as the data increases.\n",
    "\n",
    "Basically, count the number of steps the algorithm takes as a function of N (and M if it involves two arrays), then drop any constants and only keep the \"worst\" term since we consider the worst case scenario.\n",
    "\n",
    "Be aware, though, that this doesn't necessarily mean a lower complexity algorithm will always be faster in practice for every use case. Take for exmaple algorithm A, which always takes 100 steps regardless of input size so is $O(1)$, and algorithm B which scales linearly with the input so in $O(N)$.\n",
    "\n",
    "If we apply these to an array with 10 elements, A will take 100 steps and B will take 10 steps. So the \"worse\" algorithm can perform better for small data.\n",
    "\n",
    "When an algorithm is $O(log(N))$, the log is implicitly $log_2$. These come up regularly in algorithms where we divide and conquer, as in the binary search, because we half the data with each iteration. If you see an algorithm has a log term in its complexity, that's generally a clue that there is some binary split happening somewhere.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recursion\n",
    "\n",
    "::: {.callout-tip}\n",
    "Definition: Recursion\n",
    "\n",
    "See \"Recursion\".\n",
    ":::\n",
    "\n",
    "Recursion is when a function calls itself.\n",
    "\n",
    "In any case where you can use a loop, you could also write it recursively. I'm not saying you *should*, but you *can*.\n",
    "\n",
    "Every recursive function should have a **base case** (or multiple base cases) where it does not recurse, to prevent it from entering an infinite loop.\n",
    "\n",
    "### 2.1 Reading Recursive Functions \n",
    "\n",
    "There is a knack to reading recursive functions. Start from the base case and work backwards.\n",
    "\n",
    "1. Identify the base case and walk through it\n",
    "2. Identify the \"second-to-last\" case and walk through it\n",
    "3. Repeat for the next-to-last case before that and walk through it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorial(number):\n",
    "    \"\"\"Recursively calculate the factorial of a number.\"\"\"\n",
    "    # The base case\n",
    "    if number == 1:\n",
    "        return 1\n",
    "    # The recursive bit\n",
    "    return number * factorial(number - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "720"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factorial(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Call Stack\n",
    "\n",
    "The computer uses a call stack to keep track of the functions to call. When we enter a new recursion, we push a function call on to the stack, and when we finish executing we pop it from the call stack.\n",
    "\n",
    "If we don't write appropriate base cases, the recursive function can loop infinitely, leading to **stack overflow**.\n",
    "\n",
    "### 2.3. How Deep Does This Go?\n",
    "\n",
    "Use recursion when the depth of the problem is unknown or arbitrary. \n",
    "\n",
    "If we have a problem where we want to go through nested structures but we don't know ahead of time how deep they go, we can't solve this using regular loops but we can with recursion.\n",
    "\n",
    "For example, if we want to traverse a directory and each of its subdirectories, and each of their subdirectories, etc.\n",
    "\n",
    "\n",
    "### 2.4. Writing Recursive Functions\n",
    "\n",
    "Recursive algorithms are useful for categories of problems where:\n",
    "\n",
    "1. The goal is to **repeatedly execute** a task\n",
    "2. The problem can be broken into **subproblems**, which are versions of the same problem but with a smaller input.\n",
    "\n",
    "##### Passing Extra Parameters\n",
    "\n",
    "If we are modifying the data structure, say an array, in place, we can pass it as an extra parameter to the recursive function so that it can be passed up and down the call stack.\n",
    "\n",
    "This is often useful for the **repeatedly execute** category of problems.\n",
    "\n",
    "\n",
    "##### Top-down Thought Process\n",
    "\n",
    "Recursion is useful when coupled with a new, slightly unintuitive, mental model of top-down problem solving.\n",
    "\n",
    "1. Imagine the function you're writing has *already been implemented*\n",
    "2. Identify the *subproblem*, often the next step along\n",
    "3. See what happens when you *call the function on the subproblem*\n",
    "4. Go from there until you reach a *base case*\n",
    "\n",
    "This is often useful for the **subproblem** category of problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dynamic Programming\n",
    "\n",
    "The idea behind dynamic programming is similar to recursion: break the problem down into smaller subproblems.\n",
    "For dynamic programming problems, the subproblems are typically **overlapping subproblems**. We also want to remove any duplicate calls.\n",
    "\n",
    "\n",
    "### 3.1. The Top-down DP Approach\n",
    "\n",
    "A common problem with recursive approaches is that we end up calculating the same function multiple times in the call stack. This can lead to some horrendous complexities like $O(N!)$ or $O(2^N)$.\n",
    "\n",
    "\n",
    "The key difference to recursion is we only solve each subproblem once and store its result. This way, we can just look it up for any other calls that require it. This storing of intermediate calculations is called **memoization**. \n",
    "\n",
    "We will need to pass this memoised object as an extra parameter and modify it in place, as noted in the recusion section.\n",
    "\n",
    "\n",
    "### 3.2. The Bottom-up DP Approach\n",
    "\n",
    "Ditch recursion and just use a loop. \n",
    "\n",
    "This is technically another way of \"solving a problem that can be solved using recursion but without making duplciate calls\" - which is what dynamic programming essentially is. In this case, we do it by removing recursion altogether."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
