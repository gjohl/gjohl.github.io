{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Hands-On LLMs: Attention Mechanism\"\n",
    "description: \"Pay attention\"\n",
    "date: \"2024-12-15\"\n",
    "# image: \"deep_learning_model.png\"\n",
    "categories: [AI, Engineering, GenerativeAI, LLM]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "Neural networks in disguise.\n",
    "\n",
    "\n",
    "# 1. Transformers Overview\n",
    "\n",
    "## 1.1. What is the model doing?\n",
    "\n",
    "Transformer LLMs are fundamentally just models that take text as an input and produce text as an output.\n",
    "\n",
    "\n",
    "```{mermaid}\n",
    "flowchart LR\n",
    "\n",
    "  A([Input text]) --> B[LLM] --> C([Output text]) \n",
    "```\n",
    "\n",
    "They are **autoregressive models**. They generate outputs one token at a time, iteratively appending the output to the input to generate the next. So the model is simply predicting the next token given a tensor of input token embeddings.\n",
    "\n",
    "```{mermaid}\n",
    "flowchart LR\n",
    "\n",
    "  A([\"Once upon a\"]) --> B[Transformer] --> C([\"time\"]) \n",
    "```\n",
    "\n",
    "## 1.1. The forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- Chapter 3 of Hands-On Large Language Models by Jay Alammar & Marten Grootendoorst\n",
    "- [https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/)\n",
    "- [Dive into Deep Learning Chapter 11](https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html)\n",
    "- [Andrej Karpathy \"Let's build GPT\" tutorial](https://www.youtube.com/watch?v=kCc8FmEb1nY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
