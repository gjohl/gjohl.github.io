{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Hands-On LLMs: Tokens and Embeddings\"\n",
    "description: \"Part 2: Tokens and Embeddings\"\n",
    "date: \"2024-11-28\"\n",
    "# image: \"deep_learning_model.png\"\n",
    "categories: [AI, Engineering, GenerativeAI, LLM]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokens and Embeddings\n",
    "\n",
    "\n",
    "# 1. Tokenization\n",
    "\n",
    "A text prompt sent to a model first needs to be broken down into tokens. The numerical token IDs are passed to the model. \n",
    "\n",
    "The tokenization scheme is designed hand-in-hand with the model, so the two are coupled: each model has a corresponding tokenizer which transforms the prompt text into the expect numerical representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Tokenization in Action\n",
    "We can load a model to see what tokenization looks like practice.\n",
    "\n",
    "We'll load a smaller open-source model and its corresponding tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    device_map = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    device_map=None\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    device_map = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. Load the model\n",
    "\n",
    "This can take a few minutes depedning on internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0842a39ae1074d239bd58ffd2a92c37d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23abd87937a428c932c2e07226d4c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3.py:   0%|          | 0.00/11.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c847f6502f64a1389389ab4b541cf7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi3.py:   0%|          | 0.00/73.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34916626c6d486880dcfc9215864f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d97389e5ddd740c0ba4529ae6183a8a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b05441fca6d5412e8731035a4be2b1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5706a89afd8d4f5ca1d43db3e4a33e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c01c3119ed24a959eb0731b9a6f0ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09acc54f372c476cad02ba26d37cfe6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "          (rotary_emb): Phi3RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm()\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_attention_layernorm): Phi3RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device_map,\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "098acb4adcb44783a09d3054aba82f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6133ee295d440397912b4f9bf345e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a556de39d5ba4ef09d38b6da9703ab27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52d829ab2034c7ba412f924356f27a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c6ef59f1e3408d9048102d5b4ef304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3. Use the model to generate text\n",
    "\n",
    "First we tokenize the input promt. Then we pass this to the model.\n",
    "We can peek in at each step to see what's actually being passed around.\n",
    "\n",
    "We'll start with the following input prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.<|assistant|>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer converts this text to a list of integers. These are the input IDs that are passed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input prompt \n",
    "input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14350,   385,  4876, 27746,  5281,   304, 19235,   363,   278, 25305,\n",
      "           293, 16423,   292,   286,   728,   481, 29889, 12027,  7420,   920,\n",
      "           372,  9559, 29889, 32001]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can \"decode\" these input IDs, converting them back to text, to see how the tokenizer splits the text. It uses sub-word tokens, so `mishap` is split into `m, ish, ap`. Punctuation is its own token and there is a special token for `<|assistant|>` Spaces are implicit; parital tokens have a special hidden character preceding them and tokens without that character are assumed to have a space before them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write\n",
      "an\n",
      "email\n",
      "apolog\n",
      "izing\n",
      "to\n",
      "Sarah\n",
      "for\n",
      "the\n",
      "trag\n",
      "ic\n",
      "garden\n",
      "ing\n",
      "m\n",
      "ish\n",
      "ap\n",
      ".\n",
      "Exp\n",
      "lain\n",
      "how\n",
      "it\n",
      "happened\n",
      ".\n",
      "<|assistant|>\n"
     ]
    }
   ],
   "source": [
    "for id in input_ids[0]: \n",
    "    print(tokenizer.decode(id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now pass this tokenized input to the model to generate new tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to a quirk of Macs, we need to explicitly pass it an attention mask as it cannot be inferred\n",
    "if device == 'mps':\n",
    "    model_kwargs = {'attention_mask': (input_ids != tokenizer.pad_token_id).long()}\n",
    "else:\n",
    "    model_kwargs = {}\n",
    "\n",
    "# Generate the text \n",
    "generation_output = model.generate(input_ids=input_ids, max_new_tokens=100, **model_kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the generation appends tokens to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14350,   385,  4876, 27746,  5281,   304, 19235,   363,   278, 25305,\n",
       "           293, 16423,   292,   286,   728,   481, 29889, 12027,  7420,   920,\n",
       "           372,  9559, 29889, 32001,  3323,   622, 29901, 17778, 29888,  2152,\n",
       "          6225, 11763,   363,   278, 19906,   292,   341,   728,   481,    13,\n",
       "            13,    13, 29928,   799, 19235, 29892,    13,    13,    13, 29902,\n",
       "          4966,   445,  2643, 14061,   366,  1532, 29889,   306,   626,  5007,\n",
       "           304,  4653,   590,  6483,   342,  3095, 11763,   363,   278,   443,\n",
       "          6477,   403, 15134,   393, 10761,   297,   596, 16423, 22600, 29889,\n",
       "            13,    13,    13,  2887,   366,  1073, 29892,   306,   505,  2337,\n",
       "          7336,  2859,   278, 15409,   322, 22024,   339,  1793,   310,   596,\n",
       "         16423, 29889,   739,   756,  1063,   263,  2752,   310,  8681, 12232,\n",
       "           363,   592, 29892,   322,   306,   471,  1468, 24455,   304,   505,\n",
       "           278, 15130,   304,  1371]], device='mps:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can decode this to see the output text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.<|assistant|> Subject: Heartfelt Apologies for the Gardening Mishap\n",
      "\n",
      "\n",
      "Dear Sarah,\n",
      "\n",
      "\n",
      "I hope this message finds you well. I am writing to express my deepest apologies for the unfortunate incident that occurred in your garden yesterday.\n",
      "\n",
      "\n",
      "As you know, I have always admired the beauty and tranquility of your garden. It has been a source of inspiration for me, and I was thrilled to have the opportunity to help\n"
     ]
    }
   ],
   "source": [
    "# Print the output \n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Tokenizer Design\n",
    "\n",
    "There are three primary decisions that determine how the tokenizer splits a given prompt:\n",
    "\n",
    "1. **Tokenization method**: byte pair encoding (BPE), WordPiece\n",
    "2. **Tokenizer parameters**: vocabulary size, choice of special tokens\n",
    "3. **Training data set**: a tokenizer trained on English text will give different results to one trained on Punjabi text or Python code, etc.\n",
    "\n",
    "Tokenizers are used on the input (to encode text -> numbers) and on the output (to decode numbers -> text).\n",
    "\n",
    "\n",
    "### 1.2.1 Tokenization Methods\n",
    "\n",
    "There are four promininent tokenization schemes:\n",
    "\n",
    "1. **Word** tokens. \n",
    "    - Pros: Simple to implement and understand; can fit more text in a given context window\n",
    "    - Cons: Unable to handle unseen words; vocab has lots of tokens for almost identical words (e.g. write, writing, written, wrote)\n",
    "\n",
    "2. **Sub-word** tokens.\n",
    "     - Pros: Can represent new words by breaking down into other known tokens\n",
    "     - Cons: Choice of partial words dictionary requires careful design\n",
    "3. **Character** tokens.\n",
    "    - Pros: Can represent any new word\n",
    "    - Cons: Modeling is more difficult; can't fit as much text in a context window\n",
    "4. **Byte** tokens. Breaks tokens down into the individual unicode character bytes. This is also called \"tokenization-free representation\".\n",
    "    - Pros: Can represent text of different alphabets, useful for multilingual models\n",
    "\n",
    "\n",
    "Some tokenizers employ a **hybrid** approach. For example, GPT-2 uses sub-word tokenization and falls back to byte tokens for other characters.\n",
    "\n",
    "Particular cases of interest that distinguish tokenization (and model) performance are the way the tokenizer handles:\n",
    "\n",
    "- Capitalization\n",
    "- Other languages\n",
    "- Emojis\n",
    "- Code - keywords and whitespace. Some models have different tokens for one space, two spaces, three spaces, four spaces etc.\n",
    "- Numbers and digits - does it encode each digit as a separate number or as a whole? E.g. `420` vs `4,2,0`. Separate seems to perform maths better.\n",
    "- Special tokens - beginning of text, end of text, user/system/assistant tags, separator token used to separate two text inputs in similarity models.\n",
    "\n",
    "\n",
    "### 1.2.2. Tokenizer Parameters\n",
    "\n",
    "The LLM designer makes decisions about the paramters of the tokenizer:\n",
    "\n",
    "- **Vocabulary size**: $\\approx 50k$ is typical currently\n",
    "- **Special tokens**: Particular use cases may warrant special tokens, e.g. coding, research citations, etc\n",
    "- **Capitalisation**: Treat upper case and lower case as separate tokens? Or convert all to lower case?\n",
    "- **Training data domain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Embeddings\n",
    "\n",
    "Now that we have represented language as a sequence of tokens, the next question is finding an efficient numerical representation of text to model the patterns we see.\n",
    "\n",
    "For neural networks, it's helpful (or even necessary) to resize inputs to be consistent length. Just like dealing with tabular data, or resizing images when dealing with CNNs.\n",
    "\n",
    "Therefore, it would be helpful to represent every word as an embedding vector of a pre-determined size. \n",
    "\n",
    "This embedding approach allows us to apply the same ideas to different levels of text: character, sub-word, word, sentence, document.\n",
    "\n",
    "Transformers take this a step further. Rather than a *static* embedding vector, the attention mechanism allows for contextualised embeddings that vary with the surrounding words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Embeddings in Action\n",
    "\n",
    "Exploring a few examples of models that operate at different levels of abstraction.\n",
    "\n",
    "### 2.1.1 Word Embeddings\n",
    "\n",
    "Deberta is a small model that produces high-quality word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer \n",
    "\n",
    "# Load a tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/debertabase\") \n",
    "\n",
    "# Load a language model \n",
    "model = AutoModel.from_pretrained(\"microsoft/deberta-v3-xsmall\") \n",
    "\n",
    "# Tokenize the sentence \n",
    "tokens = tokenizer('Hello world', return_tensors='pt') \n",
    "\n",
    "# Process the tokens \n",
    "output = model(**tokens)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Sentence/Document Embeddings\n",
    "\n",
    "Some models operate on sentences or entire documents. \n",
    "\n",
    "A simple approach is to take the word embeddings for each word in the document, then average them. Some LLMs produce \"text embeddings\" which represent the whole text as an embedding vector directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer \n",
    "\n",
    "# Load model \n",
    "model = SentenceTransformer(\"sentence-transformers/all-mpnetbase-v2\") \n",
    "\n",
    "# Convert text to text embeddings \n",
    "vector = model.encode(\"Best movie ever!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Non-LLM-based Embeddings\n",
    "\n",
    "Embeddings are useful in NLP more generally, and some techniques, such as Word2Vec and GloVe, predate LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thellmbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
