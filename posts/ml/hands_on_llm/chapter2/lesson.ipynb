{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Hands-On LLMs: Tokens and Embeddings\"\n",
    "description: \"Part 2: Tokens and Embeddings\"\n",
    "date: \"2024-11-28\"\n",
    "# image: \"deep_learning_model.png\"\n",
    "categories: [AI, Engineering, GenerativeAI, LLM]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokens and Embeddings\n",
    "\n",
    "\n",
    "## 1. Tokenization\n",
    "\n",
    "A text prompt sent to a model first needs to be broken down into tokens. The numerical token IDs are passed to the model. \n",
    "\n",
    "The tokenization scheme is designed hand-in-hand with the model, so the two are coupled: each model has a corresponding tokenizer which transforms the prompt text into the expect numerical representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Tokenization in Action\n",
    "We can load a model to see what tokenization looks like practice.\n",
    "\n",
    "We'll load a smaller open-source model and its corresponding tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    device_map = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    device_map=None\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    device_map = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model\n",
    "\n",
    "This can take a few minutes depedning on internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0842a39ae1074d239bd58ffd2a92c37d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23abd87937a428c932c2e07226d4c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3.py:   0%|          | 0.00/11.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c847f6502f64a1389389ab4b541cf7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi3.py:   0%|          | 0.00/73.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34916626c6d486880dcfc9215864f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d97389e5ddd740c0ba4529ae6183a8a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b05441fca6d5412e8731035a4be2b1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5706a89afd8d4f5ca1d43db3e4a33e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c01c3119ed24a959eb0731b9a6f0ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09acc54f372c476cad02ba26d37cfe6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "          (rotary_emb): Phi3RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm()\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_attention_layernorm): Phi3RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device_map,\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "098acb4adcb44783a09d3054aba82f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6133ee295d440397912b4f9bf345e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a556de39d5ba4ef09d38b6da9703ab27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52d829ab2034c7ba412f924356f27a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c6ef59f1e3408d9048102d5b4ef304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the model to generate text\n",
    "\n",
    "First we tokenize the input promt. Then we pass this to the model.\n",
    "We can peek in at each step to see what's actually being passed around.\n",
    "\n",
    "We'll start with the following input prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.<|assistant|>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer converts this text to a list of integers. These are the input IDs that are passed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input prompt \n",
    "input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14350,   385,  4876, 27746,  5281,   304, 19235,   363,   278, 25305,\n",
      "           293, 16423,   292,   286,   728,   481, 29889, 12027,  7420,   920,\n",
      "           372,  9559, 29889, 32001]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can \"decode\" these input IDs, converting them back to text, to see how the tokenizer splits the text. It uses sub-word tokens, so `mishap` is split into `m, ish, ap`. Punctuation is its own token and there is a special token for `<|assistant|>` Spaces are implicit; parital tokens have a special hidden character preceding them and tokens without that character are assumed to have a space before them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write\n",
      "an\n",
      "email\n",
      "apolog\n",
      "izing\n",
      "to\n",
      "Sarah\n",
      "for\n",
      "the\n",
      "trag\n",
      "ic\n",
      "garden\n",
      "ing\n",
      "m\n",
      "ish\n",
      "ap\n",
      ".\n",
      "Exp\n",
      "lain\n",
      "how\n",
      "it\n",
      "happened\n",
      ".\n",
      "<|assistant|>\n"
     ]
    }
   ],
   "source": [
    "for id in input_ids[0]: \n",
    "    print(tokenizer.decode(id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now pass this tokenized input to the model to generate new tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to a quirk of Macs, we need to explicitly pass it an attention mask as it cannot be inferred\n",
    "if device == 'mps':\n",
    "    model_kwargs = {'attention_mask': (input_ids != tokenizer.pad_token_id).long()}\n",
    "else:\n",
    "    model_kwargs = {}\n",
    "\n",
    "# Generate the text \n",
    "generation_output = model.generate(input_ids=input_ids, max_new_tokens=100, **model_kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the generation appends tokens to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14350,   385,  4876, 27746,  5281,   304, 19235,   363,   278, 25305,\n",
       "           293, 16423,   292,   286,   728,   481, 29889, 12027,  7420,   920,\n",
       "           372,  9559, 29889, 32001,  3323,   622, 29901, 17778, 29888,  2152,\n",
       "          6225, 11763,   363,   278, 19906,   292,   341,   728,   481,    13,\n",
       "            13,    13, 29928,   799, 19235, 29892,    13,    13,    13, 29902,\n",
       "          4966,   445,  2643, 14061,   366,  1532, 29889,   306,   626,  5007,\n",
       "           304,  4653,   590,  6483,   342,  3095, 11763,   363,   278,   443,\n",
       "          6477,   403, 15134,   393, 10761,   297,   596, 16423, 22600, 29889,\n",
       "            13,    13,    13,  2887,   366,  1073, 29892,   306,   505,  2337,\n",
       "          7336,  2859,   278, 15409,   322, 22024,   339,  1793,   310,   596,\n",
       "         16423, 29889,   739,   756,  1063,   263,  2752,   310,  8681, 12232,\n",
       "           363,   592, 29892,   322,   306,   471,  1468, 24455,   304,   505,\n",
       "           278, 15130,   304,  1371]], device='mps:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can decode this to see the output text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.<|assistant|> Subject: Heartfelt Apologies for the Gardening Mishap\n",
      "\n",
      "\n",
      "Dear Sarah,\n",
      "\n",
      "\n",
      "I hope this message finds you well. I am writing to express my deepest apologies for the unfortunate incident that occurred in your garden yesterday.\n",
      "\n",
      "\n",
      "As you know, I have always admired the beauty and tranquility of your garden. It has been a source of inspiration for me, and I was thrilled to have the opportunity to help\n"
     ]
    }
   ],
   "source": [
    "# Print the output \n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Tokenizer Design\n",
    "\n",
    "There are three primary decisions that determine how the tokenizer splits a given prompt:\n",
    "1. **Tokenization method**: byte pair encoding (BPE), WordPiece\n",
    "2. **Tokenizer parameters**: vocabulary size, choice of special tokens\n",
    "3. **Training data set**: a tokenizer trained on English text will give different results to one trained on Punjabi text or python code, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thellmbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
