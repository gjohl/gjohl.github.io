{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: 'FastAI Lesson 7: Collaborative Filtering'\n",
        "description: 'Practical Deep Learning for Coders: Lesson 7'\n",
        "date: '2024-02-14'\n",
        "image: fastai.png\n",
        "categories:\n",
        "  - AI\n",
        "  - Engineering\n",
        "  - FastAI\n",
        "format:\n",
        "  html:\n",
        "    code-fold: true\n",
        "---"
      ],
      "id": "fbc762e5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Collaborative Filtering\n",
        "These are notes from lesson 7 of Fast AI Practical Deep Learning for Coders.\n",
        "\n",
        "::: {.callout-tip title=\"Homework Task\"}\n",
        "\n",
        "- Recreate the collaborative filtering spreadsheet. \n",
        ":::\n",
        "\n",
        "## 1. The Intuition Behind Collaborative Filtering\n",
        "We have users ratings of movies. \n",
        "\n",
        "Say we had “embeddings” of a set of categories for each. So for a given **movie**, we have a vector of `[action, sci-fi, romance]` and for a given **user** we have their preference for `[action, sci-fi, romance]`. Then we could do the dot product between user embedding and movie embedding to get the probability that the user likes that movie. That is, the predicted user rating. \n",
        "\n",
        "So the problem boils down to: \n",
        "\n",
        "1. What are the embeddings? i.e. the salient factors (`[action, sci-fi, romance]` in the example above)\n",
        "2. How do we get them?\n",
        "\n",
        "The answer to both questions is: we just let the model learn them.\n",
        "\n",
        "Let’s just pick a randomised embedding for each movie and each user. Then we have a loss function which is the MAE between predicted user rating for a movie and actual rating. \n",
        "Now we can use SGD to optimise those embeddings to find the best values. \n",
        "\n",
        "## 2. Assorted Notes on Implementing Collaborative Filtering\n",
        "\n",
        "How should we choose the number of latent factors? (3 in the example above).\n",
        "Jeremy wrote down some ballpark values for models of different sizes in excel, then fit a function to it to get a heuristic measure. This is the default used by fast AI. \n",
        "\n",
        "An embedding is just “look up in an array”. \n",
        "\n",
        "Doing a matrix multiply by a one hot encoded vector is the same as doing a lookup in an array, just in a more computationally efficient way. Recall the softmax example. \n",
        "\n",
        "Putting a sigmoid_range on the final layer to squish ratings to fit 0 to 5 means “the model doesn’t have to work as hard” to get movies in the right range. \n",
        "In practice we use 5.5 as the sigmoid scale value as a sigmoid can never hit 1, but we want ratings to be able to hit 5. \n",
        "\n",
        "Adding a user bias term and a movie bias term to the prediction call helps account for the fact that some users always rate high (4 or 5) but other users always rate low. And similarly for movies if everyone always rates it a 5 or a 1. \n",
        "\n",
        "We want to avoid overfitting, but data augmentation isn’t possible here. \n",
        "We use weight decay AKA L2 regularisation. \n",
        "Add sum of weights squared to the loss function. \n",
        "\n",
        "\n",
        "## References\n",
        "- [Course lesson page](https://course.fast.ai/Lessons/lesson7.html)"
      ],
      "id": "7112e1a3"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}